\documentclass[crop=false,class=book,oneside]{standalone}
%----------------------------Preamble-------------------------------%
\input{../../preamble.tex}
%--------------------------Main Document----------------------------%
\begin{document}
    \ifx\ifplanetdiff\undefined
        \newif\ifintro
        \title{Introduction}
        \author{Ryan Maguire}
        \date{\vspace{-5ex}}
        \maketitle
        \tableofcontents
        \clearpage
        \chapter{Preliminaries}
    \else
        \chapter{Preliminaries}
    \fi
    \section{Mathematical Preliminaries}
        We begin by briefly discussing a few import topics in
        mathematics that one comes across in the study of
        electromagnetism and diffraction theory. This is
        particularly useful for the study of occultation
        observations of planetary rings. We will discuss complex
        analysis, Fourier analysis, a bit of approximation theory,
        and a very brief discussion of multivariate calculus so
        that we may transition from the core of electromagnetism:
        \textit{Maxwell's Equations}, and derive the
        \textit{Fresnel-Huygens Principle}. This is the fundamental
        equation for which diffraction theory is based.
        \subsection{Complex Analysis}
            We begin some simple notions. A complex number can
            be represented in the plane as a point $z=(x,y)$, but
            we often write:
            \begin{equation}
                z=x+iy
            \end{equation}
            And call $i$ the \textit{imaginary unit}.
            We call $x$ the \textit{real part}, and
            $y$ the \textit{imaginary part}. This is
            often denoted
            $\Re(z)$ and $\Im(z)$. The arithmetic
            goes as follows:
            \begin{subequations}
                \begin{align}
                    (a+ib)+(c+id)&=(a+c)+i(c+d)\\
                    (a+ib)\cdot(c+id)&=(ac-bd)+i(bc+ad)
                \end{align}
            \end{subequations}
            It is often convenient to think of $i$ as a number
            such that $i^{2}=-1$. While this usually won't get you
            into trouble, there are plenty of examples as to why
            this may be a bad way of thinking. Square roots and
            logarithms can become inconsistent. Nevertheless, it
            is useful for intuition. There are two fundamental
            notions worth mentioning: The complex conjugate
            and the modulus of a complex number.
            \begin{definition}
                The complex conjugate a complex number
                $z=x+iy$ is:
                \begin{equation}
                    \overline{z}=x-iy
                \end{equation}
            \end{definition}
            The complex conjugate of a complex number $z$
            is a reflection of the number about the
            $x$ axis in the complex plane. It is used to
            define the modulus, or absolute value, of
            a complex number.
            \begin{definition}
                The modulus of a complex number $z=x+iy$ is:
                \begin{equation}
                    |z|=\sqrt{x^{2}+y^{2}}
                \end{equation}
            \end{definition}
            The modulus can also be written as
            $|z|=\sqrt{z\overline{z}}$, where $\overline{z}$
            is the complex conjugate of $z$. Together this allows
            us to define multiplicative \textit{inverses}
            of complex numbers. That is, division by
            non-zero complex numbers is a well defined
            notion.
            \begin{theorem}
                \label{thm:Diff_Theory_Complex_Inverse}
                If $z$ is a non-zero complex number, then there is
                a $z^{-1}$ such that $z\cdot{z}^{-1}=1$. The
                inverse of $z=x+iy$ is:
                \begin{equation}
                    z^{-1}=\frac{x-iy}{x^{2}+y^{2}}
                \end{equation}
            \end{theorem}
            \begin{proof}
                If $a+ib\ne{0}$, then $a^2+b^2>0$, so
                $\frac{a-ib}{a^2+b^2}$ is a complex number. But:
                \begin{equation}
                    (a+ib)\cdot \frac{a-ib}{a^2+b^2}
                    =\frac{(a+ib)(a-ib)}{a^2+b^2}
                    =\frac{a^2+b^2}{a^2+b^2}=1
                \end{equation}
                The uniqueness of inverses gives us our result
            \end{proof}
            The uniqueness of inverses can be found in any
            abstract algebra textbook.
            If $|z|$ is the modulus of $z$, and
            $\overline{z}$ is it's complex conjugate,
            $z^{-1}$ can be written as:
            \begin{equation}
                z^{-1}=\frac{\overline{z}}{|z|^{2}}
            \end{equation}
            Euler's Theorem is a crucial
            part of complex analysis, and allows one to define
            the \textit{polar representation} of a complex number.
            \begin{theorem}[Euler's Exponential Formula]
                \label{thm:Diff_Theory_Euler_Expo_Formula}
                If $\theta$ is a real number, then:
                \begin{equation}
                    \exp(i\theta)=\cos(\theta)+i\sin(\theta)
                \end{equation}
            \end{theorem}
            \begin{proof}
                The definition of $\exp(x)$ is:
                \begin{subequations}
                    \begin{equation}
                        \exp(x)=\sum_{n=0}^{\infty}
                        \frac{x^{n}}{n!}
                    \end{equation}
                    Plugging in $i\theta$, we obtain:
                    \begin{equation}
                        \exp(i\theta)=\sum_{n=0}^{\infty}
                        i^{n}\frac{\theta^{n}}{n!}
                    \end{equation}
                    But $i^{n}$ cycles between $i,-1,-i,$
                    and $1$. So
                    we may split this sum into two parts,
                    a real part
                    and an imaginary part, to obtain:
                    \begin{equation}
                        \sum_{n=0}^{\infty}\exp(i\theta)
                        =\sum_{n=0}^{\infty}(-1)^{n}
                        \frac{x^{2n}}{(2n)!}+
                        i\sum_{n=0}^{\infty}
                        (-1)^{n}\frac{x^{2n+1}}{(2n+1)!}
                    \end{equation}
                \end{subequations}
                But the left sum is the Taylor expansion for
                $\cos(\theta)$, and the right sum is the Taylor
                expansion for $i\sin(\theta)$.
                This completes the proof.
            \end{proof}
            Euler's Theorem can also be proved by showing the two
            expressions satisfy the same initial value problem.
            A cute corallary of this is often hailed as the most
            beautiful result in mathematics.
            This is Euler's Identity:
            \begin{equation}
                e^{i\pi}+1=0
            \end{equation}
            We care about the fact that complex
            number can be written in polar form.
            \begin{theorem}
                If $z$ is a complex number, then there is a
                unique real number $r\geq{0}$ and a
                real number $\theta\in[0,2\pi)$ such that:
                \begin{equation}
                    z=r\exp(i\theta)
                \end{equation}
            \end{theorem}
            \begin{proof}
                Let $z=x+iy$. Define $r$ and $\theta$ as:
                \begin{subequations}
                    \begin{equation}
                        r=\sqrt{x^{2}+y^{2}}
                    \end{equation}
                    \begin{equation}
                        \theta=
                        \begin{cases}
                            \arctan\big(\frac{y}{x}\big),
                            &x>0, y\geq{0}\\
                            \frac{\pi}{2}+
                            \arctan\big(\frac{y}{|x|}\big),
                            &x<0, y\geq{0}\\
                            \pi+\arctan\big(\frac{y}{x}\big),
                            &x<0,y\leq{0}\\
                            \frac{3\pi}{2}+
                            \arctan\big(\frac{|y|}{x}\big),
                            &x<0, y\geq{0}\\
                            \frac{\pi}{2}\sgn(y),&x=0
                        \end{cases}
                    \end{equation}
                \end{subequations}
                Here $\sgn(y)$ is the sign of $y$.
                Euler's Theorem completes the proof.
                Uniqueness of $r$ comes from the fact that
                $|\exp(i\theta)|=1$, so if
                $z=r_{1}\exp(i\theta_{1})$
                and $z=r_{2}\exp(i\theta_{2})$, then
                $|r_{1}|=|r_{2}|$. But $r_{1}$ and $r_{2}$ are
                non-negative, and thus $r_{1}=r_{2}$.
            \end{proof}
            We define the $n^{th}$ root
            of a complex number to be:
            \begin{equation}
                \sqrt[n]{z}=
                \sqrt[n]{r}\exp\big(\frac{i\theta}{n}\big)
            \end{equation}
            This is well defined for all complex numbers since
            the $n^{th}$ root of a non-negative real number $r$
            is well defined, and $\exp(i\theta/n)$ is well
            defined for all real $\theta$. Thus we have avoided
            the messiness of square roots that occurs in
            the real world. By $\sqrt[n]{r}$, we still mean
            the principle positive root. So
            $\sqrt{4}=2$. As our first example:
            \begin{equation}
                \sqrt{i}=\frac{1+i}{\sqrt{2}}
            \end{equation}
            In the study of diffraction theory
            we will often come across two
            special functions: The Fresnel Sine Integral
            $S(x)$, and the Fresnel Cosine Integral
            $C(x)$. In particular we are often interested
            in the values they approach as $x$ tends to
            infinity. A result from Gauss aids us in
            this problem.
            \begin{theorem}
                \begin{equation}
                    \int_{-\infty}^{\infty}\exp(-x^{2})\diff{x}
                    =\sqrt{\pi}
                \end{equation}
            \end{theorem}
            \begin{proof}
                \begin{subequations}
                    Convergence can be shown, since for
                    all $x\in\mathbb{R}$:
                    \begin{equation}
                        0<\exp(-x^{2})\leq\frac{1}{1+x^2}
                    \end{equation}
                    And therefore:
                    \begin{equation}
                        0\leq\int_{-\infty}^{\infty}
                        \exp(-x^{2})\diff{x}\leq
                        \int_{-\infty}^{\infty}
                        \frac{1}{1+x^2}\diff{x}
                        =\tan^{-1}(x)\big|_{-\infty}^{\infty}=\pi
                    \end{equation}
                    Define the following:
                    \begin{equation}
                        \mathcal{I}=\int_{-\infty}^{\infty}
                        \exp(-x^{2})\diff{x}
                    \end{equation}
                    Squaring $\mathcal{I}$, we obtain:
                    \begin{align}
                        \mathcal{I}^{2}&=
                        \Big(\int_{-\infty}^{\infty}
                        \exp(-x^{2})\diff{x}\Big)
                        \Big(\int_{-\infty}^{\infty}
                        \exp(-y^{2})\diff{y}\Big)\\
                        &=\int_{-\infty}^{\infty}
                        \int_{-\infty}^{\infty}
                        \exp(-(x^2+y^2))\diff{x}\diff{y}
                    \end{align}
                    Switching from Cartesian to
                    Polar coordinates, we have:
                    \begin{equation}
                        \mathcal{I}^{2}=
                        \int_{0}^{2\pi}\int_{0}^{\infty}
                        r\exp(-r^{2})\diff{r}\diff{\phi}=
                        2\pi \int_{0}^{\infty}
                        r\exp(-r^{2})\diff{r}
                    \end{equation}
                    This final integral can be computed from basic
                    methods one would find in a Calculus textbook.
                    Letting $u=r^{2}$, we have
                    $\diff{u}=2r\diff{r}$,
                    so the integral becomes:
                    \begin{equation}
                        \mathcal{I}^{2}=
                        \pi\int_{0}^{\infty}\exp(-u)\diff{u}
                    \end{equation}
                    Hence, $\mathcal{I}^{2}=\pi$, and therefore
                    $\mathcal{I}=\pm\sqrt{\pi}$.
                    But $\mathcal{I} > 0$, and thus
                    $\mathcal{I}=\sqrt{\pi}$.
                \end{subequations}
            \end{proof}
            This result has many fundamental applications in
            probability theory and in statistics, where
            it is used to define the normal distribution.
            For us, as we will see later, it pertains to
            the \textit{Fresnel Approximation} used in
            the theory of diffraction. In this text we wish to
            provide all of the necessary mathematics and
            derivations to rigorously lay the foundation for
            diffraction theory as applied to occultation
            observations. As such we will need various theorems
            to aid us in deriving key results. We take a brief
            moment to talk about what it means to be
            analytic, the Cauchy-Riemann Equations, and
            Green's Theorem. This will tie back later to prove
            some useful results about the
            \textit{Fresnel Integrals}.
            \begin{definition}
                An entire function is a function
                $f:\mathbb{C}\rightarrow\mathbb{C}$ such that
                for all $z_{0}\in\mathbb{C}$, the following limit
                exists:
                \begin{equation}
                    f'(z_{0})=\underset{z\rightarrow{z_{0}}}{\lim}
                    \frac{f(z)-f(z_{0})}{z-z_{0}}
                \end{equation}
            \end{definition}
            An entire function is simply a complex function that
            is \textit{differentiable} at every point in the
            complex plane. We can weaken this definition to include
            only some parts of the complex plane, and these are
            called \textit{holomorphic} functions.
            A function $f$ is analytic about the point $z_{0}$
            if it's Taylor Series converges for all $z$ in some
            nearby neighborhood of $z_{0}$:
            \begin{equation}
                f(z)=\sum_{n=0}^{\infty}\frac{f^{(n)}(z_{0})}{n!}
                (z-z_{0})^{n}
            \end{equation}
            The remarkable fact of entire functions
            is that they are automatically analytic.
            This is certainly not true for real valued functions.
            One only need consider the example
            $f(x)=x|x|$. The derivative of is
            $f'(x)=2|x|$, and this has no derivative at the
            origin. Complex functions are thus very strange and
            yet very beautiful. There is a set of equations
            that will be absolutely essential for later results,
            and we derive them now. These are the
            \textit{Cauchy-Riemann Equations}.
            \begin{theorem}[Cauchy-Riemann Theorem]
                If $f:\mathbb{C}\rightarrow\mathbb{C}$ is an
                entire function:
                \begin{equation}
                    f(z)=u(x,y)+iv(x,y)
                \end{equation}
                Then:
                \par\hfill\par
                \vspace{-2ex}
                \begin{subequations}
                    \begin{minipage}{0.49\textwidth}
                        \begin{equation}
                            \frac{\partial{u}}{\partial{x}}=
                            \frac{\partial{v}}{\partial{y}}
                        \end{equation}
                    \end{minipage}
                    \hfill
                    \begin{minipage}{0.49\textwidth}
                        \begin{equation}
                            \frac{\partial{u}}{\partial{y}}=
                            -\frac{\partial{v}}{\partial{x}}
                        \end{equation}
                    \end{minipage}
                \end{subequations}
                \par\hfill\par
            \end{theorem}
            \begin{proof}
                As $f$ is entire, it's complex derivative exists
                at every point in the complex plain.
                Let $z_{0}=x_{0}+iy_{0}$, and let
                $z=x+iy_{0}$. Taking the limit, we have:
                \begin{subequations}
                    \begin{align}
                        f'(z_{0})
                        &=\underset{x\rightarrow{x_{0}}}{\lim}
                        \frac{u(x,y_{0})+iv(x,y_{0})-
                              u(x_{0},y_{0})-iv(x_{0},y_{0})}
                             {x-x_{0}}\\
                        &=\underset{x\rightarrow{x_{0}}}{\lim}
                        \frac{\big(u(x,y_{0})-u(x_{0},y_{0})\big)
                        +i\big(v(x,y_{0})-iv(x_{0},y_{0})\big)}
                        {x-x_{0}}\\
                        &=\underset{x\rightarrow{x_{0}}}{\lim}
                        \Big(\frac{u(x,y_{0})-u(x_{0},y_{0})}
                                  {x-x_{0}}\Big)+
                        i\underset{x\rightarrow{x_{0}}}{\lim}   
                        \Big(\frac{v(x,y_{0})-v(x_{0},y_{0})}
                                  {x-x_{0}}\Big)
                    \end{align}
                \end{subequations}
                Using the definition of
                \textit{partial derivatives}, we obtain:
                \begin{equation}
                    \label{eqn:Cauchy_Riemann_x_Limit}
                    f'(z_{0})=
                    \frac{\partial{u}}{\partial{x}}+
                    i\frac{\partial{v}}{\partial{x}}
                \end{equation}
                Now we evaluate the limit along the path
                $z=x_{0}+iy$. Since the function is complex
                differentiable, this means \textit{any} path
                (or limit) we take as $z\rightarrow{z_{0}}$
                will give us the same value. Thus, we have:
                \begin{subequations}
                    \begin{align}
                        f'(z_{0})
                        &=\underset{y\rightarrow{y_{0}}}{\lim}
                        \frac{u(x_{0},y)+iv(x_{0},y)-
                              u(x_{0},y_{0})-iv(x_{0},y_{0})}
                             {i(y-y_{0})}\\
                        &=\underset{y\rightarrow{y_{0}}}{\lim}
                        \frac{\big(u(x_{0},y)-u(x_{0},y_{0})\big)+
                              i\big(v(x_{0},y)-iv(x_{0},y_{0})\big)}
                             {i(y-y_{0})}\\
                        &=\frac{1}{i}
                        \underset{x\rightarrow{x_{0}}}{\lim}
                        \Big(\frac{u(x_{0},y)-u(x_{0},y_{0})}
                                  {i(y-y_{0})}\Big)+
                        \underset{y\rightarrow{y_{0}}}{\lim}   
                        \Big(\frac{v(x_{0},y)-v(x_{0},y_{0})}
                                  {(y-y_{0})}\Big)
                    \end{align}
                \end{subequations}
                Recalling our result from
                Thm.~\ref{thm:Diff_Theory_Complex_Inverse}, the
                inverse of $1/i$ is $-i$. Again using the
                definition of partial derivatives, we obtain:
                \begin{equation}
                    \label{eqn:Cauchy_Riemann_y_Limit}
                    f'(z_{0})=
                    -i\frac{\partial{u}}{\partial{y}}+
                    \frac{\partial{v}}{\partial{y}}
                \end{equation}
                Thus, equating
                Eqn.~\ref{eqn:Cauchy_Riemann_x_Limit} and
                Eqn.~\ref{eqn:Cauchy_Riemann_y_Limit}, we obtain
                \begin{equation}
                    \frac{\partial{u}}{\partial{x}}+
                    i\frac{\partial{v}}{\partial{x}}=
                    \frac{\partial{v}}{\partial{y}}
                    -i\frac{\partial{u}}{\partial{y}}
                \end{equation}
                Comparing real and imaginary parts gives us
                the Cauchy-Riemann Equations.
            \end{proof}
            The remarkable fact about the Cauchy-Riemann equations,
            which we will not go into, is that they are a
            necessary and \textit{sufficient}
            condition for analyticity. That is, a function has
            a Taylor series \textit{if and only if} it satisfies
            the Cauchy-Riemann Equations.
            We proved analytic functions
            satisfy these equations. Going the other way, showing
            that functions that satisfy these equations are
            analytic, is a little more difficult. The curious
            should consult a textbook on complex analysis.
            We will be studying a function of the form
            $\exp(i\psi)$, where $\psi$ is an analytic function
            in the \textit{real} sense of the word. Because of
            this $\exp(i\psi)$ is analytic in the
            \textit{complex} sense of the word. We'll return
            to this later when we discuss solutions of Maxwell's
            Equations and need to use the
            \textit{Fresnel Kernel}. Now we need to introduce
            the idea of contour integrals. We start with the
            definition of \textit{Jordan Curves}.
            \begin{definition}
                A Jordan Curve in the Complex Plane is a
                continuous function
                $\Gamma:[0,1]\rightarrow\mathbb{C}$ such
                that $\Gamma(0)=\Gamma(1)$, and there are
                no values $0<x_{1}<x_{2}<1$ such that
                $\Gamma(x_{1})=\Gamma(x_{2})$.
            \end{definition}
            A simple example of a Jordan curve is a circle. Jordan
            curves are \textit{closed}, meaning they start where
            they end, and do not self-intersect. A Figure-8 is
            thus \textbf{not} a Jordan curve, but an ellipse is.
            An example of a Jordan curve is given below in
            Fig.~\ref{fig:Diff_Theory_Ex_of_Jordan_Curve}. Much the way
            the closed unit interval $[0,1]$ has an ordering on it, a
            Jordan curve has a direction associated with it. Given a Jordan
            curve $\Gamma(t)$, one may change directions by defining
            $\reflectbox{\ensuremath{\Gamma}}(t)=\Gamma(1-t)$.
            While this will plot out the same
            curve in the complex plane, the direction is different and thus
            it represents a different path. When evaluating contour
            integrals, the direction matters.
            \begin{figure}[H]
                \captionsetup{type=figure}
                \centering
                \begin{tikzpicture}[thick, font=\scriptsize]
                    % Axes:
                    \draw[>=Latex, ->] (-0.5, 0) -- (4.4, 0) 
                        node [above left] {$\Re\{z\}$};
                    \draw[>=Latex, ->] (0, -0.5) -- (0, 4.4) 
                        node [below right] {$\Im\{z\}$};

                        % Axes labels:
                        \foreach\n in {1,2,3,4}{%
                            \draw (\n,-3pt) -- (\n,3pt)
                                node [above] {$\n$};
                            \draw (-3pt,\n) -- (3pt,\n)
                                node [right] {$\n i$};
                        }
                    \begin{scope}[%
                        >=Latex,
                        ->-/.style={%
                            decoration={%
                                markings,
                                mark=at position 0 with \arrow{>},
                                mark=at position .15 with \arrow{>},
                                mark=at position .4 with \arrow{>},
                                mark=at position .6 with \arrow{>},
                                mark=at position .8 with \arrow{>}
                            },
                            postaction={decorate}
                        }
                    ]
                        \draw[->-]
                            (1.5,3) to [out=70, in=140] (3,3)
                                    to [out=-50, in=90] (4,1)
                                    to [out=-90, in=-30] (2,1.5)
                                    to [out=150, in=-90] (1,2)
                                    to [out=90, in=-110] cycle;
                    \end{scope}
                    \node at (4,4) {\Large{$\mathbb{C}$}};
                    \node at (3.5,3.2) {\normalsize{$\Gamma(t)$}};
                \end{tikzpicture}
                \caption{Jordan Curve in the Complex Plane}
                \label{fig:Diff_Theory_Ex_of_Jordan_Curve}
            \end{figure}
            For the sake of computation, we will stick to
            Jordan curves that are differentiable at all but
            finitely many points. A \textit{sector}, which is
            the region contained within an arc of a circle,
            is an example of a Jordan curve that is differentiable
            at all but three points. We prove Green's Theorem
            for such curves, particularly curves that can be
            broken into a \textit{top} part and a
            \textit{bottom} part. While we wish to prove and
            derive everything needed in the various computations
            for diffraction theory, some theorems are too
            difficult to include. We state the
            \textit{Jordan Curve Theorem}, which is arguable the
            most obvious theorem that is incredibly difficult
            to prove. The proof can be found in a textbook on
            advanced topology, or algebraic topology.
            \begin{theorem}
                If $\Gamma:\mathbb{R}\rightarrow\mathbb{R}^{2}$
                is a Jordan curve, then $\Gamma$ separates the
                plane in to two disjoint parts:
                The interior, denoted
                $\Int(\Gamma)$, and the exterior. The interior
                is bounded, the exterior is unbounded, and
                $\Gamma$ is their common boundary.
            \end{theorem}
            A quick look at
            Fig.~\ref{fig:Diff_Theory_Ex_of_Jordan_Curve} can
            convince one of the validity of this statement.
            We use the fact that a Jordan curve has an interior
            to prove Green's Theorem, which is useful for the
            evaluation of complex integrals. A student of
            electromagnetism will already understand the
            importance and usefulness of Green's Theorem.
            The Weak Green's Theorem applies to \textit{simple}
            regions, which are the one's we study.
            \begin{definition}
                A simple region is a subset $D$ of the plane
                $\mathbb{R}^{2}$ such that there are two functions
                $g_{1},g_{2}:[a,b]\rightarrow\mathbb{R}^{2}$
                where:
                \begin{equation}
                    D=\{(x,y):a\leq{x}\leq{b},
                        g_{1}(x)\leq{y}\leq{g_{2}(x)}\}
                \end{equation}
            \end{definition}
            \begin{theorem}[Weak Green's Theorem]
                If $M:\mathbb{R}^{2}\rightarrow\mathbb{R}$ and
                $N:\mathbb{R}^{2}\rightarrow\mathbb{R}$ are
                differentiable functions, if
                $\Gamma$ is a Jordan curve that is differentiable
                at all but finitely many points,
                and if the interior of $\Gamma$ is simple, Then:
                \begin{equation}
                    \oint_{\Gamma}(M\diff{x}+N\diff{y})=
                    \iint_{\Int(\Gamma)}\Big(
                    \frac{\partial{N}}{\partial{x}}-
                    \frac{\partial{M}}{\partial{y}}\Big)\diff{A}
                \end{equation}
            \end{theorem}
            \begin{proof}
                As $\Gamma$ is differentiable at all but finitely
                many points, we may split the closed unit interval
                $[0,1]$ into finitely many sub-intervals, of which
                $\Gamma$ is differentiable on each. That is,
                let $t_{1},\hdots,t_{n}$ be the points where
                $\Gamma$ is not differentiable.
            \end{proof}
            With this we prove one final result before we begin
            computations.
            \begin{theorem}[Weak Cauchy's Integral Theorem]
                If $f:\mathbb{C}\rightarrow\mathbb{C}$ is an
                entire function, if
                $\Gamma:[0,1]\rightarrow\mathbb{C}$ is a
                Jordan curve differentiable at all but finitely
                many points, and if the interior of
                $\Gamma$ is simple, then:
                \begin{equation}
                    \oint_{\Gamma}f(z)\diff{z}=0
                \end{equation}
            \end{theorem}
            \begin{proof}
                For let $f(z)=u(x,y)+iv(x,y)$. Then:
                \begin{subequations}
                    \begin{align}
                        \oint_{\Gamma}f(z)\diff(z)
                        &=\oint_{\Gamma}\big(u(x,y)+iv(x,y)\big)
                        \big(\diff{x}+i\diff(y)\big)\\
                        &=\oint_{\Gamma}
                        \big(u(x,y)\diff{x}-v(x,y)\diff{y}\big)+
                        i\oint_{\Gamma}
                        \big(v(x,y)\diff{x}+u(x,y)\diff{y}\big)
                    \end{align}
                \end{subequations}
                As $f$ is entire, $u$ and $v$ are differentiable.
                Applying Green's Theorem, we obtain:
                \begin{equation}
                    \oint_{\Gamma}f(z)\diff{z}
                    =\iint_{\Int(\Gamma)}
                    \Big(\frac{\partial{u}}{\partial{y}}+
                         \frac{\partial{v}}{\partial{x}}\Big)+
                    i\iint_{\Int(\Gamma)}
                    \Big(\frac{\partial{u}}{\partial{x}}-
                         \frac{\partial{v}}{\partial{y}}\Big)
                \end{equation}
                But since $f$ is entire, $u$ and $v$ satisfy
                the Cauchy-Riemann equations. That is:
                \par\hfill\par
                \begin{subequations}
                    \begin{minipage}{0.49\textwidth}
                        \begin{equation}
                            \frac{\partial{u}}{\partial{x}}-
                            \frac{\partial{v}}{\partial{y}}=0
                        \end{equation}
                    \end{minipage}
                    \hfill
                    \begin{minipage}{0.49\textwidth}
                        \begin{equation}
                            \frac{\partial{u}}{\partial{y}}+
                            \frac{\partial{v}}{\partial{x}}=0
                        \end{equation}
                    \end{minipage}
                \end{subequations}
                \par\hfill\par
                Thus the integrands of both the left and right
                double integrals are zero, and hence the integrals
                are zero. This completes the proof.
            \end{proof}
            We arrive at the first computation that will be
            directly used in diffraction theory. We first define
            the \textit{Fresnel Integrals}, and then do some
            computations with them.
            \begin{definition}
                The Fresnel Sine and Fresnel Cosine, denoted
                $S(x)$ and $C(x)$, respectively, are real valued
                functions defined by:
                \par\hfill\par
                \vspace{-1ex}
                \begin{subequations}
                    \begin{minipage}{0.49\textwidth}
                        \begin{equation}
                            S(x)=\int_{0}^{x}
                            \sin(t^{2})\diff{t}
                        \end{equation}
                    \end{minipage}
                    \hfill
                    \begin{minipage}{0.49\textwidth}
                        \begin{equation}
                            S(x)=\int_{0}^{x}
                            \cos(t^{2})\diff{t}
                        \end{equation}
                    \end{minipage}
                \end{subequations}
            \end{definition}
            As stated before, we will be interested in functions
            of the form $\exp(i\psi)$ later on. The Fresnel
            Approximation uses the Taylor expansion of $\psi$
            up to the quadratic term, and hence we will see
            something of the form $\exp(i(a+bx+cx^2))$. From
            elementary algebra we can complete the square, and
            do a change of variables to obtain
            $\exp(i(u^{2}-d^{2}))$, where $d$ is some constant.
            We are interested in the integral of this across the
            entire real line. From Euler's Formula
            (Thm.~\ref{thm:Diff_Theory_Euler_Expo_Formula}) we
            see that $\exp(ix^{2})$ looks like
            $\cos(x^{2})+i\sin(x^{2})$. But $x^{2}$ grows rapidly,
            and thus $\sin(x^{2})$ and $\cos(x^{2})$ are two
            rapidly oscillating functions. The oscillation are
            so rapid that the areas cancel out, and hence
            $S(x)$ and $C(x)$ are well defined as
            $x\rightarrow\infty$.
            \begin{figure}[H]
                \captionsetup{type=figure}
                \centering
                \begin{tikzpicture}[scale=0.8]
                    \begin{axis}[%
                        width=\linewidth,
                        axis lines=center,
                        axis line style={->},
                        xtick distance=1,
                        xlabel=$x$,
                        xmin=-0.2,
                        xmax=5.2,
                        ytick distance=1,
                        ylabel=$y$,
                        ymin=-1.1,
                        ymax=1.2,
                        >=Latex,
                        line width=0.3mm
                    ]
                        \addplot[%
                            samples=200,
                            domain=0:5,
                            draw=red
                        ]   {sin(deg(x*x))};
                        \addplot[%
                            samples=200,
                            domain=0:5,
                            draw=blue
                        ]   {cos(deg(x*x))};
                        \node at (axis cs:1,-0.3) 
                            {$\color{blue}\cos(x^{2})$};
                        \node at (axis cs:1.8,1)
                            {$\color{red}\sin(x^{2})$};
                    \end{axis}
                \end{tikzpicture}
                \caption{Graphs of $\sin(x^{2})$ and $\cos(x^{2})$.}
                \label{fig:Diff_Theory_Graphs_of_Sinx2_and_Cosx2}
            \end{figure}
            \begin{theorem}
                $\int_{-\infty}^{\infty}e^{i\frac{\pi}{2}x^2}dx=1+i$
            \end{theorem}
            \begin{proof}
                Let $x = i\sqrt{\frac{2}{i\pi}}z$. Then
                $i\frac{\pi}{2}x^2 = -z^2$, and
                $dx=i\sqrt{\frac{2}{i\pi}}dz$. So we have:
                \begin{equation*}
                    \int_{-\infty}^{\infty}e^{i\frac{\pi}{2}x^2}dx
                    =i\sqrt{\frac{2}{i\pi}}
                    \int_{-\infty}^{\infty}e^{-z^2}dz    
                \end{equation*}
                But $\sqrt{i}=\frac{1+i}{\sqrt{2}}$, and
                $\frac{1}{\sqrt{i}}=\frac{\sqrt{2}}{1+i}= \frac{1-i}{\sqrt{2}}$,
                from the previous theorems. Therefore:
                \begin{equation*}
                    i\sqrt{\frac{2}{\pi}}\frac{1-i}{\sqrt{2}}
                    \int_{-\infty}^{\infty}e^{-z^2}dz
                    =(1+i)\frac{1}{\sqrt{\pi}}
                    \int_{-\infty}^{\infty}e^{-z^2}dz
                \end{equation*}
                But
                $\int_{-\infty}^{\infty}e^{-z^2}dz=\sqrt{\pi}$.
                Thus, we have $1+i$.
            \end{proof}
            \begin{theorem}
                $\mathcal{F}\big(e^{i\frac{\pi}{2}(\frac{\rho}{F})^2}\big) = (1+i)Fe^{-i2\pi F^2 \xi^2}$
            \end{theorem}
            \begin{proof}
            For:
            \begin{equation*}
                \mathcal{F}\big(e^{i\frac{\pi}{2} \big(\frac{\rho}{F}\big)^2}\big) = \int_{-\infty}^{\infty} e^{i\frac{\pi}{2}\big(\frac{\rho}{F}\big)^2}e^{-2\pi i \rho \xi}d\rho = \int_{-\infty}^{\infty} e^{i\frac{\pi}{2}\big(\frac{\rho}{F}\big)^2-2\pi i \rho \xi}d\rho = \int_{-\infty}^{\infty} e^{i\frac{\pi}{2F^2}\big[\rho^2-4F^2\rho \xi\big]}d\rho    
            \end{equation*}
            Completing the square, we get $(\rho - 2F^2 \xi)^2 - 4F^4\xi^2$. So, the integral becomes:
            \begin{equation*}
                \int_{-\infty}^{\infty} e^{i\frac{\pi}{2F^2}(\rho - 2F^2\xi)^2}e^{-2\pi i F^2 \xi^2}d\rho = e^{-2\pi i F^2 \xi^2}\int_{-\infty}^{\infty} e^{i\frac{\pi}{2F^2}(\rho - 2F^2\xi)^2}d\rho
            \end{equation*}
            Let $u = \rho - 2F^2\xi$, so then $du = d\rho$. We obtain:
            \begin{equation*}
                e^{-2\pi i F^2 \xi^2}\int_{-\infty}^{\infty} e^{i\frac{\pi}{2}\big(\frac{u}{F}\big)^2}du
            \end{equation*}
            Letting $s = \frac{u}{F}$, get:
            \begin{equation*}
            Fe^{-2\pi i F^2 \xi^2} \int_{-\infty}^{\infty} e^{i\frac{\pi}{2}s^2}ds
            \end{equation*}
            But this integral is $1+i$. So, we have $(1+i)Fe^{-2\pi i F^2 \xi^2}$.
            \end{proof}
            \begin{theorem}
            $\mathcal{F}(e^{-i\frac{\pi}{2}\big(\frac{\rho_0}{F}\big)^2}\big) = (1-i)Fe^{2\pi i F^2 \xi^2}$.
            \end{theorem}
            \begin{proof}
            For:
            \begin{equation*}
                \int_{-\infty}^{\infty} e^{-i\frac{\pi}{2}\big(\frac{\rho_0}{F}\big)^2}e^{-2\pi i \rho_0 \xi}d\rho_0 = \int_{-\infty}^{\infty} e^{-i\frac{\pi}{2F^2}\big({\rho_0}^2 + 4F^2 \rho_0 \xi\big)}d\rho_0 = \int_{-\infty}^{\infty} e^{-i\frac{\pi}{2F^2}\big((\rho_0+2F^2\xi)^2 - 4F^4\xi^2\big)}d\rho_0
            \end{equation*}
            But $e^{2\pi i F^2 \xi^2}$ is constant with respect to $\rho_0$, so we have:
            \begin{equation*}
                e^{2\pi i F^2 \xi^2} \int_{-\infty}^{\infty} e^{-i\frac{\pi}{2F^2}(\rho_0+2F^2\xi)}d\rho_0    
            \end{equation*}
            Let $u = \frac{\rho_0 + 2F^2 \xi}{F}$, then $Fdu = d\rho_0$, so we have $Fe^{2\pi i F^2 \xi^2} \int_{-\infty}^{\infty} e^{-i\frac{\pi}{2}u^2}du$. Let $u = -is$, then $du = -ids$, and $u^2 = -s^2$. So we have $-i e^{2\pi i F^2 \xi^2}\int_{-\infty}^{\infty} e^{i\frac{\pi}{2}s^2}ds$. But this integral is $1+i$. So, we have $-iFe^{2\pi i F^2 \xi^2}(1+i) = (1-i)Fe^{2\pi i F^2 \xi^2}$.
            \end{proof} 
            
            \begin{theorem}
            If $\int_{-\infty}^{\infty} |f(t)|dt < \infty$, $\int_{-\infty}^{\infty} |g(t)|dt < \infty$, and $f* g = \int_{-\infty}^{\infty} f(\tau)g(\tau-t)d\tau$, then $\mathcal{F}\big(f * g\big) = \mathcal{F}(f)\cdot \mathcal{F}(g)$.
            \end{theorem}
            \begin{proof}
            Let $\int_{-\infty}^{\infty} |f(t)|dt = \norm{f}_{1}$ and $\int_{-\infty}^{\infty} |g(t)|dt = \norm{g}_{1}$. Then:
            \begin{align*}
            \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}|f(\tau)g(\tau-t)|d\tau dt&\leq\int_{-\infty}^{\infty}|f(\tau)|\int_{-\infty}^{\infty}|g(\tau-t)|d\tau dt\\
            &=\int_{\infty}^{\infty}|f(x)|\norm{g}_{1}dx=\norm{f}_{1}\norm{g}_{1}    
            \end{align*}
            Thus, $h(t) = f* g$ is such that $\int_{-\infty}^{\infty} |h(t)|dt < \infty$. Let $H(\xi) = \mathcal{F}(h)$. Then:
            \begin{align*}
            H(\xi)&=\int_{-\infty}^{\infty}h(t)e^{-2\pi it\xi}dt\\
            &=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}f(\tau)g(t-\tau)d\tau e^{-2\pi i t\xi}dt
            \end{align*}
            But $|e^{-2\pi i t \xi}f(\tau)g(t-\tau)| = |f(\tau)g(t-\tau)|$, and $\int_{-\infty}^{\infty}|f(\tau)g(t-\tau)|d\tau < \infty$. Thus, by Fubini's Theorem we may swap the integral. Let $y = t-\tau$. Then we have:
            \begin{equation*}
            H(\xi)=\int_{-\infty}^{\infty}f(\tau)\int_{-\infty}^{\infty}g(t-\tau)e^{-2\pi it\xi}dtd\tau=\int_{-\infty}^{\infty}f(\tau)e^{-2\pi i\tau\xi}d\tau\int_{-\infty}^{\infty}g(y)e^{-2\pi iy\xi}dy=\mathcal{F}(f)\cdot\mathcal{F}(g)    
            \end{equation*}
            Therefore, etc.
            \end{proof}
            
            \begin{theorem}
            If $\hat{T}(\rho_0) = \frac{1-i}{2F}\int_{-\infty}^{\infty}T(\rho) e^{i\frac{\pi}{2}\big(\frac{\rho-\rho_0}{F}\big)^2}d\rho$, then $T(\rho) = \frac{1+i}{2F}\int_{-\infty}^{\infty}\hat{T}(\rho_0)e^{-i\frac{\pi}{2}\big(\frac{\rho-\rho_0}{F}\big)^2}d\rho_0$.
            \end{theorem}
            \begin{proof}
            For $\hat{T}(\rho) = \frac{1-i}{2F}(T* e^{i \frac{\pi}{2}\big(\frac{\rho}{F}\big)^2}\big)$. Therefore:
            \begin{align*}
            \mathcal{F}(\hat{T})&=\frac{1-i}{2F}\mathcal{F}(T)\cdot \mathcal{F}(e^{i\frac{\pi}{2}\big(\frac{\rho_0}{F}\big)^2})\\
            &=\frac{1-i}{2F}\mathcal{F}(T)\cdot(1+i)Fe^{-2\pi i F^2 \xi^2}\\
            &=\mathcal{F}(T)e^{-2\pi i F^2 \xi^2}
            \end{align*}
            But $e^{2\pi iF^{2}\xi^{2}}=\frac{1}{(1-i)F}\mathcal{F}\big(e^{-i\frac{\pi}{2}\big(\frac{\rho_0}{F}\big)^2}\big) = \frac{1+i}{2F} \mathcal{F}\big(e^{-i\frac{\pi}{2}\big(\frac{\rho_0}{F}\big)^2}$. Therefore:
            \begin{align*}
            \mathcal{F}(T)&=\frac{1+i}{2F}\mathcal{F}(\hat{T})\cdot\mathcal{F}\big(e^{-i\frac{\pi}{2}\big(\frac{\rho_0}{F}\big)^2}\big)\\
            &=\frac{1+i}{2F}\mathcal{F}(\hat{T}*e^{-i\frac{\pi}{2}\big(\frac{\rho_0}{F}\big)^2})\\
            &=\mathcal{F}\bigg(\frac{1+i}{2F}\int_{-\infty}^{\infty} \hat{T}(\rho_0)e^{-i\frac{\pi}{2}\big(\frac{\rho - \rho_0}{F}\big)^2} d\rho_0\bigg)
            \end{align*}
            Therefore, $T(\rho) = \frac{1+i}{2F}\int_{-\infty}^{\infty}\hat{T}(\rho_0)e^{-i\frac{\pi}{2}\big(\frac{\rho - \rho_0}{F}\big)^2}d\rho_0$.
            \end{proof}
            
            \begin{theorem}
            If $T,\psi \in L^2(\mathbb{R})$, and if $\hat{T}(\rho_0) = \int_{-\infty}^{\infty} T(\rho)e^{i\psi(\rho_0-\rho)}d\rho_0$, then $T(\rho) = \mathcal{F}^{-1}_{\rho}\big(\frac{\mathcal{F}(\hat{T})}{\mathcal{F}(e^{i\psi})}\big)$.
            \end{theorem}
            \begin{proof}
            For $\hat{T}(\rho_0) = T* e^{i\psi}$. But then $\mathcal{F}_{\xi}(\hat{T}) = \mathcal{F}_{\xi}\big(T* e^{i\psi}\big) = \mathcal{F}_{\xi}\big(T\big)\cdot \mathcal{F}_{\xi}\big(e^{i\psi}\big)$. So then $\mathcal{F}_{\xi}(T) = \frac{\mathcal{F}_{\xi}(\hat{T})}{\mathcal{F}_{\xi}(e^{i\psi})}$. Therefore $T(\rho) = \mathcal{F}^{-1}_{\rho}\big(\frac{\mathcal{F}_{\xi}(\hat{T})}{\mathcal{F}_{\xi}(e^{i\psi})}\big)$
            \end{proof}
        \subsection{Fourier Analysis}
            \begin{definition}
                The Spectrum of an integrable function
                $f:\mathbb{R}\rightarrow\mathbb{R}$ is the
                function $F$ defined by:
                \begin{equation}
                    F(\omega)=
                    \int_{-\infty}^{\infty}f(t)
                    \exp(-2\pi{i}\omega{t})\diff{t}
                \end{equation}
            \end{definition}
            The requirement that $f$ be integrable is to avoid
            strange issues in mathematics. For the sake of
            physical application, one may assume every function
            is integrable. Mathematically this is far from true,
            but oh well. For the sake of Fourier Analysis, when
            we say integrable we mean Lebesgue integrable. This
            simply means that:
            \begin{equation}
                \int_{-\infty}^{\infty}|f(x)|\diff{x}<\infty
            \end{equation}
            We now prove what is probably the most useful theorem
            in Fourier Analysis.
            \begin{theorem}
                If $f:\mathbb{R}\rightarrow\mathbb{R}$ is a
                continuous Lebesgue integrable function,
                and if its spectrum $F$ is also continuous
                and Lebesgue integrable, then:
                \begin{equation}
                    f(t)=\int_{-\infty}^{\infty}F(\omega)
                    \exp(2\pi{i}\omega{t})\diff{\omega}
                \end{equation}
            \end{theorem}
            A powerful application of this is
            Shannon's Sampling Theorem.
            \begin{theorem}[Shannon's Sampling Theorem]
                If $f(t)$ is a continuous
                Lebesgue integrable function such that
                its spectrum $F(\omega)$ is differentiable and zero
                outside the interval $[-W,W]$,
                then $f(t)$ is uniquely determined
                by the points $f(\frac{n}{2W})$, $n\in\mathbb{N}$.
            \end{theorem}
            \begin{proof}
                For let $F$ be the spectrum of $f$. That is:
                \begin{equation}
                    f(t)=\int_{-\infty}^{\infty}F(\omega)
                    \exp(-2\pi{i}\omega{t})\diff{\omega}
                \end{equation}
                But $F(\omega)=0$ for $|\omega|>W$. Thus we have:
                \begin{equation}
                    f(t)=\int_{-W}^{W}F(\omega)
                    \exp(-2\pi{i}\omega{t})\diff{\omega}
                \end{equation}
                Then for $n\in\mathbb{N}$, we have:
                \begin{equation}
                    f\big(\frac{n}{2W}\big)=\int_{-W}^{W}F(\omega)
                    \exp(-2\pi{i}\frac{n}{2W}\omega)\diff{\omega}
                \end{equation}
                But $F$ is differentiable, and thus it's Fourier
                series converges. That is:
                \begin{subequations}
                    \begin{align}
                        F(\omega)
                        &=\sum_{n=-\infty}^{\infty}e^{2\pi{i}n\omega}
                        \int_{W}^{W}F(\tau)
                        \exp(-2\pi{i}\frac{n}{2W}\tau)\diff{\tau}\\
                        &=\sum_{n=-\infty}^{\infty}f
                          \big(\frac{n}{2W}\big)e^{2\pi{i}n\omega}
                    \end{align}
                \end{subequations}
                Therefore $f(\frac{n}{2W})$, $n\in \mathbb{N}$
                uniquely determines $F(\omega)$. But the
                spectrum $F(\omega)$ uniquely determines
                $f(t)$. Therefore $f(t)$ is
                uniquely determined and:
                \begin{equation}
                    f(t)=\sum_{n=-\infty}^{\infty}\int_{-W}^{W}
                    f\big(\frac{n}{2W}\big)
                    \exp(2\pi{i}\omega(n+t))\diff{\omega}
                \end{equation}
            \end{proof}
        \subsection{Partial Differential Equations}
        \subsection{Approximation Theory}
            Suppose $g$ is an analytical function about
            the origin (i.e. it has a convergent MacLaurin series),
            and consider the integral:
            \begin{equation}
                I(k) = \int_{a}^{b}e^{ikg(x)}dx
            \end{equation}
            Suppose that there is a $c\in(a,b)$ such
            that $g'(c)=0$ and $g''(c)\ne 0$. Then:
            \begin{align}
                \nonumber I(k)&=e^{ikg(c)}\int_{a}^{b}e^{ik[g(x)-g(c)]}dx\\
                &=e^{ikg(c)}\int_{a}^{b}e^{ik[\frac{g''(c)}{2}(x-c)^{2}+\hdots]}dx
            \end{align}
            Higher terms are extremely oscillatory, and so we neglect them.
            \begin{remark}
                Note that higher terms can indeed cancel each
                other out, meaning these neglected terms may
                not be negligible. For example, if
                $g(x)=-sin(\pi x)$, then $\exp(ig(x))$ is never
                too oscillatory. However, so long as the interval
                $[a,b]$ is small enough, the approximation is still
                valid. The previously mentioned $g(x)$ is how
                one approximates the $J_{0}(x)$ Bessel function.
            \end{remark}
            Out integral then becomes:
            \begin{align}
                I(k)&\approx e^{ikg(c)}\int_{a}^{b}e^{ik\frac{g''(c)}{2}(x-c)^{2}}dx\\
                &\approx e^{ikg(c)}\int_{\infty}^{\infty}e^{ik\frac{g''(c)}{2}(x-c)^{2}}dx\\
                &=e^{ikg(c)}\sqrt{\frac{2\pi i}{kg''(c)}}
            \end{align}
            We can use this for our double integral,
            and make it a single integral. The first and
            second integrals of $\psi$ are nasty, however.
            \begin{equation}
                \frac{\partial \psi}
                     {\partial \phi}
                =kD\Big[\frac{2D\rho\cos(B)\sin(\phi)+2\rho\rho_{0}
                \sin(\phi-\phi_{0})}{2D^2\sqrt{1+2\cos(B)
                \frac{\rho_{0}\cos(\phi_{0})-\rho\cos(\phi)}{D}+
                \frac{\rho^{2}+\rho_{0}^{2}-
                2\rho\rho_{0}\cos(\phi-\phi_{0})}{D^2}}}-
                \frac{\rho\cos(B)\sin(\phi)}{D}\Big]
            \end{equation}
            The second derivative is equally bad.
            Solving for $\frac{\partial\psi}{\partial\phi}=0$
            must be done iteratively by successive approximations.
            A further approximation can be made as $\psi$
            is analytic in $\phi$. Let $\phi_{s}$ be
            the solution to $\frac{\partial\psi}{\partial\phi}$
            and let $\phi_{s_{n}}$ be a sequence such that
            $\phi_{s_{n}}\rightarrow\phi_{s}$.
    
    \section{Maxwell's Equations}
    \section{The Basics of Diffraction}
    \section{The Main Problem}
\end{document}