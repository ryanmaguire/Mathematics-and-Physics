\documentclass[crop=false,class=article,oneside]{standalone}
%----------------------------Preamble-------------------------------%
\input{../../../../preamble.tex}
%--------------------------Main Document----------------------------%
\begin{document}
    \ifx\ifgeoalg\undefined
        \section*{Electromagnetism I}
        \setcounter{section}{1}
    \fi
\subsection{Geometry, Algebra, and Algorithms}
    This section introduces the basic ideas. Affine varieties and ideals in the polynomial ring $k[x_1,\hdots ,x_n]$ are studied. Finally, polynomials in one variable are studied to introduce the role of algorithms.
    \subsubsection{Polynomials and Affine Space}
    To link algebra and geometry, we will study polynomials over a field. Fields are important because linear algebra works over any field $k$. There are three particular fields that will be used the most:
    \begin{enumerate}
        \item $\mathbb{Q}$: This field is used for computer examples.
        \item $\mathbb{R}$: This field is used for drawing pictures of curves and surfaces.
        \item $\mathbb{C}$: This field is used for proving many theorems.
    \end{enumerate}
    \begin{definition}
    A monomial in $x_1,\hdots,x_n$ is a product $\prod_{i=1}^{n}x_{i}^{\alpha_{i}}$, where $\alpha_{1},\hdots,\alpha_{n}\in\mathbb{N}_0$
    \end{definition}
    \begin{definition}
    The total degree of a monomial $x_1^{\alpha_1}\cdots x_n^{\alpha_n}$ is the sum $\sum_{i=1}^{n}\alpha_{i}$
    \end{definition}
    \begin{notation}
    For $\alpha_1,\hdots, \alpha_n\in \mathbb{N}_0$, let $\alpha = (\alpha_1,\hdots ,\alpha_n)$. We write $\prod_{i=1}^{n}x_{i}^{\alpha_{i}}=x^{\alpha}$
    \end{notation}
    \begin{definition}
    A polynomial $f$ in $x_1,\hdots, x_n$ is a finite linear combination of monomials over $k$.
    \end{definition}
    \begin{notation}
    The set of all polynomials in $n$ variables with coefficients in $k$ is denoted $k[x_1,\hdots ,x_n]$
    \end{notation}
    \begin{definition}
    For a polynomial $f = \sum_{\alpha} a_\alpha x^\alpha \in k[x_1,\hdots ,x_n]$, $a_\alpha$ is called the coefficient of $x^\alpha$
    \end{definition}
    \begin{definition}
    A term of $f=\sum_{\alpha} a_\alpha x^\alpha \in k[x_1,\hdots ,x_n]$ is a product $a_\alpha x^\alpha$ where $a_\alpha \ne 0$
    \end{definition}
    \begin{definition}
    The total degree of $f=\sum_{\alpha}a_\alpha x^\alpha$, denoted $\deg(f)$, is $\deg(f) = \max\{|\alpha|:a_\alpha \ne 0\}$
    \end{definition}
    \begin{definition}
    The zero polynomial is the polynomial with all zero coefficients.
    \end{definition}
    \begin{theorem}
    The sum and product of polynomials in $k[x_1,\hdots ,x_n]$ is a polynomial in $k[x_1,\hdots ,x_n]$
    \end{theorem}
    \begin{definition}
    A divisor of $f\in k[x_{1},\hdots ,x_{n}]$, is a $g\in k[x_{1},\hdots,x_{n}]$ such that $\exists_{h\in k[x_{1},\hdots,x_{n}]}:f = gh$
    \end{definition}
    \begin{theorem}
    For all $n\in\mathbb{N}$, $k[x_1,\hdots ,x_n]$ is a commutative ring.
    \end{theorem}
    \begin{remark}
    Because of this we call $k[x_1,\hdots ,x_n]$ a polynomial ring.
    \end{remark}
    \begin{definition}
    The $n-$dimensional affine space over $k$ is the set $k^{n}=\{(a_1,\hdots, a_n):a_1,\hdots,a_n \in k\}$
    \end{definition}
    \begin{remark}
    A polynomial defines a function $f:k^n \rightarrow k$
    \end{remark}
    \begin{definition}
    A zero function $f:k^n \rightarrow k$ is a function such that $f(x) = 0$ for all $x\in k^n$
    \end{definition}
    \begin{remark}
    A zero function and the zero polynomial are not necessarily the same thing. That is, there are fields $k$ with non-zero polynomials that evaluate to zero at every point.
    \end{remark}
    \begin{theorem}
    There exists fields $k$, $f\in k[x]$ such that $f$ is a non-zero polynomial and $\forall_{a\in k},f(a)=0$
    \end{theorem}
    \begin{theorem}
    If $k$ is an infinite field, $f\in k[x_1,\hdots ,x_n]$, then $f$ is a zero function if and only if it is the zero polynomial.
    \end{theorem}
    \begin{theorem}
    If $k$ is an infinite field and $f,g\in k[x_1,\hdots ,x_n]$, then $f=g$ if and only if $f:k^n\rightarrow k$ and $g:k^n \rightarrow k$ give the same function.
    \end{theorem}
    There is a special property for polynomials over the complex numbers $\mathbb{C}$.
    \begin{theorem}
    Every non-constant polynomial $f\in \mathbb{C}[x]$ has a root in $\mathbb{C}$.
    \end{theorem}
    \begin{definition}
    An algebraically closed field is a field such that for non-constant $f$, $\exists_{x\in k}:f(x)=0$.
    \end{definition}
    \subsubsection{Affine Varieties}
    \begin{definition}
    The affine variety of $f_1,\hdots, f_s \in k[x_1,\hdots ,x_n]$ is $\{x\in k^n:\forall_{1\leq i \leq s},f_i(x) = 0\}$
    \end{definition}
    \begin{notation}
    The affine variety of $f_{1},\hdots,f_{s}\in k[x_{1},\hdots,x_{n}]$ is denoted $\mathbf{V}(f_1,\hdots, f_s)$
    \end{notation}
    The affine variety of a finite set of polynomials is the solution set of the system of equations $f_{i}(x)=0$
    \begin{example}
    $\mathbf{V}(x^2+y^2-1)\subset\mathbb{R}^2$ is the set of solutions to $x^2+y^2-1 = 0$: The unit circle.
    \end{example}
    \begin{example}
    The conic sections (Circles, ellipses, parabolas, and hyperbolas) are affine varieties. The graphs of rational functions are also affine varieties. For if $y = \frac{P(x)}{Q(x)}$, where $P,Q\in \mathbb{R}[x]$, then $\mathbf{V}\big(yQ(x)-P(x)\big)$ is an affine variety equivalent to that graph.
    \end{example}
    \begin{example}
    The surfaces the represent affine varieties need not be smooth everywhere. Indeed, $\mathbf{V}(z^2-x^2-y^2)$ is the graph of a cone with its apex at the origin. As such, the surface obtained is not smooth at the origin. Such points are called singular points.
    \end{example}
    \begin{example}
    The twisted cubic is $\mathbf{V}(y-x^2,z-x^3)$, with the parametrization $\{(t,t^2,t^3):t\in \mathbb{R}\}$
    \end{example}
    The notion of dimension is very subtle. In previous examples, if we have $m$ polynomials in $\mathbb{R}^n$, we expect a surface of $n-m$ dimension. This is not always the case, however.
    \begin{example}
    $\mathbf{V}(xz,yz)$ is the set of solutions to $xy=yz=0$. If $z=0$, then any point $(x,y,0)\in \mathbb{R}^3$ satisfies this. If $z\ne 0$, then $x=y=0$ and thus and point $(0,0,z)\in \mathbb{R}^3$ is a solution. Thus, $\mathbf{V}(xz,yz)$ is the union of the $xy$ plane and the $z$ axis. So $\mathbf{V}(xz,yz)$ is two dimensional, not one.
    \end{example}
    \begin{definition}
    A linear variety is an affine variety in which the defining polynomials are linear.
    \end{definition}
    \begin{example}
    Let $k$ be a field and consider the following polynomials:
    \begin{align*}
        a_{11}x_{1}+\hdots+a_{1n}x_{n}&=b_{1}\\
        \vdots&\\
        a_{m1}x_{1}+\hdots+a_{mn}x_{n}&=b_{m}
    \end{align*}
    From linear algebra we know that the method of Gaussian Elimination and row-reduction gives us the solution set of the system of equations. We also know that the dimension of the solutions set is $n-r$, where $r$ is the number of independent equations (Also known as the rank of the coefficient matrix). The dimension of an affine variety is also determined by the number of independent equations, however the term ``Independent," is much more subtle.
    \end{example}
    \begin{example}
    Find the maximum of $f(x,y,z) = x^3+2xyz-z^2$ subject to $g(x,y,z) = x^2+y^2+z^2=1$. From multivariable calculus, specifically the method of Lagrange Multipliers, we know this occurs when $\nabla(f) = \lambda \nabla(g)$, for some $\lambda \in \mathbb{R}$. This gives us the following:
    \begin{align*}
        x^{2}+2yz&=2\lambda x & 2xy-2z&=2\lambda z\\
        2xz&=2\lambda y & x^{2}+y^{2}+z^2&=1
    \end{align*}
    Solving this via algebraic means can be a nightmare. Various algorithms exist, however.
    \end{example}
    \begin{remark}
    It is possible for an affine variety to be the empty set. Let $k=\mathbb{R}$, and $f=x^{2}+y^{2}+1$. Then $\mathbf{V}(f)=\emptyset$. That is, there is no real solution to $x^2+y^2 = -1$.
    \end{remark}
    \begin{example}
    Consider a robot arm. The ``Armpit," is at the origin, and the ``Elbow," is at the point $(x,y)\in \mathbb{R}^2$ where $x^2+y^2 = r^2$ ($r$ is the length of ``Bicep.") The ``Hand," will then be at $(z,w)\in \mathbb{R}^2$ where $(x-z)^2 + (y-w)^2 = \ell^2$ ($\ell$ is the length of the ``Forearm.") Not every point $(x,y,z,w)\in \mathbb{R}^4$ represents a possible position of the robot arm, there are the following constraints:
    \begin{align*}
        x^{2}+y^{2}&=r^{2}\\ 
        (x-z)^{2}+(y-z)^{2}&=\ell^{2}
    \end{align*}
    The solution set defines an affine variety in $\mathbb{R}^4$. For arms in $\mathbb{R}^3$, the solution set would be in $\mathbb{R}^6$.
    \end{example}
    \begin{theorem}
    If $V,W\subset k^n$ are affine varieties, then so are $V\cup W$ and $V\cap W$. Moreover:
    \begin{enumerate}
        \item $\mathbf{V}(f_{1},\hdots,f_{s})\cap\mathbf{V}(g_{1},\hdots,g_{s})=\mathbf{V}(f_{1},\hdots,f_{s},g_{1},\hdots,g_{t})$
        \item $\mathbf{V}(f_{1},\hdots,f_{s})\cup\mathbf{V}(g_{1},\hdots,g_{s})=\mathbf{V}(f_{i}g_{j}:1\leq i\leq s,1\leq j\leq t)$
    \end{enumerate}
    \end{theorem}
    \begin{example}
    $\mathbf{V}(xz,yz)=\mathbf{V}(xy)\cup\mathbf{V}(z)$. $\mathbf{V}(xz,yz)$ is the union of the $xy$ plane and the $z$ axis.
    \end{example}
    \begin{example}
    For the twisted cubic: $\mathbf{V}(y-x^{2},z-x^{3})=\mathbf{V}(y-x^{2})\cap\mathbf{V}(z-x^{3})$
    \end{example}
    Several problems arise concerning affine varieties:
    \begin{enumerate}
        \item Can we determine if $\mathbf{V}(f_{1},\hdots,f_{s})\ne\emptyset$?\hfill [Consistency]
        \item Can we determine if $\mathbf{V}(f_{1},\hdots,f_{s})$ is finite?\hfill [Finiteness]
        \item Can we determine the ``Dimension," of $\mathbf{V}(f_{1},\hdots,f_{s})$?
    \end{enumerate}
    The answer to these questions is yes, although we must be careful in choosing the field we work with. 
    \subsubsection{Parametrizations of Affine Varieties}
    We now arrive at the problem of describing all of the points in an affine variety. 
    \begin{example}
    Consider the system in $\mathbb{R}[x,y,z]$:
    \begin{align*}
        x+y+z&=1\\
        x+2y-z&=3
    \end{align*}
    From linear algebra we get the row echelon matrix:
    \begin{equation*}
        \begin{bmatrix*}[r]1&0&3&\vline&-1\\0&\phantom{-}1&-2&\vline&2\end{bmatrix*}    
    \end{equation*}
    Letting $z=t$, we get $x = -3t-1$ and $y = 2+2t$. The parametrization of the affine variety is thus $\{(-3t-1,2t+2,t):t\in \mathbb{R}\}$. We call $t$ a parameter, and $(-3t-1,2t+2,t)$ a parametrization.
    \end{example}
    \begin{example}
    One way to parametrize the unit circle uses trigonometric functions: $(\cos(t),\sin(t))$. A rational way to do this is $\big(\frac{1-t^2}{1+t^2}, \frac{2t}{1+t^2}\big)$. This parametrizes the entire unit circle, with the exception of the point $(-1,0)$. This point is $\underset{t\rightarrow\infty}{\lim}\big(\frac{1-t^{2}}{1+t^{2}},\frac{2t}{1+t^{2}}\big)$. So in a sense, $(-1,0)$ is a ``Point at infinity."
    \end{example}
    \begin{definition}
    A parametrization of an affine variety $\mathbf{V}(f_1,\hdots, f_s) \subset k^n$ is a set of $j$ equations $x_{j}=f_{j}(t_{1},\hdots,t_{m})$, whose solution set $S$ is such that $S\subset\mathbf{V}(f_{1},\hdots,f_{s})$, and for all $g_{1},\hdots,g_{t}$ such that $S\subset\mathbf{V}(g_{1},\hdots,g_{t})$, $\mathbf{V}(f_{1},\hdots,f_{s})\subset\mathbf{V}(g_{1},\hdots,g_{s})$
    \end{definition}
    \begin{remark}
    Solutions to $x_k = f_k(t_1,\hdots, t_m)$ lie in $\mathbf{V}(f_1,\hdots, f_s)$ and $\mathbf{V}(f_1,\hdots, f_s)$ is the smallest affine variety containing these points.
    \end{remark}
    \begin{definition}
    A rational function in $x_1,\hdots, x_n$ is a quotient $\frac{P(x)}{Q(x)}:P,Q \in k[x_1,\hdots ,x_n],Q\ne 0$.
    \end{definition}
    \begin{definition}
    $k(x_1,\hdots ,x_n)$ is the set of all rational functions over a field $k$ in $x_1,\hdots, x_n$.
    \end{definition}
    \begin{definition}
    Equal rational functions are functions $\frac{P_1}{Q_1}, \frac{P_2}{Q_2} \in k(x_1,\hdots ,x_n)$ where $P_1Q_2 = P_2Q_1$.
    \end{definition}
    \begin{theorem}
    If $k$ is a field, then $k(x_1,\hdots ,x_n)$ is a field.
    \end{theorem}
    \begin{definition}
    A rational representation of an affine variety is a rational parametrization.
    \end{definition}
    \begin{definition}
    A polynomial representation of an affine variety is a polynomial parametrization.
    \end{definition}
    \begin{remark}
    Writing out an affine variety as $V=\mathbf{V}(f_{1},\hdots,f_{s})$ is called an implicit representation.
    \end{remark}
    There are two questions that arise from parametrization:
    \begin{enumerate}
        \item Does every affine variety have a rational parametric representation?
        \item Given a parametric representation of an affine variety, can we find the implicit representation?
    \end{enumerate}
    The answers are: No to the first question, yes to the second. Indeed, most affine varieties cannot be parametrized by rational functions.
    \begin{example}
    Find the affine variety parametrized by:
    \begin{align*}
        x&=1+t\\
        y&=1+t^{2}
    \end{align*}
    We have that $t=x-1$, and thus $y=1+(x-1)^{2}=x^{2}-2x+2$.
    \end{example}
    \begin{remark}
    The process described above involved eliminating the variable $t$ and creating a polynomial in $x$ and $y$. This illustrates the role played by elimination theory.
    \end{remark}
    \begin{example}
    Let's parametrize the unit circle in a rational manner. Let $(x,y)$ be a point on the unit circle and draw a line from the point $(-1,0)$ to $(x,y)$. This line intersects the $y-$axis at some point $(0,t)$. We have that the slope of this line is $m =\frac{t-0}{0-(-1)}=\frac{y-0}{x-(-1)}=\frac{y}{x+1}$. So $y=t(x+1)$. But $x^{2}+y^{2}=1$, so $x^2+t^{2}(x+1)^{2}=1\Leftrightarrow x^{2}+x\frac{2t^{2}}{1+t^{2}}=\frac{1-t^2}{1+t^2}\Leftrightarrow (x+\frac{t^{2}}{1+t^{2}})^{2}=\frac{1}{(1+t^{2})^{2}}\Leftrightarrow x=\frac{-t^{2}\pm 1}{1+t^{2}}$. But $x\in [-1,1]$, and thus we get $x = \frac{1-t^2}{1+t^2}$. But $y = t(x+1)$, and thus $y = \frac{2t}{1+t^2}$. $(x,y)=(\frac{1-t^{2}}{1+t^{2}},\frac{2t}{1+t^{2}})$.
    \end{example}
    \begin{definition}
    The tangent surface of a smooth curve $\Gamma:\mathbb{R}\rightarrow \mathbb{R}^n$ is $\{\Gamma(t)+u\Gamma'(t):t,u\in \mathbb{R}\}$
    \end{definition}
    \begin{remark}
    The tangent surface is obtained by taking the union of all of the tangent lines to every point on the curve. $t$ tells us which point on the curve we are one, and $u$ tells us how far along the tangent line we are.
    \end{remark}
    \begin{example}
    The twisted cubic is the curve defined by $\mathbf{r}(t) = (t,t^2,t^3)$. It's tangent surface is $\mathbf{r}+u\mathbf{r}'(t)=(t,t^2,t^3)+u(1,2t,3t^3)=(t+u,t^2+2ut,t^3+3ut^2)$. One question that arises is ``Is this an affine variety? If so, what are the defining polynomials." The answer for this particular surface is yes. The graph of this surface is equal to $\mathbf{V}(-4x^3z+3x^2y^2-4y^3+6xyz-z^2)$.
    \end{example}
    An application of this is in the design of complex objects such as automobile hoods and airplane wings. Engineers need curves and surfaces that are easy to describe, quick to draw, and varied in shape. Polynomials and rational functions satisfy this criteria. Complicated curves are usually formed by joining together simpler curves. Suppose a design engineer needs to draw a curve in the plane. The curves in question need to join smoothly, and thus the tangent directions need to match at the endpoints. The engineer must control the following:
    \begin{enumerate}
        \item The starting and ending points of the curve.
        \item The tangent directions at the starting and ending points.
    \end{enumerate}
    The B\'{e}zier Cubic does this.
    \begin{definition}
    The B\'{e}zier Cubic in $\mathbb{R}^2$ is defined by:
    \begin{align*}
        x &= (1-t)^3 x_0+3t(1-t)^2x_1+3t^2(1-t)x_2+t^3x_3 \\
        y &= (1-t)^2 y_0+3t(1-t)^2y_1+3t^2(1-t)y_2+t^3y_3
    \end{align*}
    Where $x_0,x_1,x_2,x_3,y_0,y_1,y_2,y_3$ are input parameters.
    \end{definition}
    When $t=0$, we have $x = x_0, y=y_0$. Thus $(x_0,y_0)$ is the starting point. Similarly $(x_3,y_3)$ is the end point. The derivatives are:
    \begin{align*}
        x' &= -3(1-t)^2x_0 + 3(1-t)(1-3t)x_1+3t(2-3t)x_2+3t^2x_3 \\
        y' &= -3(1-t)^2y_0 + 3(1-t)(1-3t)y_1+3t(2-3t)y_2+3t^2y_3
    \end{align*}
    So $(x'(0),y'(0)) = \big(3(x_1-x_0),3(y_1-y_0)\big)$ and $(x'(1),y'(1)) = \big(3(x_3-x_2),3(y_3-y_2)\big)$. Hence, choosing $x_1,x_2$ and $y_1,y_2$ carefully allows that designer to control the tangent of the curve at the endpoints. Moreover, choosing the point $(x_1,y_1)$ makes the tangent at $(x(0),y(0))$ point in the same direction as the line from $(x_0,y_0)$ to $(x_1,y_1)$. Similarly, choosing $(x_2,y_2)$ makes the tangent at $(x(1),y(1))$ point in the same direction as the line from $(x_2,y_2)$ to $(x_3,y_3)$.
    \begin{definition}
    The control polygon of a B\'{e}zier Cubic in $\mathbb{R}^2$ is the polygon formed by the lines $(x_0,y_0)\rightarrow(x_1,y_1)\rightarrow(x_2,y_2)\rightarrow(x_3,y_3)\rightarrow (x_0,y_0)$.
    \end{definition}
    Interestingly enough, the B\'{e}zier Cubic always lies inside the control polygon. The final thing to control is the length of the tangents at the endpoint. But from equations $1.3$ and $1.4$, the lengths are three times the distance from $(x_0,y_0)$ to $(x_1,y_1)$ and $(x_2,y_2)$ to $(x_3,y_3)$, respectively. 
    \subsubsection{Ideals}
    \begin{definition}
    An ideal of a polynomial ring $k[x_1,\hdots ,x_n]$ is a set $I\subset k[x_1,\hdots ,x_n]$ such that:
    \begin{enumerate}
    \begin{multicols}{3}
        \item $0\in I$
        \item $\forall_{f,g\in I}, f+g\in I$ 
        \item $\forall_{f\in I, h\in k[x_1,\hdots ,x_n]}, hf\in I$
    \end{multicols}
    \end{enumerate}
    \end{definition}
    \begin{definition}
    The ideal generated by a set $\{f_1,\hdots, f_s\} \subset k[x_1,\hdots ,x_n]$ is the set $\langle f_1,\hdots, f_s\rangle = \{\sum_{i=1}^{s} h_i f_i:h_1,\hdots, h_s\in k[x_1,\hdots ,x_n]\}$.
    \end{definition}
    \begin{theorem}
    If $f_1,\hdots, f_s\in k[x_1,\hdots ,x_n]$, then $\langle f_1,\hdots, f_s\rangle$ is an ideal.
    \end{theorem}
    \begin{remark}
    The ideal $\langle f_1,\hdots, f_s\rangle$ has a nice interpretation. If $x\in k$ such that $f_1(x) = \hdots = f_s(x) = 0$, then for any set of polynomials $h_1,\hdots, h_s$, we have $h_1(x)f_1(x) = \hdots = h_s(x)f_s(x) = 0$, and adding the equations we get $h_1(x)f_1(x)+\hdots + h_s(x)f_s(x) = 0$. Thus we can think of $\langle f_1,\hdots, f_s\rangle$ as the set of all "Polynomial consequences," of the equations $f_1 = \hdots = f_s = 0$.
    \end{remark}
    \begin{example}
    Consider the following system:
    \begin{align*}
        x&=1+t&y&=1+t^{2}
    \end{align*}
    We can eliminate $t$ to obtain $y = x^2-2x+2$. To see this, write
    \begin{align*}
        x-1-t&=0&-y+1+t^{2}&=0
    \end{align*}
    Multiplying this first by by $x-1+t$ and adding, we get $(x-1)^2-y+1 = 0$. Thus $y = x^2-2x+2$.
    \end{example}
    \begin{definition}
    A finitely generated ideal is an ideal such that $\exists_{f_1,\hdots, f_s}: I = \langle f_1,\hdots, f_s\rangle$.
    \end{definition}
    \begin{definition}
    A basis of an ideal is a set $\{f_1,\hdots, f_s\}\subset k[x_1,\hdots ,x_n]$ such that $I=\langle f_{1},\hdots,f_{s}\rangle$
    \end{definition}
    Hilbert's Basis Theorem, to be proved later, states that every ideal in $k[x_{1},\hdots,x_{n}]$ is finitely generated. An ideal in $k[x_{1},\hdots,x_{n}]$ is similar to a subspace in linear algebra. Both must be closed under multiplication and addition, except that in a subspace we multiply by scalars and in an ideal we multiply by polynomials. 
    \begin{theorem}
    If $\langle f_1,\hdots, f_s\rangle = \langle g_1,\hdots, g_t\rangle$, then $\mathbf{V}(f_1,\hdots, f_s) = \mathbf{V}(g_1,\hdots, g_s)$.
    \end{theorem}
    \begin{example}
    $\langle2x^2+3y^2-11,x^2-y^2-3\rangle = \langle x^2-4,y^2-1\rangle$. So, $\mathbf{V}(2x^2+3y^2-11,x^2-y^2-3) = \{(2,1),(-2,1),(2,-1),(-2,-1)\}$. Changing basis simplifies the problem.
    \end{example}
    \begin{definition}
    The ideal of an affine variety $V \subset k^n$ is $\mathbf{I}(V)=\{f\in k[x_1,\hdots ,x_n]:\forall_{x\in V}f(x)=0\}$
    \end{definition}
    \begin{theorem}
    If $V\subset k^n$ is an affine variety, then $\mathbf{I}(V)$ is an ideal of $k[x_1,\hdots ,x_n]$.
    \end{theorem}
    \begin{theorem}
    For any field $k$, $\mathbf{I}\big(\{(0,0)\}\big) = \langle x,y\rangle$.
    \end{theorem}
    \begin{theorem}
    For any infinite field $k$, $\mathbf{I}(k^n) = \{0\}$.
    \end{theorem}
    \begin{theorem}
    If $V = \mathbf{V}(y-x^2,z-x^3)\subset \mathbb{R}^3$, $f\in \mathbf{I}(V)$, then $\exists_{h_1,h_2,r(x)\in \mathbb{R}[x,y,z]}$ such that $f=h_1(y-x^2)+h_2(z-x^3)+r$.
    \end{theorem}
    \begin{theorem}
    If $V = \mathbf{V}(y-x^2,z-x^3)\subset \mathbb{R}^3$, then $\mathbf{I}(V) = \langle y-x^2,z-x^3\rangle$
    \end{theorem}
    \begin{remark}
    It is not always true that $\mathbf{I}(\mathbf{V}(f_1,\hdots, f_s)) = \langle f_1,\hdots, f_s\rangle$.
    \end{remark}
    \begin{theorem}
    If $f_1,\hdots, f_s \in k[x_1,\hdots ,x_n]$, then $\langle f_1,\hdots, f_s \rangle \subset \mathbf{I}(\mathbf{V}(f_1,\hdots, f_s))$.
    \end{theorem}
    \begin{theorem}
    There exists fields $k$ and polynomials such that $\langle f_1,\hdots,f_s\rangle \ne \mathbf{I}(\mathbf{V}(f_1,\hdots, f_s))$
    \end{theorem}
    \begin{theorem}
    If $k$ is a field, $V,W\subset k^n$ are affine varieties, then $V\subset W$ if and only if $\mathbf{I}(W)\subset \mathbf{I}(V)$.
    \end{theorem}
    \begin{theorem}
    If $k$ is a field, $W,W\subset k^n$ are affine varieties, then $V=W$ if and only if $\mathbf{I}(W)=\mathbf{I}(V)$.
    \end{theorem}
    Three questions arise concerning ideals in $k[x_1,\hdots ,x_n]$.
    \begin{enumerate}
        \item Can every ideal $I\subset k[x_1,\hdots ,x_n]$ be written as $\langle f_1,\hdots, f_s\rangle$ for some $f_1,\hdots, f_s \in k[x_1,\hdots ,x_n]$?
        \item If $f_1,\hdots, f_s\in k[x_1,\hdots ,x_n]$, $f\in k[x_1,\hdots ,x_n]$, is there an algorithm to see if $f\in\langle f_{1},\hdots,f_{s}\rangle$?
        \item Is there a relation between $\langle f_1,\hdots, f_s\rangle$ and $\mathbf{I}(\mathbf{V}(f_1,\hdots, f_s))$?
    \end{enumerate}
    \subsubsection{Polynomials in One Variable}
    This section studies the division algorithm of polynomials in one variable.
    \begin{definition}
    The leading term of $f=\sum_{k=1}^{n}a_kx^{k}\in k[x]$, where $a_{n}\ne 0$, is $\LT(f)=a_nx^{n}$.
    \end{definition}
    \begin{example}
    If $f=2x^{3}-4x+3$, then $\LT(f)=2x^{3}$.
    \end{example}
    \begin{theorem}
    If $k$ is a field and $g\in k[x]\setminus\{0\}$, then $\forall_{f\in k[x]},\exists_{q,r\in k[x]}:f=qg+r$, where either $r=0$ or $\deg(r)<\deg(g)$. Furthermore, $q$ and $r$ are unique.
    \end{theorem}
    \begin{remark}
    From the uniqueness of $r$, we call $r$ the remainder of $f$ with respect to $g$.
    \end{remark}
    \begin{theorem}
    If $k$ is a field and $f\in k[x]$ is a non-zero polynomial, then $f$ has at most $\deg(f)$ roots.
    \end{theorem}
    \begin{definition}
    A principal ideal is an ideal generated by a single element.
    \end{definition}
    \begin{theorem}
    If $k$ is a field, then every ideal of $k[x]$ is principal.
    \end{theorem}
    \begin{theorem}
    If $\langle f\rangle=\langle g$ are ideals in $k[x]$, then there is a constant $h$ such that $f=hg$.
    \end{theorem}
    \begin{definition}
    A greatest common divisor of $f,g\in k[x]$ is a polynomial $h\in k[x]$ such that $h$ divides $f$ and $g$ and $\forall_{p\in k[x]}$ such that $p$ divides $f$ and $g$, $p$ divides $h$. 
    \end{definition}
    \begin{theorem}
    If $f,g\in k[x]$, then there is a greatest common divisor of $f$ and $g$.
    \end{theorem}
    \begin{theorem}
    If $f,g\in k[x]$, and $h_{1},h_{2}$ are greatest common divisors of $f$ and $g$, then there is a constant $c\in k$ such that $h_{1}=ch_{2}$.
    \end{theorem}
    \begin{remark}
    The Euclidean Algorithm is used for computational purposes to compute the greatest common divisor of two polynomials. Let $f,g\in k[x]$.
    \begin{enumerate}
        \item Let $h_{1}=f$
        \item Let $s_{1}=g$
        \item While $s_{n}\ne 0$, do the following:
        \begin{enumerate}
            \item $r_{n}=remainder(h_{n},s_{n})$
            \item $h_{n+1}=s_{n}$
            \item $s_{n+1}=r_{n}$
        \end{enumerate}
    \end{enumerate}
    There is an $N\in \mathbb{N}$ such that for all $n>N$, $h_n = h_N$. Letting $h = h_N$, this is the greatest common divisor of $f$ and $g$. This comes from $\GCD(f,g)=\GCD(f-qg,g)=\GCD(r,g)$ and the fact that $\deg(r)<\deg(g)$. So $\deg(r_{n+1})<\deg(r_{n})$, and eventually $\deg(r_{N})=0$.
    \end{remark}
    \begin{definition}
    A greatest common divisor of polynomials $f_1,\hdots, f_s \in k[x]$ is a polynomial $h\in k[x]$ such that $h$ divides $f_1,\hdots, f_s$ and if $p\in k[x]$ such that $p$ divides $f_1,\hdots, f_s$, then $p$ divides $h$.
    \end{definition}
    \begin{theorem}
    If $f_{1},\hdots, f_{s}\in k[x]$, then there is a polynomial $h\in k[x]$ that is a greatest common divisor of $f_{1},\hdots,f_{s}$.
    \end{theorem}
    \begin{theorem}
    If $f_{1},\hdots,f_{s}\in k[x]$, and if $h$ is a GCD of $f_{1},\hdots, f_{s}$, then $\langle h\rangle=\langle f_{1},\hdots, f_{s}\rangle$
    \end{theorem}
\end{document}