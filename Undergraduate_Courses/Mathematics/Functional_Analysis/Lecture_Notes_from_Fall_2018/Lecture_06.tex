\documentclass[crop=false,class=article,oneside]{standalone}
%----------------------------Preamble-------------------------------%
\input{../../../../preamble.tex}
%--------------------------Main Document----------------------------%
\begin{document}
    \ifx\ifmathcoursesfunctional\undefined
        \section*{Functional Analysis}
        \setcounter{section}{1}
    \fi
    \subsection{Lecture 6: October 15, 2018}
        \subsubsection{Normed Spaces}
            We're finally going to put some structure on these
            sets, and talk about vector spaces. In a metric
            space, the only thing you can really talk about
            is the distance between points. In a vector space
            we have a lot more structure. We will start off
            with vector spaces over the reals $\mathbb{R}$.
            The main properties are that there is a
            $\mathbf{0}$ element, addition is well defined
            and is both associative and commutative,
            there is a notion of scalar multiplication that
            is associative, and the distributive law holds.
            \begin{example}
                $\mathbb{R}^{n}$, with it's usual notion
                of addition, and with scalar multiplication
                defined over $\mathbb{R}$, is a vector space.
            \end{example}
            \begin{definition}
                A norm on a vector space $X$ over $\mathbb{R}$
                is a function $\norm{}:X\rightarrow\mathbb{R}$
                such that:
                \begin{enumerate}
                    \item For all $\mathbf{x}\in{X}$,
                          $\norm{\mathbf{x}}\geq{0}$ and
                          $\norm{\mathbf{x}}=0$ if and only
                          if $\mathbf{x}=\mathbf{0}$.
                          \hfill[Positive Definiteness]
                    \item For all $\mathbf{x}\in{X}$ and
                          $c\in\mathbb{R}$,
                          $\norm{c\mathbf{x}}%
                           =|c|\norm{\mathbf{x}}$
                          \hfill[Homogeneity]
                    \item For all $\mathbf{x},\mathbf{y}\in{X}$,
                          $\norm{\mathbf{x}+\mathbf{y}}%
                           \leq\norm{\mathbf{x}}%
                           +\norm{\mathbf{y}}$
                          \hfill[Triangle Inequality]
                \end{enumerate}
            \end{definition}
            We have seen before that
            $d(\mathbf{x},\mathbf{y})%
             =\norm{\mathbf{x}-\mathbf{y}}$
            defines a metric, and thus $(X,d)$ is a metric space.
            Thus, for every vector space there is an associated
            metric space, the metric $d$ called the
            \textit{induced} metric.
            \begin{definition}
                A normed vector space is a vector space
                $X$ over $\mathbb{R}$ with a norm
                $\norm{}$ on $X$.
            \end{definition}
            \begin{example}
                $\mathbb{R}^{n}$ with
                $\norm{\mathbf{x}}_{p}$, for $p\geq{1}$,
                is a normed vector space.
            \end{example}
            \begin{example}
                $\ell^{p}$ with $\norm{x}_{p}$ is
                also a normed vector space.
            \end{example}
            \begin{example}
                $C[a,b]$ equipped with the supremum norm,
                $\norm{x(t)}_{\infty}$,
                is a normed vector space.
            \end{example}
        \subsubsection{Inner Product Spaces}
            \begin{definition}
                An inner product on a vector space
                $X$ over $\mathbb{R}$ is a function
                $\langle\rangle:X\rightarrow\mathbb{R}$
                such that:
                \begin{enumerate}
                    \item For all $x\in{X}$,
                          $\langle{\mathbf{x},\mathbf{x}}%
                           \rangle\geq{0}$
                          and
                          $\langle\mathbf{x},\mathbf{x}\rangle=0$
                          if and only
                          if $\mathbf{x}=\mathbf{0}$.
                          \hfill[Positive Definiteness]
                    \item For all $\mathbf{x},\mathbf{y}\in{X}$,
                          $\langle\mathbf{x},\mathbf{y}\rangle%
                           =\langle\mathbf{y},\mathbf{x}\rangle$
                          \hfill[Symmetry]
                    \item For all
                          $\mathbf{x},\mathbf{y},\mathbf{z}%
                           \in{X}$
                          and all $\alpha,\beta\in\mathbb{R}$,
                          $\langle\alpha\mathbf{x}%
                           +\beta\mathbf{y},\mathbf{z}\rangle%
                           =\alpha\langle\mathbf{x},\mathbf{z}%
                           \rangle+\beta\langle\mathbf{y},%
                           \mathbf{z}\rangle$
                          \hfill[Linearity]
                \end{enumerate}
            \end{definition}
            \begin{example}
                $\mathbb{R}^{2}$ with
                $\langle(x_{1},x_{2}),(y_{1},y_{2})\rangle%
                 =x_{1}y_{1}+x_{2}y_{2}$ is an inner product.
                Replacing this with $\mathbb{R}^{n}$ and doing
                $\sum_{k=1}^{n}x_{k}y_{k}$ is also an inner
                product. This is the usual dot product that one
                sees in a vector calculus course. In $\ell^{2}$,
                $\sum_{k=1}^{\infty}x_{k}y_{k}$ is an inner
                product as well. Note also that
                $\sum|x_{i}y_{i}|$ converges since
                $|x_{i}y_{i}|\leq\frac{1}{2}|x_{i}^{2}|%
                 +\frac{1}{2}|y_{i}|^{2}$.
            \end{example}
            \begin{example}
                In $C[a,b]$, let
                $\langle{x(t),y(t)}\rangle%
                 =\int_{a}^{b}x(t)y(t)dt$. This defines an
                inner product.
            \end{example}
            \begin{definition}
                An inner product space is a vector space
                $X$ over $\mathbb{R}$ with an inner product
                $\langle\rangle$.
            \end{definition}
            \begin{theorem}[Cauchy-Schwarz Inequality]
                If $X$ is an inner product space
                and $x,y\in{X}$, then
                $|\langle{x,y}\rangle<\norm{x}\norm{y}$
            \end{theorem}
            \begin{proof}
                For all $y\in\mathbb{R}$,
                $\langle{x+ty,x+ty}\rangle%
                 =\langle{x,x}\rangle%
                 +2t\langle{x,y}\rangle%
                 +t^{2}\langle{y,y}\rangle%
                 =\norm{x}^{2}+2t\langle{x,y}\rangle%
                 +t^{2}\norm{y}^{2}$. Thus we have a
                quadratic in $t$. But this is always positive,
                and thus the discriminant must be non-positive. Therefore
                $(2\langle{x,y})^{2}-4\norm{x}^{2}\norm{y}^{2}%
                 \leq{0}$
                and thus
                $|\langle{x,y})|\leq\norm{x}\norm{y}$.
            \end{proof}
            \begin{theorem}
                If $X$ is a vector space over $\mathbb{R}$
                and $\langle\rangle$ is an inner product,
                then
                $\norm{\mathbf{x}}%
                 =\sqrt{\langle\mathbf{x},\mathbf{y}\rangle}$
                is a norm on $X$.
            \end{theorem}
            \begin{proof}
                Positivity, homogeneity, and definiteness are
                pretty easy. The only tricky thing to check is
                the triangle inequality. We have that
                $\norm{x+y}=\langle{x+y,x+y}\rangle$,
                and this simplify to
                $\norm{x}^{2}+2\langle{x,y}\rangle+\norm{y}^{2}$.
                But from the Cauchy-Schwartz inequality, we
                have $\langle{x,y}\rangle\leq\norm{x}\norm{y}$.
                Thus
                $\norm{x+y}^{2}\leq\norm{x}^{2}%
                 +2\norm{x}\norm{y}+\norm{y}^{2}%
                 =(\norm{x}+\norm{y})^{2}$. Taking square roots
                 completes the theorem.
            \end{proof}
            In $\mathbb{R}^{n}$, the Cauchy-Schwartz inequality
            says that the dot product of two vectors is less
            than or equal to the product of the magnitude
            of the two vectors.
            This is obvious from the fact that the dot product
            of two vector is the product of the magnitudes and
            the \textit{cosine} of the angle between them.
            Since the cosine of a number is less than or equal
            to one, this would complete the theorem.
            In $\ell^{p}$ and $L^{p}$ spaces, this is the
            special case of the H\"{o}lder inequality for
            when $p=q=2$.
        \subsubsection{Convergence in Normed Spaces}
            In a metric space, convergence meant that
            $d(x_{n},x)\rightarrow{0}$. In a normed space
            we have the induced metric, and thus we may define
            convergence as $\norm{x_{n}-x}\rightarrow{0}$.
            \begin{definition}
                A convergent sequence in a normed space $X$
                is a sequence $x_{n}$ such that there is an
                $x\in{X}$ such that
                $\norm{x_{n}-x}\rightarrow{0}$.
            \end{definition}
            Since
            $\norm{y}=\norm{(y-x)+x}\leq\norm{y-x}+\norm{x}$,
            it follows that
            $|\norm{x}-\norm{y}|\leq\norm{x-y}$.
            But then if $x_{n}\rightarrow{x}$, then
            $|\norm{x_{n}}-\norm{x}|\leq\norm{x_{n}-x}$,
            and $\norm{x_{n}-x}\rightarrow{0}$. Therefore
            $\norm{x_{n}}\rightarrow\norm{x}$. That is,
            the norm function is a continuous function.
            Similarly, if $x_{n}\rightarrow{x}$, then
            $\langle{x_{n},y}\rangle\rightarrow%
             \langle{x,y}\rangle$.
            In fact, if $x_{n}\rightarrow{x}$ and
            $y_{n}\rightarrow{y}$, then
            $\langle{x_{n},y_{n}}\rangle%
             \rightarrow\langle{x,y}\rangle$. To see this, we
            have
            $\langle{x_{n},y_{n}}\rangle-\langle{x,y}\rangle%
             =\langle{x_{n}-x,y}\rangle+\langle{x,y-y_{n}}\rangle$
            and therefore
            $|\langle{x_{n},y_{n}}\rangle-\langle{x,y}\rangle%
             \leq\norm{x_{n}-x}\norm{y_{n}}%
             +\norm{x}\norm{y-y_{n}}$. But $\norm{x-x_{n}}\rightarrow{0}$
            and $\norm{y-y_{n}}\rightarrow{0}$. But also
            $\norm{y_{n}}=\norm{(y_{n}-y)+y}\leq\norm{y_{n}-y}+\norm{y}$,
            which is bounded. Therefore
            $\langle{x_{n},y_{n}}-\langle{x,y}\rangle\rightarrow{0}$.
            So inner product spaces and normed spaces are metric spaces
            and we can define everything we did for metric spaces and all
            of the previous results remain true. That is, the notions and
            theorems pertaining to convergence, completeness, compactness,
            the notion of open and closed. All of these still make sense in
            these new spaces.
        \subsubsection{Banach Spaces and Hilbert Spaces}
            \begin{definition}
                A Banach Space is a normed vector space $X$ that is
                complete with respect to the induced metric.
            \end{definition}
            \begin{definition}
                A Hilbert Space is an inner product space $X$ that is
                complete with respect to the induced metric.
            \end{definition}
        \subsubsection{Linear Operators}
            Let $X$ and $Y$ be normed spaces. A mapping
            $T:X\rightarrow{Y}$ is called a linear operator if, for
            all $x,y\in{X}$, and for all $\alpha,\beta\in\mathbb{R}$,
            $T(\alpha{x}+\beta{y})=\alpha{T(x)}+\beta{T(y)}$. Usually, with
            operators, we simply write $Tx$ and $Ty$. Similar to how
            we write matric multiplication over vectors. In $\mathbb{R}^{n}$,
            every $n\times{n}$ matrix defines a linear operator.
            \begin{definition}
                A linear operator from a normed vector space $X$ to
                a normed vector space $Y$ is a function
                $T:X\rightarrow{Y}$ such that, for all $x,y\in{X}$
                and for all $\alpha,\beta\in\mathbb{R}$,
                $T(\alpha{x}+\beta{y})=\alpha{Tx}+\beta{Ty}$.
            \end{definition}
            \begin{definition}
                A bounded linear operator from a normed vector space
                $X$ to a normed vector space $Y$ is a linear operator
                $T:X\rightarrow{Y}$ such that there is a $K\in\mathbb{R}$
                such that for all $x\in{X}$, $\norm{Tx}\leq{K}\norm{x}$
            \end{definition}
            In a just world, ``bounded'' would mean
            $\norm{Tx}\leq{K}$. However, the only linear mapping that does
            this is the zero mapping. For if $\norm{Tx}=1$,
            then $\norm{T(2x)}=2$, and so on, and thus no linear mapping
            is bounded (With the exception of the zero mapping).
            Boundedness of a norm $T:X\rightarrow{Y}$ depends on
            the norms of the space.
            \begin{theorem}
                Bounded linear operators are continuous.
            \end{theorem}
            \begin{proof}
                If $x_{n}\rightarrow{x}$, then
                $\norm{Tx_{n}-Tx}=\norm{T(x_{n}-x)}$. But
                $T$ is bounded, and thus there is a $K$ such that
                $\norm{T(x_{n}-x)}\leq{K}\norm{x_{n}-x}$. But
                $\norm{x_{n}-x}\rightarrow{0}$. Therefore, etc.
            \end{proof}
            The converse is also true.
            \begin{theorem}
                If $T$ is a continuous linear operator,
                than there exists a $\delta>0$ such that for
                all $x\in{B}_{\delta}(0)$,
                $\norm{Tx-T0}<1$. But from linearity,
                $T0=0$, and thus $\norm{Tx}<1$. Then for any
                $z\in{Z}$, we have
                $\norm{\frac{\delta}{2}\frac{z}{\norm{z}}}=\frac{\delta}{2}$,
                and thus $\norm{T(\frac{\delta}{2}\frac{z}{\norm{z}})}<1$.
                Letting $K=\delta$, we have
                $\norm{Tx}<K\norm{x}$. Thus, $T$ is bounded.
            \end{theorem}
            Continuity at 0 implies uniform continuity since
            if $x_{n}-y_{n}\rightarrow{0}$, then
            $\norm{Tx_{n}-Ty_{n}}=\norm{T(x_{n}-y_{n})}%
             \leq{K}\norm{x_{n}-y_{n}}\rightarrow{0}$.
            The set of bounded linear operators form a vector space,
            where addition is $(S+t)(x)=(Sx)+(Tx)$, and scalar multiplication
            is defined by $(\alpha{T})(x)=\alpha(Tx)$. We must show that
            when you add two bounded linear operators, the result is a
            bounded linear operator.
            \begin{theorem}
                If $T_{1}:X\rightarrow{Y}$ and $T_{2}:X\rightarrow{Y}$
                are bounded linear operators, then $T_{1}+T_{2}$ is a
                bounded linear operator.
            \end{theorem}
            \begin{proof}
                For let $T_{1}$ and $T_{2}$ be bounded. Then there are
                $K_{1},K_{2}$ such that, for all $x\in{X}$,
                $\norm{T_{1}x}\leq{K_{1}}\norm{x}$ and
                $\norm{T_{2}x}\leq{K_{2}}\norm{x}$. But then
                $\norm{(T_{1}+T_{2})x}=\norm{T_{1}x+T_{2}x}%
                 \leq\norm{T_{1}x}+\norm{T_{2}x}%
                 \leq{K_{1}}\norm{x}+K_{2}\norm{x}$. Let $K=K_{1}+K_{2}$.
            \end{proof}
            \begin{theorem}
                If $T:X\rightarrow{Y}$ is a bounded linear operator, and
                $\alpha\in\mathbb{R}$, then $\alpha{T}$ is a bounded
                linear operator.
            \end{theorem}
            \begin{proof}
                For
                $\norm{\alpha{Tx}}=|\alpha|\norm{Tx}%
                 \leq|\alpha|K\norm{x}=K\norm{\alpha{x}}$.
            \end{proof}
            We write $B(X,Y)$ to denote the set of bounded linear
            operators from $X$ to $Y$. That is, linear operators
            $T:X\rightarrow{Y}$.
            We can define a norm on $B(X,Y)$ as follows:
            $\norm{T}_{B}%
             =\sup_{x\in{X},x\ne{0}}\{\frac{\norm{Tx}}{\norm{x}}\}$.
            This is the ``Smallest $K$,'' used as a bounded for the linear
            operator $T$. This shows that
            $\norm{Tx}_{Y}\leq\norm{T}_{B}\norm{x}_{X}$.
\end{document}