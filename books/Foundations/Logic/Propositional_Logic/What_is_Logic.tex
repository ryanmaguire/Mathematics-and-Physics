\section{What is Logic?}
    It may seem strange to start a work on mathematics with an entire
    development of logic, as one might think such conversations should reside
    in philosophy. And indeed, much of classical logic was developed by
    philosophers, rather than mathematicians. Many problems, which we will
    discuss in Chapt.~\ref{chapt:Zermelo_Fraenkel_Set_Theory}, arose in the
    early 1900s with the very core of mathematics. Arguments once considered
    sound were shattered, and contradictions were discovered. On the other hand,
    other methods of proof that are very intuitive were shown to be able to
    prove the existence of non-intuitive and almost impossible objects.
    \begin{example}
        A student of calculus most likely knows well the
        \textit{intermediate value theorem}%
        \index{Theorem!Intermediate Value Theorem}%
        \index{Intermediate Value Theorem}. To those who don't, fear not, we
        shall draw pictures. Given a \textit{continuous} function $f$ of real
        numbers (roughly speaking, a curve one can draw from left to right
        without lifting up ones pencil), if $0$ evaluates to a negative number
        and $1$ evaluates to a positive number, then there is some number in the
        middle which evaluates to zero. The method of proof is quite simple:
        We first look at what happens at the point $\frac{1}{2}$. If $f$
        is zero at this point, we are done and the theorem is proved, otherwise
        if $f$ evaluates to a positive number then we may suspect there's a
        point in between 0 and $\frac{1}{2}$ that evaluates to zero. If $f$ is
        negative at the point, then there's probably a zero in between
        $\frac{1}{2}$ and 1. In either case, we divide the range of
        possibilities in half once again and see what happens at $\frac{1}{4}$
        in the first case, and $\frac{3}{4}$ in the latter. We continue
        \textit{inductively} (whatever this means) and obtain a
        \textit{sequence} of real numbers which we then show
        \textit{converge} to some real number between 0 and 1. We then invoke
        continuity to show that $f$ evaluates to zero at this point, and we are
        done (See Fig.~\ref{fig:Sketch_of_IVP}).
    \end{example}
    \begin{figure}[H]
        \centering
        \captionsetup{type=figure}
        \includegraphics{images/Intermediate_Value_Theorem_Sketch.pdf}
        \caption{Sketch of the Intermediate Value Theorem}
        \label{fig:Sketch_of_IVP}
    \end{figure}
    We can see why this may work. After a few iterations we've narrowed down the
    zero point to a very small range between $x_{3}$ and $x_{5}$, and this is a
    nice algorithm that we can tell a computer to execute to arbitrary
    precision, but what went into the proof? That is, if we were to phrase this
    with absolute precision, what definitions, assumptions, and previous
    theorems are we relying on? For one, the existence of \textit{real number},
    a notion of \textit{continuity}, and the definition of a \textit{sequence}.
    Our exposition of logic is to make clear what is required for valid proofs.
    \subsection{Truth}
        Since the aim of mathematics is to prove the validity of mathematical
        statements, we start with a definition of truth. We run into a wall
        instantly with this, since this is essentially an impossible task. Any
        definition will ultimately be circular, and indeed it is a theorem of
        Alfred Tarski\index{Tarski, Alfred} that if one has somehow already
        defined arithmetic, then one cannot use arithmetic to define truth. That
        is to say, if we take upon the assumption of the existence of the
        natural numbers 0, 1, 2, $\dots$ with the familiar notion of addition
        $+$ (i.e. $1+1=2$ and other mathematical gems), then \textit{arithmetic}
        truth cannot be defined using this arithmetic. We propose the following
        work around: Truth is a primitive notion that needs no definition. We
        can then define false to mean \textit{not} true.
        \par\hfill\par
        Tarski's result came about in the 1930's when he tried to mathematically
        work out the \textit{liar's paradox}\index{Paradox!Liar's}%
        \index{Liar's Paradox}. Consider the following sentence:
        \begin{equation}
            \text{This sentence is false.}
        \end{equation}
        Similar statements have been considered throughout the ages, including
        the variant known as Epimenides' paradox\index{Paradox!Epimenides'}.
        Epimenides, who was from Crete, proclaims ``All Cretans are liars.''
        The question was considered again 200 years later in Miletus when
        Eubulides considered the sentence ``I am lying.'' Further still, in the
        Book of Psalms, king David states that all men are liars. Needless to
        say, the paradox is quite old and well studied. Now we ask, is the
        statement \textit{true} or \textit{false} (Assuming that such notions
        have been defined)? Let's work through it, and suppose truth. If
        this sentence is false is true, then the sentence is false even though
        we just claimed it to be true. Hence, it must be false. But if this
        sentence is false is false, then the sentence is true, but we just
        showed it cannot be true. So, which one is it? There are two
        interpretations: The statement is \textit{neither} true nor false, and
        the sentence is \textit{both} true and false. Suppose we accept that the
        statement is neither true nor false. This only leads to another
        sentence in which we cannot make such a conclusion:
        \begin{equation}
            \text{This sentence is not true.}
        \end{equation}
        If this is neither true nor false, then it is not true, and hence true,
        bringing us back to the paradox. So perhaps we claim that is it both
        true and false, but then we arrive at:
        \begin{equation}
            \text{The sentence is false and not true.}
        \end{equation}
        And hence there seems to be no workaround. The problem intensifies if we
        consider pairs of sentences:
        \twocolumneq{%
            \label{eqn:That_Sentence_Is_True}%
            \text{The statement \ref{eqn:That_Statement_Is_False} is true.}
        }
        {%
            \label{eqn:That_Statement_Is_False}%
            \text{The statement \ref{eqn:That_Sentence_Is_True} is false.}
        }
        and now we go round and round in an endless circle. As Alfred Tarski
        pointed out, the problem arises in languages in which statements are
        allowed to be self-referential. To that this is indeed a self refering
        claim, we can write $P$ for the proposition and we arrive at the
        equation:
        \begin{equation}
            P=P\text{ is false.}
        \end{equation}
        if we substitute $P$, we obtain:
        \begin{equation}
            P=(P\textit{ is false})\text{ is false.}
             =\big((P\text{ is false})\text{ is false}\big)\text{ is false.}
        \end{equation}
        While it may seem like this is an unnecessary discussion, the liar's
        paradox actually plays a role in mathematics. For one, as stated before,
        it motivates Tarski's theorem on the defineability of truth, and perhaps
        more famously it allowed Kurt G\"{o}del\index{G\"{o}del, Kurt} to prove
        his \textit{incompleteness theorems}, which really shook most of modern
        mathematics. Indeed, this theorem allegedly made Albert
        Einstein\index{Einstein, Albert} believe that there could be no
        \textit{theory of everything}\index{Theory of Everything}, a theory of
        physics that could solve all problems great and small. For the sake of
        moving on to mathematics, we accept truth to be a primitive notion and
        acknowledge that the foundations of this concept are very shaky.
    \subsection{Sets}
        The main objects in mathematics are \textit{sets}. This development came
        about in the 1800's with figures such as Georg
        Cantor\index{Cantor, Georg}, Augusts De
        Morgan\index{De Morgan, Augustus}, and Bernard
        Bolzano\index{Bolzano, Bernard} making the first strides in the theory.
        The early history is very loose, but intuitive, but the vagueness
        ultimately led to Russell's Paradox\index{Paradox!Russell's} which
        showed the naivity of set theory to be inconsistent. We'll discuss this
        in Chapt.~\ref{chapt:Zermelo_Fraenkel_Set_Theory} when we develop
        Zermelo-Fraenkel set theory, for now we just need a definition. In
        Cantor's 1895 work
        \textit{Beitr\"{a}ge zur Begr\"{u}ndung der transfiniten Mengenlehre}
        (Contributions in Support of Transfinite Set Theory), he writes
        \textit{A set is a gathering together in whole of definite distinct}
        \textit{objects of our perception or of our thought, which are called}
        \textit{the elements of the set}. Beautifully phrased, but circular. The
        word \textit{gathering} is not defined, nor is \textit{objects}. Any
        attempt at defining these will lead to the same problem, and thus we
        find ourselves in need of introducing another primitive notion: The
        notion of set. We adopt the following definition.
        \begin{fdefinition}{Set}{Set}
            A \gls{set} is a collection of objects called the elements of the
            set.\index{Set}
        \end{fdefinition}
        If we wish to stand on a truly solid foundation, it seems we're off to a
        bad start. In defining sets we've used the words \textit{collection} and
        \textit{objects}, neither of which have been defined. This is the same
        problem as the circularity we pointed out in Cantor's definition. To
        begin stating definitions and theorems we need the existence of a
        \textit{thing}. Sets act as our thing. We know they exist, but we don't
        know how to define them all to well. Nevertheless, we can describe how
        they behave and what they can do, as well as how to obtain new sets from
        pre-existing ones.
        \begin{fnotation}{Element Notation}{Element_Notation}
            If $A$ is a \gls{set} and if $x$ is an element\index{Set!Element of}
            of $A$, then we denote this by writing
            \glslink{containmentsymb}{$x\in{A}$}. If $x$ is not an element
            of $A$, we write $x\notin{A}$.\index{Containment $\in$}
        \end{fnotation}
        We are not developing set theory just yet, since we have yet to build up
        logic. In the most elementary systems such as Peano arithmetic
        there is a notion of set, and hence we need to define this first.
        Pedagogically it is poor to proceed without examples, and hence we do
        this presently.
        \begin{example}
            The first three letters of the Latin alphabet can be expressed in
            set notation as follows. If let let the symbol $A$ denote this set,
            we may write:
            \begin{equation}
                A=\{\,a,\,b,\,c\,\}
            \end{equation}
            If we let $B$ denote the first three positive integers, we obtain:
            \begin{equation}
                B=\{\,1,\,2,\,3\,\}
            \end{equation}
            Using element notation (Not.~\ref{not:Element_Notation}) we see that
            $1\in{B}$, but $4\notin{B}$. That is, $B$ contains the number 1 but
            does not contain the number 4. Similarly, $a\in{A}$ and
            $d\notin{A}$. The symbol $\in$ should read \textit{is in}, or
            \textit{is an element of}, or \textit{is contained inside of}. Thus
            $a\in{A}$ read $a$ is an element of $A$, or simply $a$ is in $A$.
            The notation $b\in{A}$ also reads as $b$ is contained inside of $A$.
            The notation $\notin$ is the negation of this: \textit{not in} or
            \textit{not and element of}. Hence, $4\notin{B}$ reads 4 is not an
            element of $B$.
        \end{example}
        \begin{example}
            \label{ex:Everything_is_a_Set}%
            In the set theory that we will be working with, Zermelo-Fraenkel set
            theory, \textit{everything} is a set. This will be explained later,
            but we quite literally mean everything. The integers will be defined
            via John von Neumann's\index{von Neumann, John} construction. We
            start with the empty set $\emptyset$ which is the set that contains
            nothing, often denoted $\emptyset=\{\,\}$, and this will be our
            zero. We proceed and define $1=\{\emptyset\}$, $2=\{0,1\}$,
            $3=\{1,2,3\}$, and so on.
        \end{example}
        Elaborating on the discussion in Ex.~\ref{ex:Everything_is_a_Set}, there
        are other theories for the foundations of mathematics that allow other
        primitive notions such as classes and universes and whatnot. Some of
        these theories are extremely weak (cannot prove much) but very safe
        (there is likely no contradiction), whereas some are very user friendly
        but almost certainly fallacious. Peano's axioms are an example of a weak
        system of which the axioms are so basic and obvious that no one is
        likely to ever find a contradiction, but are so weak that they can't
        even assert the existence of negative integers, let alone the rationals
        or reals. On the other hand, any theory that allows one to say the
        \textit{collection} of all sets (whether the collection is a class, or a
        universe, or whatever) is one that should be treated with skepticism.
        The theory of Zermelo and Fraenkel is a healthy middle ground. Strong
        enough to do most mathematics, and no contradiction found (yet), though
        much of the $20^{th}$ century was spent searching to no avail.
    \subsection{Predicates and Propositions}
        Propositions and predicates will be two more of our primitive notions
        which will will vaguely define, but mostly rely on intuition.
        \begin{fdefinition}{Predicate}{Predicate}
            A predicate $P$ on a collection of variables is a sentence such that
            for any valid input one may state that the sentence is either true
            or false. That is, either $P$ is true or $P$ is false.
        \end{fdefinition}
        Predicates are the main tool used in set theory for defining and
        building new sets. All of this will be discussed later when we
        encounter the \textit{axiom schema of specificaiton}%
        \index{Axiom!Schema of Specification}. Many describe predicates as
        \textit{functions} from the set $A$ to the Boolean-valued set
        $\{\text{True},\text{False}\}$ and this is a fine way of doing this
        provided the notion function has been defined. Many take function as
        another primitive, but we'll adopt the definition of a function in terms
        of subsets using the axioms of set theory. Hence we have the following
        predicamant: Do we take the word \textit{predicate} as a primitive
        concept, or the word \textit{function}? We're adopting predicate as our
        primitive, but the argument can be made for the latter choice.
        \begin{example}
            Let $A$ be the set $A=\{1,2,3\}$ and let $P$ be the predicate
            \textit{x is greater than 2}. The set of values in $A$ that satisfy
            this claim is $B=\{3\}$. If we let $Q$ represent
            \textit{x is negative}, then there are no elements in $A$ that
            satify $Q$ and hence the resulting set is the empty set $\emptyset$.
        \end{example}
        We first express the na\'{i}ve Aristotelian definition of proposition
        that came about in the 300's B.C.E., put forward by
        Aristotle\index{Aristotle}.
        \begin{equation}
            \text{A proposition is a sentence which affirms or denies a }
            \text{predicate.}
        \end{equation}
        The classic example is the so-called
        \textit{Socrates syllogism}\index{Socrates!Syllogism of}.
        \begin{example}
            We wish to conclude that the ancient greek philosopher
            Socrates\index{Socrates} was
            mortal. We start with the following proposition:
            \textit{All men are mortal}. This is a sentence which we are
            asserting to be true and that we may use later in proofs of other
            claims. Second, we state \textit{Socrates is a man}. This is another
            proposition, a statement that we accept as true. To conclude that
            Socrates is mortal we need some \textit{rule of inference} that
            allows us to tie these two propositions together. One such rule,
            known as \textit{modus ponens}\index{Modus Ponens} does exaclty what
            we need. It states that if $P$ and $Q$ are propositions, if $P$
            implies $Q$, and if $P$ is true, then $Q$ is true. Using this, since
            all men are mortal, and since Socrates is a man, we conclude that
            Socrates is mortal.
        \end{example}
        An easier proof that Socrates is mortal goes as follows: He's dead.
        Nevertheless, we are starting to see what is needed to construct valid
        proofs. We give the following rough definition of a proposition.
        \begin{fdefinition}{Proposition}{Proposition}
            A proposition is a predicate evaluated at a particular input of
            variables.
        \end{fdefinition}
        \begin{example}
            Let's use the previous example of Socrates to motivate what we mean.
            Let $P$ be the predicate \textit{x is a man}. If we input Socrates
            we obtain $P(\text{Socrates})=\text{True}$. If we input Hatshepsut,
            we get $P(\text{Hatshepsut})=\text{False}$. This is how we
            distinguish a predicate from a proposition.
        \end{example}
    \subsection{Rules of Inference}
        Given a collection of proposition, we often wish to derive new ones.
        Indeed, that is the entirety of mathematics: Proving new theorems. We
        must precisely state what rules we accept as valid and then attempt to
        stay consistent with these rules. The first rule we have already
        discussed, and is that of \textit{modus ponens}. This rule applies to
        \textit{implications}, one of the two primitives we adopt in our
        lanuage of deducing new claims, the second being negation which we will
        discuss soon enough.
        \begin{fdefinition}{Implication}{Implication}
            An implication on a proposition $Q$ by a proposition $P$ is the
            sentence that if $P$, then $Q$. We denote this $P\Rightarrow{Q}$.
        \end{fdefinition}
        We can express implication by means of a \textit{truth table}. Truth
        tables for a finite collection of propositions exhaust all possible
        combinations of True and False, and then apply these combinations to the
        logical question at hand. For example, we consider implication.
        \begin{table}[H]
            \centering
            \captionsetup{type=table}
            \begin{tabular}{c|c|c}
                $P$&$Q$&$P\Rightarrow{Q}$\\
                \hline
                False&False&True\\
                False&True&True\\
                True&False&False\\
                True&True&True
            \end{tabular}
            \caption{Truth Table for Implication}
            \label{tab:Truth_Table_Implication}
        \end{table}
        This shows that the only possible way for $P\Rightarrow{Q}$ to be false
        is if $P$ is true, yet $Q$ is false. This may be strange according to
        everyday language and there is a common tendency to confuse the order of
        implication. We spell this out in an example.
        \begin{example}
            Consider the proposition $P=\textit{I am late for work}$ together
            with $Q=\textit{I will be fired}$, and let's exhaust the four
            possible scenarios of if $P$, then $Q$. That is, we consider the
            claim \textit{if I am late fo work, then I will be fired}. Suppose
            I was not late for work, and I was not fired. Is $P\Rightarrow{Q}$
            true? Well, the criterion for $P$ was not satisfied and hence the
            statement is \textit{not} false, and so we claim it is true. Next, I
            was not late for work yet I was still fired (harsh). Again, the
            criterion for $P$ was not satisfied and so the claim is not false,
            and hence we accept it is true. Third, I was late for work and I was
            not fired (nice boss). Here we see that $P\Rightarrow{Q}$ is
            \textit{false}. The criterion for $P$ is satisfied, yet $Q$ was not,
            and therefore $P\Rightarrow{Q}$ is a false statement. Lastly, I was
            late for work and I was fired. This is perhaps the easiest one to
            handle since it is verbatim what one thinks of when they here
            \textit{if, then} claims. Here, $P\Rightarrow{Q}$ is true.
        \end{example}
        Given this definition of implication the axiom of \textit{modus ponens},
        short for \textit{modus ponendo ponens} which in Latin means
        \textit{mode that by affirmings affirms}, seems redundantly obvious.
        Nevertheless, we write it out.
        \begin{faxiom}{Modus Ponens}{Modus Ponens}
            If $P$ and $Q$ are propositions, if $P\Rightarrow{Q}$, and if $P$ is
            true, then $Q$ is true.\index{Axiom!of Modus Ponens}
        \end{faxiom}
        It is often easier and more useful to write out truth tables using
        zero and one. For one this hints at a means for computers to be able to
        understand and manipulate logical statements and proofs, and second it
        makes tables less cumbersome and allows us to systematically run through
        the possibilities. That is, we start with all propositions set to 0 and
        then flick the right-most one to 1, then the second right-most, and so
        on. This is difficult to show when we consider statements with only two
        propositions, but later it wil be clearer. The symbol 0 denotes false,
        and 1 represents truth. For implication, we obtain:
        \begin{table}[H]
            \centering
            \captionsetup{type=table}
            \begin{tabular}{c|c|c}
                $P$&$Q$&$P\Rightarrow{Q}$\\
                \hline
                0&0&1\\
                0&1&1\\
                1&0&0\\
                1&1&1
            \end{tabular}
            \caption{Alternate Truth Table for Implication}
            \label{tab:Alternate_Truth_Table_Implication}
        \end{table}
        Given $n$ propositions there are $2^{n}$ possible scenarios and so these
        truth tables get very big very fast. We can see that there are $2^{n}$
        possibilities since that are $n$ choices to be made from 0 and 1, and
        hence there are $2\cdot{2}\cdots{2}$ possibilities, with $n$ 2's in the
        product.
        \par\hfill\par
        The next two commonly accepted rules of inference, known as
        \textit{modus tollens}\index{Axiom!of Modus Tollens} and
        \textit{contraposition}\index{Axiom!of contraposition} are widely used
        in the mathematical world and relate to a statements contrapositive.
        The contrapositive is related to the \textit{negation} of a proposition,
        and so we define this now.
        \begin{fdefinition}{Negation}{Negation}
            The negation of a proposition $P$ is the proposition \textit{not}
            $P$, denoted $\neg{P}$.
        \end{fdefinition}
        Negation is a \textit{unary} operation, an operation on a single
        variable. The truth table is thus rather simple.
        \begin{table}[H]
            \centering
            \captionsetup{type=table}
            \begin{tabular}{c|c}
                $P$&$\neg{P}$\\
                \hline
                0&1\\
                1&0
            \end{tabular}
            \caption{Truth Table for Negation}
            \label{tab:Truth_Table_Negation}
        \end{table}
        That is, negaiton takes true to false and maps false to true. Negation
        and implication form our two primitive logical operations, and the other
        familiar terms (disjunction, conjunction, equivalence) can be expressed
        using these two. The axiom of \textit{modus tollens} states that if
        $P\Rightarrow{Q}$, and if $\neg{Q}$ is true, then $\neg{P}$ is true. We
        are not directly adopting this axiom since it is provable from the
        axiom of \textit{modus ponens} if one accept the standard axioms of set
        theory. Indeed, \textit{modus tollens} is implied by the
        \textit{law of the excluded middle}\index{Law of the Excluded Middle},
        which is in turn implied by the
        \textit{axiom of choice}\index{Axiom!of Choice}, two topics that will be
        discussed in Chapt.~\ref{chapt:Zermelo_Fraenkel_Set_Theory}. Similar to
        \textit{modus tollens}, the axiom of contraposition states that if
        $P\Rightarrow{Q}$ is equivalent to $\neg{Q}\rightarrow\neg{P}$. That is,
        if the statement $P\Rightarrow{Q}$ is true, then the
        \textit{contrapositive}\index{Contrapositive}
        $\neq{Q}\rightarrow\neg{P}$ is also true.
        \begin{example}
            Consider once again the description of Socrates. We start with the
            proposition \textit{all humans are mammals}. If Socrates is a human,
            then Socrates is a mammal. Therefore if Socrates is \textit{not} a
            mammal, then Socrates is \textit{not} a human. We could also say
            that all bears are mammals, and hence if you come across an animal
            that is \textit{not} a mammal, then this animal is \textit{not} a
            bear. Reasoning like this follows from contraposition.
        \end{example}
        Before moving on to Hilbert systems, it is worth while mentioning two
        \textit{invalid} forms of reasoning. These are not included as rules of
        inference because they lead to contradiction, although students often
        make these fallacious mistakes when exploring proofs for the first time.
        The first is known as
        \textit{Denying the Antecedent}\index{Fallacy!Denying the Antecedent},
        though it is commonly known as the inverse fallacy, or the converse
        fallacy. The \textit{converse}\index{Converse} of the statement if $P$,
        then $Q$ is the sentence if $Q$, then $P$.
    \subsection{Misc}
        \begin{table}[H]
            \centering
            \captionsetup{type=table}
            \begin{tabular}{c c c c c c}
                \hline
                $p$&$q$&$r$&$\neg{q}$&$p\lor\neg{q}$&$(p\lor\neg{q})\land{r}$\\
                \hline
                0&0&0&1&1&0\\
                0&0&1&1&1&1\\
                0&1&0&0&0&0\\
                0&1&1&0&0&0\\
                1&0&0&1&1&0\\
                1&0&1&1&1&1\\
                1&1&0&0&1&0\\
                1&1&1&0&1&1\\
                \hline
            \end{tabular}
            \caption{Truth Table for $(p\lor\neg{q})\land{r}$}
            \label{tab:Truth_Table_Example}
        \end{table}
        \begin{theorem}
            If $a\rightarrow{b}$, if $\neg{c}\rightarrow\neg{b}$, and if
            $\neg{c}$, then $\neg{a}$.
        \end{theorem}
        \begin{proof}
            For if $a\rightarrow{b}$, then $\neg{b}\rightarrow\neg{a}$. But
            $\neg{c}\rightarrow\neg{b}$. But if $\neg{c}\rightarrow\neg{b}$ and
            $\neg{b}\rightarrow\neg{a}$, then $\neg{c}\rightarrow\neg{a}$. Thus
            $a\rightarrow{b}$, $\neg{c}\rightarrow\neg{b}$, and thus
            $\neg{c}\Rightarrow\neg{a}$.
        \end{proof}
        \begin{problem}
            If $a\rightarrow{b}$, if $\neg{c}\rightarrow\neg{b}$, and if
            $\neg{c}$, then $\neg{a}$.
        \end{problem}
        \begin{proof}
            For if $\neg c \rightarrow \neg b$, then $b\rightarrow c$. But if
            $a\rightarrow b$ and $b\rightarrow c$, then $a\rightarrow c$.
            Therefore $a\rightarrow c$. But if $a\rightarrow c$, then
            $\neg c \rightarrow \neg a$. Therefore,
            $a\rightarrow b,\neg c\rightarrow\neg b,\neg c\Rightarrow\neg a$.
        \end{proof}