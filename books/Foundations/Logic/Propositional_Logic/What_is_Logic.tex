\section{What is Logic?}
    It may seem strange to start a work on mathematics with an entire
    development of logic, as one might think such conversations should reside
    in philosophy. And indeed, much of classical logic was developed by
    philosophers, rather than mathematicians. Many problems, which we will
    discuss in Chapt.~\ref{chapt:Zermelo_Fraenkel_Set_Theory}, arose in the
    early 1900s with the very core of mathematics. Arguments once considered
    sound were shattered, and contradictions were discovered. On the other hand,
    other methods of proof that are very intuitive were shown to be able to
    prove the existence of non-intuitive and almost impossible objects.
    \begin{example}
        A student of calculus most likely knows well the
        \textit{intermediate value theorem}%
        \index{Theorem!Intermediate Value Theorem}%
        \index{Intermediate Value Theorem}. To those who don't, fear not, we
        shall draw pictures. Given a \textit{continuous} function $f$ of real
        numbers (roughly speaking, a curve one can draw from left to right
        without lifting up ones pencil), if $0$ evaluates to a negative number
        and $1$ evaluates to a positive number, then there is some number in the
        middle which evaluates to zero. The method of proof is quite simple:
        We first look at what happens at the point $\frac{1}{2}$. If $f$
        is zero at this point, we are done and the theorem is proved, otherwise
        if $f$ evaluates to a positive number then we may suspect there's a
        point in between 0 and $\frac{1}{2}$ that evaluates to zero. If $f$ is
        negative at the point, then there's probably a zero in between
        $\frac{1}{2}$ and 1. In either case, we divide the range of
        possibilities in half once again and see what happens at $\frac{1}{4}$
        in the first case, and $\frac{3}{4}$ in the latter. We continue
        \textit{inductively} (whatever this means) and obtain a
        \textit{sequence} of real numbers which we then show
        \textit{converge} to some real number between 0 and 1. We then invoke
        continuity to show that $f$ evaluates to zero at this point, and we are
        done (See Fig.~\ref{fig:Sketch_of_IVP}).
    \end{example}
    \begin{figure}[H]
        \centering
        \captionsetup{type=figure}
        \includegraphics{images/Intermediate_Value_Theorem_Sketch.pdf}
        \caption{Sketch of the Intermediate Value Theorem}
        \label{fig:Sketch_of_IVP}
    \end{figure}
    We can see why this may work. After a few iterations we've narrowed down the
    zero point to a very small range between $x_{3}$ and $x_{5}$, and this is a
    nice algorithm that we can tell a computer to execute to arbitrary
    precision, but what went into the proof? That is, if we were to phrase this
    with absolute precision, what definitions, assumptions, and previous
    theorems are we relying on? For one, the existence of \textit{real number},
    a notion of \textit{continuity}, and the definition of a \textit{sequence}.
    Our exposition of logic is to make clear what is required for valid proofs.
    \subsection{Truth}
        Since the aim of mathematics is to prove the validity of mathematical
        statements, we start with a definition of truth. We run into a wall
        instantly with this, since this is essentially an impossible task. Any
        definition will ultimately be circular, and indeed it is a theorem of
        Alfred Tarski\index{Tarski, Alfred} that if one has somehow already
        defined arithmetic, then one cannot use arithmetic to define truth. That
        is to say, if we take upon the assumption of the existence of the
        natural numbers 0, 1, 2, $\dots$ with the familiar notion of addition
        $+$ (i.e. $1+1=2$ and other mathematical gems), then \textit{arithmetic}
        truth cannot be defined using this arithmetic. We propose the following
        work around: Truth is a primitive notion that needs no definition. We
        can then define false to mean \textit{not} true.
        \par\hfill\par
        Tarski's result came about in the 1930's when he tried to mathematically
        work out the \textit{liar's paradox}\index{Paradox!Liar's}%
        \index{Liar's Paradox}. Consider the following sentence:
        \begin{equation}
            \text{This sentence is false.}
        \end{equation}
        Similar statements have been considered throughout the ages, including
        the variant known as Epimenides' paradox\index{Paradox!Epimenides'}.
        Epimenides, who was from Crete, proclaims ``All Cretans are liars.''
        The question was considered again 200 years later in Miletus when
        Eubulides considered the sentence ``I am lying.'' Further still, in the
        Book of Psalms, king David states that all men are liars. Needless to
        say, the paradox is quite old and well studied. Now we ask, is the
        statement \textit{true} or \textit{false} (Assuming that such notions
        have been defined)? Let's work through it, and suppose truth. If
        this sentence is false is true, then the sentence is false even though
        we just claimed it to be true. Hence, it must be false. But if this
        sentence is false is false, then the sentence is true, but we just
        showed it cannot be true. So, which one is it? There are two
        interpretations: The statement is \textit{neither} true nor false, and
        the sentence is \textit{both} true and false. Suppose we accept that the
        statement is neither true nor false. This only leads to another
        sentence in which we cannot make such a conclusion:
        \begin{equation}
            \text{This sentence is not true.}
        \end{equation}
        If this is neither true nor false, then it is not true, and hence true,
        bringing us back to the paradox. So perhaps we claim that is it both
        true and false, but then we arrive at:
        \begin{equation}
            \text{The sentence is false and not true.}
        \end{equation}
        And hence there seems to be no workaround. The problem intensifies if we
        consider pairs of sentences:
        \twocolumneq{%
            \label{eqn:That_Sentence_Is_True}%
            \text{The statement \ref{eqn:That_Statement_Is_False} is true.}
        }
        {%
            \label{eqn:That_Statement_Is_False}%
            \text{The statement \ref{eqn:That_Sentence_Is_True} is false.}
        }
        and now we go round and round in an endless circle. As Alfred Tarski
        pointed out, the problem arises in languages in which statements are
        allowed to be self-referential. To that this is indeed a self refering
        claim, we can write $P$ for the proposition and we arrive at the
        equation:
        \begin{equation}
            P=P\text{ is false.}
        \end{equation}
        if we substitute $P$, we obtain:
        \begin{equation}
            P=(P\textit{ is false})\text{ is false.}
             =\big((P\text{ is false})\text{ is false}\big)\text{ is false.}
        \end{equation}
        While it may seem like this is an unnecessary discussion, the liar's
        paradox actually plays a role in mathematics. For one, as stated before,
        it motivates Tarski's theorem on the defineability of truth, and perhaps
        more famously it allowed Kurt G\"{o}del\index{G\"{o}del, Kurt} to prove
        his \textit{incompleteness theorems}, which really shook most of modern
        mathematics. Indeed, this theorem allegedly made Albert Einstein believe
        that there could be no \textit{theory of everything}, a theory of
        physics that could solve all problems great and small.
    \subsection{Misc}
        \begin{table}[H]
            \centering
            \captionsetup{type=table}
            \begin{tabular}{c c c c c c}
                \hline
                $p$&$q$&$r$&$\neg{q}$&$p\lor\neg{q}$&$(p\lor\neg{q})\land{r}$\\
                \hline
                0&0&0&1&1&0\\
                0&0&1&1&1&1\\
                0&1&0&0&0&0\\
                0&1&1&0&0&0\\
                1&0&0&1&1&0\\
                1&0&1&1&1&1\\
                1&1&0&0&1&0\\
                1&1&1&0&1&1\\
                \hline
            \end{tabular}
            \caption{Truth Table for $(p\lor\neg{q})\land{r}$}
            \label{tab:Truth_Table_Example}
        \end{table}
        \begin{theorem}
            If $a\rightarrow{b}$, if $\neg{c}\rightarrow\neg{b}$, and if
            $\neg{c}$, then $\neg{a}$.
        \end{theorem}
        \begin{proof}
            For if $a\rightarrow{b}$, then $\neg{b}\rightarrow\neg{a}$. But
            $\neg{c}\rightarrow\neg{b}$. But if $\neg{c}\rightarrow\neg{b}$ and
            $\neg{b}\rightarrow\neg{a}$, then $\neg{c}\rightarrow\neg{a}$. Thus
            $a\rightarrow{b}$, $\neg{c}\rightarrow\neg{b}$, and thus
            $\neg{c}\Rightarrow\neg{a}$.
        \end{proof}
        \begin{problem}
            If $a\rightarrow{b}$, if $\neg{c}\rightarrow\neg{b}$, and if
            $\neg{c}$, then $\neg{a}$.
        \end{problem}
        \begin{proof}
            For if $\neg c \rightarrow \neg b$, then $b\rightarrow c$. But if
            $a\rightarrow b$ and $b\rightarrow c$, then $a\rightarrow c$.
            Therefore $a\rightarrow c$. But if $a\rightarrow c$, then
            $\neg c \rightarrow \neg a$. Therefore,
            $a\rightarrow b, \neg c \rightarrow \neg b, \neg c \Rightarrow \neg a$.
        \end{proof}