We'll begin our discussion of logic with the most primitive kind:
propositional\index{Logic!Propositional}.%
\footnote{Also called \textit{sentential logic}}\index{Logic!Sentential}
This deals with the structure of sentences, how the English language is used to
formulate arguments and deduce new facts. It is for this reason that one
should start with the foundations of logic, for at the heart of
mathematics is the concept of \textit{definition}, \textit{theorem}, and
\textit{proof}. The word definition, it is hoped, is understood as a
primitive word in the English language (much like the word \textit{the})
but theorem and proof need an explaination if we are to use them
consistently. We'll discuss propositions, predicates, and the concepts
of implication and negation. Later we will study mathematical logic when
developing Boolean algebras\index{Boolean Algebra} in
Book~\ref{book:Foundations} and Stone spaces in
Book~\ref{book:Topology}, culminating in Stone's representation
theorem\index{Theorem!Stone's Representation}.
\section{What is Logic?}
    It may seem strange to begin a study of mathematics with the development of
    logic as one might think such conversations should reside in philosophy.
    Indeed, most of classical logic was developed by philosophers rather than
    mathematicians. Many problems, which we will discuss in
    Chapt.~\ref{chapt:Zermelo_Fraenkel_Set_Theory}, arose in the early 1900s
    with the very core of mathematics. Arguments once considered sound were
    shattered and contradictions were discovered. On the other hand
    other methods of proof that are very intuitive were shown to be able to
    prove the existence of non-intuitive and almost impossible objects. Our goal
    is to discuss which forms of reasoning we shall consider valid, provide
    examples, and propose the \textit{axioms} of mathematical logic.
    \begin{example}
        \label{ex:Logic_IVP}%
        A student of calculus has likely heard of the intermediate value
        theorem\index{Theorem!Intermediate Value}. Fear not those who haven't,
        we shall draw pictures. Given a \textit{continuous}%
        \footnote{%
            Intuitively a \textit{curve} one can draw from left to right without
            lifting up their pencil.%
        }
        function $f$ of real numbers, if 0 evaluates to a negative number
        and 1 to a positive number, then there is some point in the middle which
        evaluates to zero. The proof is quite simple: We first look at what
        happens to the point $\frac{1}{2}$. If $f$ is zero here we are done,
        otherwise if $f$ is positive then we may suspect there's a value in
        between 0 and $\frac{1}{2}$ which evaluates to zero and if $f$ is
        negative at this point, then there's probably a zero between
        $\frac{1}{2}$ and 1. In either case we divide the range of possibilities
        in half and see what happens at $\frac{1}{4}$ in the first case and
        $\frac{3}{4}$ in the latter. We continue \textit{inductively} (whatever
        this means) and obtain a \textit{sequence} of real numbers which we then
        show \textit{converges} somewhere between 0 and 1. We invoke continuity
        showing $f$ evaluates to zero at this limit and we are done
        (see Fig.~\ref{fig:Sketch_of_IVP}).
    \end{example}
    \begin{figure}[H]
        \centering
        \captionsetup{type=figure}
        \if\compilefigures1
            \includegraphics{images/Intermediate_Value_Theorem_Sketch.pdf}
        \fi
        \caption{Sketch of the Intermediate Value Theorem}
        \label{fig:Sketch_of_IVP}
    \end{figure}
    We can see why this may work. After a few iterations we've narrowed down a
    zero point to a small range between $x_{3}$ and $x_{5}$ and this is a
    nice algorithm we can tell a computer to execute to arbitrary precision%
    \footnote{%
        This is known as the \textit{bisection} method%
        \index{Root Finding!Bisection}\index{Bisection Method} of root finding.%
    }
    but what went into the proof? If we are to phrase this with absolute rigor,
    what definitions, assumptions, and previous theorems are we relying on? For
    starters, the existence of \textit{real numbers}, a notion of
    \textit{continuity}, and the definition of a \textit{sequence}. Our
    exposition of logic is to make clear what is required for valid proofs.
    \begin{example}
        \label{ex:Logic_Gauss_Sum}%
        It has been alleged, though as always one should exhibit skepticism,
        that at a very early age the great mathematician Carl Friedrich
        Gauss\index{Gauss, Carl Friedrich} (1777-1855 C.E.) demonstrated
        to his teacher that $1+2+\dots+99+100=5050$. It has also been claimed
        he had revelations about the normal distribution while counting the
        number of steps on his way to school, and that he corrected his fathers
        mathematical calculations at the age of three.%
        \footnote{%
            Many such claims were made by Gauss' biographer
            \textit{Wolfgang Sartorious von Waltershausen}, born 30 years after
            Gauss, publishing these stories in
            \textit{Gauss zum Ged\"{a}chtnis}.
        } Alas, mathematicians are
        a few tales away from forming the religion of Gauss. Nevertheless, let's
        see what the argument is. We rearrange this sum as follows:
        \begin{table}[H]
            \centering
            \captionsetup{type=table}
            \begin{tabular}{ccccccccccc}
                &&$1$&$+$&$2$&$+$&$\cdots$&$+$&$99$&$+$&$100$\\
                \hline\\
                $=$&&$1$&$+$&$2$&$+$&$\cdots$&$+$&$49$&$+$&$50$\\
                &$+$&$100$&$+$&$99$&$+$&$\cdots$&$+$&$52$&$+$&$51$\\
                \hline\\
                $=$&&$101$&$+$&$101$&$+$&$\cdots$&$+$&$101$&$+$&$101$\\
                \hline\\
                $=$&&$50$&$\times$&$101$\\
                \hline\\
                $=$&&$5050$
            \end{tabular}
            \caption{Gauss' Sum of 1 to 100}
        \end{table}
        While this seems to be a concrete proof of the claim, is it valid? Can
        we generalize it? What assumptions about integers and arithmetic are we
        making?
    \end{example}
    \begin{example}
        \label{ex:Logic_Euler_Sum}%
        Gauss is often called the prince of Mathematics, and the king is the
        great Leonhard Euler\index{Euler, Leonhard} (1707-1783 C.E.). He too
        studied sums and considered the bizarre series $1+2+3+4+\dots$ arriving
        at the answer $\minus\frac{1}{12}$. It should be noted that while this
        sum was studied in the $18^{th}$ century, there seems to be ambiguity as
        to whether or not Euler earns the credit on this one. Anyways, let's
        see how we can arrive at this answer. First, we consider
        Grandi's series\index{Grandi's series} which was studied by
        Luigi Guido Grandi\index{Grandi, Luigi Guido} (1671-1742 C.E.). We have:
        \begin{equation}
            G=1-1+1-1+1-1+\cdots
        \end{equation}
        Is there a meaningful number to assign to $G$? Suppose there is and
        write:
        \begin{table}[H]
            \centering
            \captionsetup{type=table}
            \begin{tabular}{ccccccccccccc}
                $1-G$&$=$&$1$&$-$&$\Big[$$1$&$-$&$1$&$+$&$1$&$-$&$\cdots$
                    &$\Big]$\\[1ex]
                \hline\\
                    &$=$&$1$&$-$&$1$&$+$&$1$&$-$&$1$&$+$&$\cdots$\\[1ex]
                \hline\\
                &$=$&$G$
            \end{tabular}
            \caption{Grandi's Series $1-1+1-1+\cdots$}
        \end{table}
        And hence we have $1-G=G$. Rearranging we get $2S=1$ and therefore
        $G=\frac{1}{2}$. Next, we consider the series:
        \begin{equation}
            T=1-2+3-4+5-6+\cdots
        \end{equation}
        Again, suppose $T$ has a legal value and apply the following
        manipulations:
        \begin{table}[H]
            \centering
            \captionsetup{type=table}
            \begin{tabular}{ccccccccccc}
                $2T$&$=$&$1$&$-$&$2$&$+$&$3$&$-$&$4$&$+$&$\cdots$\\
                    &   &   &$+$&$1$&$-$&$2$&$+$&$3$&$-$&$\cdots$\\
                \hline\\
                    &$=$&$1$&$-$&$1$&$+$&$1$&$-$&$1$&$+$&$\cdots$\\
                \hline\\
                &$=$&$\frac{1}{2}$
            \end{tabular}
            \caption{The Sum of $1-2+3-4+\cdots$}
        \end{table}
        And hence $T=\frac{1}{4}$. We now return to Euler's sum which we'll
        denote $S$:
        \begin{equation}
            S=1+2+3+4+\dots
        \end{equation}
        We subtract from $S$ the series $T$ which we computed above, obtaining:
        \begin{table}[H]
            \centering
            \captionsetup{type=table}
            \begin{tabular}{ccccccccccccc}
                $S-T$&$=$&&       &$1$&$+$&$2$&$+$&$3$&$+$&$4$&$\cdots$\\
                     &&$-$&$\Big[$&$1$&$-$&$2$&$+$&$3$&$-$&$4$&$\cdots$&$\Big]$
                \\[1ex]
                \hline\\
                     &$=$&&&$0$&$+$&$4$&$+$&$0$&$+$&$8$&$\cdots$\\[1ex]
                \hline\\
                &$=$&&$4\Big[$&$1$&$+$&$2$&$+$&$3$&$+$&$4$&$\cdots$&$\Big]$
                    \\[1ex]
                \hline\\
                &$=$&&$4S$
            \end{tabular}
            \caption{The Sum of $1+2+3+4+\cdots$}
        \end{table}
        so now we have a nice formula for $S$:
        \begin{equation}
            S-T=4S
        \end{equation}
        But $T=\frac{1}{4}$, so we have $3S=\minus\frac{1}{4}$ and therefore
        $S=\minus\frac{1}{12}$.
    \end{example}
    We now contemplate the previous examples and ask which are valid. It is
    tempting to say that Ex.~\ref{ex:Logic_IVP} and Ex.~\ref{ex:Logic_Gauss_Sum}
    were presented with accurate proofs, whereas Ex.~\ref{ex:Logic_Euler_Sum} is
    garbage, but why? We ``proved'' Gauss' and Euler's sum in the same manner:
    Rearranged terms, used \textit{dot dot dot} notation to indicate some
    pattern, added things together in a convincing way, and then simplified. One
    might suggest that Gauss' proof involved a finite scheme and that if asked
    one could laboriously write out the numbers indicated by the dots, whereas
    Euler's sum is infinite. But if we were to perform Gauss' problem with a
    number so large that it would take longer than the age of the universe to
    write down, even with the aid of computer, would we reject his method of
    proof then? There is some solace, and we can show that the Euler sum is
    invalid if we accept that $1\ne{0}$. Consider the sum $1+1+1+1+\cdots$ which
    we shall denote $B$ for bad.
    \begin{equation}
        B=1+1+1+1+\cdots
    \end{equation}
    Using Grandi's series $G$, we subtract and obtain:
    \begin{table}[H]
        \centering
        \captionsetup{type=table}
        \begin{tabular}{ccccccccccccc}
            $B-G$&$=$&&       &$1$&$+$&$1$&$+$&$1$&$+$&$1$&$\cdots$\\
                 &&$-$&$\Big[$&$1$&$-$&$1$&$+$&$1$&$-$&$1$&$\cdots$&$\Big]$
            \\[1ex]
            \hline\\
                 &$=$&&&$0$&$+$&$2$&$+$&$0$&$+$&$2$&$\cdots$\\[1ex]
            \hline\\
            &$=$&&$2\Big[$&$1$&$+$&$1$&$+$&$1$&$+$&$1$&$\cdots$&$\Big]$
                \\[1ex]
            \hline\\
            &$=$&&$2B$
        \end{tabular}
        \caption{The Sum of $1+1+1+1+\cdots$}
    \end{table}
    And so we obtain $B-G=2B$, so $B=\minus{G}$. But we know Grandi's series is
    $G=\frac{1}{2}$, and hence $B=\minus\frac{1}{2}$. We can also do the
    following:
    \begin{equation}
        1+B=1+(1+1+1+1+\cdots)=1+1+1+1+\cdots=B
    \end{equation}
    And hence $1+B=B$, so $1=0$ which is a contradiction. For the rest of the
    chapter we wish to hone in on what we are to consider as valid mathematics.
    \subsection{Truth}
        Since the aim of mathematics is to prove the validity of mathematical
        statements, we should start with a definition of truth. We run into a
        wall instantly since this is essentially an impossible task. Any
        definition will be circular, and it is a theorem of
        Alfred Tarski\index{Tarski, Alfred} (1901-1983 C.E.) that if one has
        defined arithmetic, then one cannot use arithmetic to define
        truth.\index{Theorem!Tarski's Undefineability Theorem}%
        \index{Tarski's Undefineability Theorem}%
        \footnote{%
            This is known as Tarski's Undefineability Theorem.%
        }
        That is, if we take upon the assumption of the existence of the
        natural numbers 0, 1, 2, $\dots$ with the familiar notion of addition%
        \footnote{%
            i.e. $1+1=2$ and other mathematical gems
         }
        then \textit{arithmetic} truth cannot be defined using this arithmetic.
        Let us consult the dictionary. The Oxford English dictionary defines
        truth to mean factual, Merriam-Webster states that truth is the body of
        real things, events, and facts, and Cambridge claims it is the quality
        of being true. As hypothesized these definitions are circular and rely
        on other predefined terms like \textit{factual}. We propose the
        following work around: Truth\index{Truth} is a primitive notion that
        needs no definition. We can then define false\index{False} to mean
        \textit{not} true.
        \par\hfill\par
        Tarski's result came about in the 1930's when he tried to mathematically
        work out the \textit{liar's paradox}\index{Paradox!Liar's}%
        \index{Liar's Paradox}. Consider the following sentence:
        \begin{equation}
            \text{This sentence is false.}
        \end{equation}
        Similar statements have been considered throughout the ages, including
        the variant known as Epimenides' paradox\index{Paradox!Epimenides'}.
        Epimenides of Cnossos (\textit{c.} 600 B.C.E.), who was from Crete,
        proclaims ``All Cretans are liars.'' The question was considered again
        200 years later when Eubulides of Miletus (\textit{c.} 400 B.C.E.)
        considered the sentence ``I am lying.'' Further still in the Book of
        Psalms king David states that all men are liars. Needless to say, the
        paradox is quite old and well studied. Now we ask, is the statement
        \textit{true} or \textit{false} (assuming such notions are defined)?
        Let's work through it and suppose truth. If this sentence is false is
        true, then the sentence is false even though we just claimed it to be
        true. Hence, it must be false. But if this sentence is false is false,
        then the sentence is true, but we just showed it cannot be true. So,
        which one is it? There are two interpretations: The statement is
        \textit{neither} true nor false, and the sentence is \textit{both} true
        and false. Suppose we accept that the statement is neither true nor
        false. This leads to another sentence where we cannot make such a
        conclusion:
        \begin{equation}
            \text{This sentence is not true.}
        \end{equation}
        If this is neither true nor false, then it is not true, and hence true,
        bringing us back to the paradox. Now we claim it is both true and false,
        leading us to:
        \begin{equation}
            \text{The sentence is false and not true.}
        \end{equation}
        The problem intensifies if we consider pairs of sentences:
        \twocolumneq[\par]{%
            \label{eqn:That_Sentence_Is_True}%
            \text{Statement \ref{eqn:That_Statement_Is_False} is true.}
        }
        {%
            \label{eqn:That_Statement_Is_False}%
            \text{Statement \ref{eqn:That_Sentence_Is_True} is false.}
        }
        and now we go round and round in an endless circle. As Alfred Tarski
        pointed out, the problem arises in languages in which statements are
        allowed to be self-referential. To see this is indeed a self
        referencing claim we write $P$ for the proposition and arrive at the
        equation:
        \begin{equation}
            P=P\text{ is false}
        \end{equation}
        if we substitute $P$, we obtain:
        \begin{equation}
            P=(P\text{ is false})\text{ is false}
             =\big((P\text{ is false})\text{ is false}\big)\text{ is false}
        \end{equation}
        While it may seem like this is an unnecessary discussion, the liar's
        paradox plays a role in mathematics. For one it motivates Tarski's
        theorem on the defineability of truth, and perhaps more famously it
        allowed Kurt G\"{o}del\index{G\"{o}del, Kurt}
        \index{G\"{o}del, Kurt}(1906-1978 C.E.) to prove his
        \textit{incompleteness theorems}, which really shook most of modern
        mathematics. Indeed, this theorem allegedly made Albert
        Einstein\index{Einstein, Albert} believe there could be no
        \textit{theory of everything}\index{Theory of Everything}.%
        \footnote{%
            A theory of physics that could solve all problems great and small.%
         } 
        For the sake of moving on to mathematics we accept truth to be a
        primitive notion and acknowledge that the foundations of this concept
        are very shaky.
        \par\hfill\par
        Let us examine a few more paradoxes of the English language. The main
        paradoxes of set theory (Russell's, Cantor's, and Burali-Forti's) will
        have to wait until we've developed more vocabulary. The first to discuss
        is \textit{Berry's paradox}\index{Paradox!Berry's}%
        \index{Berry's Paradox}, named after G. G. Berry\index{Berry, G. G.}
        (1867-1928 C.E.), a junior librarian at one of Oxford's library who
        relayed the paradox to Bertrand Russell in 1906.%
        \footnote{%
            Berry's original paradox dealt with Cantor's theory of
            \textit{ordinal} numbers.
        }
        The paradox arises from the following sentence:
        \begin{center}
            \textit{The smallest positive integer not defineable in less than}
            \textit{60 letters}
        \end{center}
        This statement is itself only 57 characters long. The English language
        has 26 letters, a space bar, and 10 numerical symbols (in addition to
        grammatical symbols like commas), so let's suppose there are 50 distinct
        characters allowed in a sentence. There are then a total of
        $50^{60}\approx{8}\times{10}^{101}$ combinations. Just about all of
        these sentences are absolute gibberish, for example:
        \begin{center}
            \textit{qjasneofiq923m woasmd fd/'?maojs 3m ansdjf aia sdf iquer sj}
        \end{center}
        One of my more poetic works, called
        \textit{bashing my keyboard and counting to 60}. Some of these
        combinations of characters do indeed correspond to integers. For
        example, \textit{The smallest positive integer} corresponds to 1. In
        just about all of the systems of arithmetic that are studied there
        exists a \textit{well-ordering}\index{Well Ordering} property of the
        integers. If you are given some collection of positive integers, and
        there's at least some integer in this collection, then there is a
        \textit{smallest} such integer. The proof is quite simple, ask yourself
        \textit{is 1 in the collection}? If yes, you are done since 1 is the
        smallest positive integer, if not proceed. Then ask
        \textit{is 2 in the collection}? Again, if yes then you are done since
        2 is the smallest positive integer greater than 1, but you've already
        checked that 1 is not in your collection. You continue until finally you
        hit an integer where the answer is \textit{yes}. You are guarenteed this
        process will stop since by hypothesis the collection has some integer
        $n$, and hence you need at most $n$ iterations of this procedure.
        \par\hfill\par
        In mathematics we describe collections of objects with sentences. This
        need not be with English, but the problem will exist in just about any
        human language. For example,
        \textit{the collection of all integers which are divisible by two} is a
        description of the \textit{even} integers. So now we propose
        \textit{the set of all integers not defineable in less than 60 letters}.
        Since there are at most $\approx{10}^{102}$ such integers that are
        defineable in less than 60 characters, and since most presume there are
        infinitely many numbers, we conclude the collection of integers defined
        by this sentence is non-empty. Then by the well ordering principle there
        is a least such element. But then this least element satisfies the
        criterion \textit{The smallest positive integer not defineable in less}
        \textit{than 60 letters}, which is less than 60 letters. But we've
        described it in fewer than 60 characters, a contradiction.%
        \footnote{%
            Some number theorists use this paradox to prove all integers are
            interesting. If not, then there is a least such integer that is
            not interesting. But being the smallest boring integer is pretty
            interesting! Hence, all integers are interesting.%
        }
        \footnote{%
            There is a semantical equivalent more familiar to most (though
            almost all are unbothered by it). In the classic Disney movie
            \textit{Aladdin}, whilst singing the song
            \textit{A Whole New World}, princess Jasmine proclaims
            \textit{indescribable feeling}, and yet she just described it.
        }
        \par\hfill\par
        The resolution has already been alluded to. There is an ambiguity with
        the word \textit{defineable}. Does this sentence indeed define an
        integer? It seems the paradox merely gives a proof that it does not. One
        proposed alternative to this solution is to create a hierarchy. Hence
        \textit{The smallest positive integer that is not $defineable_{0}$}
        \textit{in less than 60 letters} is a number that is $defineable_{1}$ in
        less than 60 letters. Like the liar's paradox, Berry's has a role in
        mathematics. In 1989 the American mathematics George
        Boolos\index{Boolos, George}%
        \footnote{Note to be confused with George Boole.}
        used the paradox to prove G\"{o}del's incompleteness theorem in a
        different manner.
        \par\hfill\par
        Next is the \textit{Grelling-Nelson Paradox}%
        \index{Grelling-Nelson Paradox}\index{Paradox!Grelling-Nelson}. This is
        less mathematical than the previous one but has a familiar ring to it.
        It is named after the German logicians Kurt
        Grelling\index{Grelling, Kurt} (1886-1943 C.E.) and Leonard
        Nelson\index{Nelson, Leonard} (1882-1927 C.E.).%
        \footnote{%
            Nelson's great grandfather is the mathematician
            \textit{Johann Peter Gustav Lejeune Dirichlet}.%
            \index{Dirichlet, Peter Gustav Lejeune}
        }
        Label an adjective of the English language as \textit{autological} if
        whatever the adjective is describing also holds for the adjective
        itself. For example, \textit{polysyllabic} describes words many with
        syllables, of which polysyllabic is such a word. Even more creative,
        \textit{pentasyllabic} words have five syllables, which pentasyllabic
        happens to have. The word \textit{word} is autological, as is the word
        \textit{English} (so long as it's read in English). Label an adjective
        \textit{heterological} otherwise. The words \textit{monosyllabic} and
        \textit{long} are heterological. Now we consider the word heterological.
        Since this is an adjective it is valid to ask if it is autological or
        heterological. If we suppose it is heterological, then it describes
        itself and is thus autological, a contradiction. If it is autological
        then it describes itself, but heterological words do not describe
        themselves, a contradiction.
        \par\hfill\par
        The game  being played here is similar to the circularity of the
        liar's paradox. Since this problem is purely semantical, we move on to
        \textit{Curry's paradox}\index{Curry's Paradox}\index{Paradox!Curry's}.%
        \footnote{%
            Also known as L\"{o}b's paradox\index{Paradox!L\"{o}b's}%
            \index{L\"{o}b's Paradox}
        }
        Like the liar's paradox, this problem has mathematical use and is often
        seen as a simplification of the Kleene-Rosser paradox%
        \index{Kleene-Rosser Paradox}\index{Paradox!Kleene-Rosser} which was
        used to proved certain formal systems are inconsistent.%
        \footnote{%
            Much the way Russell's paradox proved na\"{i}ve set theory to
            be inconsistent.
        }
        The problem is named after the American mathematician
        Haskell Curry\index{Haskell Curry}%
        \footnote{%
            Programming enthusiasts should note the language
            \textit{Haskell} is named after Curry.
        }
        (1900-1982 C.E.).%
        \footnote{%
            Curry's paradox shows the inconsistentency of na\"{i}ve set theory,
            the original \textit{lambda calculus}, and Curry's
            \textit{combinatory logic}.
        }
        \par\hfill\par
        Let $P$ be any sentence, and let $Q$ be the sentence
        \textit{if this sentence is true, then P is true}. If $Q$ is true, then
        \textit{if this sentence is true, then P is true} is a true statment.
        Hence, $P$ is true. From this \textit{anything} can be proven. Like the
        liar's paradox the problem is in the allowance of self-referencing
        sentencing. $Q$ can be written as the statement \textit{if Q, then P},
        which is certainly self-referential.
        \par\hfill\par
        We've quite a lot of cleaning up to do before we can delve into the core
        of mathematics. One might think this is a waste of time, but an
        inconsistent theory is truly horrible. The
        \textit{principle of explosion}%
        \footnote{We will prove the principle later in this chapter.}
        allows one to prove, given an inconsistent set of assumptions, that
        \textit{everything} is provably true (and also provably false). This is
        rather boring and one would hope to avoid this. While it will only play
        a small role throughout our investigations, we should discuss
        Bertrand Russell's\index{Russell, Bertrand} \textit{type theory}%
        \index{Type Theory}, invented in collaboration with
        Alfred North Whitehead\index{Whitehead, Alfred North}
        (1861-1947 C.E.).%
        \footnote{%
            Not to be confused with the great topologist J. H. C. Whitehead.
        }
        This was one of the first attempts at resolving these paradoxes. In this
        theory everything has a \textit{type}, which may be thought of as a
        non-negative integer. When discussing containment, for example
        \textit{the collection A is contained in the collection B}, we only
        give this meaning if $B$ has a type that is 1 greater than the type of
        $A$. In general operations can only be applied to objects of the correct
        type. This has applications in computer science and programming.
        Programming languages like Python allow one to define functions that in
        take undefined inputs. The program will crash if an input is given that
        the function cannot safely handle. For example:
        \par
        \begin{tcblisting}{
            before=\par\vspace{2ex},
            boxsep=0.5\topsep,
            after=\par\vspace{2ex},
            top=0ex,
            bottom=0ex,
            title=Python Code with Ambiguous Input,
            listing only,
            listing options={
                language=Python,
                basicstyle=\ttfamily,
                keywordstyle=\color{blue}\ttfamily,
                stringstyle=\color{red}\ttfamily,
                commentstyle=\color{green}\ttfamily,
                morecomment={[l][\color{magenta}]{\#}},
                numbers=left,
                framexleftmargin=-16ex,
                numbersep=-16ex,
                xleftmargin=-16ex
            }
        }
            def f(x):
                return x+1
        \end{tcblisting}
        If we enter $x=1$, then type $f(x)$, this will return 2 without error.
        However Python allows strings and if we enter $x=\textit{``Bob''}$, then
        Python will return an error. This implementation of type checking is
        called \textit{Duck typing}%
        \footnote{%
            If your input looks like a duck and sounds like a duck, it is
            probably a duck.
        }
        in which the type of the input is checked at run time and if the type
        looks correct, the program will attempt to execute it. This is in
        contrasts to languages which require functions and variables to have
        their types declared prior to compiling or executation. For example, if
        we write this same program in C we have:%
        \footnote{%
            For simplicity, \textit{double} in C simply means a real number.
        }
        \begin{tcblisting}{
            before=\par\vspace{2ex},
            boxsep=0.5\topsep,
            after=\par\vspace{2ex},
            top=0ex,
            bottom=0ex,
            title=C Code with Declared Input,
            listing only,
            listing options={
                language=C,
                basicstyle=\ttfamily,
                keywordstyle=\color{blue}\ttfamily,
                stringstyle=\color{red}\ttfamily,
                commentstyle=\color{green}\ttfamily,
                morecomment={[l][\color{magenta}]{\#}},
                numbers=left,
                framexleftmargin=-16ex,
                numbersep=-16ex,
                xleftmargin=-16ex
            }
        }
            double f(double x){
                return x+1;
            }
        \end{tcblisting}
        If we try to use this function on anything that is not a real number
        the program will refuse to compile.%
        \footnote{%
            Unless you're using a \textit{really} outdated compiler.
        }
        The entirety of type theory is laid out in the three volume
        treatise \textit{Principia Mathematica}. We will adopt Zermelo-Fraenkel
        set theory\index{Zermelo-Fraenkel Set Theory} in addition to the logical
        axioms of Hilbert\index{Hilbert, David} to formulate mathematics, both
        of which being introduced 20 years after Russell and Whitehead's
        efforts.
    \subsection{Sets}
        The main objects in mathematics are \textit{sets}. This development came
        about in the 1800's with figures like Georg Cantor\index{Cantor, Georg},
        Augustus De Morgan\index{De Morgan, Augustus}, and Bernard
        Bolzano\index{Bolzano, Bernard} making the first strides in the theory.
        The early history is vague and intuitive, but the obscurity
        led to Russell's Paradox\index{Paradox!Russell's} which showed the
        na\"{i}vity of set theory to be inconsistent. We'll discuss this in
        Chapt.~\ref{chapt:Zermelo_Fraenkel_Set_Theory}, for now we just need a
        definition. Georg Cantor (1845-1918 C.E.) wrote the following:
        \begin{center}
            \textit{A set is a gathering together into a whole of definite}
            \textit{distinct objects of our perception or of our thought, which}
            \textit{are called the elements of the set.}
            \par\hfill
            \textit{Beitr\"{a}ge zur Begr\"{u}ndung der Transfiniten}
            \textit{Mengenlehre}%
            \footnote{%
                English:
                \textit{Contributions in Support of Transfinite Set Theory}%
            }
            \par\hfill
            \textit{Georg Cantor, 1985 C.E.}
        \end{center}
        Beautifully phrased, but circular since neither \textit{gathering} nor
        \textit{object} are defined. Felix Hausdorff\index{Hausdorff, Felix}
        (1868-1942 C.E.) tried to define sets by explaining how one obtains
        them. He writes
        \begin{center}
            \textit{A set is formed by the grouping together of single objects}
            \textit{into a whole. A set is a plurality thought of as a unit.}
            \par\hfill
            \textit{Felix Hausdorff}
        \end{center}
        Again, \textit{grouping} is undefined. This form of circularity was
        addressed by Alfred Tarski in his 1946 book \textit{Introduction to}
        \textit{Logic and the Methodology of the Deductive Science}. He
        expresses the need for primitive undefined notions that we take for
        granted and use freely. We collect the smallest number of primitives
        possible, motivated by intuition, and then define other terms in our
        theory by means of sentences involving these primitives and previously
        defined terms.
        \par\hfill\par
        We do not wish to imply the circularity of the foundations of
        mathematics was created with the advent of set theory. In what is
        perhaps the most important textbook ever written,
        \textit{The Elements}\index{Elements, The (Euclid)} by Euclid of
        Alexandria (\textit{c.} 300 B.C.E), we find the first known work
        that employs the \textit{axiomatic method}\index{Axiomatic Method}. It
        starts with definitions, \textit{postulates}, and
        \textit{common notions}, and proceeds to prove a plethora of important
        theorems in a logical manner deriving results from these primitives and
        previously proved theorems. In modern language postulates and common
        notions are known as \textit{axioms}, which are statements that we
        accept as true without evidence or proof. It is not without flaw since
        his primitive definitions are circular. For example, the
        first definition is of a point:
        \begin{center}
            \textit{A point is that which has no part.}
            \par
            \hfill\textit{The Elements}\par
            \hfill\textit{Euclid of Alexandria, c. 300 B.C.E.}
        \end{center}
        The word part is never defined. Similarly, a line is defined as
        \textit{breadthless length}. This is not to detract from his efforts but
        to show the problem of \textit{infinite regress}\index{Infinite Regress}
        is unavoidable unless we assert that certain terms need no definition.
        For us, the word set will have no real definition. Nevertheless, we
        write the following:
        \begin{fdefinition}{Set}{Set}
            A \gls{set} is a collection of objects called the elements of the
            set.\index{Set}\index{Set!Definition}
        \end{fdefinition}
        The circularity we pointed out in Cantor's definition arises here since
        neither \textit{collection} nor \textit{object} have been defined. To
        begin doing mathematics we need a \textit{thing}. Sets act as our thing.
        We know they exist but cannot define them very well. Instead we describe
        describe how they behave and how to obtain new sets from pre-existing
        ones via \textit{axioms}.
        \begin{fnotation}{Element Notation}{Element_Notation}
            If $A$ is a \gls{set} and if $x$ is an element\index{Set!Element of}
            of $A$, then we denote this by writing
            \glslink{containmentsymb}{$x\in{A}$}. If $x$ is not an element
            of $A$, we write $x\notin{A}$.\index{Containment $\in$}%
            \index{Element}\index{Element!Notation}
        \end{fnotation}
        We are not developing set theory yet since we have yet to build up
        logic. In the most elementary systems such as Peano arithmetic there is
        a notion of set, and hence we need to define this first. Pedagogically
        it is poor to proceed without examples, so we provide some now.
        \begin{example}
            The first three letters of the Latin alphabet can be expressed in
            set notation as follows. If let let the symbol $A$ denote this set,
            we may write:
            \begin{equation}
                A=\{\,a,\,b,\,c\,\}
            \end{equation}
            If we let $B$ denote the first three positive integers, we obtain:
            \begin{equation}
                B=\{\,1,\,2,\,3\,\}
            \end{equation}
            Using element notation (Not.~\ref{not:Element_Notation}) we have
            $1\in{B}$, but $4\notin{B}$. That is, $B$ contains the number 1 but
            does not contain the number 4. Similarly, $a\in{A}$ and
            $d\notin{A}$. The symbol $\in$ reads \textit{is in}, or
            \textit{is an element of}, or \textit{is contained inside of}. Thus
            $a\in{A}$ reads $a$ is an element of $A$, or simply $a$ is in $A$.
            The notation $b\in{A}$ also reads as $b$ is contained inside of $A$.
            The notation $\notin$ is the negation of this: \textit{not in} or
            \textit{not and element of}, so $4\notin{B}$ reads 4 is not an
            element of $B$.
        \end{example}
        \begin{example}
            \label{ex:Everything_is_a_Set}%
            In the set theory that we will be working with, Zermelo-Fraenkel set
            theory\index{Zermelo-Fraenkel Set Theory}, \textit{everything} is a
            set. This will be explained later, but we quite literally mean
            everything. The integers will be defined via John von
            Neumann's\index{von Neumann, John} construction. We start with the
            empty set\index{Empty Set} $\emptyset$ which is the set that
            contains nothing, often denoted $\emptyset=\{\,\}$, and this will be
            our zero. We proceed and define $1=\{\emptyset\}$, $2=\{0,1\}$,
            $3=\{0,1,2\}$, and so on. Moreover \textit{functions} are defined as
            sets, as are \textit{ordered pairs} $(a,b)$, and even
            \textit{orderings}.
        \end{example}
        Elaborating on the discussion in Ex.~\ref{ex:Everything_is_a_Set}, there
        are other theories for the foundations of mathematics that allow other
        primitive notions such as classes and universes. Some of these theories
        are extremely weak (cannot prove much) but very safe (there is likely no
        contradiction), whereas some are very user friendly but almost certainly
        fallacious. Peano's axioms are an example of a weak system of which the
        axioms are so basic and obvious that no one is likely to ever find a
        contradiction, but they cannot assert the existence of negative
        integers, let alone the reals. On the other hand, any theory that
        allows one to say the \textit{collection} of all sets whether the
        collection is a class, or a universe, or whatever, is one that should be
        treated with skepticism. The theory of Zermelo and Fraenkel is a healthy
        middle ground. Strong enough to do most mathematics, and no
        contradiction found yet, though much of the $20^{th}$ century was spent
        searching to no avail.
    \subsection{Predicates and Propositions}
        Propositions and predicates will be two more of our primitive notions
        which we will vaguely define, but mostly rely on intuition.
        \begin{fdefinition}{Predicate}{Predicate}
            A \gls{predicate} $P$ on a \gls{set} of variables is a sentence
            such that for any valid input one may state that $P$ is either true
            or false.\index{Predicate}\index{Predicate!Definition}
        \end{fdefinition}
        We did not define \textit{variable}\index{Variable}, but this primitive
        is understood as a symbol or placeholder that may represent
        \textit{things}. Predicates are the main tool used in set theory for
        defining and building new sets via the
        \textit{axiom schema of specification}%
        \index{Axiom!Schema of Specification}. Many describe predicates as
        \textit{functions} from a set $A$ to the Boolean-valued set
        $\{\text{True},\,\text{False}\}$ and this is a fine way of doing this
        provided the notion function has been defined. Many take function as
        another primitive, and thus one has the following decision to make: Do
        we accept \textit{predicate} as a primitive, or \textit{function}? We'll
        adopt predicate and later \textit{define} functions using elementary
        notions from the axioms of set theory.
        \begin{example}
            Let $A$ be the set $A=\{1,2,3\}$ and let $P$ be the predicate
            \textit{x is greater than 2}. The set of values in $A$ that satisfy
            this claim is $B=\{3\}$. If we let $Q$ represent
            \textit{x is negative}, then there are no elements in $A$ that
            satify $Q$ and hence the resulting set is the empty set $\emptyset$.
        \end{example}
        Moving on to propositions, we first express the na\"{i}ve Aristotelian
        definition put forward by Aristotle\index{Aristotle} (384-322 B.C.E.).
        \begin{equation}
            \text{A proposition is a sentence which affirms or denies a }
            \text{predicate.}
        \end{equation}
        The classic example is the so-called
        \textit{Socrates syllogism}\index{Socrates!Syllogism of}.
        \begin{example}
            We wish to conclude the ancient greek philosopher
            Socrates\index{Socrates} was mortal. We start with the following
            proposition: \textit{All men are mortal}. We assert this sentence is
            true and we may be used later in the proofs of other claims. Second,
            we state \textit{Socrates is a man}. This is another proposition, a
            statement that we accept as true. To conclude Socrates is mortal we
            need some \textit{rule of inference} that allows us to tie these two
            propositions together. One such rule, known as
            \textit{modus ponens}\index{Modus Ponens}%
            \index{Axiom!of Modus Ponens} does exactly what we need. It states
            that if $P$ and $Q$ are propositions, if $P$ implies $Q$, and if $P$
            is true, then $Q$ is true. Using this, since all men are mortal, and
            since Socrates is a man, we conclude that Socrates is mortal.
        \end{example}
        An easier proof that Socrates is mortal goes as follows: He's dead.
        Nevertheless, we are starting to see what is needed to construct valid
        proofs.
        \begin{fdefinition}{Proposition}{Proposition}
            A \gls{proposition} is a \gls{predicate} evaluated at a particular
            \gls{set} of variables, taken in a particular order, which is then
            affirmed or denied to be true.%
            \index{Proposition}\index{Proposition!Definition}
        \end{fdefinition}
        The order of the input of variables is important. Sets do not have
        order, and to give rise to such notions requires the bulk of the axioms
        of set theory. Axioms are themselves propositions which we assert to be
        true without proof, and hence we stumble upon circularity. To define
        ordered sets we need the definition of proposition, and to define
        proposition we need ordered sets. The solution is to accept proposition
        as a primitive which needs no real definition.
        \begin{example}
            Let's use the previous example of Socrates to motivate what we mean.
            Let $P$ be the predicate \textit{x is a man}. If we input Socrates
            we obtain $P(\text{Socrates})=\text{True}$. If we input Hatshepsut,
            we get $P(\text{Hatshepsut})=\text{False}$. This is how we
            distinguish a predicate from a proposition.
        \end{example}
        \begin{example}
            Suppose we have the predicate \textit{x is a man, y is a woman}
            and let us label this $P(x,y)$. The \textit{order} of the inputs now
            matters. For example,
            $P(\text{Socrates},\text{Hatshepsut})=\text{True}$, but
            $P(\text{Hatshepsut},\text{Socrates})=\text{False}$.
        \end{example}
    \subsection{Rules of Inference}
        Given a collection of propositions we often wish to derive new ones.
        Indeed, that is the entirety of mathematics: Proving new theorems. We
        must precisely state which rules we accept as valid and then attempt to
        stay consistent with them. The first we have already discussed,
        \textit{modus ponens}. This rule applies to \textit{implications}, one
        of the two primitives we adopt in our lanuage of deducing new claims,
        the second being negation which we will discuss soon enough.
        \begin{fdefinition}{Implication}{Implication}
            An \gls{implication} on a \gls{proposition} $Q$ by a proposition $P$
            is the sentence that if $P$, then $Q$. We denote this
            $P\Rightarrow{Q}$.\index{Implication}\index{Implication!Definition}
        \end{fdefinition}
        The proposition $P$ is called the \textit{hypothesis}\index{Hypothesis},
        and $Q$ is the \textit{conclusion}\index{Conclusion}. We can describe
        implication via \textit{truth tables}\index{Truth Table}. Truth tables
        for a finite collection of propositions exhaust all possible
        combinations of True and False, and then apply these combinations to the
        logical question at hand. Implication is depicted in
        Tab.~\ref{tab:Truth_Table_Implication}. Given
        $n$ propositions there are $2^{n}$ possible scenarios and so these truth
        tables get very big very fast. We can see that there are $2^{n}$
        possibilities since that are $n$ choices to be made from 0 and 1, so
        there are $2\cdot{2}\cdots{2}$ possibilities with $n$ 2's
        (of course, we've yet to define what $2^{n}$ means).
        \par
        \begin{minipage}[b]{0.49\textwidth}
            \centering
            \begin{table}[H]
                \centering
                \captionsetup{type=table}
                \begin{tabular}{l|l|l}
                    \multicolumn{1}{c|}{$P$}&\multicolumn{1}{c|}{$P$}&
                    \multicolumn{1}{c}{$P\Rightarrow{Q}$}\\
                    \hline
                    False&False&True\\
                    False&True&True\\
                    True&False&False\\
                    True&True&True
                \end{tabular}
                \caption{Truth Table for Implication}
                \label{tab:Truth_Table_Implication}
            \end{table}
        \end{minipage}\hfill
        \begin{minipage}[b]{0.49\textwidth}
            \centering
            \begin{table}[H]
                \centering
                \captionsetup{type=table}
                \begin{tabular}{c|c|c}
                    $P$&$Q$&$P\Rightarrow{Q}$\\
                    \hline
                    0&0&1\\
                    0&1&1\\
                    1&0&0\\
                    1&1&1
                \end{tabular}
                \caption{Alternate Table for Implication}
                \label{tab:Alternate_Truth_Table_Implication}
            \end{table}
        \end{minipage}
        \par\vspace{2.5ex}
        It is often easier and more useful to write these tables using zeros and
        ones. For one this hints at a means for computers to be able to
        understand and manipulate logical statements and proofs, but it is also
        less cumbersome and allows us to systematically run through the
        possibilities. That is, we start with all propositions set to 0 and then
        flick the right-most one to 1, then the second right-most, and so on.
        The symbol 0 denotes false, and 1 represents truth. This alternative
        truth table for implication is shown in
        Tab.~\ref{tab:Alternate_Truth_Table_Implication}.
        This shows that the only possible way for $P\Rightarrow{Q}$ to be false
        is if $P$ is true, yet $Q$ is false. This may be strange according to
        everyday language and there is a common tendency to confuse the order of
        implication. We spell this out in an example.
        \begin{example}
            Consider the proposition $P=\textit{I am late for work}$ together
            with $Q=\textit{I will be fired}$, and let's exhaust the four
            possible scenarios of if $P$, then $Q$. That is, we consider the
            claim \textit{if I am late fo work, then I will be fired}. Suppose
            I was not late for work, and I was not fired. Is $P\Rightarrow{Q}$
            true? Well, the criterion for $P$ was not satisfied and hence the
            statement is \textit{not} false, and so we claim it is true. Next, I
            was not late for work yet I was still fired (harsh). Again, the
            criterion for $P$ was not satisfied and so the claim is not false,
            and hence we accept it is true. Third, I was late for work and I was
            not fired (nice boss). Here we see that $P\Rightarrow{Q}$ is
            \textit{false}. The criterion for $P$ was satisfied, yet $Q$ was not
            and therefore $P\Rightarrow{Q}$ is a false statement. Lastly, I was
            late for work and I was fired. This is perhaps the easiest one to
            handle since it is verbatim what one thinks of when they hear
            \textit{if, then} claims. Here, $P\Rightarrow{Q}$ is true.
        \end{example}
        Given this definition of implication the axiom of \textit{modus ponens},
        short for \textit{modus ponendo ponens} which in Latin means
        \textit{mode that by affirmings affirms}, seems redundantly obvious.
        Nevertheless, we must write it out. First, we must define
        \textit{axiom} and \textit{proof}. Proof is another primitive, but axiom
        is not. We may define axiom in terms of previous notions.
        \begin{fdefinition}{Proof}{Proof}
            A \gls{proof} of a \gls{proposition} is a valid argument that
            rigorously affirms the proposition.
        \end{fdefinition}
        We've yet to see what a valid argument is. These are formed by combining
        previously proved propositions, together with definitions, to arrive at
        a conclusion using the allowed rules of inference. As of yet we have not
        claimed what rules of inference we will accept, nor do we have any
        proposition to build from. Both of these issues are addressed by axioms.
        \begin{fdefinition}{Axiom}{Axiom}
            An \gls{axiom} is a \gls{proposition} that is affirmed to be true
            without \gls{proof}.
        \end{fdefinition}
        In a good system the axioms should be intuitively obvious and not too
        controversial. In \textit{the Elements}\index{Elements, The (Euclid)}
        there are 10 axioms (five are called postulates and the others common
        notions), most of which are not too awe inspiring. The five postulates
        are phrased as follows:
        \begin{center}
            \begin{enumerate}
                \item \textit{To draw a straight line from any point}
                      \textit{to any point.}
                \item \textit{To produce a finite straight line continuous in a}
                      \textit{straight line.}
                \item \textit{To describe a circle with any centre and}
                      \textit{distance.}
                \item \textit{That all right angles are equal to one another.}
                \item \textit{That if a straight line falling on two straight}
                      \textit{lines make the interior angles on the same side}
                      \textit{less than two right angles, the straight lines,}
                      \textit{if produced indefinitely, meet on that side on}
                      \textit{which are the angles less than two right angles.}
            \end{enumerate}
            \hfill\textit{The Elements,}\par
            \hfill\textit{Euclid of Alexandria, c. 300 B.C.E.}
        \end{center}
        Only the fifth postulate, known famously as
        \textit{Euclid's Fifth Axiom}\index{Axiom!Euclid's Fifth}, is
        non-obvious. These five constitute the study of
        \textit{Euclidean geometry}%
        \index{Geometry!Euclidean}\index{Euclidean Geometry}, the first four of
        which make up \textit{absolute geometry}\index{Geometry!Absolute}%
        \index{Absolute Geometry}. For almost two thousand years a handful of
        mathematicians attempting to prove that absolute geometry and Euclidean
        geometry are the same. That is, the fifth postulate can be proved from
        the other four. Many minds of antiquity such as Claudius Ptolemny%
        \index{Ptolemy, Claidus} (\textit{c.} 100-170 C.E.), Proclus
        Lycaeous\index{Lycaeous, Proclus} (\textit{c.} 412-485 C.E.), Omar
        Khayy\'{a}m\index{Khayy\'{a}m, Omar} (1050-1123 C.E.), and others
        made the attemps, but all merely introduced an alternative axiom either
        explicitly or implicity which was equivalent to Euclid's. Euclid himself
        was hesitant in his use of the postulate, only using it nearly 30
        theorems into book one when he needed it. So the question remains, can
        the fifth be proven from the first four? The answer is \textit{no}. We
        know this because there are \textit{models}\index{Model} of the first
        four where the fifth one fails.
        \par\hfill\par
        In the $18^{th}$ century the Swiss mathematician Johann Heinrich
        Lambert\index{Lambert, Johann Heinrich} (1728-1777 C.E.) made
        investigations into a quadrilateral where three of the angles are right.
        If one could prove the fourth angles must also be right, then with this
        Euclid's fifth could be proven (Omar Khayy\'{a}m did precisely this but
        claimed the fourth angle being right is self-evident). Lambert discarded
        the possibility of the fourth angle to be obtuse, but then made
        investigations into the acute case. He was never able to prove the acute
        case was impossible. This leads to another \textit{model} of absolute
        geometry, \textit{hyperbolic geometry}\index{Geometry!Hyperbolic} in
        which the fourth angle is allowed to be acute.
        \par\hfill\par
        The revelation that Euclid's fifth is not provable from the other four
        is made more intuitive by the work of John
        Playfair\index{Playfair, John} (1748-1819 C.E.) who described the
        following axiom, now known as \textit{Playfair's axiom}:
        \begin{center}
            \textit{In a plane, given a line and a point not on it, at most one}
            \textit{line parallel to the given line can be drawn through the}
            \textit{point}
            \par
            \hfill\textit{Elements of Geometry}\par
            \hfill\textit{John Playfair, 1795 C.E.}
        \end{center}
        In combination with the first four axioms this is equivalent to Euclid's
        fifth. The first four can prove parallel lines exist, and hence we
        cannot claim refute this, but we may change Playfair's axiom to state
        that \textit{many} such lines exist, again giving us the model of
        hyperbolic geometry. Perhaps we wish to claim there are no parallel
        lines, in which case we must do away with Euclid's second axiom. This
        leads to \textit{elliptic geometry}\index{Geometry!Elliptic}%
        \index{Elliptic Geometry}, which is the geometry on a sphere. This model
        shows that Euclid's second cannot be proved from the others as well.
        \begin{faxiom}{Modus Ponens}{Modus Ponens}
            If $P$ and $Q$ are propositions, if $P\Rightarrow{Q}$, and if $P$ is
            true, then $Q$ is true.\index{Axiom!of Modus Ponens}
        \end{faxiom}
        \begin{example}
            Let $P$ be the proposition \textit{bears are mammals} and $Q$ be the
            proposition \textit{mammals are animals}. If $P$, then $Q$ reads
            if bears are mammals, then bears are animals. Since bears are
            mammals, we infer that bears are animals.
        \end{example}
        The next two commonly accepted rules of inference, known as
        \textit{modus tollens}\index{Axiom!of Modus Tollens} and
        \textit{contraposition}\index{Axiom!of Contraposition} are widely used
        in the mathematical world and relate to a statements contrapositive.
        The contrapositive is related to the \textit{negation} of a proposition,
        and so we define this now.
        \begin{fdefinition}{Negation}{Negation}
            The negation of a proposition $P$ is the proposition \textit{not}
            $P$, denoted $\neg{P}$.
        \end{fdefinition}
        Negation is a \textit{unary} operation acting on a single variable. The
        truth table is short.
        \begin{table}[H]
            \centering
            \captionsetup{type=table}
            \begin{tabular}{c|c}
                $P$&$\neg{P}$\\
                \hline
                0&1\\
                1&0
            \end{tabular}
            \caption{Truth Table for Negation}
            \label{tab:Truth_Table_Negation}
        \end{table}
        That is, negaiton takes true to false and maps false to true. Negation
        and implication form our two primitive logical operations, and the other
        familiar terms (disjunction, conjunction, equivalence) can be expressed
        using these two. With negation defined, we now present a
        \textit{logical fallacy}\index{Logical Fallacy}, a form of reasoning
        that is invalid and leads to contradiction. This is the fallacy of
        \textit{affirming the consequent}%
        \index{Logical Fallacy!Affirming the Consequent}.
        \begin{lexample}{Affirming the Consequent}{Affirming_the_Consequent}
            The fallacy of affirming the consequent is also known as the
            inverse fallacy\index{Logical Fallacy!Inverse Fallacy}, and in
            mathematics it is called the \textit{converse}
            fallacy\index{Logical Fallacy!of the Converse}.
            \textit{Modus ponens} tells us that if $P$ and $Q$ are propositions,
            if $P\Rightarrow{Q}$, and if $P$ is true, then $Q$ is true. The
            \textit{converse}\index{Converse} of the statement
            \textit{if P, then Q} is the sentence \textit{if Q, then P}. The
            validity of $P\Rightarrow{Q}$ does \textbf{not} verify the converse,
            much to the dismay of mathematicians. Theorems are often held in
            higher regard if they express the \textit{equivalence} of two
            propositions: $P\Rightarrow{Q}$ and $Q\Rightarrow{P}$. That is, both
            the statement and its converse are true. There are everyday and
            mathematical examples showing that this line of reasoning if faulty.
            Previously we discussed the classification of bears:
            \textit{if an animal is a bear, then it is a mammal}. The converse
            states if an animal is a mammal, then it is a bear. Humans are a
            counterexample to this claim since humans are mammals but are not
            bears. In the mathematical world we can consider the proposition
            \textit{if n is an odd integer, then n is not 2}. The converse
            states that if $n$ is not 2, then $n$ is not an odd integer. But
            1 is not 2, yet 1 is indeed an odd integer.
        \end{lexample}
        That affirming the consequent is invalid can be see from truth tables
        (see Tab.~\ref{tab:Truth_Table_Converse}).
        \begin{table}[H]
            \centering
            \captionsetup{type=table}
            \begin{tabular}{c|c|c|c}
                $P$&$Q$&$P\Rightarrow{Q}$&$Q\Rightarrow{P}$\\
                \hline
                0&0&1&1\\
                0&1&1&0\\
                1&0&0&1\\
                1&1&1&1
            \end{tabular}
            \caption{Truth Table for the Converse}
            \label{tab:Truth_Table_Converse}
        \end{table}
        Since the columns for $P\Rightarrow{Q}$ and $Q\Rightarrow{P}$ are
        different, we see that these are different propositions. Negation
        further allows us to define the
        \textit{contrapositive}\index{Contrapositive} of the proposition
        $P\Rightarrow{Q}$, which is the new proposition
        $\neg{Q}\Rightarrow\neg{P}$. As it turns out, this is not new
        proposition at all and is equivalent to $P\Rightarrow{Q}$. Note
        $P\Rightarrow{Q}$ is false only when $P$ is true, yet $Q$ is false.
        Similarly, $\neg{Q}\Rightarrow\neg{P}$ is false only if $\neg{Q}$ is
        true and $\neg{P}$ is false. But if $\neg{Q}$ is true, then $Q$ is
        false (Def.~\ref{def:Negation}) and if $\neg{P}$ is false, then $P$ is
        true (Def.~\ref{def:Negation}). Thus $\neg{Q}\Rightarrow\neg{P}$ is only
        false when $P$ is true and $Q$ is false. We can further examine this via
        truth tables (see Tab.~\ref{tab:Truth_Table_for_Contrapositive}).
        \begin{table}[H]
            \centering
            \captionsetup{type=table}
            \begin{tabular}{c|c|c|c|c|c}
                $P$&$Q$&$\neg{P}$&$\neg{Q}$&$P\Rightarrow{Q}$
                    &$\neg{Q}\Rightarrow\neg{P}$\\
                \hline
                0&0&1&1&1&1\\
                0&1&1&0&1&1\\
                1&0&0&1&0&0\\
                1&1&0&0&1&1
            \end{tabular}
            \caption{Truth Table for the Contrapositive}
            \label{tab:Truth_Table_for_Contrapositive}
        \end{table}
        \begin{example}
            Suppose $a$ and $b$ are variables representing real numbers and $P$
            is the proposition $a<1/2$ and $b<1/2$, and let $Q$ be the
            proposition $a+b<1$. What is the contrapositive of
            $P\Rightarrow{Q}$? This would be $\neg{Q}\Rightarrow\neg{P}$, where
            $\neg{Q}$ is the negation of $Q$ which reads $a+b\geq{1}$.
            Similarly, $\neg{P}$ is the statement $a\geq{1}/2$ or $b\geq{1}/2$.
            Thus, the contrapositive says that if $a+b\geq{1}$, then either
            $a\geq{1}/2$ or $b\geq{1}/2$ (or both). While the contrapositive of
            a statement is always equivalent to the original statement, the
            converse need not be. Indeed, this statement is true (once one knows
            the order structure of real numbers), but the converse is not. The
            converse states that if $a+b<1$, then $a<1/2$ and $b<1/2$, but
            letting $a=2$ and $b=\minus{3}$ contradicts this claim.
        \end{example}
        The axiom of \textit{modus tollens} states that if
        $P\Rightarrow{Q}$, and if $\neg{Q}$ is true, then $\neg{P}$ is true. We
        are not directly adopting this axiom since it is provable from the
        axiom of \textit{modus ponens} if one accepts the standard axioms of set
        theory. Indeed, \textit{modus tollens} is implied by the
        \textit{law of the excluded middle}\index{Law of the Excluded Middle},
        which is in turn implied by the
        \textit{axiom of choice}\index{Axiom!of Choice}, two topics that will be
        discussed in Chapt.~\ref{chapt:Zermelo_Fraenkel_Set_Theory}. Similar to
        \textit{modus tollens}, the axiom of contraposition states that
        $P\Rightarrow{Q}$ is equivalent to $\neg{Q}\rightarrow\neg{P}$. That is,
        if the statement $P\Rightarrow{Q}$ is true, then the contrapositive
        $\neg{Q}\rightarrow\neg{P}$ is also true.
        \begin{example}
            Consider once again the description of Socrates. We start with the
            proposition \textit{all humans are mammals}. If Socrates is a human,
            then Socrates is a mammal. Therefore if Socrates is \textit{not} a
            mammal, then Socrates is \textit{not} a human. We could also say
            that all bears are mammals, and hence if you come across an animal
            that is \textit{not} a mammal, then this animal is \textit{not} a
            bear. Reasoning like this follows from contraposition.
        \end{example}
        Before moving to connectives Hilbert systems it is worth while
        mentioning a few \textit{invalid} forms of reasoning. We do not include
        these as rules of inference since they lead to contradiction, although
        students often make these fallacious mistakes when exploring proofs for
        the first time. We've discussed \textit{affirming the consequent} and
        now identify \textit{denying the antecedent}%
        \index{Logical Fallacy!Denying the Antecedent}, which is another type of
        converse fallacy and is the most common to make. Denying the antecedent
        goes as follows, if $P$ implies $Q$, and \textit{not} $P$, then
        \textit{not} $Q$. This is false, and we will provide plenty of examples
        to indicate this.
        \begin{example}
            Harking back to a previous example, consider the statement
            \textit{if I am late to work, then I will be fired}. Now suppose I
            was not late to work. Does this mean I was not fired? No! Perhaps I
            was lazy on the job, or uttered too many vulgarities (I do have a
            sailor's mouth). Knowing that I was not late tells us nothing about
            whether or not I was fired. It is only if I \textit{was} late that
            we can then appropriately apply \textit{modus ponens} and conclude
            that I was fired.
        \end{example}
        \begin{example}
            Consider the proposition
            \textit{If n is an odd integer, then n is not 2}. Now suppose we are
            told than $n$ is \textit{not} an odd integer. Can we conclude that
            $n$ is 2? No! It may be 4, or 6, or any other even integer.
        \end{example}
        We can consider $P\Rightarrow{Q}$ and $\neg{P}\Rightarrow\neg{Q}$ by
        means of truth table. The statement $\neg{P}\Rightarrow\neg{Q}$ is
        called the \textit{inverse} of $P\Rightarrow{Q}$.
        \begin{table}[H]
            \centering
            \captionsetup{type=table}
            \begin{tabular}{c|c|c|c|c|c}
                $P$&$Q$&$\neg{P}$&$\neg{Q}$&$P\Rightarrow{Q}$
                                           &$\neg{P}\Rightarrow\neg{Q}$\\
                \hline
                0&0&1&1&1&1\\
                0&1&1&0&1&0\\
                1&0&0&1&0&1\\
                1&1&0&0&1&1
            \end{tabular}
            \caption{Truth Table for the Inverse}
            \label{tab:Truth_Table_Inverse}
        \end{table}
        The next fallacy is known as the
        \textit{fallacy of the undistributed middle}%
        \index{Logical Fallacy!of the Undistributed Middle}. This is the first
        argument that takes three propositions. It falsely concludes that
        if $P_{1}\Rightarrow{Q}$, and if $P_{2}\Rightarrow{Q}$, then
        $P_{1}\Rightarrow{P}_{2}$. This is false as the truth table below
        demonstates.
        \begin{table}[H]
            \centering
            \captionsetup{type=table}
            \begin{tabular}{c|c|c|c|c|c}
                $P_{1}$&$P_{2}$&$Q$&$P_{1}\Rightarrow{Q}$&$P_{2}\Rightarrow{Q}$
                                   &$P_{1}\Rightarrow{P}_{2}$\\
                \hline
                0&0&0&1&1&1\\
                0&0&1&1&1&1\\
                0&1&0&1&0&1\\
                0&1&1&1&1&1\\
                1&0&0&0&1&0\\
                1&0&1&1&1&0\\
                1&1&0&0&0&1\\
                1&1&1&1&1&1
            \end{tabular}
            \caption{Fallacy of the Undistributed Middle}
            \label{tab:Fallcy_of_Undistributed_Middle}
        \end{table}
        There's a column on this table where $P_{1}\Rightarrow{Q}$ is true, and
        $P_{2}\Rightarrow{Q}$ is true, yet $P_{1}\Rightarrow{P}_{2}$ is false.
        Namely, choose $P_{1}=True$, $P_{2}=False$, and $Q=True$.
        \begin{example}
            Consider the claim \textit{all mathematicians love geometry} which
            one could only hope is true. Consider also
            \textit{all physicists love geometry}. Given that these two
            statements are true, it would be wrong to conclude that all
            mathematicians are physicists.
        \end{example}
        \begin{example}
            We can consider a mathematical proposition. If $n$ is divisible by
            2, then $n$ is even, and if $n$ is divisible by 4, then $n$ is even.
            We cannot conclude that if $n$ is divisible by 2, then $n$ is
            divisible by 4 since the number 6 serves as a counterexample. That
            is, $6=2\cdot{3}$ and hence 6 is divisible by 2, but it is not
            divisible by 4.
        \end{example}
        Lastly, we discuss the difference between a \textit{valid} argument and
        a \textit{sound} one. A valid argument is one that proves a claim from
        hypothesized propositions by correctly using the rules of inference. A
        sound argument is a valid argument of which the hypothesized
        propositions are true.
        \begin{example}
            Consider the proposition \textit{all birds can fly}. We invoke
            \textit{modus ponens} and arrive at the following absurdity:
            \begin{subequations}
                \begin{align}
                    &\text{All birds can fly}.\\
                    &\text{Penguins are birds}.\\
                    &\text{Therefore, penguins can fly}.
                \end{align}
            \end{subequations}
            It is currently believed that penguins are incapable of flight no
            matter how hard they may try, and hence we have used our rules of
            inference correctly, but we've arrived at a false claim. That is,
            our argument is valid, but it cannot be sound. And indeed, the flaw
            is that the proposition \textit{all birds can fly} is false since
            penguins serve as a counterexample.
        \end{example}
        Another example is known as the \textit{masked-man fallacy}%
        \index{Logical Fallacy!Masked-Man Fallacy}.
        \begin{lexample}{Masked-Man Fallacy}{Masked_Man_Fallacy}
            Suppose I have a friend \textit{Bob}. Since he is my friend, the
            proposition \textit{I know Bob} is true. Suppose further that there
            is a man wearing a mask. Since he is wearing a mask, I do not know
            who he is and hence the proposition
            \textit{I do not know the masked man} is true. We obtain the
            following:
            \begin{subequations}
                \begin{align}
                    &\text{I know Bob}.\\
                    &\text{I do not know that masked man}.\\
                    &\text{Therefore, Bob is not the masked man}.
                \end{align}
            \end{subequations}
            Our argument is valid and follows from \textit{modus tollens}. That
            is, we have the proposition
            \textit{if the man is Bob, then I know him}. Hence, if I don't know
            the man, then it is not Bob. However this argument is not sound
            since it is perfectly possible for Bob to be wearing the mask. The
            flaw comes from the proposition
            \textit{I do not know the masked man}. In truth, it may be possible
            that I do. This knowledge is not available to me and cannot be used
            or refuted in the argument.
        \end{lexample}