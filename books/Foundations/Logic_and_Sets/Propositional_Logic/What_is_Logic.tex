\section{What is Logic?}
    It may seem strange to start a work on mathematics with an entire
    development of logic, as one might think such conversations should reside
    in philosophy. And indeed, much of classical logic was developed by
    philosophers rather than mathematicians. Many problems, which we will
    discuss in Chapt.~\ref{chapt:Zermelo_Fraenkel_Set_Theory}, arose in the
    early 1900s with the very core of mathematics. Arguments once considered
    sound were shattered, and contradictions were discovered. On the other hand,
    other methods of proof that are very intuitive were shown to be able to
    prove the existence of non-intuitive and almost impossible objects. Thus our
    goal is to discuss what forms of reasoning we shall consider valid, provide
    examples, and propose the \textit{axioms} of mathematical logic.
    \begin{example}
        \label{ex:Logic_IVP}%
        A student of calculus has likely heard of the intermediate value
        theorem\index{Theorem!Intermediate Value}. Fear not those who haven't,
        we shall draw pictures. Given a \textit{continuous} function $f$ of real
        numbers (roughly speaking, a curve one can draw from left to right
        without lifting up their pencil), if 0 evaluates to a negative number
        and 1 to a positive number, then there is some point in the middle which
        evaluates to zero. The proof is quite simple: We first look at what
        happens to the point $\frac{1}{2}$. If $f$ is zero here we are done,
        otherwise if $f$ is positive then we may suspect there's a value in
        between 0 and $\frac{1}{2}$ which evaluates to zero and if $f$ is
        negative at this point, then there's probably a zero between
        $\frac{1}{2}$ and 1. In either case we divide the range of possibilities
        in half and see what happens at $\frac{1}{4}$ in the first case and
        $\frac{3}{4}$ in the latter. We continue \textit{inductively} (whatever
        this means) and obtain a \textit{sequence} of real numbers which we then
        show \textit{converge} to some real number between 0 and 1. We invoke
        continuity showing that $f$ evaluates to zero at this limit point, and
        we are done (see Fig.~\ref{fig:Sketch_of_IVP}).
    \end{example}
    \begin{figure}[H]
        \centering
        \captionsetup{type=figure}
        \includegraphics{images/Intermediate_Value_Theorem_Sketch.pdf}
        \caption{Sketch of the Intermediate Value Theorem}
        \label{fig:Sketch_of_IVP}
    \end{figure}
    We can see why this may work. After a few iterations we've narrowed down a
    zero point to a very small range between $x_{3}$ and $x_{5}$, and this is a
    nice algorithm that we can tell a computer to execute to arbitrary
    precision (this is known as the \textit{bisection method}%
    \index{Root Finding!Bisection}\index{Bisection Method} of root finding), but
    what went into the proof? That is, if we were to phrase this with absolute
    precision, what definitions, assumptions, and previous theorems are we
    relying on? For one, the existence of \textit{real number}, a notion of
    \textit{continuity}, and the definition of a \textit{sequence}. Our
    exposition of logic is to make clear what is required for valid proofs.
    \begin{example}
        \label{ex:Logic_Gauss_Sum}%
        It has been alleged, though as always one should exhibit skepticism,
        that at a very early age the great mathematician Carl Friedrich
        Gauss\index{Gauss, Carl Friedrich} (1777-1855 C.E.) demonstrated
        to his teacher that $1+2+\dots+99+100=5050$. It's also been claimed that
        he had revelations about the normal distribution while counting the
        number of steps on his way to school, and that he corrected his fathers
        mathematical calculations at the age of three. Alas, mathematicians are
        a few tales away from forming the religion of Gauss. Nevertheless, let's
        see what the argument is. We rearrange this sum as follows:
        \begin{table}[H]
            \centering
            \captionsetup{type=table}
            \begin{tabular}{ccccccccccc}
                &&$1$&$+$&$2$&$+$&$\cdots$&$+$&$99$&$+$&$100$\\
                \hline\\
                $=$&&$1$&$+$&$2$&$+$&$\cdots$&$+$&$49$&$+$&$50$\\
                &$+$&$100$&$+$&$99$&$+$&$\cdots$&$+$&$52$&$+$&$51$\\
                \hline\\
                $=$&&$101$&$+$&$101$&$+$&$\cdots$&$+$&$101$&$+$&$101$\\
                \hline\\
                $=$&&$50$&$\times$&$101$\\
                \hline\\
                $=$&&$5050$
            \end{tabular}
            \caption{Gauss' Sum of 1 to 100}
        \end{table}
        While this seems to be a concrete proof of the claim, is it valid? Can
        we generalize it? What assumptions about integers and arithmetic are we
        making? We'll explore the axioms of arithmetic and try and resolve these
        questions.
    \end{example}
    \begin{example}
        \label{ex:Logic_Euler_Sum}%
        Gauss is often called the prince of Mathematics, and the king is the
        great Leonhard Euler\index{Euler, Leonhard} (1707-1783 C.E.). He too
        studied sums and considered the bizarre series $1+2+3+4+\dots$ arriving
        at the answer $\minus\frac{1}{12}$. It should be noted that while this
        sum was studied in the $18^{th}$ century, there seems to be ambiguity as
        to whether or not Euler earns the credit on this one. Anyways, let's
        see how we can arrive at this answer. First, we consider
        Grandi's series\index{Grandi's series} which was studied by
        Luigi Guido Grandi\index{Grandi, Luigi Guido} (1671-1742 C.E.). We have
        (Using $G$ for Grandi):
        \begin{equation}
            G=1-1+1-1+1-1+\cdots
        \end{equation}
        Is there a meaningful number to assign to $G$? Well, suppose there is
        and we write:
        \begin{table}[H]
            \centering
            \captionsetup{type=table}
            \begin{tabular}{ccccccccccccc}
                $1-G$&$=$&$1$&$-$&$\Big[$&$1$&$-$&$1$&$+$&$1$&$-$&$\cdots$
                    &$\Big]$\\[1ex]
                \hline\\
                    &$=$&&&&$1$&$-$&$1$&$+$&$1$&$-$&$\cdots$\\[1ex]
                \hline\\
                &$=$&&&&$G$
            \end{tabular}
            \caption{Grandi's Series $1-1+1-1+\cdots$}
        \end{table}
        And hence we have $1-G=G$. Rearranging we get $2S=1$ and therefore
        $G=\frac{1}{2}$. Next, we consider the series:
        \begin{equation}
            T=1-2+3-4+5-6+\cdots
        \end{equation}
        Again, supposing $T$ has some legal value, we apply some manipulations
        and obtain:
        \begin{table}[H]
            \centering
            \captionsetup{type=table}
            \begin{tabular}{ccccccccccc}
                $2T$&$=$&$1$&$-$&$2$&$+$&$3$&$-$&$4$&$+$&$\cdots$\\
                    &   &   &$+$&$1$&$-$&$2$&$+$&$3$&$-$&$\cdots$\\
                \hline\\
                    &$=$&$1$&$-$&$1$&$+$&$1$&$-$&$1$&$+$&$\cdots$\\
                \hline\\
                &$=$&$\frac{1}{2}$
            \end{tabular}
            \caption{The Sum of $1-2+3-4+\cdots$}
        \end{table}
        And hence $T=\frac{1}{4}$. We now turn to Euler's sum of
        $1+2+3+4+\dots$. We'll give this the symbol $S$:
        \begin{equation}
            S=1+2+3+4+\dots
        \end{equation}
        We now subtract from $S$ the series $T$ which we computed above. We get:
        \begin{table}[H]
            \centering
            \captionsetup{type=table}
            \begin{tabular}{ccccccccccccc}
                $S-T$&$=$&&       &$1$&$+$&$2$&$+$&$3$&$+$&$4$&$\cdots$\\
                     &&$-$&$\Big[$&$1$&$-$&$2$&$+$&$3$&$-$&$4$&$\cdots$&$\Big]$
                \\[1ex]
                \hline\\
                     &$=$&&&$0$&$+$&$4$&$+$&$0$&$+$&$8$&$\cdots$\\[1ex]
                \hline\\
                &$=$&&$4\Big[$&$1$&$+$&$2$&$+$&$3$&$+$&$4$&$\cdots$&$\Big]$
                    \\[1ex]
                \hline\\
                &$=$&&$4S$
            \end{tabular}
            \caption{The Sum of $1+2+3+4+\cdots$}
        \end{table}
        so now we have a nice formula for $S$:
        \begin{equation}
            S-T=4S
        \end{equation}
        But $T=\frac{1}{4}$, so we have $3S=\minus\frac{1}{4}$ and therefore
        $S=\minus\frac{1}{12}$.
    \end{example}
    We now contemplate the previous examples and ask which are valid. It is
    tempting to say that Ex.~\ref{ex:Logic_IVP} and Ex.~\ref{ex:Logic_Gauss_Sum}
    were presented with accurate proofs, whereas Ex.~\ref{ex:Logic_Euler_Sum} is
    garbage, but why? We ``proved'' Gauss' and Euler's sum in the same manner:
    Rearranged terms, used \textit{dot dot dot} notation to indicate some
    pattern, added things together in a convincing way, and then simplified. One
    might suggest that Gauss' proof involved a finite scheme and that if asked
    one could laboriously write out the numbers indicated by the dots, whereas
    Euler's sum is infinite. But if we were to perform Gauss' problem with a
    number so large that it would take longer than the age of the universe to
    write down, even with the aid of computer, would we reject his method of
    proof then? There is some solace, and we can show that the Euler sum is
    invalid if we accept that $1\ne{0}$. Consider the sum $1+1+1+1+\cdots$ which
    we shall denote $B$ for bad.
    \begin{equation}
        B=1+1+1+1+\cdots
    \end{equation}
    Using Grandi's series $G$, we subtract and obtain:
    \begin{table}[H]
        \centering
        \captionsetup{type=table}
        \begin{tabular}{ccccccccccccc}
            $B-G$&$=$&&       &$1$&$+$&$1$&$+$&$1$&$+$&$1$&$\cdots$\\
                 &&$-$&$\Big[$&$1$&$-$&$1$&$+$&$1$&$-$&$1$&$\cdots$&$\Big]$
            \\[1ex]
            \hline\\
                 &$=$&&&$0$&$+$&$2$&$+$&$0$&$+$&$2$&$\cdots$\\[1ex]
            \hline\\
            &$=$&&$2\Big[$&$1$&$+$&$1$&$+$&$1$&$+$&$1$&$\cdots$&$\Big]$
                \\[1ex]
            \hline\\
            &$=$&&$2B$
        \end{tabular}
        \caption{The Sum of $1+1+1+1+\cdots$}
    \end{table}
    And so we obtain $B-G=2B$, so $B=\minus{G}$. But we know Grandi's series is
    $G=\frac{1}{2}$, and hence $B=\minus\frac{1}{2}$. This isn't the only thing
    we could have done. For consider:
    \begin{equation}
        1+B=1+(1+1+1+1+\cdots)=1+1+1+1+\cdots=B
    \end{equation}
    And hence $1+B=B$, so $1=0$ which is a contradiction. For the rest of the
    chapter we wish to hone in on what we are to consider as valid mathematics.
    \subsection{Truth}
        Since the aim of mathematics is to prove the validity of mathematical
        statements, we should start with a definition of truth. We run into a
        wall instantly since this is essentially an impossible task. Any
        definition will be circular, and it is a theorem of
        Alfred Tarski\index{Tarski, Alfred} (1901-1983 C.E.) that if one has
        somehow defined arithmetic, then one cannot use arithmetic to define
        truth. That is, if we take upon the assumption of the existence of the
        natural numbers 0, 1, 2, $\dots$ with the familiar notion of addition
        $+$ (i.e. $1+1=2$ and other mathematical gems), then \textit{arithmetic}
        truth cannot be defined using this arithmetic. Let us consult the
        dictionary. The Oxford English dictionary defines truth to mean factual,
        Merriam-Webster states that truth is the body of real things, events,
        and facts, and Cambridge claims its the quality of being true. As
        hypothesized these definitions are circular and rely on other predefined
        terms like \textit{factual}. We propose the following work around: Truth
        is a primitive notion that needs no definition. We can then define false
        to mean \textit{not} true.
        \par\hfill\par
        Tarski's result came about in the 1930's when he tried to mathematically
        work out the \textit{liar's paradox}\index{Paradox!Liar's}%
        \index{Liar's Paradox}. Consider the following sentence:
        \begin{equation}
            \text{This sentence is false.}
        \end{equation}
        Similar statements have been considered throughout the ages, including
        the variant known as Epimenides' paradox\index{Paradox!Epimenides'}.
        Epimenides of Cnossos (\textit{c.} 600 B.C.E.), who was from Crete,
        proclaims ``All Cretans are liars.'' The question was considered again
        200 years later when Eubulides of Miletus (\textit{c.} 400 B.C.E.)
        considered the sentence ``I am lying.'' Further still in the Book of
        Psalms king David states that all men are liars. Needless to say, the
        paradox is quite old and well studied. Now we ask, is the statement
        \textit{true} or \textit{false} (Assuming such notions are defined)?
        Let's work through it, and suppose truth. If this sentence is false is
        true, then the sentence is false even though we just claimed it to be
        true. Hence, it must be false. But if this sentence is false is false,
        then the sentence is true, but we just showed it cannot be true. So,
        which one is it? There are two interpretations: The statement is
        \textit{neither} true nor false, and the sentence is \textit{both} true
        and false. Suppose we accept that the statement is neither true nor
        false. This only leads to another sentence in which we cannot make such
        a conclusion:
        \begin{equation}
            \text{This sentence is not true.}
        \end{equation}
        If this is neither true nor false, then it is not true, and hence true,
        bringing us back to the paradox. Now we claim is it both true and false,
        leading us to:
        \begin{equation}
            \text{The sentence is false and not true.}
        \end{equation}
        And hence there seems to be no workaround. The problem intensifies if we
        consider pairs of sentences:
        \twocolumneq{%
            \label{eqn:That_Sentence_Is_True}%
            \text{Statement \ref{eqn:That_Statement_Is_False} is true.}
        }
        {%
            \label{eqn:That_Statement_Is_False}%
            \text{Statement \ref{eqn:That_Sentence_Is_True} is false.}
        }
        and now we go round and round in an endless circle. As Alfred Tarski
        pointed out, the problem arises in languages in which statements are
        allowed to be self-referential. To see that this is indeed a self
        referencing claim, we can write $P$ for the proposition and we arrive at
        the equation:
        \begin{equation}
            P=P\text{ is false.}
        \end{equation}
        if we substitute $P$, we obtain:
        \begin{equation}
            P=(P\textit{ is false})\text{ is false.}
             =\big((P\text{ is false})\text{ is false}\big)\text{ is false.}
        \end{equation}
        While it may seem like this is an unnecessary discussion, the liar's
        paradox plays a role in mathematics. For one it motivates Tarski's
        theorem on the defineability of truth, and perhaps more famously it
        allowed Kurt G\"{o}del\index{G\"{o}del, Kurt} (1906-1978 C.E.) to prove
        his \textit{incompleteness theorems}, which really shook most of modern
        mathematics. Indeed, this theorem allegedly made Albert
        Einstein\index{Einstein, Albert} believe there could be no
        \textit{theory of everything}\index{Theory of Everything}, a theory of
        physics that could solve all problems great and small. For the sake of
        moving on to mathematics, we accept truth to be a primitive notion and
        acknowledge that the foundations of this concept are very shaky.
    \subsection{Sets}
        The main objects in mathematics are \textit{sets}. This development came
        about in the 1800's with figures like Georg Cantor\index{Cantor, Georg},
        Augustus De Morgan\index{De Morgan, Augustus}, and Bernard
        Bolzano\index{Bolzano, Bernard} making the first strides in the theory.
        The early history is very loose and intuitive, but the vagueness
        ultimately led to Russell's Paradox\index{Paradox!Russell's} which
        showed the naivity of set theory to be inconsistent. We'll discuss this
        in Chapt.~\ref{chapt:Zermelo_Fraenkel_Set_Theory} when we develop
        Zermelo-Fraenkel set theory, for now we just need a definition. Georg
        Cantor (1845-1918 C.E.) wrote in his 1895 work
        \textit{Beitr\"{a}ge zur Begr\"{u}ndung der transfiniten Mengenlehre}
        (Contributions in Support of Transfinite Set Theory), that
        \textit{a set is a gathering together in whole of definite distinct}
        \textit{objects of our perception or of our thought, which are called}
        \textit{the elements of the set}. Beautifully phrased, but circular. The
        word \textit{gathering} is not defined, nor is \textit{object}. Any
        attempt at defining these will lead to the same problem, and thus we
        find ourselves in need of introducing another primitive notion: The
        notion of set. We adopt the following definition.
        \begin{fdefinition}{Set}{Set}
            A \gls{set} is a collection of objects called the elements of the
            set.\index{Set}
        \end{fdefinition}
        This is the same problem as the circularity we pointed out in Cantor's
        definition arises here since neither \textit{collection} nor
        \textit{object} has been defined. To begin doing mathematics we need a
        \textit{thing}. Sets act as our thing. We know they exist, but we don't
        know how to define them all to well. Nevertheless, we can describe how
        they behave and how to obtain new sets from pre-existing ones.
        \begin{fnotation}{Element Notation}{Element_Notation}
            If $A$ is a \gls{set} and if $x$ is an element\index{Set!Element of}
            of $A$, then we denote this by writing
            \glslink{containmentsymb}{$x\in{A}$}. If $x$ is not an element
            of $A$, we write $x\notin{A}$.\index{Containment $\in$}
        \end{fnotation}
        We are not developing set theory just yet, since we have yet to build up
        logic. In the most elementary systems such as Peano arithmetic there is
        a notion of set, and hence we need to define this first. Pedagogically
        it is poor to proceed without examples, and we do so now.
        \begin{example}
            The first three letters of the Latin alphabet can be expressed in
            set notation as follows. If let let the symbol $A$ denote this set,
            we may write:
            \begin{equation}
                A=\{\,a,\,b,\,c\,\}
            \end{equation}
            If we let $B$ denote the first three positive integers, we obtain:
            \begin{equation}
                B=\{\,1,\,2,\,3\,\}
            \end{equation}
            Using element notation (Not.~\ref{not:Element_Notation}) we see that
            $1\in{B}$, but $4\notin{B}$. That is, $B$ contains the number 1 but
            does not contain the number 4. Similarly, $a\in{A}$ and
            $d\notin{A}$. The symbol $\in$ should read \textit{is in}, or
            \textit{is an element of}, or \textit{is contained inside of}. Thus
            $a\in{A}$ reads $a$ is an element of $A$, or simply $a$ is in $A$.
            The notation $b\in{A}$ also reads as $b$ is contained inside of $A$.
            The notation $\notin$ is the negation of this: \textit{not in} or
            \textit{not and element of}. Hence $4\notin{B}$ reads 4 is not an
            element of $B$.
        \end{example}
        \begin{example}
            \label{ex:Everything_is_a_Set}%
            In the set theory that we will be working with, Zermelo-Fraenkel set
            theory, \textit{everything} is a set. This will be explained later,
            but we quite literally mean everything. The integers will be defined
            via John von Neumann's\index{von Neumann, John} construction. We
            start with the empty set $\emptyset$ which is the set that contains
            nothing, often denoted $\emptyset=\{\,\}$, and this will be our
            zero. We proceed and define $1=\{\emptyset\}$, $2=\{0,1\}$,
            $3=\{0,1,2\}$, and so on.
        \end{example}
        Elaborating on the discussion in Ex.~\ref{ex:Everything_is_a_Set}, there
        are other theories for the foundations of mathematics that allow other
        primitive notions such as classes and universes. Some of these theories
        are extremely weak (cannot prove much) but very safe (there is likely no
        contradiction), whereas some are very user friendly but almost certainly
        fallacious. Peano's axioms are an example of a weak system of which the
        axioms are so basic and obvious that no one is likely to ever find a
        contradiction, but they cannot assert the existence of negative
        integers, let alone the reals. On the other hand, any theory that
        allows one to say the \textit{collection} of all sets whether the
        collection is a class, or a universe, or whatever, is one that should be
        treated with skepticism. The theory of Zermelo and Fraenkel is a healthy
        middle ground. Strong enough to do most mathematics, and no
        contradiction found yet, though much of the $20^{th}$ century was spent
        searching to no avail.
    \subsection{Predicates and Propositions}
        Propositions and predicates will be two more of our primitive notions
        which we will vaguely define, but mostly rely on intuition.
        \begin{fdefinition}{Predicate}{Predicate}
            A predicate $P$ on a collection of variables is a sentence such that
            for any valid input one may state that the sentence is either true
            or false. That is, either $P$ is true or $P$ is false.
        \end{fdefinition}
        Predicates are the main tool used in set theory for defining and
        building new sets via the \textit{axiom schema of specification}%
        \index{Axiom!Schema of Specification}. Many describe predicates as
        \textit{functions} from a set $A$ to the Boolean-valued set
        $\{\text{True},\,\text{False}\}$ and this is a fine way of doing this
        provided the notion function has been defined. Many take function as
        another primitive, and thus one has the following decision to make: Do
        we accept \textit{predicate} as a primitive, or \textit{function}? We'll
        adopt predicate and later \textit{define} functions using elementary
        notions from the axioms of set theory.
        \begin{example}
            Let $A$ be the set $A=\{1,2,3\}$ and let $P$ be the predicate
            \textit{x is greater than 2}. The set of values in $A$ that satisfy
            this claim is $B=\{3\}$. If we let $Q$ represent
            \textit{x is negative}, then there are no elements in $A$ that
            satify $Q$ and hence the resulting set is the empty set $\emptyset$.
        \end{example}
        Moving on to propositions, we first express the na\"{i}ve Aristotelian
        definition put forward by Aristotle\index{Aristotle} (384-322 B.C.E.).
        \begin{equation}
            \text{A proposition is a sentence which affirms or denies a }
            \text{predicate.}
        \end{equation}
        The classic example is the so-called
        \textit{Socrates syllogism}\index{Socrates!Syllogism of}.
        \begin{example}
            We wish to conclude that the ancient greek philosopher
            Socrates\index{Socrates} was mortal. We start with the following
            proposition: \textit{All men are mortal}. We assert this sentence is
            true and we may be used later in the proofs of other claims. Second,
            we state \textit{Socrates is a man}. This is another proposition, a
            statement that we accept as true. To conclude Socrates is mortal we
            need some \textit{rule of inference} that allows us to tie these two
            propositions together. One such rule, known as
            \textit{modus ponens}\index{Modus Ponens}%
            \index{Axiom!of Modus Ponens} does exactly what we need. It states
            that if $P$ and $Q$ are propositions, if $P$ implies $Q$, and if $P$
            is true, then $Q$ is true. Using this, since all men are mortal, and
            since Socrates is a man, we conclude that Socrates is mortal.
        \end{example}
        An easier proof that Socrates is mortal goes as follows: He's dead.
        Nevertheless, we are starting to see what is needed to construct valid
        proofs.
        \begin{fdefinition}{Proposition}{Proposition}
            A proposition is a predicate evaluated at a particular input of
            variables.
        \end{fdefinition}
        \begin{example}
            Let's use the previous example of Socrates to motivate what we mean.
            Let $P$ be the predicate \textit{x is a man}. If we input Socrates
            we obtain $P(\text{Socrates})=\text{True}$. If we input Hatshepsut,
            we get $P(\text{Hatshepsut})=\text{False}$. This is how we
            distinguish a predicate from a proposition.
        \end{example}
    \subsection{Rules of Inference}
        Given a collection of propositions we often wish to derive new ones.
        Indeed, that is the entirety of mathematics: Proving new theorems. We
        must precisely state what rules we accept as valid and then attempt to
        stay consistent with these rules. The first we have already discussed,
        \textit{modus ponens}. This rule applies to \textit{implications}, one
        of the two primitives we adopt in our lanuage of deducing new claims,
        the second being negation which we will discuss soon enough.
        \begin{fdefinition}{Implication}{Implication}
            An implication on a proposition $Q$ by a proposition $P$ is the
            sentence that if $P$, then $Q$. We denote this $P\Rightarrow{Q}$.
        \end{fdefinition}
        The proposition $P$ is called the \textit{hypothesis}, and $Q$ is the
        \textit{conclusion}. We can describe implication via
        \textit{truth tables}. Truth tables for a finite collection of
        propositions exhaust all possible combinations of True and False, and
        then apply these combinations to the logical question at hand.
        Implication is depicted in Tab.~\ref{tab:Truth_Table_Implication}. Given
        $n$ propositions there are $2^{n}$ possible scenarios and so these truth
        tables get very big very fast. We can see that there are $2^{n}$
        possibilities since that are $n$ choices to be made from 0 and 1, so
        there are $2\cdot{2}\cdots{2}$ possibilities with $n$ 2's.
        \par\hfill\par
        It is often easier and more useful to write these tables using zeros and
        ones. For one this hints at a means for computers to be able to
        understand and manipulate logical statements and proofs, but it is also
        less cumbersome and allows us to systematically run through the
        possibilities. That is, we start with all propositions set to 0 and then
        flick the right-most one to 1, then the second right-most, and so on.
        The symbol 0 denotes false, and 1 represents truth. This alternative
        truth table for implication is shown in
        Tab.~\ref{tab:Alternate_Truth_Table_Implication}.
        \par
        \begin{minipage}[b]{0.49\textwidth}
            \centering
            \begin{table}[H]
                \centering
                \captionsetup{type=table}
                \begin{tabular}{l|l|l}
                    \multicolumn{1}{c|}{$P$}&\multicolumn{1}{c|}{$P$}&
                    \multicolumn{1}{c}{$P\Rightarrow{Q}$}\\
                    \hline
                    False&False&True\\
                    False&True&True\\
                    True&False&False\\
                    True&True&True
                \end{tabular}
                \caption{Truth Table for Implication}
                \label{tab:Truth_Table_Implication}
            \end{table}
        \end{minipage}\hfill
        \begin{minipage}[b]{0.49\textwidth}
            \centering
            \begin{table}[H]
                \centering
                \captionsetup{type=table}
                \begin{tabular}{c|c|c}
                    $P$&$Q$&$P\Rightarrow{Q}$\\
                    \hline
                    0&0&1\\
                    0&1&1\\
                    1&0&0\\
                    1&1&1
                \end{tabular}
                \caption{Alternate Table for Implication}
                \label{tab:Alternate_Truth_Table_Implication}
            \end{table}
        \end{minipage}
        \par\vspace{2.5ex}
        This shows that the only possible way for $P\Rightarrow{Q}$ to be false
        is if $P$ is true, yet $Q$ is false. This may be strange according to
        everyday language and there is a common tendency to confuse the order of
        implication. We spell this out in an example.
        \begin{example}
            Consider the proposition $P=\textit{I am late for work}$ together
            with $Q=\textit{I will be fired}$, and let's exhaust the four
            possible scenarios of if $P$, then $Q$. That is, we consider the
            claim \textit{if I am late fo work, then I will be fired}. Suppose
            I was not late for work, and I was not fired. Is $P\Rightarrow{Q}$
            true? Well, the criterion for $P$ was not satisfied and hence the
            statement is \textit{not} false, and so we claim it is true. Next, I
            was not late for work yet I was still fired (harsh). Again, the
            criterion for $P$ was not satisfied and so the claim is not false,
            and hence we accept it is true. Third, I was late for work and I was
            not fired (nice boss). Here we see that $P\Rightarrow{Q}$ is
            \textit{false}. The criterion for $P$ was satisfied, yet $Q$ was not
            and therefore $P\Rightarrow{Q}$ is a false statement. Lastly, I was
            late for work and I was fired. This is perhaps the easiest one to
            handle since it is verbatim what one thinks of when they hear
            \textit{if, then} claims. Here, $P\Rightarrow{Q}$ is true.
        \end{example}
        Given this definition of implication the axiom of \textit{modus ponens},
        short for \textit{modus ponendo ponens} which in Latin means
        \textit{mode that by affirmings affirms}, seems redundantly obvious.
        Nevertheless, we write it out.
        \begin{faxiom}{Modus Ponens}{Modus Ponens}
            If $P$ and $Q$ are propositions, if $P\Rightarrow{Q}$, and if $P$ is
            true, then $Q$ is true.\index{Axiom!of Modus Ponens}
        \end{faxiom}
        \begin{example}
            Let $P$ be the proposition \textit{bears are mammals} and $Q$ be the
            proposition \textit{mammals are animals}. If $P$, then $Q$ reads
            if bears are mammals, then bears are animals. Since bears are
            mammals, we infer that bears are animals.
        \end{example}
        The next two commonly accepted rules of inference, known as
        \textit{modus tollens}\index{Axiom!of Modus Tollens} and
        \textit{contraposition}\index{Axiom!of Contraposition} are widely used
        in the mathematical world and relate to a statements contrapositive.
        The contrapositive is related to the \textit{negation} of a proposition,
        and so we define this now.
        \begin{fdefinition}{Negation}{Negation}
            The negation of a proposition $P$ is the proposition \textit{not}
            $P$, denoted $\neg{P}$.
        \end{fdefinition}
        Negation is a \textit{unary} operation acting on a single variable. The
        truth table is short.
        \begin{table}[H]
            \centering
            \captionsetup{type=table}
            \begin{tabular}{c|c}
                $P$&$\neg{P}$\\
                \hline
                0&1\\
                1&0
            \end{tabular}
            \caption{Truth Table for Negation}
            \label{tab:Truth_Table_Negation}
        \end{table}
        That is, negaiton takes true to false and maps false to true. Negation
        and implication form our two primitive logical operations, and the other
        familiar terms (disjunction, conjunction, equivalence) can be expressed
        using these two. With negation defined, we now present a
        \textit{logical fallacy}\index{Logical Fallacy}, a form of reasoning
        that is invalid and leads to contradiction. This is the fallacy of
        \textit{affirming the consequent}%
        \index{Logical Fallacy!Affirming the Consequent}.
        \begin{fexample}{Affirming the Consequent}{Affirming_the_Consequent}
            The fallacy of affirming the consequent is also known as the
            inverse fallacy\index{Logical Fallacy!Inverse Fallacy}, and in
            mathematics it is called the \textit{converse}
            fallacy\index{Logical Fallacy!of the Converse}.
            \textit{Modus ponens} tells us that if $P$ and $Q$ are propositions,
            if $P\Rightarrow{Q}$, and if $P$ is true, then $Q$ is true. The
            \textit{converse}\index{Converse} of the statement
            \textit{if P, then Q} is the sentence \textit{if Q, then P}. The
            validity of $P\Rightarrow{Q}$ does \textbf{not} verify the converse,
            much to the dismay of mathematicians. Theorems are often held in
            higher regard if they express the \textit{equivalence} of two
            propositions: $P\Rightarrow{Q}$ and $Q\Rightarrow{P}$. That is, both
            the statement and its converse are true. There are everyday and
            mathematical examples showing that this line of reasoning if faulty.
            Previously we discussed the classification of bears:
            \textit{if an animal is a bear, then it is a mammal}. The converse
            states if an animal is a mammal, then it is a bear. Humans are a
            counterexample to this claim since humans are mammals but are not
            bears. In the mathematical world we can consider the proposition
            \textit{if n is an odd integer, then n is not 2}. The converse
            states that if $n$ is not 2, then $n$ is not an odd integer. But
            1 is not 2, yet 1 is indeed an odd integer.
        \end{fexample}
        That affirming the consequent is invalid can be see from truth tables
        (see Tab.~\ref{tab:Truth_Table_Converse}).
        \begin{table}[H]
            \centering
            \captionsetup{type=table}
            \begin{tabular}{c|c|c|c}
                $P$&$Q$&$P\Rightarrow{Q}$&$Q\Rightarrow{P}$\\
                \hline
                0&0&1&1\\
                0&1&1&0\\
                1&0&0&1\\
                1&1&1&1
            \end{tabular}
            \caption{Truth Table for the Converse}
            \label{tab:Truth_Table_Converse}
        \end{table}
        Since the columns for $P\Rightarrow{Q}$ and $Q\Rightarrow{P}$ are
        different, we see that these are different propositions. Negation
        further allows us to define the
        \textit{contrapositive}\index{Contrapositive} of the proposition
        $P\Rightarrow{Q}$, which is the new proposition
        $\neg{Q}\Rightarrow\neg{P}$. As it turns out, this is not new
        proposition at all and is equivalent to $P\Rightarrow{Q}$. Note
        $P\Rightarrow{Q}$ is false only when $P$ is true, yet $Q$ is false.
        Similarly, $\neg{Q}\Rightarrow\neg{P}$ is false only if $\neg{Q}$ is
        true and $\neg{P}$ is false. But if $\neg{Q}$ is true, then $Q$ is
        false (Def.~\ref{def:Negation}) and if $\neg{P}$ is false, then $P$ is
        true (Def.~\ref{def:Negation}). Thus $\neg{Q}\Rightarrow\neg{P}$ is only
        false when $P$ is true and $Q$ is false. We can further examine this via
        truth tables (see Tab.~\ref{tab:Truth_Table_for_Contrapositive}).
        \begin{table}[H]
            \centering
            \captionsetup{type=table}
            \begin{tabular}{c|c|c|c|c|c}
                $P$&$Q$&$\neg{P}$&$\neg{Q}$&$P\Rightarrow{Q}$
                    &$\neg{Q}\Rightarrow\neg{P}$\\
                \hline
                0&0&1&1&1&1\\
                0&1&1&0&1&1\\
                1&0&0&1&0&0\\
                1&1&0&0&1&1
            \end{tabular}
            \caption{Truth Table for the Contrapositive}
            \label{tab:Truth_Table_for_Contrapositive}
        \end{table}
        \begin{example}
            Suppose $a$ and $b$ are variables representing real numbers and $P$
            is the proposition $a<1/2$ and $b<1/2$, and let $Q$ be the
            proposition $a+b<1$. What is the contrapositive of
            $P\Rightarrow{Q}$? This would be $\neg{Q}\Rightarrow\neg{P}$, where
            $\neg{Q}$ is the negation of $Q$ which reads $a+b\geq{1}$.
            Similarly, $\neg{P}$ is the statement $a\geq{1}/2$ or $b\geq{1}/2$.
            Thus, the contrapositive says that if $a+b\geq{1}$, then either
            $a\geq{1}/2$ or $b\geq{1}/2$ (or both). While the contrapositive of
            a statement is always equivalent to the original statement, the
            converse need not be. Indeed, this statement is true (once one knows
            the order structure of real numbers), but the converse is not. The
            converse states that if $a+b<1$, then $a<1/2$ and $b<1/2$, but
            letting $a=2$ and $b=\minus{3}$ contradicts this claim.
        \end{example}
        The axiom of \textit{modus tollens} states that if
        $P\Rightarrow{Q}$, and if $\neg{Q}$ is true, then $\neg{P}$ is true. We
        are not directly adopting this axiom since it is provable from the
        axiom of \textit{modus ponens} if one accepts the standard axioms of set
        theory. Indeed, \textit{modus tollens} is implied by the
        \textit{law of the excluded middle}\index{Law of the Excluded Middle},
        which is in turn implied by the
        \textit{axiom of choice}\index{Axiom!of Choice}, two topics that will be
        discussed in Chapt.~\ref{chapt:Zermelo_Fraenkel_Set_Theory}. Similar to
        \textit{modus tollens}, the axiom of contraposition states that
        $P\Rightarrow{Q}$ is equivalent to $\neg{Q}\rightarrow\neg{P}$. That is,
        if the statement $P\Rightarrow{Q}$ is true, then the contrapositive
        $\neg{Q}\rightarrow\neg{P}$ is also true.
        \begin{example}
            Consider once again the description of Socrates. We start with the
            proposition \textit{all humans are mammals}. If Socrates is a human,
            then Socrates is a mammal. Therefore if Socrates is \textit{not} a
            mammal, then Socrates is \textit{not} a human. We could also say
            that all bears are mammals, and hence if you come across an animal
            that is \textit{not} a mammal, then this animal is \textit{not} a
            bear. Reasoning like this follows from contraposition.
        \end{example}
        Before moving to connectives Hilbert systems it is worth while
        mentioning a few \textit{invalid} forms of reasoning. We do not include
        these as rules of inference since they lead to contradiction, although
        students often make these fallacious mistakes when exploring proofs for
        the first time. We've discussed \textit{affirming the consequent} and
        now identify \textit{denying the antecedent}%
        \index{Logical Fallacy!Denying the Antecedent}, which is another type of
        converse fallacy and is the most common to make. Denying the antecedent
        goes as follows, if $P$ implies $Q$, and \textit{not} $P$, then
        \textit{not} $Q$. This is false, and we will provide plenty of examples
        to indicate this.
        \begin{example}
            Harking back to a previous example, consider the statement
            \textit{if I am late to work, then I will be fired}. Now suppose I
            was not late to work. Does this mean I was not fired? No! Perhaps I
            was lazy on the job, or uttered too many vulgarities (I do have a
            sailor's mouth). Knowing that I was not late tells us nothing about
            whether or not I was fired. It is only if I \textit{was} late that
            we can then appropriately apply \textit{modus ponens} and conclude
            that I was fired.
        \end{example}
        \begin{example}
            Consider the proposition
            \textit{If n is an odd integer, then n is not 2}. Now suppose we are
            told than $n$ is \textit{not} an odd integer. Can we conclude that
            $n$ is 2? No! It may be 4, or 6, or any other even integer.
        \end{example}
        We can consider $P\Rightarrow{Q}$ and $\neg{P}\Rightarrow\neg{Q}$ by
        means of truth table. The statement $\neg{P}\Rightarrow\neg{Q}$ is
        called the \textit{inverse} of $P\Rightarrow{Q}$.
        \begin{table}[H]
            \centering
            \captionsetup{type=table}
            \begin{tabular}{c|c|c|c|c|c}
                $P$&$Q$&$\neg{P}$&$\neg{Q}$&$P\Rightarrow{Q}$
                                           &$\neg{P}\Rightarrow\neg{Q}$\\
                \hline
                0&0&1&1&1&1\\
                0&1&1&0&1&0\\
                1&0&0&1&0&1\\
                1&1&0&0&1&1
            \end{tabular}
            \caption{Truth Table for the Inverse}
            \label{tab:Truth_Table_Inverse}
        \end{table}
        The next fallacy is known as the
        \textit{fallacy of the undistributed middle}%
        \index{Logical Fallacy!of the Undistributed Middle}. This is the first
        argument that takes three propositions. It falsely concludes that
        if $P_{1}\Rightarrow{Q}$, and if $P_{2}\Rightarrow{Q}$, then
        $P_{1}\Rightarrow{P}_{2}$. This is false as the truth table below
        demonstates.
        \begin{table}[H]
            \centering
            \captionsetup{type=table}
            \begin{tabular}{c|c|c|c|c|c}
                $P_{1}$&$P_{2}$&$Q$&$P_{1}\Rightarrow{Q}$&$P_{2}\Rightarrow{Q}$
                                   &$P_{1}\Rightarrow{P}_{2}$\\
                \hline
                0&0&0&1&1&1\\
                0&0&1&1&1&1\\
                0&1&0&1&0&1\\
                0&1&1&1&1&1\\
                1&0&0&0&1&0\\
                1&0&1&1&1&0\\
                1&1&0&0&0&1\\
                1&1&1&1&1&1
            \end{tabular}
            \caption{Fallacy of the Undistributed Middle}
            \label{tab:Fallcy_of_Undistributed_Middle}
        \end{table}
        There's a column on this table where $P_{1}\Rightarrow{Q}$ is true, and
        $P_{2}\Rightarrow{Q}$ is true, yet $P_{1}\Rightarrow{P}_{2}$ is false.
        Namely, choose $P_{1}=True$, $P_{2}=False$, and $Q=True$.
        \begin{example}
            Consider the claim \textit{all mathematicians love geometry} which
            one could only hope is true. Consider also
            \textit{all physicists love geometry}. Given that these two
            statements are true, it would be wrong to conclude that all
            mathematicians are physicists.
        \end{example}
        \begin{example}
            We can consider a mathematical proposition. If $n$ is divisible by
            2, then $n$ is even, and if $n$ is divisible by 4, then $n$ is even.
            We cannot conclude that if $n$ is divisible by 2, then $n$ is
            divisible by 4 since the number 6 serves as a counterexample. That
            is, $6=2\cdot{3}$ and hence 6 is divisible by 2, but it is not
            divisible by 4.
        \end{example}
        Lastly, we discuss the difference between a \textit{valid} argument and
        a \textit{sound} one. A valid argument is one that proves a claim from
        hypothesized propositions by correctly using the rules of inference. A
        sound argument is a valid argument of which the hypothesized
        propositions are true.
        \begin{example}
            Consider the proposition \textit{all birds can fly}. We invoke
            \textit{modus ponens} and arrive at the following absurdity:
            \begin{subequations}
                \begin{align}
                    &\text{All birds can fly}.\\
                    &\text{Penguins are birds}.\\
                    &\text{Therefore, penguins can fly}.
                \end{align}
            \end{subequations}
            It is currently believed that penguins are incapable of flight no
            matter how hard they may try, and hence we have used our rules of
            inference correctly, but we've arrived at a false claim. That is,
            our argument is valid, but it cannot be sound. And indeed, the flaw
            is that the proposition \textit{all birds can fly} is false since
            penguins serve as a counterexample.
        \end{example}
        Another example is known as the \textit{masked-man fallacy}%
        \index{Logical Fallacy!Masked-Man Fallacy}.
        \begin{fexample}{Masked-Man Fallacy}{Masked_Man_Fallacy}
            Suppose I have a friend \textit{Bob}. Since he is my friend, the
            proposition \textit{I know Bob} is true. Suppose further that there
            is a man wearing a mask. Since he is wearing a mask, I do not know
            who he is and hence the proposition
            \textit{I do not know the masked man} is true. We obtain the
            following:
            \begin{subequations}
                \begin{align}
                    &\text{I know Bob}.\\
                    &\text{I do not know that masked man}.\\
                    &\text{Therefore, Bob is not the masked man}.
                \end{align}
            \end{subequations}
            Our argument is valid and follows from \textit{modus tollens}. That
            is, we have the proposition
            \textit{if the man is Bob, then I know him}. Hence, if I don't know
            the man, then it is not Bob. However this argument is not sound
            since it is perfectly possible for Bob to be wearing the mask. The
            flaw comes from the proposition
            \textit{I do not know the masked man}. In truth, it may be possible
            that I do. This knowledge is not available to me and cannot be used
            or refuted in the argument.
        \end{fexample}