\chapter{Normed and Inner Product Spaces}
    \section{Basic Definitions}
        We're finally going to put some structure on these
        sets, and talk about vector spaces. In a metric
        space, the only thing you can really talk about
        is the distance between points. In a vector space
        we have a lot more structure. We will start off
        with vector spaces over the reals $\mathbb{R}$.
        The main properties are that there is a
        $\mathbf{0}$ element, addition is well defined
        and is both associative and commutative,
        there is a notion of scalar multiplication that
        is associative, and the distributive law holds.
        \begin{example}
            $\mathbb{R}^{n}$, with it's usual notion
            of addition, and with scalar multiplication
            defined over $\mathbb{R}$, is a vector space.
        \end{example}
        \begin{definition}
            A norm on a vector space $X$ over $\mathbb{R}$
            is a function $\norm{}:X\rightarrow\mathbb{R}$
            such that:
            \begin{enumerate}
                \item For all $\mathbf{x}\in{X}$,
                      $\norm{\mathbf{x}}\geq{0}$ and
                      $\norm{\mathbf{x}}=0$ if and only
                      if $\mathbf{x}=\mathbf{0}$.
                      \hfill[Positive Definiteness]
                \item For all $\mathbf{x}\in{X}$ and
                      $c\in\mathbb{R}$,
                      $\norm{c\mathbf{x}}%
                       =|c|\norm{\mathbf{x}}$
                      \hfill[Homogeneity]
                \item For all $\mathbf{x},\mathbf{y}\in{X}$,
                      $\norm{\mathbf{x}+\mathbf{y}}%
                       \leq\norm{\mathbf{x}}%
                       +\norm{\mathbf{y}}$
                      \hfill[Triangle Inequality]
            \end{enumerate}
        \end{definition}
        We have seen before that
        $d(\mathbf{x},\mathbf{y})=\norm{\mathbf{x}-\mathbf{y}}$
        defines a metric, and thus $(X,d)$ is a metric space.
        Thus, for every vector space there is an associated
        metric space, the metric $d$ called the
        \textit{induced} metric.
        \begin{definition}
            A normed vector space is a vector space
            $X$ over $\mathbb{R}$ with a norm
            $\norm{}$ on $X$.
        \end{definition}
        \begin{example}
            $\mathbb{R}^{n}$ with
            $\norm{\mathbf{x}}_{p}$, for $p\geq{1}$,
            is a normed vector space.
        \end{example}
        \begin{example}
            $\ell^{p}$ with $\norm{x}_{p}$ is
            also a normed vector space.
        \end{example}
        \begin{example}
            $C[a,b]$ equipped with the supremum norm,
            $\norm{x(t)}_{\infty}$,
            is a normed vector space.
        \end{example}
    \subsection{Inner Product Spaces}
        \begin{definition}
            An inner product on a vector space
            $X$ over $\mathbb{R}$ is a function
            $\langle\rangle:X\rightarrow\mathbb{R}$
            such that:
            \begin{enumerate}
                \item For all $x\in{X}$,
                      $\langle{\mathbf{x},\mathbf{x}}%
                       \rangle\geq{0}$
                      and
                      $\langle\mathbf{x},\mathbf{x}\rangle=0$
                      if and only
                      if $\mathbf{x}=\mathbf{0}$.
                      \hfill[Positive Definiteness]
                \item For all $\mathbf{x},\mathbf{y}\in{X}$,
                      $\langle\mathbf{x},\mathbf{y}\rangle%
                       =\langle\mathbf{y},\mathbf{x}\rangle$
                      \hfill[Symmetry]
                \item For all
                      $\mathbf{x},\mathbf{y},\mathbf{z}%
                       \in{X}$
                      and all $\alpha,\beta\in\mathbb{R}$,
                      $\langle\alpha\mathbf{x}%
                       +\beta\mathbf{y},\mathbf{z}\rangle%
                       =\alpha\langle\mathbf{x},\mathbf{z}%
                       \rangle+\beta\langle\mathbf{y},%
                       \mathbf{z}\rangle$
                      \hfill[Linearity]
            \end{enumerate}
        \end{definition}
        \begin{example}
            $\mathbb{R}^{2}$ with
            $\langle(x_{1},x_{2}),(y_{1},y_{2})\rangle%
             =x_{1}y_{1}+x_{2}y_{2}$ is an inner product.
            Replacing this with $\mathbb{R}^{n}$ and doing
            $\sum_{k=1}^{n}x_{k}y_{k}$ is also an inner
            product. This is the usual dot product that one
            sees in a vector calculus course. In $\ell^{2}$,
            $\sum_{k=1}^{\infty}x_{k}y_{k}$ is an inner
            product as well. Note also that
            $\sum|x_{i}y_{i}|$ converges since
            $|x_{i}y_{i}|\leq\frac{1}{2}|x_{i}^{2}|%
             +\frac{1}{2}|y_{i}|^{2}$.
        \end{example}
        \begin{example}
            In $C[a,b]$, let
            $\langle{x(t),y(t)}\rangle%
             =\int_{a}^{b}x(t)y(t)dt$. This defines an
            inner product.
        \end{example}
        \begin{definition}
            An inner product space is a vector space
            $X$ over $\mathbb{R}$ with an inner product
            $\langle\rangle$.
        \end{definition}
        \begin{theorem}[Cauchy-Schwarz Inequality]
            If $X$ is an inner product space
            and $x,y\in{X}$, then
            $|\langle{x,y}\rangle<\norm{x}\norm{y}$
        \end{theorem}
        \begin{proof}
            For all $y\in\mathbb{R}$,
            $\langle{x+ty,x+ty}\rangle%
             =\langle{x,x}\rangle%
             +2t\langle{x,y}\rangle%
             +t^{2}\langle{y,y}\rangle%
             =\norm{x}^{2}+2t\langle{x,y}\rangle%
             +t^{2}\norm{y}^{2}$. Thus we have a
            quadratic in $t$. But this is always positive,
            and thus the discriminant must be non-positive. Therefore
            $(2\langle{x,y})^{2}-4\norm{x}^{2}\norm{y}^{2}%
             \leq{0}$
            and thus
            $|\langle{x,y})|\leq\norm{x}\norm{y}$.
        \end{proof}
        \begin{theorem}
            If $X$ is a vector space over $\mathbb{R}$
            and $\langle\rangle$ is an inner product,
            then
            $\norm{\mathbf{x}}%
             =\sqrt{\langle\mathbf{x},\mathbf{y}\rangle}$
            is a norm on $X$.
        \end{theorem}
        \begin{proof}
            Positivity, homogeneity, and definiteness are
            pretty easy. The only tricky thing to check is
            the triangle inequality. We have that
            $\norm{x+y}=\langle{x+y,x+y}\rangle$,
            and this simplify to
            $\norm{x}^{2}+2\langle{x,y}\rangle+\norm{y}^{2}$.
            But from the Cauchy-Schwartz inequality, we
            have $\langle{x,y}\rangle\leq\norm{x}\norm{y}$.
            Thus
            $\norm{x+y}^{2}\leq\norm{x}^{2}%
             +2\norm{x}\norm{y}+\norm{y}^{2}%
             =(\norm{x}+\norm{y})^{2}$. Taking square roots
             completes the theorem.
        \end{proof}
        In $\mathbb{R}^{n}$, the Cauchy-Schwartz inequality
        says that the dot product of two vectors is less
        than or equal to the product of the magnitude
        of the two vectors.
        This is obvious from the fact that the dot product
        of two vector is the product of the magnitudes and
        the \textit{cosine} of the angle between them.
        Since the cosine of a number is less than or equal
        to one, this would complete the theorem.
        In $\ell^{p}$ and $L^{p}$ spaces, this is the
        special case of the H\"{o}lder inequality for
        when $p=q=2$.
    \subsection{Convergence in Normed Spaces}
        In a metric space, convergence meant that
        $d(x_{n},x)\rightarrow{0}$. In a normed space
        we have the induced metric, and thus we may define
        convergence as $\norm{x_{n}-x}\rightarrow{0}$.
        \begin{definition}
            A convergent sequence in a normed space $X$
            is a sequence $x_{n}$ such that there is an
            $x\in{X}$ such that
            $\norm{x_{n}-x}\rightarrow{0}$.
        \end{definition}
        Since
        $\norm{y}=\norm{(y-x)+x}\leq\norm{y-x}+\norm{x}$,
        it follows that
        $|\norm{x}-\norm{y}|\leq\norm{x-y}$.
        But then if $x_{n}\rightarrow{x}$, then
        $|\norm{x_{n}}-\norm{x}|\leq\norm{x_{n}-x}$,
        and $\norm{x_{n}-x}\rightarrow{0}$. Therefore
        $\norm{x_{n}}\rightarrow\norm{x}$. That is,
        the norm function is a continuous function.
        Similarly, if $x_{n}\rightarrow{x}$, then
        $\langle{x_{n},y}\rangle\rightarrow%
         \langle{x,y}\rangle$.
        In fact, if $x_{n}\rightarrow{x}$ and
        $y_{n}\rightarrow{y}$, then
        $\langle{x_{n},y_{n}}\rangle%
         \rightarrow\langle{x,y}\rangle$. To see this, we
        have
        $\langle{x_{n},y_{n}}\rangle-\langle{x,y}\rangle%
         =\langle{x_{n}-x,y}\rangle+\langle{x,y-y_{n}}\rangle$
        and therefore
        $|\langle{x_{n},y_{n}}\rangle-\langle{x,y}\rangle%
         \leq\norm{x_{n}-x}\norm{y_{n}}%
         +\norm{x}\norm{y-y_{n}}$. But $\norm{x-x_{n}}\rightarrow{0}$
        and $\norm{y-y_{n}}\rightarrow{0}$. But also
        $\norm{y_{n}}=\norm{(y_{n}-y)+y}\leq\norm{y_{n}-y}+\norm{y}$,
        which is bounded. Therefore
        $\langle{x_{n},y_{n}}-\langle{x,y}\rangle\rightarrow{0}$.
        So inner product spaces and normed spaces are metric spaces
        and we can define everything we did for metric spaces and all
        of the previous results remain true. That is, the notions and
        theorems pertaining to convergence, completeness, compactness,
        the notion of open and closed. All of these still make sense in
        these new spaces.
    \subsection{Banach Spaces and Hilbert Spaces}
        \begin{definition}
            A Banach Space is a normed vector space $X$ that is
            complete with respect to the induced metric.
        \end{definition}
        \begin{definition}
            A Hilbert Space is an inner product space $X$ that is
            complete with respect to the induced metric.
        \end{definition}
    \subsection{Linear Operators}
        Let $X$ and $Y$ be normed spaces. A mapping
        $T:X\rightarrow{Y}$ is called a linear operator if, for
        all $x,y\in{X}$, and for all $\alpha,\beta\in\mathbb{R}$,
        $T(\alpha{x}+\beta{y})=\alpha{T(x)}+\beta{T(y)}$. Usually, with
        operators, we simply write $Tx$ and $Ty$. Similar to how
        we write matric multiplication over vectors. In $\mathbb{R}^{n}$,
        every $n\times{n}$ matrix defines a linear operator.
        \begin{definition}
            A linear operator from a normed vector space $X$ to
            a normed vector space $Y$ is a function
            $T:X\rightarrow{Y}$ such that, for all $x,y\in{X}$
            and for all $\alpha,\beta\in\mathbb{R}$,
            $T(\alpha{x}+\beta{y})=\alpha{Tx}+\beta{Ty}$.
        \end{definition}
        \begin{definition}
            A bounded linear operator from a normed vector space
            $X$ to a normed vector space $Y$ is a linear operator
            $T:X\rightarrow{Y}$ such that there is a $K\in\mathbb{R}$
            such that for all $x\in{X}$, $\norm{Tx}\leq{K}\norm{x}$
        \end{definition}
        In a just world, ``bounded'' would mean
        $\norm{Tx}\leq{K}$. However, the only linear mapping that does
        this is the zero mapping. For if $\norm{Tx}=1$,
        then $\norm{T(2x)}=2$, and so on, and thus no linear mapping
        is bounded (With the exception of the zero mapping).
        Boundedness of a norm $T:X\rightarrow{Y}$ depends on
        the norms of the space.
        \begin{theorem}
            Bounded linear operators are continuous.
        \end{theorem}
        \begin{proof}
            If $x_{n}\rightarrow{x}$, then
            $\norm{Tx_{n}-Tx}=\norm{T(x_{n}-x)}$. But
            $T$ is bounded, and thus there is a $K$ such that
            $\norm{T(x_{n}-x)}\leq{K}\norm{x_{n}-x}$. But
            $\norm{x_{n}-x}\rightarrow{0}$. Therefore, etc.
        \end{proof}
        The converse is also true.
        \begin{theorem}
            If $T$ is a continuous linear operator,
            than there exists a $\delta>0$ such that for
            all $x\in{B}_{\delta}(0)$,
            $\norm{Tx-T0}<1$. But from linearity,
            $T0=0$, and thus $\norm{Tx}<1$. Then for any
            $z\in{Z}$, we have
            $\norm{\frac{\delta}{2}\frac{z}{\norm{z}}}=\frac{\delta}{2}$,
            and thus $\norm{T(\frac{\delta}{2}\frac{z}{\norm{z}})}<1$.
            Letting $K=\delta$, we have
            $\norm{Tx}<K\norm{x}$. Thus, $T$ is bounded.
        \end{theorem}
        Continuity at 0 implies uniform continuity since
        if $x_{n}-y_{n}\rightarrow{0}$, then
        $\norm{Tx_{n}-Ty_{n}}=\norm{T(x_{n}-y_{n})}%
         \leq{K}\norm{x_{n}-y_{n}}\rightarrow{0}$.
        The set of bounded linear operators form a vector space,
        where addition is $(S+t)(x)=(Sx)+(Tx)$, and scalar multiplication
        is defined by $(\alpha{T})(x)=\alpha(Tx)$. We must show that
        when you add two bounded linear operators, the result is a
        bounded linear operator.
        \begin{theorem}
            If $T_{1}:X\rightarrow{Y}$ and $T_{2}:X\rightarrow{Y}$
            are bounded linear operators, then $T_{1}+T_{2}$ is a
            bounded linear operator.
        \end{theorem}
        \begin{proof}
            For let $T_{1}$ and $T_{2}$ be bounded. Then there are
            $K_{1},K_{2}$ such that, for all $x\in{X}$,
            $\norm{T_{1}x}\leq{K_{1}}\norm{x}$ and
            $\norm{T_{2}x}\leq{K_{2}}\norm{x}$. But then
            $\norm{(T_{1}+T_{2})x}=\norm{T_{1}x+T_{2}x}%
             \leq\norm{T_{1}x}+\norm{T_{2}x}%
             \leq{K_{1}}\norm{x}+K_{2}\norm{x}$. Let $K=K_{1}+K_{2}$.
        \end{proof}
        \begin{theorem}
            If $T:X\rightarrow{Y}$ is a bounded linear operator, and
            $\alpha\in\mathbb{R}$, then $\alpha{T}$ is a bounded
            linear operator.
        \end{theorem}
        \begin{proof}
            For
            $\norm{\alpha{Tx}}=|\alpha|\norm{Tx}%
             \leq|\alpha|K\norm{x}=K\norm{\alpha{x}}$.
        \end{proof}
        We write $B(X,Y)$ to denote the set of bounded linear
        operators from $X$ to $Y$. That is, linear operators
        $T:X\rightarrow{Y}$.
        We can define a norm on $B(X,Y)$ as follows:
        $\norm{T}_{B}%
         =\sup_{x\in{X},x\ne{0}}\{\frac{\norm{Tx}}{\norm{x}}\}$.
        This is the ``Smallest $K$,'' used as a bounded for the linear
        operator $T$. This shows that
        $\norm{Tx}_{Y}\leq\norm{T}_{B}\norm{x}_{X}$.
    \section{Lecture 7: October 22, 2018}
        \subsection{Bounded Linear Operators}
            A bounded linear operator is a function
            $T:X\rightarrow{Y}$ between normed spaces
            $X$ and $Y$ such that $T$ is linear, and
            there exists a $K\in\mathbb{R}$ such that,
            for all $x\in{X}$,
            $\norm{Tx}_{Y}\leq{K}\norm{x}_{X}$. The
            norm of $T$, $\norm{T}$, is then defined
            as the smallest such $K$. Equivalently:
            \begin{equation*}
                \norm{T}=
                \sup\Big\{\frac{\norm{Tx}_{Y}}{\norm{x}_{X}}:
                          x\in{X},x\ne{0}\Big\}
                =\sup\{\norm{Tx}_{Y}:\norm{x}_{X}=1\}
            \end{equation*}
            The set of all bounded linear operators
            from a normed space $X$ to a normed space
            $Y$ is denoted $B(X,Y)$. This is a vector
            space with addition defined as
            $(T+S)x=(Tx)+(Sx)$ and $(aT)x=a(Tx)$.
            \begin{theorem}
                $\norm{T}$ defines a norm on
                $B(X,Y)$.
            \end{theorem}
            \begin{proof}
                For $\norm{T}\geq{0}$ and
                $\norm{Tx}=0$ if and only if
                $Tx=0$ for all $x\in{X}$, and thus
                $T$ is the zero operator. If
                $\alpha\in\mathbb{R}$, then:
                \begin{align*}
                    \norm{\alpha{T}}
                    &=\sup\Big\{
                        \frac{\norm{\alpha{T}x}_{Y}}{\norm{x}_{X}}:
                        x\in{X},x\ne{0}\Big\}\\
                    &=|\alpha|\sup\Big\{
                        \frac{\norm{Tx}_{Y}}{\norm{x}_{X}}:
                        x\in{X},x\ne{0}\Big\}\\
                    &=|\alpha|\norm{T}
                \end{align*}
                Finally, if $S,T\in{B}(X,Y)$, then:
                \begin{align*}
                    \norm{S+T}&=\sup\Big\{
                        \frac{\norm{(S+t)x}_{Y}}{\norm{x}_{X}}:
                        x\in{X},x\ne{0}\}\\
                    &=\sup\Big\{
                        \frac{\norm{Sx+Tx}_{Y}}{\norm{x}_{X}}:
                        x\in{X},x\ne{0}\Big\}\\
                    &\leq\sup\Big\{
                        \frac{\norm{Sx}_{y}+\norm{Tx}_{Y}}
                             {\norm{x}_{X}}:
                        x\in{X},x\ne{0}\Big\}\\
                    &\leq\norm{T}+\norm{S}
                \end{align*}
            \end{proof}
            \begin{theorem}
                If $Y$ is a Banach space, and
                if $X$ is a normed space, then
                $B(X,Y)$ is a Banach space.
            \end{theorem}
            \begin{proof}
                For let $T_{n}$ be a Cauchy sequence
                in $B(X,Y)$ and let $\varepsilon>0$.
                Then there exists $N_{0}\in\mathbb{N}$
                such that for all $n,m>N_{0}$,
                $\norm{T_{n}-T_{m}}<\varepsilon$. That is,
                for all $n,m>N_{0}$:
                \begin{align*}
                    \sup\Big\{
                        \frac{\norm{T_{n}x-T_{m}x}_{Y}}
                             {\norm{x}_{X}}:
                        x\in{X},x\ne{0}\Big\}
                    &\leq\varepsilon\\
                    \Rightarrow
                    \frac{\norm{T_{n}x-T_{m}y}_{Y}}
                         {\norm{x}_{X}}
                    &\leq\varepsilon
                \end{align*}
                That is, $T_{n}x$ is a Cauchy sequence
                in $Y$ for any fixed value $x\in{X}$.
                But $Y$ is a Banach space, and is therefore
                complete. But then if $T_{n}x$ is a Cauchy
                sequence in $Y$ it has a limit $y\in{Y}$.
                Let $Tx=\lim_{n\rightarrow\infty}T_{n}x$
                for all $x\in{X}$.
                Then $T\in{B(X,Y)}$. For:
                \begin{equation*}
                    T(x+y)
                    =\lim_{n\rightarrow\infty}T_{n}(x+y)
                    =\lim_{n\rightarrow\infty}(T_{n}x+T_{n}y)
                    =Tx+Ty
                \end{equation*}
                And similarly $(\alpha{T})x=\alpha{T}x$.
                Lastly, $T$ is bounded. For all $n,m>N$ we have
                $\norm{T_{n}x-T_{m}x}_{Y}/\norm{x}_{X}<\varepsilon$.
                Taking the limit on $m$, we have
                $\norm{Tx-T_{n}x}_{Y}/\norm{x}_{X}\leq\varepsilon$
                for all $n>N_{0}$. Thus,
                $\norm{T_{n}x-Tx}_{X}\leq\varepsilon\norm{x}_{X}$.
                But
                $\norm{Tx-T_{n}x}_{Y}=\norm{T_{n}x-(T_{n}-Tx)}_{Y}$,
                and therefore
                $\norm{Tx}\leq\varepsilon\norm{x}_{X}+\norm{T_{n}x}$,
                and $\norm{T_{n}x}\leq\norm{T_{n}}$, and therefore
                $\norm{Tx}\leq\varepsilon\norm{X}_{X}+%
                 \norm{T}\norm{x}_{X}$. But then
                $\norm{Tx}_{Y}\leq%
                 (\varepsilon+\norm{T_{n}})\norm{x}_{X}$.
                But $T_{n}$ is bounded, and therefore
                $T$ is bounded. Finally, we must show that
                $T_{n}\rightarrow{T}$ in $B(X,Y)$ with respect
                to the norm $\norm{T_{n}-T}$. That is, we must
                show that $\norm{T-T_{n}}\rightarrow{0}$. This
                follows since
                $\norm{Tx-T_{n}x}_{Y}/\norm{x}_{X}<\varepsilon$
                for $n>N_{0}$, and therefore
                $\norm{T-T_{n}}<\varepsilon$. Therefore, etc.
            \end{proof}
        \subsection{Dual Spaces}
            So if $Y$ is a Banach space, and $X$ is any normed
            space, then $B(X,Y)$ is a Banach space. One of the
            most important cases is $Y=(\mathbb{R},||)$, where
            $||$ is the normal absolute value ``norm.''
            $B(X,\mathbb{R})$ is a Banach space, and it is
            called the continuous dual space of $X$, written
            $X'$. Elements of $X'$ are called bounded linear
            functionals. These are bounded linear operators
            whose range of the operator is the real numbers.
            The characterization, or the representation, or
            realization, of these dual spaces is a major
            topic in functional analysis. A lot of these
            theorems are do to a mathematician by the name
            of Riesz.
            \begin{example}
                A functional takes an element of a normed
                space $X$ and spits out a real number. For
                example, if $X$ is the space of continuous
                functions, then the following are
                functionals:
                \begin{align*}
                    f_{1}(x)&=\int_{0}^{1}x(t)t^{2}\diff{t}
                    &
                    f_{2}(x)&=x(0.5)
                    &
                    f_{3}(x)&=0
                \end{align*}
            \end{example}
            Let $X=(\mathbb{R}^{2},\ell^{1}$. What does
            $X'$ look like? That is, what is the dual
            space of $X$? let $f:X\rightarrow\mathbb{R}$
            be defined by
            $f(x_{1},x_{2})=2x_{1}-5x_{2}$. Then
            $f\in{X'}$ and $\norm{f}=5$. More generally,
            every element of $\mathbb{R}^{2}$ defines
            and element of $X'$. Given
            $(a,b)\in\mathbb{R}^{2}$, we define
            $f(x_{1},x_{2})=ax_{1}+bx_{2}$. $f$ is then linear,
            and:
            \begin{align*}
                |f(x_{1},x_{2})|
                &=|ax_{1}+bx_{2}|\\
                &\leq|a||x_{1}|+|b||x_{2}|\\
                &\leq\max\{|a|,|b|\}(|x_{1}|+|x_{2}|)\\
                &=\norm{(a,b)}_{\infty}
                \norm{(x_{1},x_{2})}_{\ell^{1}}
            \end{align*}
            And therefore $f$ is bounded, as $\norm{(a,b)}_{\infty}$
            is a bound. That is, $\norm{f}\leq\norm{(a,b)}_{\infty}$.
            By choosing $x=(x_{1},x_{2})$, where $x_{1}=1$ and
            $x_{2}=0$ if $|b|\leq|a|$, and $x_{1}=0$ and
            $x_{2}=1$ otherwise, we ge
            $|f|=\max\{(a,b)\}=\norm{(a,b)}_{\infty}$.
            Therefore $\norm{f}=\norm{(a,b)}_{\infty}$. On
            the other hand, if $f\in{X'}$, let
            $a=f(1,0)$ and $b=f(0,1)$. Then, for all
            $(x_{1},x_{2})\in\mathbb{R}^{2}$:
            \begin{equation*}
                f(x_{1},x_{2})
                =f(x_{1}(1,0)+x_{2}(0,1))
                =x_{1}f(1,0)+x_{2}f(0,1)
                =ax_{1}+bx_{2}
            \end{equation*}
            So the dual of $(\mathbb{R}^{2},\ell^{1})$
            looks very much like $(\mathbb{R}^{2},\ell^{\infty})$.
            In fact, $(\mathbb{R}^{2},\ell^{1})'$ and
            $(\mathbb{R}^{2},\ell^{\infty})$ are isometric
            and isomorphic. That is, we really can't tell them
            apart and we can consider them as the same thing.
            More generally,
            $(\mathbb{R}^{n},\ell^{n})'=(\mathbb{R}^{n},\ell^{\infty})$.
            Even more general, if $p$ and $q$ are exponential
            conjugates of each other (That is,
            $\frac{1}{q}+\frac{1}{p}=1$), then
            $(\mathbb{R}^{n},\ell^{p})'=(\mathbb{R}^{n},\ell^{p})$
            for all $1\leq{p}\leq\infty$. Saying $p=\infty$ is
            equivalent to saying $q=1$. Setting $p=q=2$, we have
            $(\mathbb{R}^{n},\ell^{2})'=(\mathbb{R}^{n},\ell^{2})$.
            This is true of any Hilbert space: The dual of any
            Hilbert Space $\mathcal{H}$ is itself. That is,
            $\mathcal{H}'=\mathcal{H}$. This is one of the
            Riesz Representation Theorems. In infinite dimensions,
            $(\ell^{p})'=\ell^{q}$, where $p$ and $q$ are such that
            $\frac{1}{p}+\frac{1}{q}=1$, and $1\leq{p}<\infty$.
            Now, we cannot allow $p=\infty$. For
            $(\ell^{\infty})'$ is not equal to $\ell^{1}$.
            \begin{theorem}
                If $1\leq{p}<\infty$ and
                $\frac{1}{p}+\frac{1}{q}=1$, then
                $(\ell^{p})'=\ell^{q}$.
            \end{theorem}
            \begin{proof}
                If $(f_{1},f_{2},\hdots)\in\ell^{q}$, then let
                $f:\ell^{p}\rightarrow\mathbb{R}$ be defined by
                $f(x_{1},x_{2},\hdots)=\sum_{k=1}^{\infty}x_{k}f_{k}$.
                This converges from H\"{o}lder's inequality:
                \begin{equation*}
                    \sum_{k=1}^{\infty}|x_{k}f_{k}|
                    \leq
                    \Big(\sum_{k=1}^{\infty}f_{k}^{q}\Big)^{1/q}
                    \Big(\sum_{k=1}^{\infty}x_{i}^{p}\Big)^{1/p}
                \end{equation*}
                And therefore
                $|fx|=\norm{(f_{1},f_{2},\hdots)}_{q}%
                 \norm{(x_{1},x_{2},\hdots)}_{p}$. That is,
                Moreover $f$ is linear. Therefore
                $f\in(\ell^{p})'$ and
                $\norm{f}\leq\norm{(f_{1},f_{2},\hdots)}_{q}$.
                On the other hand, let
                $x_{i}=|f_{i}|^{q/p}\sgn(f_{i}$. Then
                \begin{equation*}
                    fx=\sum_{k=1}^{\infty}f_{k}x_{k}
                    =\sum_{k=1}^{\infty}|f_{k}|^{q/p+1}
                \end{equation*}
                But $\frac{1}{p}+\frac{1}{q}=1$, and thus
                $\frac{q}{p}+1=q$. Thus:
                \begin{align*}
                    |fx|
                    &=\sum_{k=1}^{\infty}|f_{k}|^{q}\\
                    &=\norm{(f_{1},f_{2},\hdots)}_{q}^{q}\\
                    &=\norm{(f_{1},f_{2},\hdots)}_{q}
                        \norm{(f_{1},f_{2},\hdots)}_{q}^{q-1}\\
                    &=\norm{(f_{1},f_{2},\hdots)}_{q}
                        \Big(\sum_{k=1}^{\infty}|f_{k}^{q}
                        \Big)^{\frac{q-1}{q}}
                    &=\norm{(f_{1},f_{2},\hdots)}_{q}
                        \Big(\sum_{k=1}^{\infty}|x_{k}|^{p}
                        \Big)^{1/p}\\
                    &=\norm{(f_{1},f_{2},\hdots)}_{q}\norm{x}_{p}
                \end{align*}
                Therefore, $\norm{f}=\norm{(f_{1},f_{2},\hdots)}_{q}$.
                Thus, for all $y\in\ell^{q}$ there is a bounded
                linear operator $f\in\ell^{p}$ such that
                $\norm{y}_{\ell^{q}}=\norm{f}_{(\ell^{p})'}$. That
                is, every $(f_{i})\in\ell^{q}$ defines an element
                of $(\ell^{p})'$ by
                $fx=\sum_{k=1}^{\infty}f_{k}x_{k}$, for any
                $(x_{i})\in\ell^{p}$. So $\ell^{q}$ can
                be \textit{embedded} into to $(\ell^{p})'$.
                Now we need to show that this embedding is
                the entirety of $(\ell^{p})'$. If
                $f\in(\ell^{p})'$, let
                $f_{i}=f(e_{i})$, where $e_{i}$ is the
                sequence $(0,0,\hdots,1,0,0,\hdots)$, where
                the 1 occurs in the $i^{th}$ spot. We need
                to show that $(f_{i})\in\ell^{q}$ and
                $fx=\sum_{k=1}^{\infty}f_{k}x_{k}$
                for all $x\in\ell^{p}$. If $x\in\ell^{p}$, then:
                \begin{align*}
                    x=\sum_{k=1}^{\infty}x_{k}e_{k}
                    \Rightarrow
                    fx=f\Big(\sum_{k=1}^{\infty}x_{k}e_{k}\Big)
                    =\sum_{k=1}^{\infty}f(x_{k}e_{k})
                    =\sum_{k=1}^{\infty}x_{k}f(x_{k})
                    =\sum_{k=1}^{\infty}x_{k}f_{k}
                \end{align*}
                Choosing $x_{k}=|f_{k}|^{q/p}\sgn(f_{k})$ and apply
                H\"{o}lder.
            \end{proof}
    \section{Lecture 8: October 29, 2018}
        \subsection{Review}
            If $X$ is a vector space, an inner product on
            $X$ is a mapping
            $\langle,\rangle:X\times{X}\rightarrow\mathbb{R}$
            such that:
            \begin{enumerate}
                \item $\langle{x,x}\rangle\geq{0}$ and
                      $\langle{x,x}\rangle=0$ if and only if
                      $x=0$
                \item $\langle{x,y}\rangle=\langle{y,x}\rangle$
                \item $\langle{ax+by,z}\rangle%
                       =a\langle{x,z}\rangle+b\langle{y,z}\rangle$
            \end{enumerate}
            Think of the dot product for vectors. This is a
            generalization of this concept. Every inner product
            on a vector space $V$ induce a norm on $V$:
            \begin{equation*}
                \norm{x}=\sqrt{\norm{x,x}}
            \end{equation*}
            An inner product that is complete with respect to
            the induced norm is called a Hilbert Space. A mapping
            $f:X\rightarrow\mathbb{R}$ is bounded if there is a
            $K\in\mathbb{R}$ such that, for all $x\in{X}$,
            $|f(x)|\leq{L\norm{x}}$. $f$ is linear if
            $f(ax+by)=af(x)+bf(y)$, for all $x,y\in{X}$ and all
            $a,b\in\mathbb{R}$. The smallest such $K$ that works
            is called the norm of $f$, denoted $\norm{f}$. For
            all $x\in{X}$, $|f(x)|\leq\norm{f}\norm{x}$. The
            vector space of all bounded linear functionals on
            $X$ is the dual space $X'$. This is also a Banach
            space with the functional norm $\norm{f}$. One
            question that arises is, how do we know that there
            are bounded linear functionals on a space $X$? In
            the case that $X$ is a Hilbert space, this is rather
            easy, but for a more general Banach space this is not
            that trivial. For any normed space $X$ we can at least
            one bounded linear functional because the zero mapping
            $f(x)=0$ is such a functional. The question is then
            does every normed space have a bounded linear functional
            on it? The answer is yes, and this is related to
            the Hahn-Banach Theorem. As said before, in the
            Hilbert case this is rather easy.
            \begin{theorem}
                If $X$ is a Hilbert space, then there is a
                non-trivial bounded linear functional
                $f:X\rightarrow\mathbb{R}$.
            \end{theorem}
            \begin{proof}
                If $X$ is an inner product and
                $z\in{X}$, let $f(x)=\langle{x,z}\rangle$ for
                all $x\in{X}$. Then $f$ is linear since the
                inner product is linear. But moreover, from
                Cauchy-Schwarz we have:
                \begin{equation*}
                    |f(x)|=|\langle{x,z}\rangle|
                    \leq\norm{x}\norm{z}
                \end{equation*}
                And thus $\norm{f}\leq\norm{z}$
                But $|f(z)|=\norm{z}$, so
                $\norm{f}=\norm{z}$. $f$ is a bounded
                linear functional.
            \end{proof}
            Riesz's Representation theorem says that this is it.
            All bounded linear functionals look like this. Thus,
            if $H$ is a Hilbert space, it's dual $H'$ is the space
            of all functions that look like
            $f(x)=\langle{x,y}\rangle$ for some $y\in{X}$. More
            precisely, if $H$ is a Hilbert space and $f\in{H'}$,
            then there is a $y\in{H}$ such that, for all
            $x\in{X}$, $f(x)=\langle{x,y\rangle}$.
        \subsection{Riesz's Representation Theorem}
            \begin{theorem}
                If $H$ is a Hilbert space, and
                $f:H\rightarrow\mathbb{R}$ is a bounded
                linear functional, then there is a unique
                $y\in{H}$ such that, for all $x\in{X}$,
                $f(x)=\langle{x,y}\rangle$. Moreover,
                $\norm{f}=\norm{y}$.
            \end{theorem}
            \begin{proof}
                Let $f\in{H'}$ and let $N=\mathrm{ker}(f)$. That is,
                $N$ is the null space of the functional $f$
                which is the set of all points
                $x\in{X}$ such that $f(x)=0$. The Null space
                actually defines a closed vector space, which
                is a subspace of $H$. If $N=H$, then
                $f(x)=0$, and thus let $y=0$. Otherwise, let $z$
                be a non-zero elements such that,
                for all $x\in{N}$, $\langle{x,y}\rangle=0$.
                For all $x,z$, $f(x)z-f(z)x\in{N}$, for:
                \begin{equation*}
                    f(f(x)z-f(z)x)
                    =f(f(x)z)-f(f(z)z)
                    =f(x)f(z)-f(z)f(z)=0
                \end{equation*}
                Therefore:
                \begin{align*}
                    \langle{f(x)z-f(z)x,z}\rangle&=0\\
                    \Rightarrow
                    |f(x)|\norm{z}^{2}-|fz|\langle{x,y}\rangle&=0\\
                    \Rightarrow
                    f(x)
                    &=\langle{x,\frac{f(z)z}{\norm{z}^{2}}}\rangle
                \end{align*}
                Therefore, let $y=\frac{f(z)}{\norm{z}^{2}}z$.
                This is unique since if for all $x\in{H}$,
                $\langle{x,y_{1}}\rangle=\langle{x,y_{2}}\rangle$,
                then $y_{1}=y_{2}$. Finally:
                \begin{equation*}
                    \norm{y}
                    =\frac{|f(z)|}{\norm{z}^{2}}\norm{z}
                    =\frac{|f(z)|}{\norm{z}}
                    \leq\norm{f}
                \end{equation*}
                But also:
                \begin{equation*}
                    |f(x)|=|\langle{x,z}\rangle|
                    \leq\norm{x}\norm{z}
                \end{equation*}
                Thus, $\norm{f}\leq\norm{z}$. But
                $\norm{z}\leq\norm{f}$. Therefore,
                $\norm{f}=\norm{z}$.
            \end{proof}
            Much like in $\mathbb{R}^{n}$, there is a notion of
            orthogonality in a general inner product space.
            \begin{definition}
                Orthognal elements in an inner product space $X$
                are elements $x,y\in{X}$ such that
                $\langle{x,y}\rangle=0$.
            \end{definition}
            There's also a notion of convexity for a general
            vector space.
            \begin{definition}
                A convex subset of a vector space $V$ space is a
                subset $S\subset{V}$ such that, for all
                $x,y\in{V}$ and for all $\lambda\in\mathbb{R}$,
                $\lambda{x}+(1-\lambda)y\in{S}$.
            \end{definition}
            \begin{theorem}
                If $S$ is a subset of $V$, then $S$ is convex.
            \end{theorem}
            Recall that for a general metric space $X$,
            if $S\subset{X}$, we defined
            $dist(x,S)=\inf\{d(x,s):s\in{S}\}$. We proved that,
            if $S$ is compact, then there is an $s\in{S}$ such
            that $dist(x,S)=d(x,s)$. We showed that, without
            compactness, this may not be true. Indeed, even
            complete spaces may lack this property. If
            $X$ is a Hilbert space, however, this property is
            guaranteed.
            \begin{theorem}
                If $H$ is a Hilbert space and if $S\subset{H}$
                is a closed convex subset of $H$, then there is
                a unique $s\in{S}$ such that
                $dist(x,S)=\norm{x-s}$.
            \end{theorem}
            \begin{proof}
                As $dist(x,S)=\inf\{d(x,s):s\in{S}\}$, there is
                a sequence $x_{n}\in{S}$ such that
                $\norm{x-x_{n}}\rightarrow{dist(x,S)}$. Then, by
                Appolonius:
                \begin{align*}
                    \norm{x-x_{n}}^{2}+\norm{x-x_{m}}^{2}
                    &=\frac{1}{2}\norm{x_{n}-x_{m}}^{2}
                    +\frac{1}{2}
                    \norm{\frac{1}{2}(x_{n}+x_{m})-x}^{2}\\
                    &\geq\frac{1}{2}\norm{x_{n}+x_{m}}^{2}
                    +2dist(x,S)
                \end{align*}
                But $\norm{x-x_{n}}\rightarrow{dist(x,S)}$ and
                $\norm{x-x_{m}}\rightarrow{dist(x,S)}$, so:
                \begin{equation*}
                    \frac{1}{2}\norm{x_{n}-x_{m}}^{2}
                    \leq\norm{x-x-{m}}^{2}
                    +\norm{x-x_{m}}^{2}-2dist(x,S)
                \end{equation*}
                Which can be made arbitrarily small. Therefore,
                $x_{n}$ is Cauchy. But $H$ is a Hilbert space, and
                is therefore complete, and thus $x_{n}$ converges.
                Let $s$ be the limit. As $S$ is closed, $s\in{S}$.
                Moreover, from construction,
                $\norm{x-s}=dist(x,S)$. If there is another point
                $v$, then $\norm{x-s}=\norm{x-v}$. From
                Appolonius:
                \begin{equation*}
                    \norm{x-s}^{2}+\norm{x-v}^{2}
                    \geq\frac{1}{2}\norm{s-v}+2dist(x,S)^{2}
                \end{equation*}
                But $\norm{x-s}=\norm{x-v}=dist(x,S)$, and
                thus $\norm{s-v}=0$. Therefore, $s=v$.
            \end{proof}
            Is $S$ is a closed subspace of $H$, then it's
            automatically convex. In this case, $x-s\perp{S}$,
            where $z\perp{S}$ means that, for all
            $s\in{S}$, $\langle{s,z}\rangle=0$. For if
            $z\in{S}$, then $s+tz\in{S}$ for all $t$. Thus:
            \begin{equation*}
                \norm{s+tz-x}\geq{dist(x,S)}=\norm{s-x}^{2}
            \end{equation*}
            And therefore:
            \begin{equation*}
                \rangle{s-x,s-x}\rangle+2t\langle{s-x,z}
                +t^{2}\langle{z,z}\geq{s-x}^{2}
                \Rightarrow{t^{2}\norm{z}^{2}+2t\langle{s-x,z}}
                \geq{0}
            \end{equation*}
            Looking at the discriminant of this polynomial, we
            have:
            \begin{equation*}
                \langle{s-x,z}\rangle=0
            \end{equation*}
            Therefore, $s-x\perp{S}$. You obtain $s\in{S}$
            by ``dropping the perpendicular of $x$,'' onto
            $S$. That is, $s$ is the orthogonal projection
            of $x$ onto $S$. $s=P_{S}x$ where
            $P_{S}:H\rightarrow{H}$ is the orthogonal
            projection. This has a few nice properties:
            \begin{enumerate}
                \item It is idempotent: $P_{S}^{2}=P_{S}$.
                \item Self adjoint:
                      $\langle{P_{S}x,y}\rangle%
                       =\langle{x,P_{S}y}\rangle=$
                \item Linear.
                \item Bounded and $\norm{P_{S}}=1$.
            \end{enumerate}
            If $S$ is a subset of an inner product space $X$,
            we write $S^{\perp}=\{x\in{X}:\langle{x,s}\rangle=0\}$.
            This is often read aloud as ``$S$ perp'' or
            ``$S$ perpendicular.''
            \begin{theorem}
                If $S\subset{X}$, then $S^{\perp}$ is a
                closed subspace.
            \end{theorem}
            \begin{theorem}
                $S\subset(S^{\perp})^{\perp}$
            \end{theorem}
            The direct sum of two subsets of a Hilbert space
            $H$ is
            $S_{1}\oplus{S_{2}}=\{ax+by:x\in{S_{1}},y\in{S_{2}}\}$.
            \begin{theorem}
                If $H$ is a Hilbert space and $S$ is a closed
                subspace of $H$, then $H=S\oplus{S^{\perp}}$
            \end{theorem}
            \begin{proof}
                For $x=P_{S}(x)+(x-P_{S}(x))$, and thus there is
                an element in $S$ and an element in $S^{\perp}$
                such that $x$ is the sum of those two elements.
                This is the only representation. For if
                $x=s_{1}+s_{1}^{\perp}$ and
                $x=s_{2}+s_{2}^{\perp}$, then stuff.
            \end{proof}
            If $X$ and $Y$ are normed spaces, and if
            $f\in{B(X,Y)}$, then
            $\{x\in{X}:f(x)=0\in{Y}\}$ is called the null
            space of $f$.
            \begin{theorem}
                If $X$ and $Y$ are normed spaces, and if
                $f\in{B(X,Y)}$, then $\mathrm{ker}(f)$ is a closed
                linear subspace of $X$.
            \end{theorem}
            \begin{proof}
                Obvious since $f$ is linear and continuous.
            \end{proof}
            In a Hilbert space $H$, then
            $H=\mathrm{ker}(f)\oplus\mathrm{ker}(f)^{\perp}$. Thus, if
            $\mathrm{ker}(f)\ne{H}$, then $\mathrm{ker}(f)^{\perp}\ne\{0\}$.
            That is, there exists a $z\in\mathrm{ker}(f)^{\perp}$ that
            is non-zero. This is the $z$ we used to prove the
            Riesz representation theorem. Riesz's
            Theorem thus says that every Hilbert space is its own dual.
    \section{Lecture 9: November 5, 2018}
        \subsection{Adjoint}
            If $H$ is a Hilbert space and
            $f\in{H'}$, then there is a $z\in{H}$
            such that $f(x)=\langle{x,z}\rangle$ for
            all $x\in{H}$. Moreover, $\norm{f}=\norm{z}$.
            The adjoint of $T\in{B(H,H)}$ is an
            operator $T^{*}:H\rightarrow{H}$ such that
            $\langle{Tx,y}\rangle=\langle{x,T^{*}}\rangle$.
            There is always such an operator for any
            $T\in{B(H,H)}$. $T^{*}$ is also bounded and
            linear. By Riesz there is a $z=T^{*}y$ such
            that $f(x)=\langle{x,T^{*}y}\rangle$. Then
            $\norm{T^{*}y}=\norm{z}=\norm{f}\leq\norm{T}\norm{y}$.
            Thus, $\norm{T^{*}}\leq\norm{T}$. Therefore
            $T^{*}\in{B(H,H)}$. $T^{*}$ is called the
            ajdoint of $T$.
            \begin{example}
                Consider $\mathbb{R}^{n}$ with the usual
                inner product. Let $T$ be the matrix
                $(T_{ij})$. Then:
                \begin{equation*}
                    (Tx)_{i}=\sum_{j=1}^{n}T_{ij}x_{j}
                \end{equation*}
                and:
                \begin{equation*}
                    \langle{Tx,y}\rangle
                    =\sum_{i=1}^{n}(Tx)_{i}y_{i}
                    =\sum_{i=1}^{n}\sum_{j=1}^{n}
                    T_{ij}x_{j}y_{i}
                    =\sum_{j=1}^{n}\sum_{i=1}^{n}
                    T_{ij}y_{i}x_{j}
                \end{equation*}
                If $T^{*}$ is the adjoint, then:
                \begin{equation*}
                    \langle{x,T^{*}y}\rangle
                    =\sum_{j=1}^{n}
                    \Big(\sum_{i=1}^{n}
                         T^{*}_{ji}y_{i}\Big)x_{j}
                \end{equation*}
                And thus $T^{*}_{ji}=T_{ij}$.
                That is, the adjoint
                of $T$ is the transpose of $T$. If we were in
                $\mathbb{C}^{n}$ we would use the complex
                conjugate of the transpose of $T$.
                In general, if $T=T^{*}$
                we say that $T$ is \textit{self-adjoint}.
                This is also called symmetric or Hermitian.
            \end{example}
            \begin{example}
                As another example, consider
                $H=\ell^{2}$ and let
                $T(x_{1},x_{2},\hdots)=(x_{2},x_{3},\hdots)$.
                This is linear, and:
                \begin{equation*}
                    \norm{T(x_{1},x_{2},\hdots)}
                    =\norm{(x_{2},x_{3},\hdots)}
                    =\sqrt{\sum_{n=2}^{\infty}x_{n}^{2}}
                    \leq
                    \sqrt{\sum_{n=1}^{\infty}x_{n}^{2}}
                    =\norm{(x_{1},x_{2},\hdots)}
                \end{equation*}
                Therefore $T$ is bounded and
                $\norm{T}\leq{1}$. But
                $T(0,1,0,0,\hdots)=(1,0,0,\hdots)$
                showing that $\norm{T}\geq{1}$.
                Thus, $\norm{T}=1$. Then, from
                the definition of $T$:
                \begin{align*}
                    \langle{Tx,y}\rangle
                    &=\langle{(x_{2},x_{3},\hdots),
                              (y_{1},y_{2},\hdots)}\rangle\\
                    &=x_{2}y_{1}+x_{3}y_{2}+\hdots\\
                    &={x_{1}}\cdot{0}+x_{2}y_{1}+
                    x_{3}y_{2}+\hdots\\
                    &=\langle{(x_{1},x_{2},\hdots),
                              (0,y_{1},y_{2},\hdots)}\rangle
                \end{align*}
                And therefore
                $T^{*}(y_{1},y_{2},\hdots)=(0,y_{1},y_{2},\hdots)$.
                Also $\norm{T^{*}}=1$. In general,
                if $T\in{B(H,H)}$
                then $\norm{T^{*}}=\norm{T}$.
            \end{example}
            \begin{theorem}
                If $T\in{B(H,H)}$, then
                $T^{**}=T$.
            \end{theorem}
            \begin{theorem}
                $\norm{T}=\norm{T^{*}}$
            \end{theorem}
            \begin{proof}
                For $\norm{T}\leq\norm{T^{*}}$ and
                $\norm{T^{*}}\leq\norm{T^{**}}$, but
                $T=T^{**}$, and therefore
                $\norm{T}=\norm{T^{*}}$.
            \end{proof}
            \begin{example}
                Let $x=C[0,1]$ and let
                $\langle{x,y}\rangle=\int_{0}^{1}x(t)y(t)\diff{t}$.
                Let $K:X\times{X}\rightarrow\mathbb{R}$
                be continuous and define $T$ by:
                \begin{equation*}
                    Tx(t)=\int_{0}^{1}K(t,s)x(s)\diff{s}
                \end{equation*}
                Then for all $x\in{X}$, $Tx\in{X}$ as well,
                since $K$ is continuous.
                Moreover, from Cauchy-Schwarz:
                \begin{equation*}
                    \norm{Tx}^{2}
                    =\int_{0}^{1}
                    \Big[\int_{0}^{1}
                         K(t,s)x(d)\diff{s}\Big]^{2}\diff{t}
                    \leq\int_{0}^{1}
                    \Big[\int_{0}^{1}K(t,s)^{2}\diff{s}
                    \int_{0}^{1}x(s)^{2}\diff{s}\Big]\diff{t}
                \end{equation*}
                But
                $\int_{0}^{1}x(s)^{2}\diff{s}=\norm{x}^{2}$.
                So:
                \begin{equation*}
                    \norm{Tx}^{2}\leq
                    \norm{x}^{2}\int_{0}^{1}\int_{0}^{1}
                    K(t,s)\diff{s}\diff{t}
                \end{equation*}
                Therefore $T$ is bounded and:
                \begin{equation*}
                    \norm{T}\leq
                    \sqrt{\int_{0}^{1}\int_{0}^{1}
                          K(t,s)\diff{s}\diff{t}}
                \end{equation*}
                Computing the adjoint:
                \begin{align*}
                    \langle{Tx,y}\rangle
                    &=\int_{0}^{1}Tx(t)y(t)\diff{t}\\
                    &=\int_{0}^{1}\Big(
                    \int_{0}^{1}K(t,s)x(s)\diff{s}\Big)
                    y(t)\diff{t}\\
                    &=\int_{0}^{1}\Big(
                    \int_{0}^{1}K(t,s)y(t)\diff{t}\Big)
                    x(s)\diff{s}\\
                    &=\int_{0}^{1}\Big(
                    \int_{0}^{1}K(s,t)y(s)\diff{s}\Big)
                    x(t)\diff{t}\\
                    &=\int_{0}^{1}Ty(s)x(s)\diff{s}\\
                    &=\langle{Ty,x}\rangle
                \end{align*}
                We may swap the order of integration since
                $K$ is continuous on a compact set.
            \end{example}
            \begin{theorem}
                $\norm{T^{*}T}=\norm{T}^{2}$
            \end{theorem}
            \begin{proof}
                For:
                \begin{equation*}
                    \norm{T^{*}Tx}\leq\norm{T^{*}}
                    \norm{Tx}\leq
                    \norm{T^{*}}\norm{T}\norm{x}
                \end{equation*}
                And therefore $\norm{T^{*}T}\leq\norm{T}^{2}$.
                On the other hand:
            \end{proof}
            \begin{theorem}
                If $T$ is self-adjoint, then
                $\norm{T}=\sup\{|\langle{Tx,x}\rangle|:\norm{x}=1\}$.
            \end{theorem}
            \begin{proof}
                \label{thm:Funct:Norm_of_Self_%
                       Adjoint_Operator}
                Let
                $\alpha=\sup\{sup\{|\langle{Tx,x}\rangle|:\norm{x}=1\}$.
                Then:
                \begin{equation*}
                    |\langle{Tx,x}\rangle|
                    \leq\norm{Tx}\norm{x}\leq
                    \norm{T}\norm{x}^{2}
                \end{equation*}
                Taking the supremum over $\norm{x}=1$,
                we have $\alpha\leq\norm{T}$.
                But if $\norm{x}=\norm{y}=1$, then:
                \begin{align*}
                    |\langle{Tx,y}\rangle|
                    &=|\frac{1}{4}\langle{T(x+y),x+y}\rangle
                    -\frac{1}{4}\langle{T(x-y),x-y}\rangle\\
                    &=|\frac{1}{4}\norm{x+y}^{2}
                    \langle{T}\frac{x+y}{\norm{x+y}},
                           \frac{x+y}{\norm{x+y}}\rangle
                    -\frac{1}{4}\norm{x-y}
                    \langle{T}\frac{x-y}{\norm{x-y}},
                           \frac{x-y}{\norm{x-y}}\rangle
                \end{align*}
                Since
                $\langle{Tx,y}\rangle=\langle{x,Ty}\rangle$,
                as $T$ is self-adjoint.
                And from the definition of $\alpha$:
                \begin{equation*}
                    |\langle{Tx,y}\rangle|
                    \leq\frac{\alpha}{4}
                    \big(\norm{x+y}^{2}+\norm{x-y}^{2}\big)
                    \leq\frac{\alpha}{4}
                    \big(2\norm{x}^{2}+2\norm{y}^{2}\big)
                    =\alpha
                \end{equation*}
                Let $y=Tx/\norm{Tx}$, we get:
                \begin{equation*}
                    \langle{Tx,\frac{Tx}{\norm{Tx}}}\rangle
                    \leq\alpha
                \end{equation*}
                And therefore $\norm{T}\leq\alpha$. But also
                $\alpha\leq\norm{T}$. Thus, $\norm{T}=\alpha$.
            \end{proof}
            Thm.~\ref{thm:Funct:Norm_of_Self_Adjoint_Operator}
            can fail if $T$ is not self-adjoint.
            In $\mathbb{R}^{2}$, let
            $T(x_{1},x_{2})=(0,x_{1})$. Then:
            \begin{equation*}
                \norm{Tx}^{2}=x_{1}^{2}\leq
                x_{1}^{2}+x_{2}^{2}
                =\norm{x}^{2}
            \end{equation*}
            And therefore $\norm{T}\leq{1}$.
            But $T(1,0)=(0,1)$, and
            thus $\norm{T}=1$. But if $(x_{1},x_{2})$
            lies on the unit circle, then
            $|x_{1}x_{2}|\leq0.5$. Thus:
            \begin{equation*}
                |\langle{Tx,x}\rangle|
                =|\langle(x_{1},x_{2}),(0,x_{1})\rangle
                =|x_{1}x_{2}|\leq\frac{1}{2}
            \end{equation*}
            Therefore
            $|\langle{Tx,x}\rangle|\leq{0.5}<\norm{T}$ for
            all $x\in\mathbb{R}^{2}$ such that $\norm{x}=1$.
        \subsection{Compact Operators}
            Compact operators can be defined in a more
            general spaces
            than that of Hilbert or Banach spaces.
            They can be defined
            on Topological spaces, but we won't go that far.
            For now we will simply define them on a
            general metric space.
            \begin{definition}
                A compact mapping from a metric space $X$
                to a metric
                space $Y$ is a function $T:X\rightarrow{Y}$
                such that
                for all bounded subsets $S$ of $X$, the image
                $T(S)$ is pre-compact in $Y$. That is,
                $\overline{T(S)}$ is compact
                (The closure of $T(S)$ is compact).
            \end{definition}
            \begin{theorem}
                If $T:X\rightarrow{Y}$ is a linear compact
                operator between normed spaces $X$ and $Y$,
                then $T$ is continuous.
            \end{theorem}
            \begin{proof}
                For let $S=\overline{B_{1}(\mathbf{0})}$.
                This is bounded, so
                $\overline{T(S)}$ is compact, and therefore bounded.
                Let $M$ be such a bound.
                Thus, for all $s\in\overline{S}$
                such that $\norm{s}=1$,
                $\norm{Ts}\leq{M}$, and therefore $\norm{T}\leq{M}$.
                Thus $T$ is bounded and linear, and is therefore
                continuous.
            \end{proof}
            \begin{example}
                Every linear mapping
                $T:\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}$
                is compact.
                As a another example, let
                $X=C[0,1]$ and equip this with the supremum norm.
                Define $T$ as:
                \begin{equation*}
                    Tx(t)=\int_{0}^{1}K(t,s)x(s)\diff{s}
                \end{equation*}
                Where $K:[0,1]\times[0,1]\rightarrow\mathbb{R}$
                is continuous. This is a compact operator. For if
                $S$ is a bounded subset then there exists
                an $M$ such
                that for all $x\in{S}$, $\norm{x}\leq{M}$. Thus:
                \begin{align*}
                    \norm{Tx}=\sup|Tx(t)|
                    &=\sup\Big|\int_{0}^{1}
                    K(t,s)x(s)\diff{s}\Big|\\
                    &\leq\sup\int_{0}^{1}
                    |K(t,s)||x(s)|\diff{s}\\
                    &\leq\kappa\int_{0}^{1}|x(s)|\diff{s}\\
                    &\leq\kappa\norm{x}
                \end{align*}
                Where $\kappa=\sup|K(t,s)|$. $\kappa$ exists
                since $K(t,s)$ is continuous on a compact
                set and is therefore
                bounded. So $T(S)$ is uniformly bounded. To apply
                Arzela-Ascoli we need to show that
                $T(S)$ is equicontinuous. That is, for all
                $\varepsilon>0$ there is a $\delta>0$ such that,
                for all $x\in{S}$, if $|t_{2}-t_{1}|<\delta$
                then $|Tx(t_{2})-Tx(t_{1})|<\varepsilon$. If we
                can show that $T$ satisfies this, then
                $\overline{T(S)}$ is compact,
                and thus $T$ is compact.
                Let's show this. If $x\in{S}$, then:
                \begin{align*}
                    |Tx(t_{2})-Tx(t_{1})|
                    &=\Big|\int_{0}^{1}K(t_{1},s)x(s)\diff{s}
                    -\int_{0}^{1}K(t_{2},s)x(s)\diff{s}\Big|\\
                    &=\Big|\int_{0}^{1}(K(t_{2},s)-
                    K(t_{1},s))x(s)\diff{s}\Big|\\
                    &\leq
                    \int_{0}^{1}|K(t_{2},s)-K(t_{1},s)||x(s)|
                    \diff{s}\\
                    &\leq{M}\int_{0}^{1}|K(t_{2},s)-K(t_{1},s)|
                    \diff{s}
                \end{align*}
                But as $K$ is uniformly continuous,
                there is a $\delta>0$
                such that, for all $s\in[0,1]$,
                $|t_{2}-t_{1}|<\delta$ implies
                $|K(t_{2},s)-K(t_{1},s)|<\varepsilon/M$.
                Thus, $T(S)$ is equicontinuous. We can replace the
                supremum norm with $L^{2}$ and $T$ is still compact.
                Indeed, it is true for $L^{p}$ if we replace the
                use of Cauchy-Schwarz with the more general
                H\"{o}lder's Inequality. From this we have that
                $T$ is a compact self-adjoint operator.
            \end{example}
    \section{Lecture 10: November 19, 2018}
        \subsection{Compact Linear Operators}
            A linear operator $T:X\rightarrow{Y}$
            is compact if $\overline{T(S)}$ is compact
            for all bounded $S\subset{X}$. Example,
            $Tx(t)=\int_{0}^{1}K(t,s)x(s)\diff{s}$,
            where $K$ is continuous on
            $[0,1]^{2}$, is a compact operator
            $T:C[0,1]\rightarrow{C[0,1]}$. If
            $K(x,t)=K(t,x)$ for all
            $(x,t)\in[0,1]^{2}$, then
            $T$ is a self-adjoint operator on
            $L^{2}[0,1]$. $L^{2}[0,1]$ can be seen
            as the completion of $C[0,1]$ with respect
            to the $L^{2}$ norm.
            \begin{theorem}
                A linear operator
                $T:X\rightarrow{Y}$ is compact if and only
                if for all bounded sequences
                $x:\mathbb{N}\rightarrow{X}$,
                $Tx$ has a congergent subsequence in $Y$
            \end{theorem}
            We're interested mainly in the case of $Y=X$,
            and when $T:X\rightarrow{X}$ is bounded in
            linear. This is the set of all operators
            $B(X,X)$. We rewrite this as $B(X)$. That is,
            $B(X)$ is the set of all bounded linear operators
            from $X$ to itself. Recall that if
            $Y$ is a Banach space, and if $X$ is a normed
            space, then $B(X,Y)$ is a Banach space. Thus,
            if $X$ is a Banach space, then $B(X)$ is a
            Banach space. But we can also multiply elements
            in $B(X)$ by using function composition.
            If $S,T\in{B(X)}$, then $ST$ is defined by
            $(ST)(x)=S(Tx)$. But then:
            \begin{equation*}
                \norm{(ST)x}=\norm{S(Tx)}
                \leq\norm{S}\norm{Tx}
                \rightarrow
                \norm{ST}\leq\norm{S}\norm{T}
            \end{equation*}
            A Banach space with such a multiplication
            property is called a Banach Algebra. The set of
            compact linear operators on $X$ is often denoted
            $C(X)$. Thus, $C(X)\subset{B(X)}$. It's
            a two-sided closed ideal in $B(X)$. That is,
            if $S,T\in{C(X)}$, and if $a$ and $b$ are scalars,
            then $aS+bT\in{C(X)}$, $ST\in{C(X)}$,
            and $TS\in{C(X)}$. Finally, if
            $F:\mathbb{N}\rightarrow{C(X)}$ is a sequence of
            compact operators, and if $F_{n}\rightarrow{T}$,
            then $T\in{C(X)}$.
            \begin{definition}
                Orthogonal Elements in an inner product
                space $(X,\langle\rangle)$ are elements
                $x,y\in{X}$ such that
                $\langle{x,y}\rangle=0$.
            \end{definition}
            \begin{definition}
                An orthonormal subset of an inner
                product space $(X,\langle\rangle)$
                is a subset $S\subset{X}$ such that for
                all $x,y\in{S}$ such that $x\ne{y}$,
                $\langle{x,y}\rangle=0$ and for all
                $x\in{S}$, $\norm{x}=1$.
            \end{definition}
            \begin{theorem}
                If $x\in{X}$ and
                $\varphi:\mathbb{N}\rightarrow{X}$
                is a sequence such that
                $A=\{\varphi_{n}:n\in\mathbb{N}\}$ is an
                orthonormal subset of $X$, then for all
                $x\in{X}$:
                \begin{equation*}
                    \norm{x}=
                    \sum_{n=1}^{N}\langle{x,\varphi_{n}}
                    \rangle^{2}
                    +\norm{x-\sum_{n=1}^{N}
                           \langle{x,\varphi_{n}}\rangle
                           \varphi_{n}}^{2}
                \end{equation*}
            \end{theorem}
            \begin{proof}
                If $m\in\mathbb{Z}_{N}$, then:
                \begin{equation*}
                    \langle{x-\sum_{n=1}^{N}
                            \langle{x,\varphi_{n}}\rangle
                            \varphi_{n},\varphi_{m}}\rangle
                    =\langle{x,\varphi_{m}}\rangle
                    -\sum_{n=1}^{N}\langle{x,\varphi_{n}}\rangle
                    \langle{\varphi_{n},\varphi_{m}}\rangle
                \end{equation*}
                But $A$ is an orthonormal subset of $X$,
                and thus if $n\ne{m}$ then
                $\langle{\varphi_{n},\varphi_{m}}\rangle=0$
                ad if $n=m$ then
                $\langle{\varphi_{n},\varphi_{m}}\rangle%
                 =\norm{\varphi_{n}}=1$. In terms of the
                Kronecker-Delta function,
                $\langle{\varphi_{n},\varphi_{m}}\rangle%
                 =\delta_{nm}$. So we have:
                \begin{equation*}
                    \langle{x-\sum_{n=1}^{N}
                            \langle{x,\varphi_{n}}\rangle
                            \varphi_{n},\varphi_{m}}\rangle
                    =0
                \end{equation*}
                But for all $N\in\mathbb{N}$,
                $x=(x-\sum_{n=1}^{N}%
                 \langle{x,\varphi_{n}}\rangle\varphi_{n})+%
                 \sum_{n=1}^{N}\langle{x,\varphi_{n}}\rangle%
                 \varphi_{n}$,
                and these two are orthogonal.
                Therefore, from Pythagoras:
                \begin{align*}
                    \norm{x}^{2}&=
                    \norm{x-\sum_{n=1}^{N}
                          \langle{x,\varphi_{n}}\varphi_{n}
                          \rangle}^{2}+
                    \norm{\sum_{n=1}^{N}
                          \langle{x,\varphi_{n}}\varphi_{n}
                          \rangle}^{2}\\
                    &=\sum_{n=1}^{N}\langle{x,\varphi_{n}}
                    \norm{\varphi_{n}}^{2}\rangle
                    +\norm{\sum_{n=1}^{N}
                    \langle{x,\varphi_{n}}\varphi_{n}
                    \rangle}^{2}
                \end{align*}
                But $\norm{\varphi_{n}}^{2}=1$ for all $n$.
                Therefore, etc.
            \end{proof}
            \begin{theorem}[Bessel's Inequality]
                $\sum_{n=1}^{N}\langle{x,\varphi_{n}}\rangle%
                 \leq\norm{x}^{2}$
            \end{theorem}
            \begin{example}
                $A=\{e_{n}:n\in\mathbb{N}\}$ is an orthonormal
                subset of $\ell^{2}$.
                $A=\{\sin(nt)/\sqrt{\pi}:n\in\mathbb{N}\}$
                is an orthonormal subset of
                $C[0,1]$ with the $L^{2}$ inner product.
            \end{example}
            \begin{definition}
                A basis of an innert product space $X$
                is an orthonormal subset $A\subset{X}$
                such that there is no orthonormal subset
                $B\subset{X}$ such that $A\subset{B}$.
            \end{definition}
            \begin{theorem}
                If $(X,\langle\rangle)$ is an inner product
                space, then there is an $A\subset{X}$
                such that $A$ is an orthonormal subset of $X$.
            \end{theorem}
            \begin{theorem}
                If $X$ is an inner product space, and if
                $\varphi:\mathbb{N}\rightarrow{X}$ is a sequence
                such that $S=\{\varphi_{n}:n\in\mathbb{N}\}$
                is a basis of $X$, then for all $x\in{X}$
                there is a sequence
                $a:\mathbb{N}\rightarrow\mathbb{R}$
                such that
                $x=\sum_{n=1}^{\infty}a_{n}\varphi_{n}$
            \end{theorem}
        \subsection{Summability}
            What does $\sum_{\alpha\in{A}}z_{\alpha}$
            if $z_{\alpha}\in\mathbb{R}$ for all
            $\alpha\in{A}$?
            \begin{definition}
                We say
                $\sum_{\alpha\in{A}}z_{\alpha}$ is summable
                to $z\in\mathbb{R}$ if for all
                $\varepsilon>0$ there is a subset
                $B\subset{A}$ such that, for all
                $C\subset{A}$ such that
                $B\subset{C}$,
                $|\sum_{\alpha\in{C}}z_{\alpha}-z|<\varepsilon$.
            \end{definition}
            It turns out that, if
            $\sum_{\alpha\in{A}}z_{\alpha}$ is summable, then
            only countably many $z_{\alpha}$ are non-zero.
            For all $n$, the set
            $\{z_{\alpha}:z_{\alpha}>1/n\}$ must be finite.
            The set of all non-zero elements is the
            union over all of these $n$, which is the
            countable union of finite sets, which is
            thus countable.
            \begin{theorem}
                If $(X,\langle\rangle)$ is an inner
                product space, if
                $\varphi:\mathbb{N}\rightarrow{X}$ is a
                sequence such that
                $S=\{\varphi_{n}:n\in\mathbb{N}\}$ is a
                basis of $X$, then:
                \begin{equation*}
                    x=\sum_{n=1}^{\infty}
                    \langle{x,\varphi_{n}}\rangle
                    \varphi_{n}
                \end{equation*}
            \end{theorem}
            \begin{example}
                If $L^{2}[0,\pi]$, let
                $\varphi_{n}(t)=\sin(nt)\sqrt{2/\pi}$. Then
                $A=\{\varphi_{n}(t):n\in\mathbb{N}\}$.
            \end{example}
    \section{Lecture 11: November 26, 2018}
        Let $T$ be a linear operator on a vector space
        $T$. We say $\lambda\in\mathbb{C}$ is an
        eigenvalue of $T$ if there exists $x\ne{0}$ in
        $X$ such that $Tx=\lambda{x}$. The corresponding
        $x$ is called the eigenvector or eigenfunction.
        We're interested in the case of compact
        self-adjoint operators $T$ on a Hilbert space
        $\mathscr{H}$.
        \begin{theorem}
            There is a sequence of real eigenvalues
            $\lambda_{n}$ of $T$, finite or infinite,
            such that $0$ is the only possible
            accumulation point of $\lambda_{n}$, and
            corresponding basis of
            orthogonormal eigenvectors $x_{n}$.
        \end{theorem}
        \begin{proof}
            We'll prove this in steps. First, either
            $\norm{T}$ or $-\norm{T}$ is an eigenvalue. This
            is because:
            \begin{equation*}
                \norm{T}=
                \underset{\norm{x}=1}{\sup}
                \{|\langle{Tx,x}\rangle|\}
            \end{equation*}
            This comes from the fact that
            $T=T^{*}$ for self-adjoint operators. Thus,
            either
            $\norm{T}=\langle{Tx,x}\rangle$ or
            $-\norm{T}=\langle{Tx,x}\rangle$. Choose a
            sequence $x_{n}$ such that
            $\norm{x_{n}}=1$ and
            $\langle{Tx_{n},x_{n}}\rangle%
             \rightarrow\pm\norm{T}$. By choosing a
            subsequence, we may assume that
            $Tx_{n}$ converges. We can not assume that
            $x_{n}$ converges, however. Thus,
            $Tx_{n}\rightarrow{y}$. Then:
            \begin{align*}
                \norm{Tx_{n}-\lambda{x}_{n}}^{2}
                &=\langle{Tx_{n}-\lambda{x}_{n},
                          Tx_{n}-\lambda{x}_{n}}\rangle\\
                &=\norm{Tx_{n}}^{2}
                -2\lambda\langle{Tx_{n},x}\rangle
                +\lambda^{2}\norm{x}^{2}\\
                &\leq
                \norm{T}^{2}\norm{x}^{2}
                -2\lambda\langle{Tx_{n},x}\rangle
                +\lambda^{2}\norm{x}^{2}
            \end{align*}
            And this converges to zero as $n$ tends to
            infinity. Thus,
            $Tx_{n}-\lambda{x}_{n}\rightarrow{0}$. It
            follows that
            $\lambda{x}_{n}=Tx_{n}-(Tx_{n}-\lambda{x}_{n})$,
            and this converges to $y$. Therefore
            $x_{n}\rightarrow{y}/\lambda$. Note that
            $\lambda$ is only equal to zero if
            $T$ is the zero operator. In this case the
            problem is trivial. Thus we may assume
            $\lambda\ne{0}$. Therefore
            $Tx_{n}\rightarrow{Ty}/\lambda$, but
            $Tx_{n}\rightarrow{y}$ as well. Thus
            $y=Ty/\lambda$. Let $\lambda_{1}=\lambda$
            and $\varphi_{1}=y/\norm{y}$. Then
            $\norm{\phi_{1}}=1$ and
            $T\varphi_{1}=\lambda_{1}\phi_{1}$. Moreover,
            let $T_{1}=T$ and let $H_{1}=\mathscr{H}$.
            Let $H_{2}=\{x\in{H}_{1}:x\perp\varphi_{1}\}$.
            Define $T_{2}:H_{2}\rightarrow{H}_{1}$ by
            $T_{2}x=T_{1}x=Tx$. This is the restriction
            of $T$ to $H_{2}$. Then
            $T_{2}H_{2}\subseteq{T}_{2}H_{2}$. For if
            $x\in{H}_{2}$, then
            $\langle{T}_{2}x,\varphi_{1}\rangle%
             \langle{T}_{1}x,\varphi_{1}\rangle$. But
            $T$ is self-adjoint, and
            $T_{1}=T$, and therefore
            $T_{1}$ is self-adjoint. But then:
            \begin{equation*}
                \langle{T}_{2}x,\varphi_{1}\rangle
                =\langle{T}_{1}x,\varphi_{1}\rangle
                =\langle{x},T_{1}\varphi_{1}\rangle
                =\lambda_{1}\langle{x}_{1},
                    \varphi_{1}\rangle
            \end{equation*}
            Therefore $T_{2}x\in{H}_{2}$, and therefore
            $T_{2}:H_{2}\rightarrow{H}_{2}$. $T_{2}$
            is self-adjoint, for:
            \begin{align*}
                \langle{T}_{2}x,y\rangle
                =\langle{T}_{1}x,y\rangle
                =\langle{x},T_{1}y\rangle
                =\langle{x},T_{2}y\rangle
            \end{align*}
            $T_{2}$ is compact since if $x_{n}$ is
            bounded in $H_{2}$, then it's bounded in
            $H_{1}$, and thus
            $T_{2}x_{n}=T_{1}x_{n}$, which has a convergent
            subsequence,
            and therefore $T_{2}$ is compact. $H_{2}$ is
            a subspace of $\mathscr{H}$ and is closed since
            $x_{n}\in{H}_{2}$ and
            $x_{n}\rightarrow{x}$ in $\mathscr{H}$ then:
            \begin{equation*}
                \langle{x},\varphi_{1}\rangle=
                \langle\lim{x}_{n},\varphi_{1}\rangle=
                \lim\langle{x}_{n},\varphi_{1}\rangle=
                0
            \end{equation*}
            Thus $H_{2}$ is closed and is therefore complete,
            and thus $H_{2}$ is a Hilbert space. As before
            there is a $\varphi_{2}$ and a $\lambda_{2}$
            such that $\varphi_{2}\in{H}_{2}$,
            $\norm{\varphi_{2}}=1$, and:
            \begin{equation*}
                T_{2}\varphi_{2}=\lambda_{2}\varphi_{2}
            \end{equation*}
            But then $T\varphi_{2}=\lambda_{2}\varphi_{2}$,
            where $\lambda_{2}=\pm\norm{T_{2}}$. Moreover,
            $|\lambda_{2}|<|\lambda_{1}|$. Continuing in
            this manner, let
            $H_{n}=\{x\in\mathscr{H}:%
             x\perp\varphi_{1},\hdots,\varphi_{n-1}\}$
            and $T_{n}:H_{n}\rightarrow{H}_{n}$ be the
            restriction of $T$ onto $H_{n}$. We obtain a
            $\varphi_{n}$ such that
            $\norm{\varphi_{n}}$ and
            $T\varphi_{n}=\lambda\varphi_{n}$. Moreover
            $|\lambda_{n}|\leq|\lambda_{n-1}|$. Thus,
            $|\lambda_{n}|$ forms a monotonically
            decreasing sequence and either there is an
            $N\in\mathbb{N}$ such that
            $\lambda_{N}=0$, in which case for all $n>N$,
            $\lambda_{n}=0$ as well, or for all
            $n\in\mathbb{N}$, $|\lambda_{n}|>0$. In the first
            case it is clear that $\lambda_{n}\rightarrow{0}$.
            In the second case we have
            $\lambda_{n}\varphi_{n}=T\varphi_{n}$ has
            a convergent subsequence for $T$ is compact.
            But $|\lambda_{n}|$ is a monotonically
            decreasing sequence bounder below by zero,
            and therefore converges. Let $c$ be the limit.
            Then $\lambda_{n}x_{n}\rightarrow{cx}$.
            Moreover $\norm{\varphi_{n}-\varphi_{m}}^{2}=2$
            since $\varphi_{n}$ and $\varphi_{m}$ are
            orthogonal when $n\ne{m}$. Therefore
            $\varphi_{n}$ is not a Cauchy sequence. Thus,
            for $\lambda_{n}\varphi_{n}$ to converge,
            $c=0$.
        \end{proof}
        \begin{theorem}[Hilbert-Schmidt Theorem]
            If $x\in\mathscr{H}$, then:
            \begin{equation*}
                Tx=\sum_{n=1}^{\infty}
                    \lambda_{n}\langle{x},\varphi_{n}\rangle
                    \varphi_{n}
            \end{equation*}
        \end{theorem}
        \begin{proof}
            For define $y_{m}$ as:
            \begin{equation*}
                y_{m}=x-\sum_{n=1}^{m-1}
                \langle{x},\varphi_{n}\rangle
                \varphi_{n}
            \end{equation*}
            Then:
            \begin{equation*}
                \langle{y}_{m},\varphi_{k}\rangle
                =\langle
                x-\sum_{n=1}^{m-1}\rangle{x},\varphi_{n}
                \rangle\varphi_{n},\varphi_{k}\rangle
                =\langle{x},\varphi_{k}\rangle-
                \sum_{n=1}^{m-1}\langle{x},\varphi_{n}
                \rangle\langle\varphi_{n},\varphi_{k}\rangle
                =0
            \end{equation*}
            If $\lambda_{N}=0$, then $Ty=T_{n}y=0$ by
            setting $m=N$. Thus:
            \begin{equation*}
                0=Ty
                =Tx-\sum_{n=1}^{N-1}
                \langle{x},\varphi_{n}\rangle{T}\varphi_{n}
                =Tx-\sum_{n=1}^{N-1}
                \lambda_{n}\langle{x},\varphi_{n}\rangle
                \varphi_{n}
            \end{equation*}
            If $\lambda_{n}\ne{0}$ for all $n\in\mathbb{N}$,
            then $y_{m}\in{H}_{m}$ and therefore:
            \begin{equation*}
                x=(x-y_{m})+y_{m}
            \end{equation*}
            But $x-y_{m}$ is orthogonal to $y_{m}$, and
            therefore by Pythagoras:
            \begin{equation*}
                \norm{x}^{2}=
                \norm{x-y_{m}}^{2}+\norm{y_{m}}^{2}
            \end{equation*}
            Therefore $\norm{y_{m}}\leq\norm{x}$. Also:
            \begin{equation*}
                \norm{Ty_{m}}=\norm{T_{m}y_{m}}
                \leq\norm{T_{m}}\norm{y_{m}}
                =|\lambda_{m}|\norm{y_{m}}
            \end{equation*}
            And therefore:
            \begin{equation*}
                \norm{Tx-\sum_{n=1}^{m-1}
                \lambda_{n}\rangle{x},\varphi_{n}\rangle
                \varphi_{n}}\leq|\lambda_{m}|\norm{x}
                \rightarrow{0}
            \end{equation*}
        \end{proof}
        \begin{theorem}
            If $T$ is a compact self-adjoint operator
            on a Hilbert Space $\mathscr{H}$, then there is
            an orthogonal basis for $\mathscr{H}$
            consisting of eigenvector of $T$.
        \end{theorem}
        \begin{proof}
            For any $x\in\mathscr{H}$:
            \begin{equation*}
                Tx=\sum_{n=1}^{\infty}\lambda_{n}
                \rangle{x},\varphi_{n}\rangle\varphi_{n}
            \end{equation*}
            This sum may be infinite.
            Let $\{\psi_{\alpha}\}_{\alpha\in{A}}$ be
            and orthogonal basis of $\mathrm{ker}(T)$. Then
            $T\psi_{\alpha}=0$ for all $\alpha\in{A}$,
            and thus $0$ is an eigenvalue for all
            $\psi_{\alpha}$. But also:
            \begin{equation*}
                \lambda_{n}\langle\varphi_{n},
                \psi_{\alpha}\rangle
                =\langle\lambda_{n}\varphi_{n},
                \psi_{\alpha}\rangle
                =\langle{T}\varphi_{n},\psi_{\alpha}\rangle
                =\langle\varphi_{n},T\psi_{\alpha}\rangle
                =0
            \end{equation*}
            Thus, for all $\alpha\in{A}$ and all
            $n\in\mathbb{N}$,
            $\varphi_{n}\perp\psi_{\alpha}$. Then by
            Hilbert-Schmidt, for every $x\in\mathscr{H}$:
            \begin{equation*}
                T\big(x-\sum_{n=1}^{\infty}
                \langle{x},\varphi_{n}\rangle\varphi_{n}\big)
                =Tx-\sum_{n=1}^{\infty}\lambda_{n}
                \langle{x},\varphi_{n}\rangle\varphi_{n}=0
            \end{equation*}
            Thus:
            \begin{equation*}
                x=\sum_{n=1}^{\infty}\lambda_{n}
                \langle{x},\varphi_{n}\rangle\varphi_{n}+
                \sum_{\alpha\in{A}}
                \langle{x},\psi_{\alpha}\rangle\psi_{\alpha}
            \end{equation*}
        \end{proof}
\chapter{More Stuffs}
    \section{Lecture 12: December 3, 2018}
        Cauchy-Schwarz says that:
        \begin{equation}
            \Big(\int_{a}^{b}x(s)y(s)\diff{s}\Big)^{2}
            \leq\Big(\int_{a}^{b}x^{2}(s)\diff{s}\Big)
            \Big(\int_{a}^{b}y^{2}(s)\diff{s}\Big)
        \end{equation}
        So:
        \begin{align}
            |Tx(t)|^{2}&=
            \Big(\int_{0}^{t}x(s)\diff{s}\Big)^{2}\\
            &\leq\Big(\int_{0}^{t}1^{2}\diff{s}\Big)
            \Big(\int_{0}^{t}x(s)^{2}\diff{s}\Big)\\
            &=t\int_{0}^{t}x(s)^{2}\diff{s}\\
            &\leq{t}\int_{0}^{1}x(s)^{2}\diff{s}\\
            &=t\norm{x}_{2}^{2}
        \end{align}
        So then:
        \begin{equation}
            \int_{0}^{1}Tx(t)^{2}\diff{t}
            \leq\int_{0}^{1}t\norm{x}_{2}^{2}\diff{t}
        \end{equation}
        This then implies:
        \begin{align}
            \norm{Tx}^{2}&\leq
            \frac{1}{2}\norm{x}_{2}^{2}\\
            \Rightarrow
            \norm{T}&\leq\frac{1}{\sqrt{2}}
        \end{align}
        Letting $x(t)=1$, we have $Tx=t$. Thus:
        \begin{equation}
            \norm{Tx}^{2}=\int_{0}^{1}t^{2}\diff{t}
            =\frac{1}{3}
        \end{equation}
        And thus $\norm{Tx}=1/\sqrt{3}$. So
        $1/\sqrt{3}\leq\norm{T}\leq{1}/\sqrt{2}$.
        If $x(t)=1-t$, then $Tx(t)=t-t^{2}/2$. So
        $\norm{Tx}=\sqrt{2/15}$. We're getting closer to
        the answer. Letting $x(t)=\cos(\pi{t}/2)$ gives
        us the norm. We now have to show this. Write:
        \begin{equation*}
            x(t)=\sum_{n=1}^{\infty}b_{n}\sin(n\pi{t})
        \end{equation*}
        Then:
        \begin{equation*}
            Tx(t)=\sum_{n=1}^{\infty}
            \frac{b_{n}}{n\pi}\big[1-\cos(n\pi{t})\big]
            =\Big(\sum_{n=1}^{\infty}\frac{b_{n}}{n\pi}\Big)
            -\Big(\sum_{n=1}^{\infty}\frac{b_{n}}{n\pi}
            \cos(n\pi{t})\Big)
        \end{equation*}
        But:
        \begin{align*}
            \norm{x}^{2}&=\sum_{n=1}^{\infty}\frac{b_{n}^{2}}{2}
            &
            \norm{Tx}^{2}&=
            \Big(\sum_{n=1}^{\infty}\frac{b_{n}}{n\pi}\Big)^{2}
            +\sum_{n=1}^{\infty}\frac{b_{n}^{2}}{2n^{2}\pi^{2}}
        \end{align*}
        Let's maximize $\norm{Tx}^{2}$ subject to
        $\norm{x}^{2}$. Using Lagrange multipliers we get:
        \begin{equation*}
            \frac{2}{n\pi}\Big(\sum_{k=1}^{\infty}
            \frac{b_{k}}{k\pi}\Big)
            +\frac{b_{n}}{n^{2}\pi^{2}}
            =\lambda^{2}b_{n}
        \end{equation*}
        Letting $A=\sum_{k=1}^{\infty}b_{k}/k\pi$, we obtain:
        \begin{equation*}
            b_{n}=\frac{2n\pi}{\lambda^{2}n^{2}\pi^{2}-1}A
        \end{equation*}
        So then:
        \begin{equation*}
            A=\sum_{n=1}^{\infty}
            \frac{2}{\lambda^{2}n^{2}\pi^{2}-1}A
        \end{equation*}
        And thus:
        \begin{equation*}
            \sum_{n=1}^{\infty}
            \frac{2}{\lambda^{2}n^{2}\pi^{2}-1}=1
        \end{equation*}
        And this is the expansion of cotangent:
        \begin{equation*}
            1-\frac{1}{\lambda}\cot\big(\frac{1}{\lambda}\big)
            =1
        \end{equation*}
        And therefore:
        \begin{equation*}
            \lambda=\frac{\pi}{2},\frac{3\pi}{2},\frac{5\pi}{2},
            \hdots
        \end{equation*}
        Finally:
        \begin{equation*}
            x(t)=\sum_{n=1}^{\infty}
            b_{n}\sin(n\pi{t})=
            \sum_{n=1}^{\infty}
            \frac{2n\pi}{4n^{2}-1}A\sin(n\pi{t})
            =\sqrt{2}\cos\big(\frac{\pi}{2}t\big)
        \end{equation*}
        But before we can do all of this we need to show
        that there is such a maximum. That is, the step that
        involves Lagrange multipliers is valid. We want
        an $x$ such that $\norm{x}=1$ and
        $\norm{Tx}=\norm{T}$. Certainly there is a sequence
        such that $\norm{x_{n}}=1$ and
        $\norm{Tx_{n}}\rightarrow\norm{T}$. Since $T$ is
        compact we may assume, taking subsequences as
        necessary, that
        $Tx_{n}\rightarrow{y}$. If $T$ is self adjoint
        and $x_{n}\rightarrow{x}$, then
        $Tx=y$. It would be nice if Bolzano-Weiestrass worked
        and we could say
        $\norm{x_{n}}$ bounded implies that
        $x_{n}\rightarrow{x}$, but this is not always
        true in infinite dimensions. We'll need to
        weaken our notion of convergence for this. We
        could simply say that everything converges to
        everything, which is the chaotic topology, but
        this is not very useful for it loses uniqueness.
        \begin{definition}
            A weakly convergent sequence in an inner product
            space $X$ is a sequence
            $x:\mathbb{N}\rightarrow{X}$ such that there
            is an $a\in{X}$ such that
            for all $z\in{X}$,
            $\langle{x_{n},z}\rangle\rightarrow%
             \langle{a,z}\rangle$. We write
             $x_{n}\overset{w}{\rightarrow}{a}$
        \end{definition}
        For example, if $X$ is a Hilbert space and
        $e_{n}$ is an orthonormal basis, then
        $e_{n}\rightarrow{0}$ since, for all $z$:
        \begin{equation*}
            \norm{z}^{2}=\sum_{n=1}^{\infty}
            \langle{e_{n},z}\rangle^{2}
        \end{equation*}
        And thus $\langle{e_{n},z}\rangle\rightarrow{0}$.
        But $\langle{0,z}\rangle=0$, so
        $e_{n}\overset{w}{\rightarrow}{0}$. Normal convergence
        is also called strong convergence. Strong convergence
        implies weak convergence. For if
        $x_{n}\rightarrow{a}$, then
        $\langle{a-x_{n},z}\rangle\rightarrow{0}$ for all
        $z$, and thus $x_{n}\overset{w}{\rightarrow}{a}$.
        Moreover, weak limits are unique. If
        $T$ is a bounded linear operator on a Hilbert space
        $H$, and if $x_{n}$ converges weakly to $a$,
        then $Tx_{n}\overset{w}{\rightarrow}{Ta}$.
        If $x$ converges weakly to $a$ and if $T$ is a
        compact linear operator on $H$, then
        $Tx_{n}$ converges strongly to $Tx$.
        \begin{theorem}
            If $H$ is a Hilbert space,
            $T$ is a compact operator on $H$,
            and if $x:\mathbb{N}\rightarrow{H}$ is a
            weakly convergent sequence such that
            $x_{n}\overset{w}{\rightarrow}{a}$,
            then the sequence
            $y:\mathbb{N}\rightarrow{H}$ defined by
            $y_{n}=Tx_{n}$ is such that
            $y_{n}\rightarrow{Tx}$. That is,
            $Tx_{n}$ converges strongly to $Ta$.
        \end{theorem}
        \begin{proof}
            For suppose not. As $T$ is compact and
            $x_{n}$ is bounded, there is
            a convergent subsequence. Let $a_{1}$ be the
            limit. If the limit is not unique then there
            is another convergent subsequence with
            a different limit $a_{2}$. But
            $Tx_{n}$ converges weakly to
            $a$, and strong convergence implies weak
            convergence. Therefore $a_{1}=a_{2}=a$, a
            contradiction. Therefore, $Tx_{n}$ converges
            strongly to $Ta$.
        \end{proof}
        \begin{theorem}
            If $x_{n}\overset{w}{\rightarrow}{a}$
            and $\norm{x_{n}}\rightarrow{C}$,
            then $\norm{a}\leq{C}$.
        \end{theorem}
        \begin{proof}
            For:
            \begin{align*}
                \norm{a}^{2}&=|\langle{a,a}\rangle|\\
                &=\underset{n\rightarrow\infty}{\lim}
                |\langle{x_{n},a}\rangle|\\
                &\leq\underset{n\rightarrow\infty}{\lim}
                \norm{x_{n}}\norm{a}\\
                &=C\norm{a}
            \end{align*}
            Dividing by $\norm{a}$ gives the result.
        \end{proof}
        We now prove that there exists $x$ such that
        $\norm{x}=1$ and $\norm{Tx}=\norm{T}$. This works
        for any compact linear operator on a Hilbert space.
        We can always find a sequence, by definition, such
        that $\norm{x_{n}}=1$ and
        $\norm{Tx_{n}}\rightarrow\norm{T}$. But since
        $x_{n}$ is bounded by 1 there is a weakly
        convergent subsequence (Still to be proved).
        This is ``Bolzano-Weierstrass,'' of infinite
        dimensions. Then $x_{n}$ converges weakly to
        $x$, and thus $Tx_{n}$ converges strongly to
        $Tx$. Then $\norm{Tx}=\norm{T}$. Finally,
        $\norm{x}\leq\lim\norm{x_{n}}=1$, and
        $\norm{T}=\norm{Tx}\leq\norm{T}\norm{x}$,
        so $\norm{x}\geq{1}$, and therefore
        $\norm{x}=1$. Let's show $T:L^{2}\rightarrow{L}^{2}$
        defined by $Tx(t)=\int_{0}^{t}x(s)\diff{s}$ is
        compact. Suppose $x_{n}$ is bounded in $L^{2}$ with
        bound $M$. That is, $\norm{x_{n}}\leq{M}$.
        Then:
        \begin{align*}
            |Tx_{n}(t)|^{2}&=
            \Big|\int_{0}^{t}x_{n}(s)^{2}\diff{s}\Big|^{2}\\
            &\leq
            \Big(\int_{0}^{1}|x_{n}(s)|\diff{s}\Big)^{2}\\
            &\leq\Big(\int_{0}^{1}\diff{s}\Big)
            \Big(\int_{0}^{1}|x_{n}(s)|^{2}\diff{s}\Big)\\
            &=\norm{x_{n}}^{2}\\
            &\leq{M}^{2}
        \end{align*}
        Taking the supremum over $t\in[0,1]$ gives:
        \begin{equation*}
            \norm{Tx_{n}}_{\infty}\leq{M}
        \end{equation*}
        So $Tx_{n}$ is bounded in $C[0,1]$. Also, if
        $0\leq{t}_{1}$ and $t_{2}\leq{1}$, then:
        \begin{align*}
            |Tx_{n}(t_{2})-Tx_{n}(t_{1})|^{2}
            &\leq\Big(\int_{t_{1}}^{t_{2}}|x_{n}(s)|
            \diff{s}\Big)^{2}\\
            &\leq\Big(\int_{t_{1}}^{t_{2}}\diff{s}\Big)
            \Big(\int_{t_{1}}^{t_{2}}|x_{n}(s)|^{2}
            \diff{s}\Big)\\
            &=(t_{2}-t_{1})\int_{t_{1}}^{t_{2}}
            |x_{n}(s)|^{2}\diff{s}\\
            &\leq(t_{2}-t_{1})\norm{x_{n}}^{2}\\
            \leq{M}^{2}(t_{2}-t_{1})
        \end{align*}
        So $|Tx_{n}(t_{2})-Tx_{n}(t_{1})|$ can be made
        arbitrarily small for $t_{2}$ and $t_{1}$ close enough,
        independent on the $n$. That is, a
        $\delta$ may be chosen independent of $n$. This is
        the criterion for equicontinuity.
        The compactness of $[0,1]$ then gives uniform
        equicontinuity. Arzela-Ascoli then says there is a
        subsequence $Tx_{n}\rightarrow{y}$,
        with $y\in{c}[0,1]$. That is,
        $\norm{Tx-y}_{\infty}\rightarrow{0}$. But then:
        \begin{align*}
            \norm{Tx_{n}-y}^{2}
            &=\int_{0}^{1}|Tx_{n}(t)-y(t)|^{2}\diff{t}\\
            &\leq\int_{0}^{1}
            \norm{Tx_{n}-y}_{\infty}^{2}\diff{t}\\
            =\norm{Tx_{n}-y}_{\infty}^{2}
        \end{align*}
        And this converges to zero. Thus,
        $Tx_{n}\rightarrow{y}$.
        \begin{theorem}[Baire Category Theorem]
            If $(X,d)$ is a complete metric and
            $C_{n}$ is a sequence of closed sets such
            that $X=\cup_{n=1}^{\infty}C_{n}$, then
            there is an $N\in\mathbb{N}$ such that
            $C_{N}$ contains an open subset.
        \end{theorem}
        \begin{proof}
            Let $r_{1}\in(0,1)$, $x_{1}\in{X}$.
            If $B_{r_{1}}(X_{1})\subset{C_{1}}$ then we're
            done. Otherwise $B_{r_{1}}(x_{1})\setminus{C_{1}}$
            is a non-empty set, so there exists
            $x_{n}\in{B}_{r_{2}}(x_{1})$, where
            $r_{2}\in(0,1/2)$. By induction, choose
            $r_{n}\in(0,1/n)$ and $x_{n}$ such that
            $x_{n}\in{B}_{r_{n-1}}(x_{n-1})$ and
            $\overline{B_{r_{n}}(x_{n})}\subset%
             B_{r_{n-1}}(x_{n-1})$. For $n<m$,
            $x_{m}\in\overline{B_{r_{n}}(x_{n})}$,
            so $d(x_{n},x_{m})<1/n$. Then $x_{n}$ is
            Cauchy, but $X$ is complete so there is a limit
            $x$. Since $\overline{B_{r_{n}}(x_{n})}$ is
            closed, $x\in\overline{B_{r_{n}}(x_{n})}$. But
            $X=\cup_{n=1}^{\infty}C_{n}$ and thus there is
            an $N$ such that $x\in{N}$. But then
            $B_{r_{n}}(x)\subset{C}_{N})$, so $C_{N}$ contains
            an open subset.
        \end{proof}
        The Baire Category Theorem is used to prove the
        Uniform Boundedness Theorem, which is also called
        the Banach-Steinhau theorem.
        \begin{theorem}[Uniform Boundedness Theorem]
            If $H$ is a Hilbert space, and if
            $x_{n}\overset{w}{\rightarrow}{a}$, then
            $\norm{x_{n}}$ is bounded.
        \end{theorem}
        \begin{proof}
            Let
            $C_{k}=\{y\in{H}:|\langle{x_{n},y}\rangle\leq{k}\}$.
            Then $C_{k}$ is closed since $y_{j}\in{C_{k}}$
            and $y_{j}\rightarrow{y}$ implies that:
            \begin{align*}
                |\langle{x_{n},y_{j}}\rangle|&\leq{k}\\
                \Rightarrow
                \underset{j\rightarrow\infty}{\lim}
                |\langle{x_{n},y_{j}}\rangle|&\leq{k}\\
                \Rightarrow
                |\langle{x_{n},y}\rangle|&\leq{k}
            \end{align*}
            Moreover $H=\cup_{k=1}^{\infty}C_{k}$. By
            the Baire Category Theorem there is a
            $k\in\mathbb{N}$ such that
            $C_{k}$ contains an open subset. Let $z_{0}\in{H}$
            and $r\in\mathbb{R}$ be such that
            $B_{r}(z_{0})\subset{C_{k}}$. Let
            $y\in{H}$, $y\ne{0}$, and set $z=z_{0}+\alpha{y}$.
            where:
            \begin{equation*}
                \alpha=\frac{r}{2\norm{y}}
            \end{equation*}
            Then $\norm{z-z_{0}}<r$, so
            $z\in{C}_{k}$. That is,
            $|\langle{x_{n},z}\rangle|\leq{k}$ for all
            $n$. Thus we have:
            \begin{align*}
                |\langle{x_{n},y}\rangle|&=
                |\langle{x_{n},\frac{z-z_{0}}{\alpha}}\rangle|\\
                &\leq
                \frac{1}{\alpha}\Big(
                |\langle{x_{n},z\rangle}|+
                |\langle{x_{n},z_{0}}\rangle|\\
                &\leq
                \frac{1}{\alpha}(k+k)\\
                &=\frac{4k}{r}\norm{y}
            \end{align*}
            This is true of any $y\in{H}$. Choosing $y=x_{n}$,
            we get:
            \begin{equation*}
                \norm{x_{n}}^{2}\leq\frac{4k}{r}\norm{x_{n}}
            \end{equation*}
            Dividing by $\norm{x_{n}}$ shows boundedness.
        \end{proof}
    \section{Lecture 13: December 10, 2018}
        \begin{equation}
            Tx(t)=\int_{0}^{1-t}x(s)\diff{s}
        \end{equation}
        If $\norm{x}_{2}\leq{M}$, then:
        \begin{equation}
            |Tx(t)|^{2}=
            \Big|\int_{0}^{1-t}x(s)\diff{s}\Big|^{2}
            \leq\Big(\int_{0}^{1-t}\diff{s}\Big)
            \Big(\int_{0}^{1-t}x(s)^{2}\diff{s}\Big)
            =(1-t)\norm{x}_{2}^{2}
        \end{equation}
        So we have:
        \begin{equation}
            \norm{Tx}^{2}=\int_{0}^{1}Tx(t)^{2}\diff{t}
            \leq\int_{0}^{1}(1-t)\norm{x}_{2}^{2}\diff{t}
            =\frac{1}{2}\norm{x}_{2}^{2}
            \leq\frac{1}{2}M^{2}
        \end{equation}
        So:
        \begin{align}
            |Tx(t_{1})-Tx(t_{2})|^{2}
            &=\Big|\int_{0}^{1-t_{1}}x(s)\diff{s}-
            \int_{0}^{1-t_{2}}x(s)\diff{s}\Big|^{2}\\
            &=\Big|\int_{1-t_{2}}^{1-t_{1}}x(s)
            \diff{s}\Big|^{2}\\
            &\leq\int_{1-t_{2}}^{1-t_{1}}\diff{s}
            \int_{1-t_{2}}^{1-t_{1}}x(s)^{2}\diff{s}\\
            &=|t_{1}-t_{2}|\norm{x}_{2}^{2}\\
            &\leq{M}^{2}|t_{1}-t_{2}|
        \end{align}
        This shows equicontinuity, and thus Arzela-Ascoli
        shows that $T$ is compact.
        \begin{theorem}[Banach-Alaoglu-Hilbert Theorem]
            If $H$ is a Hilbert space and
            $x:\mathbb{N}\rightarrow{H}$ is a bounded sequence,
            then there is a weakly convergent subsequence.
        \end{theorem}
        The next question would be ``What about Banach Space?''
        If $X$ is a normed space, we say $x_{n}$ converges
        weakly to $x$, denoted
        $x_{n}\overset{w}{\rightarrow}{x}$ if for all
        bounded linear functional $f\in{X'}$,
        $f(x_{n})\rightarrow{f(x)}$.
        For example let $X=\ell^{1}$. We have proven that the
        dual of $\ell^{1}$ is $\ell^{\infty}$ and elements
        of the dual take the form:
        \begin{equation}
            f(x)=\sum_{n=1}^{\infty}z_{i}x_{i}
        \end{equation}
        Where $z_{i}\in\ell^{\infty}$. Thus, $z_{i}$ is
        bounded and $x_{i}$ is absolutely convergent,
        since $x_{i}\in\ell^{1}$, and thus the product
        is absolutely convergent. That is,
        $x_{i}z_{i}\in\ell^{1}$. The problem with this is
        that we don't know that $X'\ne\{0\}$ for a given
        Banach space. There are plenty these, enough to
        separate points, thanks to the Hahn-Banach theorem.
        This says that if $f$ is a bounded linear functional
        on a subspace $M$ of $X$, then there exists
        $F\in{X'}$ such that
        $\norm{F}_{X'}=\norm{f}_{M'}$ and
        $F(x)=f(x)$ for all $x\in{M}$. So given $m\in{M}$,
        then $\alpha{m}\in{M}$ for al $\alpha\in\mathbb{R}$.
        Define $f(m)=k$, for some $k\in\mathbb{R}$. Then
        $f(\alpha{m})=\alpha{k}$ and
        $|f(\alpha{m})|=|\alpha||f(m)|%
         =|k|\norm{\alpha{m}}/\norm{m}$. And
         $|f(\alpha{m})|/\norm{\alpha{m}}=|k|/\norm{m}$.
        Thus $f\in{M'}$. Thus, Hahn-Banach can be exteneded
        to all of $X$. So we can for all $x\in{X}$ and for
        all $r\in\mathbb{R}$, there is a bounded linear
        function $f\in{X'}$ such that
        $f(x)=r$. If $m_{1}$ and $m_{2}$ are independent
        (That is, $m_{1}\ne\alpha{m}_{2}$ for any real
        number $\alpha$), then let
        $M=\{am_{1}+bm_{2}:a,b\in\mathbb{R}\}$. Define
        $f(am_{1}+bm_{2})=a\norm{m_{1}}$.
        Then $f\in{M'}$ so this
        can be extended to all of $X$ by the Hahn-Banach
        theorem. But $f(m_{1})=1$ and
        $f(m_{2})=0$, so $f$ separates points. Thus,
        if $x_{n}$ converges weakly to $x$ in a
        Banach space, and if $x_{n}$ also converges weakly
        to $y$, then $x=y$. The uniform boundedness theorem
        also holds if $X$ is complete. The
        Banach-Alaoglu-Hilbert theorem fails in a general
        Banach space. For example, $\ell^{1}$. We now talk
        about weak* convergence. In $X'$, we say
        $f_{n}$ converges weak* to $f$ if
        $f_{n}(x)\rightarrow{f(x)}$ for all
        $x\in{X}$. Banach-Alaoglu holds if weak is replaced
        with weak* and if $X'$ is separable. The double
        dual, $X''$, is the dual of $X'$. $X$ is
        embedded in $X''$. That is, $X$ embeds naturally
        in $X''$ as follows: Define
        $C:X\rightarrow{X''}$ as follows. If $x\in{X}$,
        $Cx(f)=f(x)$ for all $f\in{X'}$. It's easy to
        show that $\norm{cx}=\norm{x}$. Indeed,
        $C$ is an isometry on $X$ to $X''$. If $C$ is
        onto, we say that $X$ is reflexive. There are
        Banach spaces that are not reflexive that can
        be isometrically embedded into their second dual,
        but the canonical map is not such an embedding.
        \begin{theorem}
            If $H$ is a Hilbert space, then $H$ is reflexive.
        \end{theorem}
        \begin{theorem}
            If $X$ is reflexive, then
            $X=X''$.
        \end{theorem}
        \begin{theorem}
            If $X$ is reflexive, weak* convergence
            implies weak convergence.
        \end{theorem}
        From topology, a subbasis for a topological space
        is a collection of sets such that every open set can
        be written as arbitrary unions and finite intersections
        of the sets. Choose as the subbasis:
        \begin{equation}
            \{f^{-1}(-\infty,a):a\in\mathbb{R},f\in{X'}\}
        \end{equation}
        If $X'$ is separable, then this space is metrizable.
        For let $A$ be a countable dense subset, and define
        the metric $d$ as:
        \begin{equation}
            d(f,g)=\sum_{x\in{A}}
            \frac{|f(x)-g(x)|}{1+|f(x)-g(x)|}2^{-n}
        \end{equation}
        \begin{theorem}[Banach-Alaoglu]
            If $X$ is a normed vector space, and if
            $\tau$ is the weak* topology, then
            $\overline{B_{1}(0)}$ is a compact subset of
            $(X',\tau)$.
        \end{theorem}
        Adams Sobolev Spaces.
\chapter{Old Notes}
    \section{Summary of Lectures}
        The boundary of a circle
        in $\mathbb{R}^{2}$ is nowhere dense,
        with respect to the metric on $\mathbb{R}^{2}$.
        Any open ball about any point on the circle contains
        points not on the circle, and thus it has empty
        interior. Something about $\varepsilon$ nets.
        \subsection{Normed Spaces and Banach Spaces}
            There are notions of subspace,
            linear combination, independence, spanning,
            dimension, basis, Hamel basis, and
            \textit{convexity}.
            Open and closed balls are convex.
            A subspace of a Banach space is complete
            iff closed. Schauder basis.
            A Schauder basis implies separable.
            If $\{x_{1},\hdots,x_{n}\}$ is independent,
            then there exists a $c>0$ such that, for all
            $\boldsymbol{\alpha}$,
            $|\boldsymbol{\alpha}\cdot\mathbf{x}|%
             \geq{c}\norm{\boldsymbol{\alpha}}$.
            Finite dimensional subspaces are complete,
            as are closed subspaces. In finite dimensional
            normed spaces, a space is compact if and only
            if it is closed and bounded.
            Riesz's Lemma says that if $Z$ is a subspace
            of a normed space $X$, and if $Y$ is a proper
            closed subspace of $Z$, then there is a $z\in{Z}$
            such that $\norm{z}=1$ and $D(z,Y)\geq{1/2}$.
            A corollary of this is that $B_{1}(0)$ is compact
            if and only if $X$ is finite dimensional.
        \subsection{Linear Operators}
            Identity, zero, differentiation,
            and integration. Domain/Range
            of a linear operator, the null space.
            Inverse of a linear operator is linear.
            $(ST)^{-1}=T^{-1}S^{-1}$.
            In finite dimension all linear operators are
            continuous. An operator is bounded
            if and only if it is continuous. If a linear
            operator is continuous at some point, then it
            is continuous everywhere. An operator is bounded
            if and only if its null space is closed.
            There is something called the extension of a
            bounded linear operator. $B(X,Y)$ is the set of
            bounder linear operators
            from $X$ to $Y$. This is complete if and only if
            $Y$ is complete. A functional is a mapping from a
            vector space $X$ into the real numbers $\mathbb{R}$.
            For continuous linear functional, continuity at
            $0$ implies continuity everywhere.
            There is something called the dual space
            $X'$, which is itself a Banach space.
        \subsection{Inner Product and Hilbert Spaces}
            If $x_{n}\rightarrow{x}$ and $y_{n}\rightarrow{y}$,
            then
            $\langle{x_{n},y_{n}}\rangle\rightarrow\langle{x,y}\rangle$.
            There's a notion of orthogonal sets,
            and orthonormality. If $(e_{n})$ is orthonormal basis,
            then $x=\sum\langle{x,e_{k}}\rangle{e_{k}}$
            for all $x$. Gram-Schmidt procedure.
            $\sum\alpha_{k}e_{k}$ converges if and only if
            $\sum|\alpha_{k}|^{2}$ converges.
            A set $M$ is total in a Hilbert space $H$ is the
            span of the closure of $M$ is equal
            to $H$. If $M$ is complete, then it is
            total if and only if $M^{\perp}=0$.
            Parseval's theorem. Legendre, Hermite, and Laguerre
            polynomials are things.
            Self adjoint, unitary, and normal operators.
            $T^{*}=T$, $T^{*}=T^{-1}$, and $T^{*}T=TT^{*}$. If
            $X$ is a vector space over the complex numbers,
            and if $T$ is self adjoint, then
            $\langle{Tx,x}\rangle$ is a real number for all $x$.
        \subsection{Compact Linear Operators}
            If $T$ is compact and linear, then it is bounded and
            continuous. An operator is compact and linear
            if and only if
            for all bounded sequences $x_{n}$,
            $Tx_{n}$ has a convergent subsequence. Compact linear
            operators form a vector space. The rank of an
            operator is
            the dimension of its image. If $T$ is linear,
            bounded, and of finite rank, then it is compact.
            If $T_{n}$ is a sequence of compact linear
            operators, if $Y$ is
            complete, and if $\norm{T_{n}-T}\rightarrow{0}$, then
            $T$ is compact. A sequence $x_{n}$ converges weakly to
            $x$ if, for all $y$,
            $\langle{x_{n},y}\rangle\rightarrow\langle{x,y}\rangle$.
            If $x_{n}$ converges weakly to $x$, then
            and if $T$ is a compact linear operator, then
            $Tx_{n}\rightarrow{Tx}$. If $H$ is a Hilbert space,
            $T$ is a compact self-adjoint operator, and if
            $x_{n}$ converges weakly to $x$, then
            $\langle{Tx_{n},x_{n}}\rangle\rightarrow\langle{Tx,x}\rangle$.
            If $T:H\rightarrow{H}$ is compact and linear, then so
            is its adjoint. The Hilbert-Schmidt theorem says that
            compact self-adjoint operators on a Hilbert space $H$
            have an orthonormal basis of eigenvectors. All of this
            has applications to integral operators and
            Sturm-Liouville Theory.
        \subsection{Fundamental Theorems}
            Zorn's Lemma. Hahn-Banach Theorem. Sublinear functionals.
            If $X$ is a normed space, and $Z$ is a subspace, and if
            $f\in{Z'}$, then $f$ be extended to $X$ such that
            $\norm{f}_{X}=\norm{f}_{Z}$.
            This extends Hilbert spaces by Riesz.
            If $X$ is a normed space
            and $x\ne{0}$, then there is an $f\in{X'}$ such that
            $\norm{f}=1$ and $f(x_{0})=\norm{x_{0}}$.
            For all $x$,
            $\norm{x}=\sup\{\norm{f(x)}/\norm{f}:f\in{X'},f\ne{0}\}$.
            There's a thing called bounded variation.
            If $x\in{X}$ and
            $g_{x}(f)=f(x)$ for $f\in{X'}$, then
            $g_{x}\in{X''}$ and $\norm{g_{x}}=\norm{x}$.
            Reflexive implies complete.
            Finite and Hilbert implies reflexive.
            $X'$ separable implies $X$ is separable.
            $X$ separable and reflexive implies
            $X'$ is separable.
            Strong convergence implies weak convergence.
            The converse is not true. If $X$ is finite
            dimensional, then weak convergence
            implies strong convergence. Weak convergence implies
            $\norm{x_{n}}$ is bounded. If
            $x_{n}\rightarrow{x}$ weakly, and if
            $\norm{x_{n}}\rightarrow\norm{x}$, then
            $x_{n}\rightarrow{x}$ strongly.
            Open mapping theorem.
            Closed graph theorem.
            Differentiation is a closed operator on
            $C^{1}[a,b]\rightarrow{C[a,b]}$.
\chapter{Normed Vector Spaces}
    \section{Basic Definitions}
        \begin{ldefinition}{Normed Vector Spaces}
              {Funct_Analysis_Normed_Vector_Space}
            A normed vector space over a field
            $\mathbb{F}\subseteq\mathbb{C}$, denoted
            $(V,\norm{\cdot})$ is a vector space $V$ over
            $\mathbb{F}$ and a norm $\norm{\cdot}$ on $V$.
        \end{ldefinition}
    \section{Banach Spaces}
    \begin{ldefinition}{Banach Space}
          {Funct_Analysis_Banach_Space}
        A Banach space is a normed vector space
        $(V,\norm{\cdot})$ such that the metric $d$ induced by
        the norm $\norm{\cdot}$ is complete on $V$.
    \end{ldefinition}
    Normed spaces are special. Give $\mathbf{v}\in{V}$, and
    for $r>0$, we have:
    \begin{equation}
        B_{r}^{(V,\norm{\cdot})}(\mathbf{x})=
        B_{r}^{(V,\norm{\cdot})}(\mathbf{0})+\mathbf{x}
    \end{equation}
    That is, open balls about arbitrary points are merely
    translations of an open ball about the origin.
    \begin{equation}
        |\norm{\mathbf{v}}-\norm{\mathbf{u}}|
        \leq\norm{\mathbf{v}-\mathbf{u}}
    \end{equation}
    And thus the map $\mathbf{v}\mapsto\norm{\mathbf{v}}$ is
    continuous. The closure of an open ball is the closed ball.
    \begin{equation}
        \overline{B_{r}^{(V,\norm{\cdot})}(\mathbf{x})}
        =\{\mathbf{y}\in{V}:
            \norm{\mathbf{x}-\mathbf{y}}\leq{r}\}
    \end{equation}
    We can also multiply open balls by constants, to get
    the following:
    \begin{equation}
        \varepsilon{B}_{r}^{(V,\norm{\cdot})}(\mathbf{x})=
        B_{\varepsilon{r}}^{(V,\norm{\cdot})}(\mathbf{x})
    \end{equation}
    \begin{theorem}
        Suppose $X$ and $Y$ are normed vector spaces over
        $\mathbb{F}\subseteq\mathbb{C}$. Let $T:X\rightarrow{Y}$
        be a linear transformation. Then the following
        are equivalent:
        \begin{enumerate}
            \item $T$ is continuous.
            \item $T$ is continuous at some $x_{0}\in{X}$.
            \item There is an $\alpha>0$ such that
                  $\norm{Tx}\leq\alpha\norm{x}$.
        \end{enumerate}
    \end{theorem}
    \begin{proof}
        Suppose $T$ is continuous at $x_{0}$. Then there is a
        $\delta>0$ such that:
        \begin{equation}
            T\Big(\overline{B_{\delta}(x_{0})}\big)
            \subseteq{B}_{1}(T(x_{0}))
        \end{equation}
        But:
        \begin{align}
            T\Big(\overline{B_{\delta}(x_{0})}\big)
            &=T\Big(\overline{B_{\delta}(0)}\big)+T(x_{0})\\
            B_{1}(T(x_{0}))=
            B_{1}(0)+T(x_{0})
        \end{align}
        Now suppose $z\ne{0}$. Then:
        \begin{equation}
            \norm{T(z)}=
            \norm{\frac{\norm{z}}{\delta}
                  T\Big(\frac{\delta{z}}{\norm{z}}\Big)}
            \leq\frac{1}{\delta}\norm{z}
        \end{equation}
        Let $\alpha=\delta^{\minus{1}}$.
        Proving the next one:
        \begin{equation}
            \norm{T(x)-T(y)}=\norm{T(x-y)}
            \leq\alpha\norm{x-y}
        \end{equation}
        And so we have continuity.
    \end{proof}
    There are linear maps that are not bounded. Let
    $\ell_{1}^{0}=\mathrm{Span}\{e_{k}:x\in\mathbb{N}\}$. Map
    $e_{k}\rightarrow{k}e_{k}$. Let
    $\norm{\cdot}_{a}$ and $\norm{\cdot}_{b}$ be norms on
    $X$ that induce the same topology on $X$. Consider the map
    $id:(X,\norm{\cdot}_{a})\rightarrow(X,\norm{\cdot}_{b})$.
    Since the topologies are the same, $id$ is continuous.
    Then there is a $c\geq{0}$ such that:
    \begin{equation}
        \norm{x}_{b}\leq{c}\norm{x}_{a}
    \end{equation}
    We can go the other way as well, and thus we see that
    equivalence implies strongly equivalent. This is not true
    in a general metric space.
    \begin{ldefinition}{Operator Norm}
          {Funct_Analysis_Operator_Norm}
        Let $\mathscr{L}(X,Y)$ be the set of bounded linear
        transformation $T:X\rightarrow{Y}$. The operator
        norm on $T$ is:
        \begin{equation}
            \norm{T}=\sup\{\norm{T}(x):\norm{x}\leq{1}\}
        \end{equation}
    \end{ldefinition}
    \begin{theorem}
        The operator norm on $\mathscr{L}(X,Y)$ is a norm.
    \end{theorem}
    \begin{proof}
        For we have:
        \begin{equation}
            \norm{S\circ{T}}\leq\norm{S}\norm{T}
        \end{equation}
    \end{proof}
    \begin{ldefinition}{Algebra Over a Field}
          {Funct_Analysis_Algebra_Over_Field}
        An algebra over a field $\mathbb{F}$ is a vector
        space $A$ over $\mathbb{F}$ such that $A$ has a ring
        structure $(A,\times,+)$ such that:
        \begin{equation}
            \lambda(xy)=(\lambda{x})y
            =x(\lambda(y))
            \quad\quad
            x,y\in{A}
            \quad\lambda\in\mathbb{F}
        \end{equation}
    \end{ldefinition}
    \begin{lexample}{}{Old_Stuff}
        $\mathbb{R}[x]$, $\mathbb{C}[x]$, $M_{n}(\mathbb{F})$,
        and $C_{b}(X)$.
    \end{lexample}
    \begin{ldefinition}{Normed Algebra}
          {Funct_Analysis_Normed_Algebra}
        A normed algebra is a normed space $(A,\norm{\cdot})$
        such that $A$ is an algebra and such that,
        for all $x,y\in{A}$:
        \begin{equation}
            \norm{xy}\leq\norm{x}\norm{y}
        \end{equation}
    \end{ldefinition}
    \begin{ldefinition}{Banach Algebra}
          {Funct_Analysis_Banach_Algebra}
        A Banach Algebra is a normed algebra $(A,\norm{\cdot})$
        such that $(A,\norm{\cdot})$ is a Banach space.
    \end{ldefinition}
    \begin{theorem}
        If $Y$ is a Banach space, then $\mathscr{L}(X,Y)$
        is a Banach space.
    \end{theorem}
    \begin{theorem}
        If $X$ is a Banach Algebra, then
        $\mathscr{L}(A)$ is a Banach algebra.
    \end{theorem}
    \begin{proof}
        For suppose $T_{n}$ is Cauchy in
        $\mathscr{L}(X,Y)$. Then for all $x\in{X}$,
        $T_{n}(x)$ is Cauchy in $Y$, and thus
        $T_{n}(x)$ converges to some $y\in{Y}$. Let
        $T:X\rightarrow{Y}$ be this limit function. Then
        $T:X\rightarrow{Y}$ is a linear map. But since
        $T_{n}$ is Cauchy, it is uniformly bounded. But then
        there is an $M\in\mathbb{R}^{+}$ such that:
        \begin{equation}
            \norm{T_{n}}\leq{M}
        \end{equation}
        And thus:
        \begin{equation}
            \norm{T{x}}=
            \underset{n\rightarrow\infty}{\lim}
            \norm{T_{n}x}\leq
            \lim\underset{n}{\sup}\norm{T_{n}}\norm{x}
            \leq{M}\norm{x}
        \end{equation}
        Therefore, etc.
    \end{proof}
\chapter{Lecture 9, I Think}
    Let $X_{\lambda}$ be a Banach space for all
    $\lambda\in\Lambda$. Then the product is:
    \begin{equation}
        \prod_{\lambda\in\Lambda}X_{\lambda}
        =\{f:\Lambda\rightarrow\bigcup_{\lambda\in\Lambda}
        X_{\lambda}:f(\lambda)\in{X}_{\lambda}\}
    \end{equation}
    Generally, we think of the indexing set to be finite,
    $\Lambda=\{1,\dots,n\}$. The product space is then:
    \begin{equation}
        \prod_{\lambda=1}^{n}X_{\lambda}=
        X_{1}\times\dots{X}_{n}
    \end{equation}
    Functions are therefore $n$ tuples. There's no reason to
    expect that this will be a Banach space in any reasonable
    way. Thus we define the Banach Space Direct-Product.
    \begin{ldefinition}{Banach Space Direct Product}
          {Funct_Analysis_Banach_Space_Direct_Product}
        The Banach Space Direct Product of a set of Banach
        spaces $X_{\lambda}$ indexed over $\Lambda$ is:
        \begin{equation}
            \prod_{\lambda\in\Lambda}^{*}X_{\lambda}
            =\{f\in\prod_{\lambda\in\Lambda}X_{\lambda}:
            \underset{\lambda\in\Lambda}{\sup}
            f(\lambda)<\infty\}
        \end{equation}
    \end{ldefinition}
    Then $\norm{x}=\sup_{\lambda}\norm{x_{\lambda}}$ is a norm
    on the product space.
    \begin{ltheorem}{Open Mapping Theorem}
          {Funct_Analysis_Open_Mapping_Theorem}
        If $X$ and $Y$ are Banach spaces and if
        $T\in\mathcal{L}(X,Y)$ is surjective, then $T$ is an
        open map.
    \end{ltheorem}
    \begin{proof}
        It will suffice to find $r>0$ such that:
        \begin{equation}
            B_{r}^{Y}\subseteq
            T\Big(B_{1}^{X}(0)\Big)
        \end{equation}
        By homogeneity, $T(B_{\delta}^{X}(0))$ is a
        neighborhood of $0$ for all $\delta>0$. By linearity,
        $T(B_{\delta}(x))$ is a neighborhood of
        $T(x)$ for all $x\in{X}$ and for all $\delta>0$. But
        if $V\subseteq{X}$ is open, and $x\in{V}$, then there
        is a $\delta>0$ such that $B_{\delta}(x)\subseteq{V}$.
        Thus, $T(B_{\delta}(x))$ is a neighborhood of $T(x)$
        in $T(V)$. There is al an $r>0$ such that:
        \begin{equation}
            B_{r}^{Y}(0)\subseteq
            \overline{T(B_{1}^{X}(0)))}
        \end{equation}
        For let $\alpha\in(0,1)$. Note that:
        \begin{equation}
            T(B_{\alpha}(x))=\alpha{T}(B_{1}(0))
        \end{equation}
        And also:
        \begin{equation}
            B_{\alpha{r}}(0)\subseteq
            \overline{\alpha{T}(B_{1}(0))}
        \end{equation}
        Let $y\in{B}_{r}(0)$. Then there is a
        $y_{1}\in{T(B_{1}(0))}$ such that:
        \begin{equation}
            \norm{y-y_{1}}<\frac{r}{2}
        \end{equation}
        But then $y-y_{1}\in{B}_{r/2}(0)$. But then there is a
        $y_{2}\in{T}(B_{1/2}(0))$ such that:
        \begin{equation}
            \norm{y_{2}}<\frac{r}{4}
        \end{equation}
        That is:
        \begin{equation}
            \norm{y-y_{1}-y_{2}}<\frac{r}{2^{2}}
        \end{equation}
        Continuing we obtain a sequence $y_{n}$ such that:
        \begin{equation}
            y_{n}\in{T}(B_{1/2^{n}}(0))
        \end{equation}
        And such that:
        \begin{equation}
            \norm{y-\sum_{k=1}^{n}y_{k}}<
            \frac{r}{2^{n}}
        \end{equation}
        Note that there exists $x_{n}\in{X}$ such that
        $T(x_{n})=y_{n}$ and:
        \begin{equation}
            \norm{x_{n}}<\frac{1}{2^{n-1}}
        \end{equation}
        But then there is an $x\in{X}$ such that:
        \begin{equation}
            x=\sum_{n=1}^{\infty}x_{n}
        \end{equation}
        Since $X$ is complete and since this series converges.
        But $T$ is continuous, and therefore $T(x)=y$. But:
        \begin{equation}
            \norm{x}\leq\sum_{n=1}^{\infty}\norm{x_{n}}
            <\sum_{n=0}^{\infty}\frac{1}{2^{n}}=2
        \end{equation}
        That is,:
        \begin{equation}
            B_{r}(0)\subseteq{T}(B_{2}(0))
        \end{equation}
        And therefore:
        \begin{equation}
            B_{r/2}(0)\subseteq
            T_{1}(B_{1}(0))
        \end{equation}
        If $T$ is surjective, we can write:
        \begin{equation}
            Y=\bigcup_{n\in\mathbb{N}}
            \overline{T(B_{n}(0))}
        \end{equation}
        But $Y$ is a Banach space, and thus by the Baire
        category theorem, there is an $n\in\mathbb{N}$ such
        that $\overline{T(B_{n}(0))}$ has interior. Therefore,
        etc.
    \end{proof}
    \begin{lexample}{}{L_p_Example}
        Recall that:
        \begin{equation}
            \ell_{0}^{p}=
            \{x\in\ell^{p}:\exists{N\in\mathbb{N}},
                \forall_{n>N},x_{n}=0\}
        \end{equation}
        Let $\ell_{0}^{p}$ be defined by:
        \begin{equation}
            \ell_{0}^{p}=\mathrm{Span}\{e_{n}:n\in\mathbb{N}\}
        \end{equation}
        Define $T:\ell_{0}^{2}\rightarrow\ell_{0}^{p}$ by:
        \begin{equation}
            T(e_{n})=\frac{1}{n}e_{n}
        \end{equation}
        Then $T$ is bounded and $\norm{T}\leq{1}$. Note that
        $T$ is bijective and has an inverse:
        \begin{equation}
            T^{\minus{1}}(e_{n})=ne_{n}
        \end{equation}
        And this is not bounded, so
        $T^{\minus{1}}\notin\mathcal{L}(\ell_{0}^{p})$.
        This can happen since $\ell_{0}^{P}$ is not complete.
    \end{lexample}
    \begin{ltheorem}{Inverse Mapping Theorem}
          {Funct_Analysis_Inverse_Mapping_Theorem}
        If $X$ and $Y$ are Banach spaces and
        $T\in\mathcal{L}(X,Y)$ is bijective, then
        $T^{\minus{1}}\in\mathcal{L}(Y,X)$.
    \end{ltheorem}
    \begin{proof}
        Note that $T^{\minus{1}}$ is linear. Since $T$ has to be
        open by the open mapping theorem, $T^{\minus{1}}$ is
        continuous. But continuous linear functions are bounded.
        Therefore, $T^{\minus{1}}$ is bounded.
    \end{proof}
    \begin{ltheorem}{Closed Graph Theorem}
          {Funct_Analysis_Closed_Graph_Theorem}
        If $X$ and $Y$ are Banach spaces and if
        $T:X\rightarrow{Y}$ is linear, then
        $T\in\mathcal{L}(X,Y)$ if and only if the graph of
        $T$ is closed in $X\times{Y}$.
    \end{ltheorem}
    \begin{proof}
        Note that $X\times{Y}$ is a Banach space and
        $(x_{n},y_{n})\rightarrow(x,y)$ if and only if
        $x_{n}\rightarrow{x}$ and $y_{n}\rightarrow{y}$.
        If $T$ is bounded, then the graph of $T$ is closed.
        Now, suppose that the graph of $T$ is closed. But
        then the graph is a Banach space. The projection map
        $P:T\rightarrow{X}$ given by $P((x,T(x))=x$ is
        a bounded bijection. Hence, $P^{\minus{1}}$ is bounded.
        Let $P_{2}$ be defined by $P_{2}(x,T(x))=T(x)$. Then
        $P_{2}$ is bounded. But:
        \begin{equation}
            T(x)=P_{2}\circ{P}_{1}^{\minus{1}}(x)
        \end{equation}
        And therefore $T$ is bounded.
    \end{proof}
    \begin{lexample}{}{L_X_Y_Example}
        Suppose $T_{n}\in\mathcal{L}(X,Y)$ and suppose, for
        all $x\in{X}$:
        \begin{equation}
            T(x)=\underset{n\rightarrow\infty}{\lim}T_{n}(x)
        \end{equation}
        Then we have that $T$ is linear. Is
        $T\in\mathcal{L}(X,Y)$? We have:
        \begin{equation}
            \norm{T}=
            \norm{\underset{n\rightarrow\infty}{\lim}T_{n}}
            \leq\underset{n\rightarrow\infty}{\lim}\sup_{n}
            \norm{T_{n}}
        \end{equation}
    \end{lexample}
    \begin{ltheorem}{Principle of Uniform Boundedness}
          {Funct_Analysis_Prin_Uni_Bounded}
        If $X$ and $Y$ are Banach spaces, if
        $T_{\lambda}\in\mathcal{L}(X,Y)$ for all
        $\lambda\in\Lambda$, and if for all $x\in{X}$ we have:
        \begin{equation}
            \norm{\{\norm{T_{\lambda}(x)}:\lambda\in\Lambda\}}
            <\infty
        \end{equation}
        Then $\{\norm{T_{\lambda}}:\lambda\in\Lambda\}$ is
        bounded.
    \end{ltheorem}
    Recall that if $X_{\lambda}$ are Banach spaces for all
    $\lambda\in\Lambda$, where $\Lambda$ is some indexing
    set, then we can form the Banach Space direct product:
    \begin{equation}
        \prod_{\lambda\in\Lambda}^{*}X_{\lambda}
        =\Big\{x\in\prod_{\lambda\in\Lambda}X_{\lambda}:
            \norm{x}_{\lambda}<\infty\Big\}
    \end{equation}
    If $\lambda_{0}\in\Lambda$, we can define:
    \begin{equation}
        P_{\lambda}:\prod_{\lambda\in\Lambda}^{*}X_{\lambda}
        \rightarrow{X}_{\lambda_{0}}
    \end{equation}
    By defining $P_{\lambda_{0}}(x)=x(\lambda_{0})$.
    Now consider the case when $X_{\lambda}=Y$ for all
    $\lambda\in\Lambda$. We'll write:
    \begin{equation}
        Y_{\Lambda}=\prod_{\lambda\in\Lambda}Y
    \end{equation}
    The principle of uniform boundedness says that if
    $X$ and $Y$ are Banach spaces and if
    $T_{\lambda}\in\mathcal{L}(X,Y)$ for all
    $\lambda\in\Lambda$, and that for all $x\in{X}$, the set
    $\{\norm{T_{\lambda}(x)}:\lambda\in\Lambda\}$ is
    bounded, then $\{\norm{T_{\lambda}}:\lambda\in\Lambda\}$
    is bounded.
    \begin{ltheorem}{Principle of Uniform Boundedness}{}
        If $X$ and $Y$ are Banach spaces, if
        $T_{\lambda}\in\mathcal{L}(X,Y)$ for all
        $\lambda\in\Lambda$, and if for all $x\in{X}$ we
        have that $\{\norm{T_{\lambda}(x)}:\lambda\in\Lambda\}$
        is bounded, then
        $\{\norm{T_{\lambda}}:\lambda\in\Lambda\}$ is bounded.
    \end{ltheorem}
    \begin{proof}
        For let:
        \begin{equation}
            Y_{\Lambda}=\prod_{\lambda\in\Lambda}Y
        \end{equation}
        And define $T:X\rightarrow{Y}_{\Lambda}$ by:
        \begin{equation}
            T(x)=T_{\lambda}(x)
        \end{equation}
        Then $T$ is well defined and linear. To see
        that $T$ is bounded, use the closed graph
        theorem. That is, suppose $x_{n}\rightarrow{x}$.
        We need to show that $T(x_{n})\rightarrow{T}(x)$.
        Let $\lambda\in\Lambda$. Then
        $P_{\lambda}(T(x_{n}))\rightarrow{P}_{\lambda}(y)$.
        Then $T_{n}(x_{n})(\lambda)\rightarrow{y}(\lambda)$.
        But $T(x_{n})(\lambda)=T_{\lambda}(x_{n})$, and this
        converges to $T_{\lambda}(x)$. But then
        $y(\lambda)=T_{\lambda}$ for all $\lambda$, and
        thus $y=T(x)$. Therefore
        $T\in\mathcal{L}(X,Y_{\Lambda})$. But if
        $\norm{x}\leq{1}$, then:
        \begin{equation}
            \norm{T_{\lambda}(x)}\leq
            \sup_{\lambda\in\Lambda}\norm{T_{\lambda}(x)}
            =\sup_{\lambda\in\Lambda}\norm{T(x)(\lambda)}
            =\norm{T(x)}\leq\norm{T}
        \end{equation}
        Thus, $\norm{T}\geq\norm{T_{\lambda}}$ for all
        $\lambda\in\Lambda$. Therefore, etc.
    \end{proof}
    \begin{ltheorem}{Banach-Stienhaus Theorem}
          {Funct_Analysis_Banach_Stienhaus}
        If $X$ and $Y$ are Banach spaces, and if
        $T_{n}\in\mathcal{L}(X,Y)$ converges point-wise to
        $T$, then $T\in\mathcal{L}(X,Y)$.
    \end{ltheorem}
    \begin{proof}
        By the principle of uniform boundedness,
        $T_{n}$ is uniformly bounded. Therefore $T$ is bounded.
    \end{proof}
    For more review, see Folland's Real Analysis and
    Pedersen's Analysis Now. Zorn's lemma is used to
    prove that every vector space has a basis. We'll
    use this to discuss the notion of duals on
    normed vector spaces.
    \begin{ldefinition}{Dual of a Normed Vector Space}
          {Funct_Analysis_Dual_of_Normed_Vec_Space}
        The dual of a normed vector space $(X,\norm{\cdot})$
        is the set $X^{*}=\mathcal{L}(X,\mathbb{F})$
        with the operator norm.
    \end{ldefinition}
    \begin{theorem}
        If $(X,\norm{\cdot})_{X}$ is a normed vector space,
        then $(X^{*},\norm{\cdot})$ is a Banach space.
    \end{theorem}
    \begin{lexample}
        Let $X=\mathbb{F}^{n}$ and let $\{e_{1},\dots,e_{n}\}$
        be the standard basis. The we can define
        $e_{k}^{*}\in(\mathbb{F}^{n})^{*}$ by:
        \begin{equation}
            e_{k}^{*}(\alpha_{1}e_{1}+\cdots+\alpha_{n}e_{n})
            =\alpha_{k}
        \end{equation}
        Then $\{e_{1}^{*},\dots,e_{n}^{*}\}$ is a basis
        for $(\mathbb{F})^{*}$, and thus
        $(\mathbb{F}^{n})^{*}\simeq\mathbb{F}^{n}$.
    \end{lexample}
    \begin{lexample}
        Let $\{e_{\lambda}\}_{\lambda\in\Lambda}$ be
        a Hamel basis. Then we can define a linear basis:
        \begin{equation}
            e_{\lambda}^{*}:X\rightarrow\mathbb{F}
        \end{equation}
        But also, the set
        $\{\lambda\in\Lambda:e_{\lambda}^{*}\in{X}^{*}\}$
        is at most finite.
    \end{lexample}
    One question that arises is, given any normed vector
    space $X$, what can we say about the dual space $X^{*}$?
    We know that the zero operator is in there, but is there
    anything else?
    \begin{ldefinition}{Minkowski Functional}
          {Funct_Analysis_Minkowski_Functional}
        A Minkowski function on a normed vector space
        $(X,\norm{\cdot})$ over a field
        $\mathbb{F}\subseteq\mathbb{C}$ is a function
        $m:X\rightarrow\mathbb{R}$ such that the following are
        true:
        \begin{subequations}
            \begin{align}
                m(x+y)&\leq{m}(x)+m(y)\\
                m(tx)&=tm(x)\quad\quad{t}\geq{0}
            \end{align}
        \end{subequations}
    \end{ldefinition}
    \begin{lexample}
        If $\norm{\cdot}$ is a semi-norm on $X$ over
        $\mathbb{R}$ or $\mathbb{C}$, then $m(x)=\norm{x}$
        is a Minkowski functional on $X$. If we let
        $X=\ell_{\mathbb{R}}^{\infty}$, then:
        \begin{equation}
            m(x)=\underset{n\rightarrow\infty}{\lim}
                \underset{k\leq{n}}{\sup}\{x_{n}\}
        \end{equation}
        Is also a Minkowski functional.
    \end{lexample}
    \begin{ltheorem}{Basic Extension Lemma}
          {Funct_Analysis_Basic_Extension_Lemma}
        If $m:X\rightarrow\mathbb{R}$ is a Minkowski functional
        on a vector space $X$ over $\mathbb{R}$ if
        $Y\subseteq{X}$ is a subspace, and if
        $\varphi:Y\rightarrow\mathbb{R}$ is a linear functional
        such that, for all $y\in{Y}$,
        $\varphi(y)\leq{m}(y)$, then there is a linear
        functional $\tilde{\varphi}:X\rightarrow\mathbb{R}$
        such that, for all $x\in{X}$,
        $\tilde{\varphi}(x)\leq{m}(x)$, and for all $y\in{Y}$,
        $\tilde{\varphi}(y)=\varphi(y)$.
    \end{ltheorem}
    If $X$ is a normed vector space, then we can identify
    $X$ with it's image $i(X)$ in $X^{**}$, where
    $i:X\rightarrow{X}^{**}$ is defined by:
    \begin{equation}
        i(x)(\phi)=\phi(x)
    \end{equation}
    If $X$ is a Banach space, then $i(X)$ is closed. The
    Otherwise, we call $\tilde{X}=\overline{i(X)}$ is the
    completion of $X$.
    \begin{ldefinition}{Reflexive Banach Space}
          {Funct_Analysis_Reflexive_Banach_Space}
        A reflexive banach space is a Banach space $X$ such
        that the natural map $i:X\rightarrow{X}^{**}$ is
        surjective.
    \end{ldefinition}
    If $X$ is reflexive, then $X$ is isometrically isomorphic
    to $X^{**}$. One might suspect that the converse is true,
    but that's not what the definition says. Indeed, the
    converse is not true. There are Banach spaces $X$ that
    are isometrically isomorphic to $X^{**}$ that are not
    reflexive.
    \begin{lexample}
        Let $X=\ell^{p}$, with $1<p<\infty$, and let
        $q$ be the conjugate exponent of $p$. If $y\in\ell^{q}$,
        there is a linear functional
        $\varphi_{q}^{y}\in(\ell^{p})^{*}$ defined by:
        \begin{equation}
            \varphi_{y}^{p}(x)=\sum_{n=1}^{\infty}x_{n}y_{n}
        \end{equation}
        From H\"{o}lder's inequality, this sum does indeed
        converge, and:
        \begin{equation}
            \norm{\varphi_{y}^{p}}\leq\norm{y}_{q}
        \end{equation}
        Moreover, equality is obtained:
        \begin{equation}
            \norm{\varphi_{y}^{p}}=\norm{y}_{q}
        \end{equation}
        and the map $y\mapsto\varphi_{y}^{p}$ is an
        isometric isomorphism of $\ell^{q}$ with
        $(\ell^{p})^{*}$. Consider
        $i(x)\in(\ell^{p})^{**}=(\ell^{q})^{*}$ by
        the identity above. Note that:
        \begin{equation}
            i(x)(\varphi_{y}^{p})=\varphi_{y}^{p}(x)
            =\varphi_{x}^{q}(y)
        \end{equation}
        And thus $i:X\rightarrow{X}^{**}$ is surjective, so
        $X$ is reflexive.
    \end{lexample}
    \begin{ldefinition}{Transpose of a Bounded Linear Operator}
          {Funct_Analysis_Tranpose_of_BLO}
        The transpose over a bounded linear operator
        $T:X\rightarrow{Y}$ between normed vector spaces $X$
        and $Y$ is the function $T^{*}:Y^{*}\rightarrow{X}^{*}$
        defined by:
        \begin{equation}
            T^{*}(\varphi)(x)=\varphi(T(x))
        \end{equation}
        For all $\varphi\in{Y}^{*}$.
    \end{ldefinition}
    \begin{theorem}
        If $X$ and $Y$ are normed vector spaced and if
        $T\in\mathcal{L}(X,Y)$, then
        $T^{*}\in\mathcal{L}(Y^{*},X^{*})$ and
        $\norm{T^{*}}=\norm{T}$.
    \end{theorem}
    \begin{proof}
        For:
        \begin{subequations}
            \begin{align}
                \norm{T^{*}(\varphi)}&=
                \underset{\norm{x}=1}{\sup}|T^{*}(\varphi)(x)|\\
                &=\underset{\norm{x}=1}{\sup}|\varphi(T(x))|\leq
                \underset{\norm{x}=1}{\sup}
                    \norm{\varphi}\norm{T}\norm{x}\\
                &=\underset{\norm{x}=1}{\sup}
                    \norm{\varphi}{\norm{T}}\\
                &\leq\norm{T}\norm{\varphi}
            \end{align}
        \end{subequations}
        Therefore, $\norm{T^{*}}\leq\norm{T}$. Let
        $\varepsilon>0$. Then there is an $x$ such that
        $\norm{x}=1$ and $\norm{T}<\norm{T(x)}+\varepsilon$.
        But there exists $\varphi\in{Y}^{*}$ such that
        $\norm{\varphi}=1$ and $\varphi(T(x))=\norm{T(x)}$.
        But then:
        \begin{subequations}
            \begin{align}
                \norm{T^{*}}\geq\norm{T^{*}(\varphi)}
                &\geq|T^{*}(\varphi(x))|\\
                &=\norm{T(x)}\\
                &>\norm{T}-\varepsilon
            \end{align}
        \end{subequations}
        By letting $\varepsilon$ tend to zero, we see that
        $\norm{T^{*}}\geq\norm{T}$. Thus,
        $\norm{T^{*}}=\norm{T}$.
    \end{proof}
    \begin{theorem}
        If $X$ and $Y$ are Banach Spaces, if $T:X\rightarrow{Y}$
        and $S:Y^{*}\rightarrow{X}^{*}$ are functions such
        that, for all $\varphi\in{Y}^{*}$ and for all $x\in{X}$,
        we have $S(\varphi)(x)=\varphi(T(x))$, then $S$ and $T$
        are bounded linear operators and $S=T^{*}$.
    \end{theorem}
    \begin{proof}
        Not if $\varphi\in{Y}^{*}$, then $S(\varphi)\in{X}^{*}$,
        and thus:
        \begin{subequations}
            \begin{align}
                \varphi\big(T(x+\lambda{y})\big)&=
                S(\varphi)(x+\lambda{y})\\
                &=S(\varphi)(x)+\lambda{S}(\varphi)(y)\\
                &=\varphi(T(x))+\lambda\varphi(T(y))\\
                &=\varphi(T(x)+\lambda{T}(y))
            \end{align}
        \end{subequations}
        Since $\varphi\in{Y}^{*}$, we have:
        \begin{equation}
            T(x+\lambda{y})=T(x)+\lambda{T}(y)
        \end{equation}
        To see that $T$ is bounded, we use the closed graph
        theorem. Suppose $x_{n}\rightarrow{x}$ and
        $T(x_{n})\rightarrow{y}$. Then, for all
        $\varphi\in{Y}^{*}$, we have:
        \begin{subequations}
            \begin{align}
                \varphi(y)&=\underset{n\rightarrow\infty}{\lim}
                    \varphi(T(x_{n}))\\
                    &=\underset{n\rightarrow\infty}{\lim}
                    S(\varphi)(x_{n})\\
                    &=S(\varphi)(x)\\
                    &=\varphi(T(x))
            \end{align}
        \end{subequations}
        Thus, by the closed graph theorem, $y=T(x)$.
    \end{proof}
        \begin{ldefinition}{Weak Topology}
              {Funct_Analysis_Weak_Topology}
            The weak topology on a normed vector space
            $X$ is the topology on $X$ generated by
            $X^{*}$.
        \end{ldefinition}
        That is, the weak topology is the smallest topology on
        $X$ such that every linear functional is continuous.
\chapter{Stuff}
    Last time, we described the initial topology on a space
    $X$ generated by a family of functions $\mathscr{F}$
    $f:X\rightarrow(Z_{F},\tau_{F})$.
    \begin{ldefinition}{Initial Topology}
          {Funct_Analysis_Initial_Topology}
        The initial topology on a normed vector space
        $(X,\norm{\cdot})$ generated by a family of functions
        $\mathscr{F}=\{f:X\rightarrow(Z_{F},\tau_{F})\}$ such
        that $f$ is continuous for all $f\in\mathscr{F}$.
    \end{ldefinition}
    \begin{theorem}
        If $\mathcal{U}$ is defined by:
        \begin{equation}
            \mathcal{U}(\varphi,x_{0},\varepsilon)=
            \{x\in{X}:|\varphi(x)-\varphi(x_{0})|<\varepsilon\}
        \end{equation}
        For $\varphi\in{X}^{*}$, $x_{0}\in{X}$, $\varepsilon>0$,
        then $\mathcal{U}$ forms a sub-basis for the weak
        topology on $X$. Moreover, the sets:
        \begin{equation}
            \mathcal{U}(\{\varphi_{1},\dots,\varphi_{n}\},
                x_{0},\varepsilon)\}
                =\{x\in{X}:|\varphi_{k}(x)-\varphi_{k}(x_{0})|
                    <\varepsilon,k\in\mathbb{Z}_{n}\}
        \end{equation}
        Form an pen neighborhood basis at $x_{0}$ in the weak
        topology.
    \end{theorem}
    \begin{proof}
        It suffices to prove the second assertion. Since open
        balls in $\mathbb{F}$ form a basis for the topology,
        the collection:
        \begin{equation}
            \varphi^{\minus{1}}(B_{r}(c))=
                \{x:|\varphi(x)-c|<r\}
        \end{equation}
        Form a sub-basis for the weak topology. But if
        $x_{0}\in\{x:|\varphi(x)-c|<r\}$, and if
        $\varepsilon=r-|\varphi(x_{0})-c|$, then
        $|\varphi(x)-\varphi(x_{0})|<\varepsilon$. Thus:
        \begin{equation}
            x_{0}\in\{x:|\varphi(x)-\varphi(x_{0})|<\varepsilon\}
                \subseteq\{x:|\varphi(x)-c|<r\}
        \end{equation}
        Therefore, etc.
    \end{proof}
    \begin{theorem}
        If $X$ is a normed vector space, and $\tau$ is the
        normed topology and $\tau_{w}$ is the weak
        topology, then $\tau_{w}\subseteq\tau$.
    \end{theorem}
    \begin{theorem}
        If $X$ is a normed vector space, if $\tau$ is the
        normed topology, if $\tau_{w}$ is the weak topology,
        and if $X$ is finite dimensional, then
        $\tau=\tau_{w}$.
    \end{theorem}
    \begin{theorem}
        If $X$ is a normed vector space, $\tau$ is the normed
        topology, $\tau_{w}$ is th weak topology, and if
        $X$ is infinite dimensional, then
        $\tau_{w}\ne\tau$.
    \end{theorem}
    In any topology space $X$, we say that a sequence $x_{n}$
    converges to $x$ if the sequence is eventually contained in
    any neighborhood of $x$.
    \begin{theorem}
        If $x_{n}$ is a sequence in a normed vector space,
        then $x_{n}\rightarrow{x}$ weakly if and only if
        $\varphi(x_{n})\rightarrow\varphi(x)$ for all
        $\varphi\in{X}^{*}$.
    \end{theorem}
    \begin{proof}
        For suppose $x_{n}\rightarrow{x}_{0}$ weakly. Then,
        for all $\varepsilon>0$, let:
        \begin{equation}
            \mathcal{U}(\varphi,x_{0},\varepsilon)=
                \{x:|\varphi(x)-\varphi(x_{0})|<\varepsilon\}
        \end{equation}
        This is a weak neighborhood of $x_{0}$. Hence,
        $x_{n}$ is eventually in $\mathcal{U}$. Thus,
        $\varphi(x_{n})$ is eventually in
        $B_{\varepsilon}(\varphi(x_{0}))$, and thus
        $\varphi(x_{n})\rightarrow\varphi(x_{0})$.
        Now suppose $\varphi(x_{n})\rightarrow\varphi(x_{0})$
        for all $\varphi\in{X}^{*}$. Let $V$ be a weak
        neighborhood of $x_{0}$. Then for some
        $\varphi_{1},\dots,\varphi_{n}$, and $\varepsilon>0$:
        \begin{equation}
            x_{0}\in\mathcal{U}(
            \{\varphi_{1},\dots,\varphi_{n}\},x_{0},
                \varepsilon\})
            =\{x:|\varphi_{k}(x)-\varphi_{k}(x_{0})|
                <\varepsilon,k\in\mathbb{Z}_{n}\}
            \subseteq{V}
        \end{equation}
        Thus, $x_{n}$ is eventually in $V$.
    \end{proof}
    \begin{theorem}
        Every weakly convergent sequence is bounded with
        respect to the normed topology.
    \end{theorem}
    \begin{proof}
        Suppose $x_{n}\rightarrow{x}$ weakly and let
        $i:X\rightarrow{X}^{**}$ be the canonical map. Then,
        for all $\varphi\in{X}^{*}$:
        \begin{equation}
            i(x_{n}(\varphi)=\varphi(x_{n})
        \end{equation}
        And this is bounded, and thus
        $i(x_{n})$ is uniformly bounded by the Uniform
        Boundedness Principle. But $i$ is isometric, and
        thus $\norm{x_{n}}$ is bounded.
    \end{proof}
    \begin{lexample}
        Let $X=\ell^{2}$. Recall that if $x\in\ell^{2}$, then:
        \begin{equation}
            \norm{x}_{2}=
                \sqrt{\sum_{n=1}^{\infty}|x_{n}|^{2}}
        \end{equation}
        Let $e_{n}$ be the usual basis:
        \begin{equation}
            e_{n}(k)=\delta_{nk}=
                \begin{cases}
                    1,&n=k\\
                    0,&n\ne{k}
                \end{cases}
        \end{equation}
        We have seen that $(\ell^{2})^{*}$ is isometric to
        $\ell^{2}$. That is, if $y\in\ell^{2}$, then
        $y$ corresponds to $\varphi_{y}$ where:
        \begin{equation}
            \varphi_{y}(x)=\sum_{n=1}^{\infty}
                x_{n}y_{n}
        \end{equation}
        And every $\varphi\in(\ell^{2})^{*}$ can be written
        this way. There is a neighborhood basis for
        $0\in\ell^{2}$ in the weak topology given by the
        sets of the form:
        \begin{equation}
            \mathcal{U}=\{x\in\ell^{2}:
                \sum_{i=1}^{n}|\varphi_{y_{i}}(x)|^{2}
                <\varepsilon\}
        \end{equation}
        Now define $T$ as follows:
        \begin{equation}
            T=\{\sqrt{n}e_{n}:n\in\mathbb{N}\}
        \end{equation}
        We want to find the \textit{weak} closure of $T$, which
        is the closure of $T$ with respect to the weak topology.
        That is:
        \begin{equation}
            W=\overline{T}^{w}=
            \bigcap\{F\subseteq\ell^{2}:
                T\subseteq{F},F^{C}\in\tau_{w}\}
        \end{equation}
        Where $F^{C}$ denotes the complement with respect
        to the weak topology. Let's show that $0$ is an
        element of $W$. Suppose not. If $0\ne{W}$, then
        there is a $\mathcal{U}$ such that
        $\mathcal{U}\cap{T}=\emptyset$. But:
        \begin{equation}
            |\varphi_{y_{i}}(\sqrt{k}e_{k})|=
            \sqrt{k}|y_{i}(k)|
        \end{equation}
        Thus, for all $k\in\mathbb{N}$:
        \begin{equation}
            \sum_{i=1}^{n}|y_{i}(k)|^{2}\geq
            \frac{\varepsilon}{k}
        \end{equation}
        But then:
        \begin{equation}
            \sum_{i=1}^{n}\norm{y_{i}}_{2}^{2}
            =\sum_{i=1}^{n}\sum_{k=1}^{\infty}
                |y_{i}(k)|^{2}
            =\sum_{k=1}^{\infty}\sum_{i=1}^{n}
                |y_{i}(k)|^{2}\geq\sum_{k=1}^{\infty}
                    \frac{\varepsilon}{k}
        \end{equation}
        But this sum diverges, a contradiction as the functions
        are in $\ell^{2}$. Thus, $0\in{W}$.
    \end{lexample}
    Note that any sequence $x_{n}\subseteq{T}$ that converges
    to zero must be bounded. But $\{\sqrt{n}e_{n}:n\leq{n}\}$
    is weakly closed. But no sequence in $T$ can converge weakly
    to zero, and thus the weak topology is not metrizable.
    As it turns out, the weak topology is not even
    first countable. We like sequences and want to continue
    using this notion, but cannot use this for the weak
    topology on infinite dimensional normed spaces. We
    generalize by describing nets.
    \begin{ldefinition}{Directed Ordered Sets}
          {Funct_Analysis_Directed_Ord_Set}
        A directed ordered set is an ordered set
        $(\Lambda,\leq)$ such that, for all $x,y\in\Lambda$,
        there is a $z\in\Lambda$ such that
        $x\leq{z}$ and $y\leq{z}$.
    \end{ldefinition}
    \begin{lexample}
        There are two common examples. Any totally ordered
        set is automatically a directed ordered set,
        since we can just choose the max of any two elements.
        Moreover, if $(X,\tau)$ is a topologicaly space, then
        $(\tau,\subseteq)$ is a directed ordered set, since
        for $\mathcal{U}$ and $\mathcal{V}$, then
        $\mathcal{U}\cup\mathcal{V}$ is larger than both.
        Similarly with reverse containment, taking
        $\mathcal{U}\cap\mathcal{V}$ as the
        \textit{larger} set.
    \end{lexample}
    \begin{ldefinition}{Nets}
        A net on a set $X$ is a function from a directed
        set $\Lambda$ into $X$, $x:\Lambda\rightarrow{X}$.
    \end{ldefinition}
    As with sequences, we denote the image of $\lambda\in\Lambda$
    under a net $x$ by writing $x(\lambda)=x_{\lambda}$.
    We say that a net converges in a topological space if
    for any neighborhood $\mathcal{U}$ of the limit point
    $x_{0}$, there is a $\lambda_{0}\in\Lambda$ such that,
    for all $\lambda\geq\lambda_{0}$, $x_{\lambda}$, we have
    $x_{\lambda}\in\mathcal{U}$. An accumulation point of
    $x_{\lambda}$. There is also a generalized notion of
    accumulation point for nets.
    \begin{lexample}
        Every sequence is a net. Take $\Lambda=\mathbb{N}$.
    \end{lexample}
    \begin{theorem}
        If $(X,\tau)$ is a topological space, and
        $\mathcal{E}\subseteq{X}$, then
        $x\in\overline{\mathcal{E}}$ if and only if there is
        a net $a:\Lambda\rightarrow\mathcal{E}$ such that
        $a_{\lambda}\rightarrow{x}$.
    \end{theorem}
    \begin{proof}
        For suppose $x_{\lambda}\rightarrow{x}$ with
        $x_{\lambda}\in\mathcal{E}$. If
        $x\notin\overline{\mathcal{E}}$, there there is a
        $\mathcal{U}\subseteq\mathcal{O}(x)$ such that
        $\mathcal{U}\cap\mathcal{U}=\emptyset$. But
        $x_{\lambda}$ is eventually in $\mathcal{U}$,
        a contradiction. Thus, etc. This direction of
        the proof is identical for sequences, and indeed holds
        for sequences. Going the other way requires the use
        of the notion of nets. Suppose
        $x\in\overline{\mathcal{U}}$. Let
        $\Lambda$ be the set of neighborhoods of $x$. Then
        this is a directed set. Pick points to get a net.
    \end{proof}
\chapter{More Normed Vector Space Stuff}
    Given a normed vector space $X$, the dual $X^{*}$ is
    a Banach space. It has a normed topology and a weak
    topology given by $X^{**}$. We can also give it the
    weak star topology: $\sigma(X^{*},X)$. This is the
    smallest (Or initial) topology determined by the
    functionals in $X$. That is, elements of $X^{*}$
    are continuous. In other words, for all
    $x\in{X}$, the mapping $\varphi\mapsto\varphi(x)$
    is continuous. Recall that a net $(\varphi_{\lambda})$
    in $X^{*}$ converges to $\varphi$ in the weak star
    topology if and only if it converges point-wise.
    That is, $\varphi_{\lambda}(x)\rightarrow\varphi(x)$
    for all $x\in{X}$.
    \begin{ltheorem}{Alaoglu's Theorem}
        If $X$ is a normed vector space, if
        $B{*}$ is the closed unit disc in the dual space:
        \begin{equation}
            B^{*}=\{\varphi\in{X}^{*}:
                \norm{\varphi}\leq{1}\}
        \end{equation}
        Then $B^{*}$ is compact in the weak star
        topology.
    \end{ltheorem}
    \begin{proof}
        Define $D_{r}$ by:
        \begin{equation}
            D_{r}=\{z\in\mathbb{F}:|z|\leq{r}\}
        \end{equation}
        For all $r>0$. Define:
        \begin{equation}
            Z=\prod_{x\in{X}}D_{\norm{x}}
        \end{equation}
        Then, by Tychonoff's theorem, $Z$ is compact in
        the product topology. But this topology is the
        initial topology determined by the projection
        maps. And a net $(z_{\lambda})\rightarrow{z}$ in
        $Z$ if and only if $z_{\lambda}(x)\rightarrow{z}(x)$
        for all $x\in{X}$. Define
        $j:B^{*}\rightarrow{Z}$ by:
        \begin{equation}
            j(\varphi)(x)=\varphi(x)
        \end{equation}
        Then $j$ is an injective map. Moreover, $j$ has
        closed range in $Z$. For suppose $\varphi_{\lambda}$
        is a net and $j(\varphi_{\lambda})\rightarrow{z}$,
        for some $z\in{Z}$. But then, for all $x\in{X}$,
        $j(\varphi_{\lambda})(x)\rightarrow{z}(x)$, and thus
        $\varphi_{\lambda}(x)\rightarrow{z}(x)$. Thus $z$
        is linear, $z(x+y)=z(x)+z(y)$ and
        $z(\alpha{x})=\alpha{z}(x)$. Thus, $\varphi(x)=z(x)$
        and $|\varphi(x)|\leq\norm{x}$, and therefore
        $\varphi\in{B}^{*}$. Therefore $j(B^{*})$ is compact.
        Moreover, $j$ is homeomorphism between
        $B^{*}$ and $j(B^{*})$, and thus $B^{*}$ is compact.
    \end{proof}
\chapter{Hilbert Spaces}
    \begin{ldefinition}{Sesqui-Linear Form}
        A Sesqui-Linear form on a vector space $V$ over a
        field $\mathbb{F}$ is a function
        $\langle{\cdot|\cdot}\rangle$ such that:
        \begin{equation}
            \langle{\alpha{x}+y|z}\rangle
            =\alpha\langle{x,z}\rangle+\langle{y,z}\rangle
            \quad\quad
            x,y,z\in{V}
            \quad
            \alpha\in\mathbb{F}
        \end{equation}
        And such that:
        \begin{equation}
            \langle{x,\alpha{y}+z}\rangle
            =\overline{\alpha}\langle{x|y}\rangle
            +\langle{x|z}\rangle
           \quad\quad
            x,y,z\in{V}
            \quad
            \alpha\in\mathbb{F}
        \end{equation}
    \end{ldefinition}
    \begin{ldefinition}{Self-Adjoint Sesqui-Linear Form}
        A self-adjoint sesqui-linear form on a vector space
        $V$ over a field $\mathbb{F}$ is a sesqui-linear
        form $\langle{\cdot|\cdot}\rangle$ such that:
        \begin{equation}
            \langle{x|y}\rangle=\overline{\langle{y|x}\rangle}
            \quad\quad
            x,y\in{V}
        \end{equation}
    \end{ldefinition}
    \begin{ldefinition}{Postive Sesqui-Linear Form}
        Positive if $\langle{x|x}\rangle\geq{0}$.
    \end{ldefinition}
    \begin{theorem}
        If $\mathbb{F}=\mathbb{C}$ and
        $\langle{\cdot|\cdot}\rangle$ is a sesqui-linear
        form, then:
        \begin{equation}
            \langle{x|y}\rangle
            =\frac{1}{4}\sum_{n=0}^{3}
                i^{n}\langle{x+i^{n}y|x+i^{n}y}\rangle
        \end{equation}
    \end{theorem}
    \begin{theorem}
        If $\mathbb{F}=\mathbb{C}$, and
        $\langle{\cdot|\cdot}\rangle$ is a sesqui-linear
        form, then it is self-adjoint if and only if
        it is positive.
    \end{theorem}
    Thus, on a complex vector space, a positive sesqui-linear
    form is always self-adjoint.
    \begin{ldefinition}{Pre-Inner Product}
        A pre-inner product is a positive self-adjoint
        sesqui-linear form.
    \end{ldefinition}
    \begin{ldefinition}{Inner Product}
        An inner product is a pre-inner product such that
        $\langle{x|x}\rangle=0$ implies that $x=0$.
    \end{ldefinition}
    \begin{ldefinition}{Induced Semi-Norm}
        Given a pre-inner product, the induced norm is:
        \begin{equation}
            \norm{v}=\sqrt{\langle{x|y}\rangle}
        \end{equation}
    \end{ldefinition}
    If $\mathbb{F}=\mathbb{R}$, then:
    \begin{equation}
        \langle{x|y}\rangle=\norm{x+y}^{2}-\norm{x-y}^{2}
    \end{equation}
    \begin{ltheorem}{Cauchy-Schwarz Inequality}
        If $\langle{\cdot|\cdot}\rangle$ is a pre-inner product
        and $\norm{\cdot}$ is the induced semi-inner product,
        then:
        \begin{equation}
            |\langle{x|y}\rangle|\leq\norm{x}\norm{y}
        \end{equation}
        A similar result holds for $\mathbb{C}$.
    \end{ltheorem}
    \begin{proof}
        For:
        \begin{equation}
            0\leq\norm{\alpha{x}+y}^{2}
            =\langle{\alpha{x}+y|\alpha{x}+y}\rangle
            =|\alpha|^{2}\norm{x}^{2}
            +\alpha\langle{x|y}\rangle
            +\overline{\alpha\langle{x|y}\rangle}+\norm{y}^{2}
        \end{equation}
        We can simplify this further to obtain:
        \begin{equation}
            0\leq|\alpha|^{2}+2\Re(\alpha\langle{x|y}\rangle)
                +\norm{y}^{2}
        \end{equation}
        Let $\tau\in\mathbb{F}$ be such that
        $\tau\langle{x|y}\rangle=|\langle{x|y}\rangle|$.
        Let $\alpha=t\tau$, with $t\in\mathbb{R}$. Then:
        \begin{equation}
            0\leq{t}^{2}\norm{x}^{2}+2t|\langle{x|y}\rangle
                +\norm{y}^{2}
        \end{equation}
        But this is a quadratic with a positive discriminant,
        and thus by the quadratic formula:
        \begin{equation}
            4\langle{x|y}\rangle^{2}-4\norm{x}^{2}\norm{y}^{2}
            \leq{0}
        \end{equation}
        Therefore, etc. Smiley face.
    \end{proof}
    \begin{theorem}
        If $\langle{\cdot|\cdot}\rangle$ is a pre-inner
        product, then the induced semi-norm is a semi-norm.
    \end{theorem}
    \begin{proof}
        Apply Cauchy-Schwarz to obtain the triangle inequality.
    \end{proof}
    \begin{ldefinition}{Induced Norm}
        An induced norm is a semi-induced norm induced
        by an inner product.
    \end{ldefinition}
    \begin{theorem}
        Induced norms are norms.
    \end{theorem}
    \begin{ldefinition}{Inner Product Space}
        An inner product space Is s vector space
        $V$ over a field $\mathbb{F}$ with an
        inner product $\langle{\cdot|\cdot}\rangle$,
        denoted $(V,\langle{\cdot|\cdot}\rangle)$.
    \end{ldefinition}
    \begin{ldefinition}{Hilbert Space}
        A Hilbert space is an inner product space
        such that the induced norm is complete.
    \end{ldefinition}
    \begin{lexample}
        Let $\mathcal{H}=\mathbb{F}^{n}$ and define:
        \begin{equation}
            \langle{x|y}\rangle
            =\sum_{k=1}^{n}x_{k}\overline{y}_{k}
        \end{equation}
        Then $\langle{\cdot|\cdot}\rangle$ is an inner product.
        The induced norm is $\ell^{2}$, which is complete.
        Thus this is a Hilbert space. Extending this to
        sequences:
        \begin{equation}
            \langle{x|y}\rangle
            =\sum_{k=0}^{\infty}x_{k}\overline{y}_{k}
        \end{equation}
        Similarly, we can define:
        \begin{equation}
            \langle{f|g}\rangle
            =\int_{X}f(x)\overline{g}(x)\diff{\mu}
        \end{equation}
        For $f,g\in{L}^{2}(X,\mathcal{M},\mu)$. Remembering
        to identity functions that differ on only a set
        of measure zero, this is a Hilbert space.
    \end{lexample}
    \begin{theorem}
        If $H$ is an inner product space, and if
        $x,y\in{H}$, then:
        \begin{equation}
            \norm{x+y}^{2}+\norm{x-y}^{2}=
            2\norm{x}^{2}+2\norm{y}^{2}
        \end{equation}
    \end{theorem}
    This theorem characterizes inner product spaces.
    \begin{ltheorem}{Jordan von-Neumann Theorem}
        If $X$ is a complex Banach space such that:
        \begin{equation}
            \norm{x+y}^{2}+\norm{x-y}^{2}
            =2\norm{x}^{2}+2\norm{y}^{2}
        \end{equation}
        Then there is an inner product that induces
        the norm.
    \end{ltheorem}
    \begin{theorem}
        If $H$ is an inner product space and
        $x_{n}\rightarrow{x}$ and
        $y_{n}\rightarrow{y}$, then:
        \begin{equation}
            \langle{x_{n}|y_{n}}\rangle
            \rightarrow\langle{x|y}\rangle
        \end{equation}
    \end{theorem}
    \begin{proof}
        Recall that the norm is continuous, so
        $\norm{x_{n}}\rightarrow\norm{x}$ and
        $\norm{y}_{n}\rightarrow\norm{y}$. By Cauchy-Schwarz:
        \begin{equation}
            |\langle{x_{n}|y_{n}}\rangle-\langle{x|y}\rangle|
            \leq\langle{x_{n}-x|y_{n}}\rangle|+
                |\langle{x|y-y_{n}}\rangle|
            \leq\norm{x-x_{n}}\norm{y_{n}}+
                \norm{x}\norm{y-y_{n}}
            \rightarrow{0}
        \end{equation}
        Therefore, etc.
    \end{proof}
    \begin{ldefinition}{Orthogonal Elements}
          {Funct_Analysis_Orthogonal_Elements}
        Orthogonal elements of an inner product space
        $(H,\langle{\cdot|\cdot}\rangle)$ are points
        $x,y\in{H}$ such that
        $\langle{x|y}\rangle=0$.
    \end{ldefinition}
    We can define a similar notion for subsets of $H$.
    \begin{ltheorem}{Pythagoras' Theorem}
        If $x_{1},\dots,x_{n}$ are pairwise orthogonal
        elements of an inner product space $H$, then:
        \begin{equation}
            \sum_{k=1}^{n}\norm{x_{k}}^{2}=
            \norm{\sum_{k=1}^{n}x_{k}}^{2}
        \end{equation}
    \end{ltheorem}
    \begin{proof}
        For:
        \begin{align}
            \norm{\sum_{k=1}^{n}x_{k}}^{2}
            &=\langle{\sum_{k=1}^{n}x_{k}|\sum_{k=1}^{n}x_{k}}
                \rangle\\
            &=\sum_{k=1}^{n}\sum_{j=1}^{n}
                \langle{x_{k}|x_{j}}\rangle
            &=\sum_{k=1}^{n}\langle{x_{k}|x_{k}}\rangle\\
            &=\sum_{k=1}^{n}\norm{x_{k}}^{2}
        \end{align}
        Therefore, etc.
    \end{proof}
    \begin{theorem}
        I $C$ is a closed non-empty convex subset of a Hilbert
        space $H$, then for all $y$ there is a unique
        $x\in{C}$ such that $\mathrm{dist}(y,C)=\norm{x-y}$.
    \end{theorem}
    Next time, on Functional analysis:
    Direct sum, proof of the previous theorem, the fact
    that there may not be a further element. Stuff.
\chapter{Even More Stuff}
    \begin{theorem}
        If $C$ is a closed non-empty convex subset of a
        Hilbert space $\mathcal{H}$, and if
        $h\in\mathcal{H}$, then there is a unique
        $c\in{C}$ such that:
        \begin{equation}
            \mathrm{dist}(h,C)=\norm{y-h}
        \end{equation}
        That is, there is a unique $x$ in $C$ that is
        closest to $h$.
    \end{theorem}
    \begin{proof}
        Translate $C$ by $C-h$, so we can assume $h=0$.
        Define the following:
        \begin{equation}
            \alpha=\inf\{\norm{x}:x\in{C}\}
        \end{equation}
        That is, $\alpha=\mathrm{dist}(0,C)$. Let $x_{n}$
        be a sequence in $C$ such that
        $\norm{x_{n}}\rightarrow\alpha$. Then, by the
        parallelogram law:
        \begin{equation}
            2(\norm{x_{n}}^{2}+\norm{x_{m}}^{2})=
            \norm{x_{n}+x_{m}}^{2}+
            \norm{x_{n}-x_{m}}^{2}
        \end{equation}
        But then, for all $n,m\in\mathbb{N}$:
        \begin{equation}
            \frac{x_{n}+x_{m}}{2}\in{C}
        \end{equation}
        Since $C$ is convex. Therefore:
        \begin{equation}
            2(\norm{x_{n}}^{2}+\norm{x_{m}}^{2})
            \geq4\alpha^{2}+\norm{x_{n}-x_{m}}^{2}
        \end{equation}
        And thus $x_{n}$ is Cauchy. Then
        $x_{n}\rightarrow{x}$ and $\norm{x}=\alpha$.
        Moreover, from convexity, $x$ is unique.
    \end{proof}
    \begin{ldefinition}{Orthogonal Complement}
        The orthogonal complement of a subset
        $S\subseteq{H}$ of an inner product space
        $H$ is the set:
        \begin{equation}
            S^{\perp}=\{y\in{H}:\forall_{x\in{A}},
                \langle{x|y}\rangle=0\}
        \end{equation}
    \end{ldefinition}
    \begin{theorem}
        If $H$ is an inner product space and
        $S\subseteq{H}$, then $S^{\perp}$ is a closed
        subspace.
    \end{theorem}
    From linear algebra, if $W_{1}$ and $W_{2}$ are
    subsapces of a vector space $V$ such that
    $W_{1}+W_{2}=V$ and $W_{1}\cap{W}_{2}=\{0\}$, then
    we say $V=W_{1}\oplus{W}_{2}$. The map
    $P_{1}:V\rightarrow{V}$ defined by taking $v\in{V}$ to
    the unique $w_{1}\in{W}_{1}$ such that
    $v=w_{1}+w_{2}$ is called the projection of $H$
    onto $W_{1}$ along $W_{2}$.
    \begin{theorem}
        If $W$ is a closed subspace of a Hilbert space
        $\mathcal{H}$, then
        $\mathcal{H}=W\oplus{W}^{\perp}$. If
        $P_{W}:\mathcal{H}\rightarrow\mathcal{H}$ is the
        projection mapping of $H$ into $W$, then
        $P_{W}(h)$ is the closest element in $W$ to $h$.
    \end{theorem}
    \begin{proof}
        For let $h\in\mathcal{H}$ and let $x$ be the
        closed element in $W$ to $h$. Let
        $x^{\perp}=h-x$. Let $w\in{W}$ and
        $\varepsilon>0$. But then:
        \begin{subequations}
            \begin{align}
                \norm{x^{\perp}}^{2}
                &=\norm{h-x}^{2}\leq
                \norm{h-(x+\varepsilon{w}}^{2}\\
                &=\norm{h-x-\varepsilon{w}}^{2}\\
                &=\norm{x^{\perp}-\varepsilon{w}}^{2}\\
                &=\norm{x^{\perp}}^{2}-
                2\varepsilon\Re(\langle{x^{\perp}|w}\rangle)
                +\varepsilon^{2}\norm{w}^{2}
            \end{align}
        \end{subequations}
        Therefore:
        \begin{equation}
            2\varepsilon\Re(\langle{x^{\perp}|w}\rangle)
            =\varepsilon^{2}\norm{w}^{2}
        \end{equation}
        Thus, $x^{\perp}\in{W}^{\perp}$. But
        $\mathcal{H}=W+W^{\perp}$ and
        $W\cap{W}^{\perp}=\{0\}$.
        Therefore, $W\oplus{W}^{+}=\mathcal{H}$
    \end{proof}
    \begin{theorem}
        If $\mathcal{H}$ is a Hilbert space and if
        $S\subseteq\mathcal{H}$, then:
        \begin{equation}
            (S^{\perp})^{\perp}
            =\mathrm{Cl}_{\norm{\cdot}}\big(\mathrm{Span}(S)\big)
        \end{equation}
    \end{theorem}
    Note that given a point $h$ in an inner product space,
    $\varphi_{h}(x)=\langle{x|v}\rangle$ defins a linear
    function. By the Cauchy-Schwarz theorem:
    \begin{equation}
        |\varphi_{h}(h)|\leq\norm{h}\norm{v}
    \end{equation}
    And thus $\varphi_{v}\in{H}^{*}$ and
    $\norm{\varphi_{v}}\leq\norm{v}$. But:
    \begin{equation}
        |\varphi_{v}(v)|=\norm{v}^{2}
    \end{equation}
    And thus $\norm{\varphi_{v}}=\norm{v}$.
    \begin{ltheorem}{Riesz's Representation Theorem}
          {thm:Funct_Analysis_Riesz_Rep_Theorem}
        If $\mathcal{H}$ is a Hilbert space, then the
        map $\Phi:\mathcal{H}\rightarrow\mathcal{H}^{*}$
        given by $\Phi(v)=\varphi_{v}$ is a conjugate
        linear isometric map of $\mathcal{H}$ onto
        $\mathcal{H}^{*}$.
    \end{ltheorem}
    \begin{proof}
        By the previous remark, it suffices to show that
        $\Phi$ is surjective. Let $\varphi\in\mathcal{H}^{*}$.
        Let $W=\ker(\varphi)$. But $\varphi$ is continuous,
        and thus $W$ is a closed subspace of $\mathcal{H}$.
        If $\varphi$ is the zero function, let $v=0$. If not,
        then there exists a non-zero element
        $v\in(\ker(\varphi))^{\perp}$. Let
        $y=v/\norm{v}$.
    \end{proof}
    \begin{theorem}
        If $\mathcal{H}$ is a Hilbert space, then the
        bijection $\Phi:\mathcal{H}\rightarrow\mathcal{H}^{*}$
        mapping $v\mapsto\varphi_{v}$ is a homeomorphism
        of $\mathcal{H}$ with the weak topology onto
        $\mathcal{H}^{*}$ with the weak star topology.
    \end{theorem}
    \begin{proof}
        Recall every $\varphi\in\mathcal{H}^{*}$ is of the
        form $\varphi_{v}$ for some $v\in\mathcal{H}$.
        Thus $h_{\lambda}\mapsto{h}$ in the weak topology
        if and only if for all $v\in\mathcal{H}$:
        \begin{subequations}
            \begin{align}
                \langle{h_{\lambda}|h}\rangle
                &\rightarrow\langle{h|v}\rangle\\
                \Longrightarrow
                \langle{v|h_{\lambda}}\rangle
                &\rightarrow\langle{v|h}\rangle\\
                \Longrightarrow\varphi_{h_{\lambda}}(v)
                &\rightarrow\varphi_{h}(v)
            \end{align}
        \end{subequations}
        But then $\varphi_{h_{\lambda}}\rightarrow\varphi_{h}$
        in the weak star topology.
    \end{proof}
    \begin{theorem}
        If $\mathcal{H}$ is a Hilbert space and if
        $B$ is the closed unit ball, then $B$ is
        weakly compact.
    \end{theorem}
    \begin{ldefinition}{Orthonormal Basis}
        An orthonormal basis of an inner product
        space $H$ is a subset $E\subseteq{H}$ such that, for
        all $e\in{E}$, $\norm{e}=1$, and for all distinct
        $e_{\alpha},e_{\beta}\in{E}$,
        $\langle{e_{\alpha}|e_{\beta}}\rangle=0$.
    \end{ldefinition}
    \begin{ltheorem}{Bessel's Inequality}
          {Funct_Analysis_Bessels_Inequality}
        If $H$ is an inner product, and if
        $e_{n}$ is an orthonormal sequence in $H$< then
        for all $x\in{H}$:
        \begin{equation}
            \sum_{n=1}^{\infty}|\langle{x|e_{n}}\rangle|^{2}
                \leq\norm{x}^{2}
        \end{equation}
    \end{ltheorem}
    \begin{proof}
        For define the following:
        \begin{equation}
            x_{n}=x-\sum_{k=1}^{n}
                \langle{x|e_{k}}\rangle{e}_{k}
        \end{equation}
        Note that $x_{n}\perp{e}_{k}$ for $k=1,\dots,n$.
        But, by the Pythagorean theorem:
        \begin{equation}
            \norm{x}^{2}=\norm{x_{n}}^{2}+
                \sum_{k=1}^{n}|\langle{x|e_{k}}\rangle|^{2}
        \end{equation}
        Thus:
        \begin{equation}
            \norm{x}^{2}\geq\underset{n\in\mathbb{N}}{\sup}
                \sum_{k=1}^{n}|\langle{x|e_{k}}\rangle|^{2}
                =\sum_{k=1}^{\infty}|\langle{x|e_{k}}\rangle|^{2}
        \end{equation}
        Therefore, etc.
    \end{proof}
    \begin{ldefinition}{Orthogonal Projection}
          {def:Funct_Analysis_Orthogonal_Projection}
        The orthogonal projection of a closed subspace $W$
        of a Hilbert space $\mathcal{H}$ is the projection
        $P_{W}:\mathcal{H}\rightarrow\mathcal{H}$ of
        $W$ along $W^{\perp}$.
    \end{ldefinition}
    \begin{theorem}
        If $E$ is an orthonormal set in a Hilbert space
        $\mathcal{H}$, and if
        $\mathcal{E}=\mathrm{Cl}(\mathrm{Span}(E))$, then for all
        $h\in{H}$, the sum over
        $\langle{h|e_{n}}\rangle{e}_{n}$ converges and:
        \begin{equation}
            P_{\mathcal{E}}(h)=\sum_{n=1}^{\infty}
                \langle{h|e_{n}}\rangle{e}_{n}
        \end{equation}
    \end{theorem}
\chapter{Even MORE Stuff!}
    \begin{theorem}
        If $S$ and $T$ are self adjoint bounded operators
        on $\mathcal{H}$, and if $S\leq{T}$, then
        $ASA^{*}\leq{A}TA^{*}$ for all
        $A\in\mathscr{L}(\mathcal{H})$.
    \end{theorem}
    \begin{theorem}
        If $S,T$ are self-adjoint operators on $\mathcal{H}$,
        if $0\leq{S}\leq{T}$, then $\norm{S}\leq\norm{T}$.
    \end{theorem}
    \begin{proof}
        For suppose $0\leq{S}\leq{T}$, and let
        $[x,y]=\langle{Sx|y}\rangle$. Then $[\cdot,\cdot]$
        is a pre-inner product on $\mathcal{H}$, and thus
        if $\norm{x}=\norm{y}=1$, then by Cauchy-Schwarz:
        \begin{equation}
            |\langle{Sx|y}\rangle|^{2}=|[x,y]|^{2}
            \leq[x,x][y,y]=
            \langle{Sx|x}\rangle\langle{Sy|y}\rangle
        \end{equation}
        But $S\leq{T}$, and therefore:
        \begin{equation}
            \langle{Sx|x}\rangle\langle{Sy|y}\rangle
            \leq\langle{Tx|x}\rangle\langle{Ty|y}\rangle
            \leq\norm{T}^{2}
        \end{equation}
        Therefore $\norm{S}^{2}\leq\norm{T}^{2}$. Taking
        square roots completes the proof.
    \end{proof}
    \begin{theorem}
        If $S$ is a self-adjoint bounded operators on
        $\mathcal{H}$, and if $S\geq{0}$, then
        $S\leq{I}$ if and only if $\norm{S}\leq{1}$.
    \end{theorem}
    \begin{proof}
        For if $0\leq{S}\leq{U}$, then
        $\norm{S}\leq\norm{I}=1$. But if $0\leq{S}$ and
        $\norm{S}\leq{1}$, then:
        \begin{equation}
            \langle{Sx|x}\rangle\leq\norm{x}^{2}=
            \langle{x|x}\rangle
        \end{equation}
        Thus, $S\leq{I}$.
    \end{proof}
    \begin{theorem}
        If $T$ is a self-adjoint bounded operator on
        $\mathcal{H}$, if $\minus{I}\leq{T}\leq{I}$, then
        $\norm{T}\leq{1}$.
    \end{theorem}
    \begin{proof}
        Suppose $T=T^{*}$ and $\minus{I}\leq{T}\leq{I}$.
        Then:
        \begin{equation}
            \langle{T(x+y)|x+y}\rangle
            \leq\norm{x+y}^{2}
        \end{equation}
        And similarly:
        \begin{equation}
            \minus\langle{T(x-y)|x-y}\rangle
            \leq\norm{x-y}^{2}
        \end{equation}
        Summing, we obtain:
        \begin{equation}
            4\Re\big(\langle{Tx|y}\rangle\big)
            \leq\norm{x+y}^{2}+\norm{x-y}^{2}
        \end{equation}
        By the parallelogram law:
        \begin{equation}
            4\Re\big(\langle{Tx|y}\rangle\big)
            \leq{2}\norm{x}^{2}+2\norm{y}^{2}
        \end{equation}
        Therefore:
        \begin{equation}
            4|\langle{Tx|y}\rangle|
            \leq{2}\norm{x}^{2}+2\norm{y}^{2}
        \end{equation}
        But, for $\norm{x}=\norm{y}=1$,
        $\sup|\langle{Tx|y}\rangle|\leq{1}$, and therefore
        $\norm{T}\leq{1}$. On the other hand, if
        $T=T^{*}$, then:
        \begin{equation}
            |\langle{Tx|x}\rangle|\leq\norm{x}^{2}
        \end{equation}
        But $T=T^{*}$, and thus
        $\langle{Tx|x}\rangle$ is a real number. Thus:
        \begin{equation}
            \minus\langle{x|x}\rangle
            \leq\langle{Tx|x}\rangle
            \leq\langle{x|x}\rangle
        \end{equation}
        Thus, $\minus{I}\leq{T}\leq{I}$.
    \end{proof}
    Let $A\in{M}_{n}(\mathbf{F})^{\dagger}$. That is,
    $A=A^{*}$ and $A=\mathcal{U}\mathcal{D}\mathcal{U}^{*}$
    for some unitary $\mathcal{U}$ and a diagonal
    $\mathcal{D}$:
    \begin{equation}
        \mathcal{D}=
            \begin{pmatrix}
                \lambda_{1}&0&\dots&0\\
                0&\lambda_{2}&\dots&0\\
                \vdots&\vdots&\ddots&0\\
                0&0&\dots&\lambda_{n}
            \end{pmatrix}
    \end{equation}
    Where $\lambda_{k}\geq{0}$.
    \begin{theorem}
        There is a sequence o polynomials with positive
        coefficients such that:
        \begin{equation}
            \sum_{n=1}^{\infty}p_{n}(t)=1-\sqrt{1-t}
        \end{equation}
        Uniformly on $[0,1]$.
    \end{theorem}
    \begin{proof}
        Let $q_{0}=0$ and define:
        \begin{equation}
            q_{n+1}(t)=\frac{1}{2}\big(t+q_{n}(t)\big)^{2}
        \end{equation}
        For all $n\in\mathbb{N}$. By induction we see that
        $q_{n}$ is a sequence of polynomials with positive
        coefficients and such that:
        \begin{equation}
            0\leq{q}_{n}(t)\leq{1}
        \end{equation}
        For all $t\in[0,1]$, and $n\in\mathbb{N}$. Define:
        \begin{equation}
            p_{n}(t)=q_{n}(t)-q_{n-1}(t)
        \end{equation}
        For all $n\in\mathbb{N}$. But then:
        \begin{equation}
            2p_{n+1}(t)
            =q_{n}(t)^{2}-q_{n-1}(t)^{2}
            =\big(q_{n}(t)-q_{n-1}(t)\big)
                \big(q_{n}(t)+q_{n-1}(t)\big)
            =p_{n}(t)\big(q_{n}(t)+q_{n-1}(t)\big)
        \end{equation}
        Thus, each $p_{n}$ has positive coefficients, and
        therefore:
        \begin{equation}
            q_{n}(t)\leq{q}_{n-1}(t)
        \end{equation}
        But $q_{n}$ is bounded by 1 and monotonic, and thus
        by completeness, there is a limit function. Let:
        \begin{equation}
            Q(t)=\underset{n\rightarrow\infty}{\lim}q_{n}(t)
        \end{equation}
        But then:
        \begin{equation}
            q(t)=\frac{1}{2}\big(t+q(t)^{2}\big)
        \end{equation}
        And therefore:
        \begin{equation}
            q(t)=1-\sqrt{1-t}
        \end{equation}
        Moreover, by Dini's theorem, the convergence is
        uniform. And the $p_{n}$ form a telescoping series,
        and therefore:
        \begin{equation}
            \sum_{n=1}^{\infty}p_{n}(t)=1-\sqrt{1-t}
        \end{equation}
        Therefore, etc.
    \end{proof}
    \begin{lexample}
        Define the following $2\times{2}$ matrices:
        \begin{equation}
            A=\begin{pmatrix}
                2&1\\
                1&1
            \end{pmatrix}
            \quad\quad
            B=\begin{pmatrix}
                4&\minus{1}\\
                \minus{1}&1
            \end{pmatrix}
        \end{equation}
        Then $A$ and $B$ are positive, but:
        \begin{equation}
            AB=\begin{pmatrix}
                7&\minus{1}\\
                2&1
            \end{pmatrix}
        \end{equation}
        And this is not symmetric, and thus not positive.
        The product of positive operators need not be
        positive.
    \end{lexample}
    \begin{theorem}
        If $S\geq{0}$, and for all $n\in\mathbb{N}$,
        $S^{n}\geq{0}$. In particular, if $p$ is a
        polynomial with positive coefficients, then
        $p(S)\geq{0}$.
    \end{theorem}
    \begin{proof}
        We have $(S^{n})^{*}=S^{n}$. Thus, yeah.
    \end{proof}
    \begin{theorem}
        If $T$ is a bounded operator, $T\geq{0}$, then
        there is a unique $A\in\mathscr{L}(\mathcal{H})$ such
        that $A\geq{0}$ and $A^{2}=T$. If $B$ commutes with
        $T$< then $B$ commutes with $A$.
    \end{theorem}
    \begin{proof}
        Let $\alpha>0$. If $A^{2}=\alpha{T}$, then
        $(\alpha^{\minus{1}/2}A)^{2}=T$. Thus we can replace
        $T$ by $\alpha{T}$ such that $\norm{T}\leq{1}$.
        Thus, $0\leq{T}\leq{I}$. Then, if $S=I-T$, then
        $0\leq{S}\leq{I}$. Let $p_{n}$ and $q_{n}$ be
        defined as before. Let:
        \begin{equation}
            S_{n}=p_{n}(S)
        \end{equation}
        Then $S_{n}\geq{0}$. Thus:
        \begin{equation}
            0\leq\sum_{k=m}^{n}p_{k}(t)=
            \sum\alpha_{r}t^{r}\leq\varepsilon
        \end{equation}
        For all $t\in[0,1]$. Note that $\alpha_{k}\geq{0}$.
        Hence, we have:
        \begin{equation}
            \norm{\sum_{k=m}^{n}S_{k}}
            \leq\sum\alpha_{r}\norm{S}^{r}\leq
            \sum\alpha{r}<\varepsilon
        \end{equation}
        Thus, $S_{k}$ forms a Cauchy sequence and therefore
        converges. Moreover:
        \begin{equation}
            R=\sum_{k=1}^{\infty}S_{k}\geq{0}
        \end{equation}
        Moreover, $0\leq{R}\leq{I}$. But:
        \begin{align}
            (I-R)^{2}
            &=\Big(I-\sum_{k=1}^{\infty}S_{k}\Big)^{2}\\
            &=\underset{n\rightarrow\infty}{\lim}
                \big(I-q_{n}(S)\big)^{2}\\
            &=\underset{n\rightarrow\infty}{\lim}
                \big(I-2q_{n}(S)+q_{n}(S)^{2}\big)\\
            &=I-S
        \end{align}
        And this is equal to $T$. Thus, $(I-R)^{2}=T$.
        Now, if $BT=TB$, then:
        \begin{equation}
            AB=(I-R)B
            =\underset{n\rightarrow\infty}{\lim}
                \big(I-q_{n}(I-T)\big)B
            =B\underset{n\rightarrow\infty}{\lim}
                \big(I-q_{n}(I-T)\big)
            =BA
        \end{equation}
        Lastly, $A$ is unique. For if $B\geq{0}$ and
        $B^{2}=T$, then $B$ commutes with $T$ and hence
        $B$ commutes with $A$. Therefore:
        \begin{equation}
            (A-B)^{2}x=(A-B)(A+B)x=(T-T)x=0
        \end{equation}
        Hence, if $y$ is in the range of $A+B$, then
        $(A-B)y=0$.Let $\mathcal{E}$ be the range of $A+B$.
        If we can show that $(A-B)y=0$ for all
        $y\in\mathcal{E}^{\perp}$, then we are done. But since
        $A$ and $B$ are self-adjoint, $\mathcal{E}^{\perp}$
        is the kernel of $A+B$. Thus:
        \begin{equation}
            \langle{Ay|y}\rangle\leq
            \langle{(A+B)z|z}\rangle=0
        \end{equation}
        Thus, for all $y\in\mathcal{E}^{\perp}$,
        $\langle{Ay|y}\rangle=0$. But $A\geq{0}$, hence there
        is a $C$ such that $A=C^{2}$ and $C\geq{0}$.
        Then:
        \begin{equation}
            \langle{Az|Z}\rangle=\norm{Cz}^{2}=0
        \end{equation}
        Thus $Cz=0$ and hence $Az=C^{2}z=0$. Similarly,
        $Bz=0$. Thus $(A-B)z=0$.
    \end{proof}
    \begin{theorem}
        If $T\geq{0}$ and $S\geq{0}$, and if
        $TS=ST$, then $TS\geq{0}$.
    \end{theorem}
    \begin{proof}
        For:
        \begin{equation}
            TS=T(\sqrt{S})^{2}=\sqrt{S}T\sqrt{S}\geq{0}
        \end{equation}
        Therefore, etc.
    \end{proof}
    Here, $A$ is called the positive square root of the
    operator $T$.
\chapter{SO MUCH STUFF}
    The numberical range of $T\in\mathcal{L}(H)$ is:
    \begin{equation}
        n(T)=\sup\{\langle{Tx|x}\rangle:\norm{x}\leq{1}\}
    \end{equation}
    If $T\geq{0}$, then $n(T)=\norm{T}$.
    \begin{theorem}
        If $H$ is a complex Hilbert space, then
        $T\mapsto{n}(T)$ is a norm on $\mathcal{L}(H)$ such
        that $n(T^{2})\leq{n}(T)^{2}$. Moreover:
        \begin{equation}
            \frac{1}{2}\norm{T}\leq{n}(T)
            \leq\norm{T}
        \end{equation}
        Moreover, if $T$ is normal, then $n(T)=\norm{T}$.
    \end{theorem}
    \begin{theorem}
        If $T\in\mathcal{L}(H)$ is self-adjoint, then:
        \begin{equation}
            n(T)=\norm{T}
        \end{equation}
    \end{theorem}
    \begin{proof}
        For, by Cauchy-Schwarz, $n(T)\leq\norm{T}$. And for
        all $x\in{H}$:
        \begin{equation}
            |\langle{Tx|x}\rangle|\leq{n}(T)\norm{x}^{2}
        \end{equation}
        However:
        \begin{subequations}
            \begin{align}
                \langle{T(x+y)|x+y}\rangle
                &=\langle{Tx|x}\rangle+\langle{Tx|y}\rangle
                +\langle{Ty|x}\rangle+\langle{Ty|y}\rangle\\
                \langle{T(x-y)|x-y}\rangle
                &=\langle{Tx|x}\rangle-\langle{Tx|y}\rangle
                -\langle{Ty|x}\rangle+\langle{Ty|y}\rangle\\
                \langle{T(x+y)|x+y}\rangle
                -\langle{T(x-y)|x-y}\rangle
                &=2\big(\langle{Tx|y}\rangle
                    +\langle{Ty|x}\rangle\big)\\
                &=4\Re(\langle{Tx|y}\rangle)
            \end{align}
        \end{subequations}
        Therefore, by Cauchy-Schwarz, and the parallellogram
        law:
        \begin{equation}
            4\Re(\langle{Tx|y}\rangle)\leq
            n(t)\big(\norm{x+y}^{2}+\norm{x-y}^{2}\big)
            \leq{n}(T)\big(2\norm{x}^{2}+2\norm{y}^{2}\big)
        \end{equation}
        Replace $y$ with $\tau{y}$, where $\tau$ is such that
        $|\tau|=1$. From this, we have $\norm{T}\leq{n}(T)$.
        Thus, $\norm{T}=n(T)$.
    \end{proof}
    \begin{theorem}
        If $x,y\in{H}$ are non-zero, and if
        $|\langle{x|y}|=\norm{x}\norm{y}$, then there is a
        $\lambda$ such that $y=\lambda{x}$.
    \end{theorem}
    \begin{proof}
        Since $x$ an $y$ are non-zero, we can divide out
        by their modulus and assume $\norm{x}=\norm{y}=1$.
    \end{proof}
    \begin{theorem}
        If $T$ is a compact self-adjoint operator on
        $H$, then $T$ has an eigenvalue $\lambda$ such that
        $|\lambda|=\norm{T}$.
    \end{theorem}
    \begin{proof}
        Since $T$ is a compact operator, the restriction of
        $T$ to the unit sphere is weak-norm continuous. Hence,
        if $x_{\lambda}\rightarrow{x}$ weakly in $H$, then
        $T(x_{\lambda})\rightarrow{T}(x)$ in norm. But then,
        by the triangle inequality and Cauchy-Schwarz:
        \begin{subequations}
            \begin{align}
                |\langle{Tx_{\lambda}|x_{\lambda}}\rangle-
                    \langle{Tx|x}\rangle|
                &\leq|\langle{Tx_{\lambda}-x|x_{\lambda}}\rangle|
                +|\langle{Tx|x-x_{\lambda}}\rangle|\\
                &\leq\norm{Tx_{\lambda}-x}\norm{x_{\lambda}}
                +|\langle{Tx|x-x_{\lambda}}\rangle|\\
                &\leq\norm{Tx_{\lambda}-Tx}+
                +|\langle{Tx|x-x_{\lambda}}\rangle|
            \end{align}
        \end{subequations}
        And this converges to zero. Thus, the map
        $x\mapsto\langle{Tx|x}\rangle$ is weakly continuous.
        Since $B$ is weakly compact, by the Extreme value
        theorem, this function attains its maximum
        $n(T)=\norm{T}$ at some $x_{0}$ in the unit sphere.
    \end{proof}
    \begin{theorem}
        If $H$ is a separable complex Hilbert space and if
        $T\in\mathcal{L}(H)$ is a compact normal operator,
        then $T$ is diagonalizable and if $\{e_{n}\}$ is an
        orthonormal basis of eigenvectors with
        $Te_{n}=\lambda{e}_{n}$, then
        $\lambda_{n}\rightarrow{0}$. Conversely, any such
        operator is normal and compact.
    \end{theorem}
    \begin{proof}
        It suffices to show that $T$ is diagonalizable.
        Let $\Lambda$ be the collection of all orthonormal
        sets of eigenvectors for $T$. Then $\Lambda$ is
        non-empty. Then $\Lambda$ is ordered by containment
        and is indeed inductively ordered. If $S_{\alpha}$ is
        totally ordered, then the union of all elements of
        $S_{\alpha}$ is a majorant. Hence by Zorn's lemma, there
        is a maximal element $S\in\Lambda$. But, since
        $H$ is separable, $S$ is countable. Let $P$ be the
        projection onto the closed linear span of $S$. If
        the span is equal to $H$, we are done. Otherwise,
        $TP(x)=PT(x)$ for all $x$, and thus
        $TP=PT$, and therefore $TP^{*}=T^{*}P$. But then:
        \begin{equation}
            S=(I-P)T=T(I-P)
        \end{equation}
        If $S=0$, let $e_{0}$ be a unit vector in
        $(I-P)H$. Then $Te_{0}=T(I-P)e_{0}=Se_{0}=0$.
        Thus, $e_{0}$ is an eigenvector for $T$ with
        eigenvalue $0$. If $S\ne{0}$ then $S$ has an eigenvector
        $e_{0}$ with $Se_{0}=\lambda{e}_{0}$ and
        $|\lambda|=\norm{S}$.
    \end{proof}
\chapter{Banach Algebras}
    A Banach algebra is a Banach space with a ring structure
    such that:
    \begin{equation}
        \lambda(xy)=(\lambda{x})y=x(\lambda{y})
    \end{equation}
    And such that:
    \begin{equation}
        \norm{xy}\leq\norm{x}\norm{y}
    \end{equation}
    We will assume that $\mathbb{F}=\mathbb{C}$.
    \begin{lexample}
        Let $X$ be a compact metric space, and let
        $A=\mathcal{C}(X)$. This is a Banach algebra.
        Another classic example is $M_{n}(\mathbb{C})$.
        The set of compact operators on a Hilbert space
        $\mathcal{K}(\mathcal{H})$ is a Banach Algebra.
    \end{lexample}
    \begin{ldefinition}{Unital Banach Algebras}
          {Unital_Banach_Algebra}
        A Unital Banach Algebra is a Banach algebra
    \end{ldefinition}
    \begin{ldefinition}{Inversion of a Banach Algebra}
        The group $\mathrm{Inv}(A)$ which is the set of
        all invertible elements of $A$.
    \end{ldefinition}
    \begin{theorem}
        If $A$ is a unital Banach Algebra and if $a$ is such
        that $\norm{a}<1$, then $I-a$ is invertible.
    \end{theorem}
    \begin{proof}
        For let:
        \begin{equation}
            b=\sum_{n=0}^{\infty}a^{n}
        \end{equation}
        But then:
        \begin{equation}
            (1-a)b=(1-a)\underset{N\rightarrow\infty}{\lim}
                \sum_{n=0}^{N}a^{n}
            =\underset{N\rightarrow\infty}{\lim}
                (1-a)a^{n}
            =\underset{N\rightarrow\infty}{\lim}
                1-a^{N+1}
        \end{equation}
        But $a^{N+1}\rightarrow{0}$, and thus $(1-a)b=1$.
    \end{proof}
    \begin{theorem}
        If $A$ is a unital Banach Algebra, then
        $\mathrm{Inv}(A)$ is open.
    \end{theorem}
    \begin{proof}
        Let $a\in\mathrm{Inv}(A)$. Then
        $a+h=a(1+a^{\minus{1}}h)$. But $1+a^{\minus{1}}h$ is
        invertible and $\norm{h}<1/\norm{a^{\minus{1}}}$.
    \end{proof}
    \begin{ldefinition}{Spectrum of Banach Algebras}
          {Spec_Banach_Alg}
        The spectrum of a unital Banach Algebra is the set:
        \begin{equation}
            \sigma(A)=\{\lambda\in\mathbb{C}:
                \lambda{I}-a\notin\mathrm{Inv}(A)\}
        \end{equation}
    \end{ldefinition}
    \begin{ldefinition}{Spectral Radius}{Spectral_Radius}
        The spectral radius of a unital Banach Algebra is:
        \begin{equation}
            \rho(A)=\sup\{|\lambda|:\lambda\in\sigma(A)\}
        \end{equation}
    \end{ldefinition}
    \section{Burling-Gelfand-Mazure Theorem}
        \begin{ltheorem}{Gelfand-Mazur Theorem}{Gelfand_Mazur}
            If $A$ is a untial Banach Algebra and $a\in{A}$, then
            $\sigma(a)$ is compact and non-empty, and:
            \begin{equation}
                \rho(a)=\underset{n\geq{1}}{\inf}
                    \norm{a^{n}}^{1/n}
                =\underset{n\rightarrow\infty}{\lim}
                    \norm{a^{n}}^{1/n}
            \end{equation}
        \end{ltheorem}
        \begin{proof}
            Let $f(\lambda)=(\lambda-a)^{\minus{1}}$ on
            $\Omega=\sigma(a)^{C}$. Then:
            \begin{equation}
                a^{n}=\frac{1}{2\pi{i}}\oint_{\Gamma_{r}}
                    \lambda^{n}f(\lambda)\diff{\lambda}
            \end{equation}
            Also, define $m(r)$ be:
            \begin{equation}
                m(r)=\underset{\lambda\in\Gamma_{r}}{\max}
                    \norm{f(\lambda)}
            \end{equation}
            But then $\norm{a^{n}}\leq{r}^{n+1}m(r)$, and
            also:
            \begin{equation}
                \underset{n\rightarrow\infty}{\lim}
                    \big(r^{n+1}m(r)\big)^{1/n}=r
            \end{equation}
            And thus:
            \begin{equation}
                \underset{n}{\lim}\sup\norm{a^{n}}^{1/n}
                    \leq{r}
            \end{equation}
            And therefore,
            \begin{equation}
                \underset{n}{\lim}\sup\norm{a^{n}}^{1/n}
                    \leq\rho(a)
            \end{equation}
            Let $\lambda\in\sigma(a)$ and note that:
            \begin{equation}
                \lambda^{n}-a^{n}=(\lambda-a)
                    \sum_{k=1}^{n-1}\lambda^{n-k}a^{k}
            \end{equation}
            Then $\lambda^{n}\in\sigma(a^{n})$. For, if not,
            then $(\lambda^{n}-a^{n})^{\minus{1}}(\cdots)$ is
            an inverse of $\lambda-a$, a contradiction. Thus:
            \begin{equation}
                |\lambda^{n}|\leq\rho(a^{n})\leq\norm{a^{n}}
            \end{equation}
            And therefore $|\lambda|\leq\norm{a^{n}}^{1/n}$.
            Thus:
            \begin{equation}
                \rho(a)\leq\underset{n}{\inf}\norm{a^{n}}^{1/n}
            \end{equation}
            Piecing this together, we obtain:
            \begin{equation}
                \underset{n}{\lim}\sup\norm{a^{n}}^{1/n}
                \leq\rho(a)\leq\underset{n\geq{1}}{\inf}
                \norm{a^{n}}^{1/n}\leq\underset{n}{\lim}\inf
                \norm{a^{n}}^{1/n}
            \end{equation}
        \end{proof}
        Therefore, etc.
        \begin{theorem}
            A unital Banach Algebra in which every non-zero
            element is invertible is isometrically isomorphic
            to $\mathbb{C}$.
        \end{theorem}
        \begin{proof}
            For let $\lambda,\mu\in\mathbb{C}$ be distinct
            elements. Then, at most, either $\lambda-a$ or
            $\mu-a$ is zero. Thus, the spectrum of $a$,
            $\sigma(a)$, must be a singleton. Let
            $a_{\lambda}$ be this element. Then the map
            $\lambda:A\rightarrow\mathbb{C}$ defined by
            $a\mapsto{a}_{\lambda}$ is an isometric isomorphism.
            Also:
            \begin{equation}
                \lambda(a)\lambda(b)-ab
                =\lambda(a)\lambda(b)-\lambda(a)b+\lambda(a)b
                    -ab
                =\lambda(a)(\lambda(b)-b)+b(\lambda(a)-a)
                =0+0
                =0
            \end{equation}
            Therefore, etc.
        \end{proof}
        A proper ideal $M$ on an algebra $A$ is called maximal
        if its contained in no strictly larger proper ideal.
        Note that if $A$ is unital, then every ideal is contained
        in a maximal ideal. By Zorn's lemma, given a unital
        algebra, there is a maximal ideal. If $A$ is a unital
        algebra, then $\mathrm{Inv}(A)$ is open and no proper
        ideal intersects $\mathrm{Inv}(A)$. Therefore, maximal
        ideals are closed.
        \begin{ldefinition}{Complex Homomorphism}{Comp_Homomorph}
            A complex homomorphism on a Banach Algebra $A$ is a
            multiplicative linear functional
            $h:A\rightarrow\mathbb{C}$. The set of complex
            homomorphisms on a Banach Algebra $A$ is denoted
            $\Delta(A)$.
        \end{ldefinition}
        \begin{theorem}
            If $A$ is a commutative unital Banach Algebra,
            then $\Delta\ne\emptyset$, and $J$ is a maximal
            ideal in $A$ if and only if there is a complex
            homomorphism $h$ such that $J=\mathrm{ker}(h)$.
            Furthermore, if $h$ is a complex homomorphism,
            then $h\in{A}^{*}$ and $\norm{h}=1$. Lastly,
            $\lambda\in\sigma(a)$ if and only if $h(a)=\lambda$
            for some $h\in\Delta(A)$.
        \end{theorem}
        \begin{proof}
            For let $J$ be a maximal ideal in $A$. Then $J$ is
            closed, and therefore the quotient map
            $\pi:A\rightarrow{A}/J$ is such that $\norm{\pi}=1$.
            Suppose $a\in{A}$ and $\pi(a)\ne{0}$. Let
            $M=\{ax+y:x\in{A},y\in{J}\}$. But $A$ is commutative,
            and thus $M$ is an ideal in $A$ which strictly
            contains $J$. Thus, by maximality, $M=A$. Therefore,
            there is an $x\in{A}$ and $y\in{J}$ such that
            $ax+y=I_{A}$. That is, $\pi(x)$ is an inverse of
            $\pi(a)$ in $A/J$. Thus every non-zero element in
            $A/J$ is invertible, and thus $A/J=\mathbb{C}$.
            Therefore, $\pi$ is a norm 1 linear functional.
            Therefore, $\Delta(A)\ne\emptyset$. Suppose
            $h^{\minus{1}}(0)$ is a maximal ideal.  Suppose
            $h^{\minus{1}}(0)\subsetneq{J}$, where $J$ is an
            ideal. Let $a\in{J}$ be such that $h(a)\ne{0}$. If
            $x\in{A}$, then $x=h(x)h^{\minus{1}}(a)a$ is
            contained in $h^{\minus{1}}(0)$. Thus, $x\in{J}$
            and therefore $J=A$. Lastly, if
            $a\in\mathrm{Inv}(A)$, then for all $h\in\Delta$,
            $h(a)h^{\minus{1}}(a)=h(1)=1$. Thus $h(a)\ne{0}$.
            If $a\in\mathrm{Inv}(A)$, then
            $J=\{ax:x\in{A}\}$ is a proper ideal. Hence,
            $J$ is contained in the kernel of some complex
            homomorphism. That is, there is an $h\in\Delta$ such
            that $h(a)=0$. Thus, $a\in\mathrm{Inv}(A)$ if and
            only if $h(a)\ne{0}$ for all $h\in\Delta$. Applying
            this to $\lambda-a$ completes the proof.
        \end{proof}
        \begin{ldefinition}{Gelfand Transform}{Gelfand_Transform}
            The Gelfand-Transform of a point $a$ in a unital
            commutative Banach Algebra is the function
            $\hat{a}:\Delta\rightarrow\mathbb{C}$ defined by
            $\hat{a}(h)=h(a)$.
        \end{ldefinition}
        \begin{ldefinition}{Gelfand Topology}{Gelfand_Topology}
            The Gelfand Topology of a unital commutative
            Banach Algebra $A$ is the initial topology $\tau$
            generated by the set of Gelfand Transforms
            $\mathscr{F}=\{\hat{a}:a\in{A}\}$.
        \end{ldefinition}
        \begin{ldefinition}{Maximal Ideal Space}{Max_Ideal_Space}
            The maximumal ideal space of a unital commutative
            Banach Algebra $A$ is the topological space
            $(\Delta,\tau_{\mathscr{F}})$, where $\Delta$ is the
            set of complex homomorphisms of $A$, and
            $\tau_{\mathscr{F}}$ is the Gelfand Topology on
            $\Delta$.
        \end{ldefinition}
        A subbasis for the topology on $\Delta$ is given by
        the sets:
        \begin{equation}
            \hat{a}^{\minus{1}}(\mathcal{U})
            =\{h\in\Delta:h(u)\in\mathcal{U}\}
        \end{equation}
        For $\mathcal{U}\in{B}_{\varepsilon}(z)$. Thus, we have
        $h_{\alpha}\rightarrow{h}$ if and only if
        $h_{\alpha}(a)\rightarrow{h}(a)$ for all $a\in{A}$.
        Therefore, the Gelfand-Topology is the relative weak
        star topology.
        \begin{theorem}
            If $A$ is a unital commutative Banach Algebra
            with a maximal ideal space $\Delta$, then $\Delta$
            is a compact Hausdorff space in the Gelfand
            topology. Moreover, the Gelfand-Transform
            $a\mapsto\hat{a}$ is an algebra homomorphism of
            $A$ into $C(\Delta)$ with kernel:
            \begin{equation}
                \mathrm{rad}(A)=\bigcap\{\;J\,:\,
                    J\textrm{ is a Maximal Ideal of }A\;\}
            \end{equation}
            And $\norm{\hat{a}}_{\infty}=\rho(a)$.
        \end{theorem}
        \begin{proof}
            By definiton, $\hat{a}\in{C}(\Delta)$. Also,
            $a\mapsto\hat{a}$ is an algebra homomorphism since
            $h$ is a homomorphism, $\hat{a}(h)=h(a)$ and
            $\hat{a}=0$ if and only if $h(a)=0$ for all
            $h\in\Delta$. Thus, the formula for $\mathrm{rad}(A)$
            follows from the previous theorem. But the weak star
            topology is Hausdorff, and therefore $\Delta$ is
            Hausdorff. But $B^{*}$ is weak star compact by
            Banach-Alaglu, and thus it suffices to show that
            $\Delta$ is closed in $B^{*}$. Suppose
            $h_{\alpha}\rightarrow\varphi$ in $B^{*}$.
            Then $h_{\alpha}(a)\rightarrow\varphi(a)$ for all
            $a\in{A}$. But $\varphi(1)=\lim{h_{\alpha}}(1)=1$,
            and therefore $\varphi(1)=1$. Morevoer:
            \begin{equation}
                \varphi(ab)
                =\underset{\alpha}{\lim}h_{\alpha}(ab)
                =\underset{\alpha}{\lim}h_{\alpha}(a)
                    h_{\alpha}(b)
                =\varphi(a)\varphi(b)
            \end{equation}
            And therefore $\varphi\in\Delta$.
        \end{proof}
    \begin{theorem}
        If $A=C(X)$, then $x\mapsto{h}_{x}$ is a
        homeomorphism of $X$ onto $\Delta$.
    \end{theorem}
    \begin{proof}
        Suppose $J$ is an ideal in $C(X)$ such that, given
        $x\in{X}$, there exists $f_{x}\in{J}$ such that
        $f_{x}(x)\ne{0}$. Since $X$ is compact, there exists
        $f_{1},\dots,f_{n}$ in $J$ such that:
        \begin{equation}
            \sum_{k=1}^{n}|f_{k}(x)|^{2}>0
        \end{equation}
        For all $x\in{X}$. Then this sum is in $J$, and:
        \begin{equation}
            (\sum_{k=1}^{n}|f_{k}|^{2})^{\minus{1}}
            \sum_{k=1}^{n}|f_{k}|^{2}=1
        \end{equation}
        And this is also in $J$. Thus, $J=A$. Thus all of the
        maximal ideals in $C(X)$ must be of the form:
        \begin{equation}
            J_{x}=\{f\in{C}(X):f(x)=0\}
        \end{equation}
        And therefore $x\mapsto{h}_{x}$ is surjective. But if
        $x_{i}\mapsto{x}$, then:
        \begin{equation}
            h_{x_{i}}(f)=f(x_{i})\mapsto{f}(x)=h_{x}(f)
        \end{equation}
        And thus $h_{x_{i}}\mapsto{h}$ in $\Delta$. Thus, this
        map is injective and is therefore a bijection. Therefore
        $x\mapsto{h}_{x}$ is a continuous bijection of a compact
        space $X$ onto a Hausdorff space $\Delta$, and is
        therefore a hoemomorphism.
    \end{proof}
    \begin{ltheorem}{Stone-Weierstrass Theorem}{Stone_Weierstrass}
        If $X$ is a compact Hausdorff space, and if $A_{0}$ is
        a sub-algebra of $C(X)$ that is closed under complex
        conjugation which separates points and contains constant
        functions, then $A_{0}$ is dense in $C(X)$ with respect
        to the supremum norm.
    \end{ltheorem}
    A corollary of this is commonly used in the analysis of
    $\mathbb{R}$.
    \begin{ltheorem}{Weierstrass Approximation Theorem}
          {Weierstrass_Approx}
        If $A=C_{\mathbb{R}}([a,b])$, and if
        $P$ is the set of polynomials on $[a,b]$, then
        $P$ is dense in $A$.
    \end{ltheorem}
    Note that if $A=C(\mathbb{T})=C_{\mathbb{C}}(\mathbb{R})$,
    then polynomials are not dense. For example, let
    $f(z)=\overline{z}$. If $p_{n}\in\mathbb{Z}[z]$, and if
    $p_{n}\rightarrow{f}$ uniformly, on $\mathbb{T}$, then:
    \begin{equation}
        0=\int_{|z|=1}p_{n}(z)\diff{z}\not\rightarrow
        \int_{|z|=1}f(z)\diff{z}=2\pi{i}
    \end{equation}
    \begin{ldefinition}{Star Homomorphism}{Star_Homo}
        A star homomorphism between star algebras $A$ and $B$
        is a homomorphism $\Phi:A\rightarrow{B}$ such that
        $\Phi(a^{*})=\Phi(a)^{*}$.
    \end{ldefinition}
    \begin{theorem}
        If $A$ is a $C^{*}$ algebra, then there is a Hilbert
        space $H$ and an isometric star homomorphism
        $\Phi:A\rightarrow\mathscr{L}(H)$, then $\Phi(A)$ is a
        $C^{*}$ algebra of $\mathscr{L}(H)$.
    \end{theorem}
    \begin{ltheorem}{Abstract Spectral Theorem}{Abs_Spectral}
        If $A$ is a unital commutative $C^{*}$ algebra, then
        the Gelfand map $\Phi:A\rightarrow{C}(\Delta)$ is an
        isometric star isomorphism of $A$ onto $C(\Delta)$.
    \end{ltheorem}
    \begin{proof}
        For if $a\in{A}$, then:
        \begin{align}
            a&=\frac{a+a^{*}}{2}+\frac{a-a^{*}}{2}\\
             &=\frac{a+a^{*}}{2}-i\frac{ia-ia^{*}}{2}\\
             &=\frac{a+a^{*}}{2}-i\frac{(ia)+(ia)^{*}}{2}
        \end{align}
        Therefore, $a=x+iy$, with $x,y\in{A}$. Thus, to show
        that $h(a^{*})=\overline{h(a)}$< it suffices to show that
        $h(a)$ is real if $a=a^{*}$. Let $h\in\Delta$ and
        $a=a^{*}$. Then, for all $t\in\mathbb{R}$, we have:
        \begin{equation}
            u_{t}=\exp(ita)=\sum_{n=0}^{\infty}
                \frac{(ita)^{n}}{n!}
        \end{equation}
        Note that if $x$ and $y$ commute, then
        $\exp(x+y)=\exp(x)\exp(x)\exp(y)$. Therefore:
        \begin{equation}
            u_{t}^{*}=\exp(\minus{i}ta)
        \end{equation}
        And thus:
        \begin{equation}
            \norm{u_{t}}^{2}=\norm{u_{t}u_{t}^{*}}
            =\norm{\exp(ita)\exp(\minus{ita}}=1
        \end{equation}
        Since $\norm{h}=1$, $|h(u_{t})|\leq{1}$. And thus:
        \begin{equation}
            |h(\exp(ita))|=|h(\exp(ith(a))|
                =\exp(t\Re(ih(a)))\leq{1}
        \end{equation}
        For all $t\in\mathbb{R}$. Thus, $h(a)\in\mathbb{R}$.
        Now we know:
        \begin{equation}
            \norm{\hat{a}}_{\infty}=\rho(a)
                =\underset{n\rightarrow\infty}{\lim}
                    \norm{a^{n}}^{1/n}
        \end{equation}
        But $\norm{a}^{2}=\norm{a^{*}a}=\norm{a^{2}}$, and thus
        by induction:
        \begin{equation}
            \norm{a}^{2^{n}}=\norm{a^{2^{n}}}
        \end{equation}
        But then $\norm{\hat{a}}_{\infty}=\norm{a}$, and thus
        $\Phi$ is isometric. And isometric implies injective.
        Therefore $\Phi(A)$ is a closed subalgebra of
        $C(\Delta)$. But $\Phi(A)$ separates points, and is closed
        under complex conjugation, and therefore by the
        Stone-Weierstrass theorem, $\Phi(A)=C(\Delta)$.
        Therefore, etc.
    \end{proof}
    Thus, such a map is injective, norm preserving, isometric,
    surjective, and thus bijective, and moreover it is star
    preserving. That is, $\Phi(a^{*})=\Phi(a)^{*}$. More
    specifically, $\hat{a^{*}}(h)=\overline{\hat{a}(h)}$ and
    $h(a^{*})=\overline{h(a)}$.
    Suppose $B$ is a unital sub-algebra of a unital Banach
    Algebra $A$. That is, $I_{A}\in{B}\subseteq{A}$. If
    $b\in\mathbb{R}$, then:
    \begin{equation}
        \sigma_{A}(b)\subseteq\sigma_{B}(b)
    \end{equation}
    It is possible for this containment to be a proper subset.
    For let $D=\{z\in\mathbb{C}:|z|<1\}$. Then
    $\overline{D}=D\cup\mathbb{T}$. Also let
    $A(D)$ be the set of functions on $\overline{D}$ such that
    $f|_{D}\in{H}(D)$. By the maximum modulus principle, the
    map $f\mapsto{f}_{\mathbb{T}}$ is an isometric
    isomorphism of $A(D)$ onto a closed subalgebra of
    $C(\mathbb{T})$. By the reflection principle, we get an
    involution on $A(D)$ and $C(\mathbb{T})$ by
    $f^{*}(z)=\overline{f(z)}$. Let $f\in{A}(D)$ be defined by
    $f(z)=z$. Then $\lambda-z$ is inertible in $A(D)$ if and
    only if $\lambda\notin\overline{D}$. Thus
    $\sigma_{A(D)}(f)=\overline{D}$, and
    $\sigma_{C(\mathbb{T})}(f)=\mathbb{T}$. Thus, the containment
    is proper.
