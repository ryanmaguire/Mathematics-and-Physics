\documentclass[crop=false,class=book,oneside]{standalone}
%----------------------------Preamble-------------------------------%
\input{../../../preamble.tex}                                       %
%--------------------------Main Document----------------------------%
\begin{document}
    \ifx\ifplanetdiff\undefined
        \pagenumbering{roman}
        \title{Preliminaries}
        \author{Ryan Maguire}
        \date{\vspace{-5ex}}
        \maketitle
        \tableofcontents
        \clearpage
        \chapter{Preliminaries}
        \pagenumbering{arabic}
    \else
        \chapter{Preliminaries}
    \fi
    We begin by briefly discussing a few import topics in
    mathematics that one comes across in the study of
    electromagnetism and diffraction theory. This is
    particularly useful for the study of occultation
    observations of planetary rings. We will discuss complex
    analysis, Fourier analysis, a bit of approximation theory,
    and a very brief discussion of multivariate calculus so
    that we may transition from the core of electromagnetism:
    \textit{Maxwell's Equations}, and derive the
    \textit{Fresnel-Huygens Principle}. This is the fundamental
    equation for which diffraction theory is based.
    \section{Complex Analysis}
        \subsection{Complex Numbers}
            A complex number can be represented as
            a point in the plane $z=(x,y)$,
            but we often write:
            \begin{equation}
                z=x+iy
            \end{equation}
            And call $i$ the \textit{imaginary unit}.
            We call $x$ the \textit{real part}, and
            $y$ the \textit{imaginary part}. This is
            denoted $\Re(z)$ and $\Im(z)$. The arithmetic
            goes as follows:
            \begin{subequations}
                \begin{align}
                    (a+ib)+(c+id)&=(a+c)+i(c+d)\\
                    (a+ib)\cdot(c+id)&=(ac-bd)+i(bc+ad)
                \end{align}
            \end{subequations}
            It is convenient to think of $i$ as a number
            such that $i^{2}=-1$. There are two fundamental
            notions worth mentioning: The complex conjugate
            and the modulus of a complex number.
            \begin{ldefinition}{Complex Conjugate}
                The complex conjugate a complex number
                $z=x+iy$ is:
                \begin{equation}
                    \overline{z}=x-iy
                \end{equation}
            \end{ldefinition}
            The complex conjugate of a complex number $z$
            is a reflection of the number about the
            $x$ axis in the complex plane. It is used to
            define the modulus, or absolute value, of
            a complex number.
            \begin{ldefinition}{Modulus of a Complex Number}
                The modulus of a complex number $z=x+iy$ is:
                \begin{equation}
                    |z|=\sqrt{x^{2}+y^{2}}
                \end{equation}
            \end{ldefinition}
            The modulus can also be written as
            $|z|=\sqrt{z\overline{z}}$, where $\overline{z}$
            is the complex conjugate of $z$. Together this allows
            us to define multiplicative \textit{inverses}
            of complex numbers. That is, division by
            non-zero complex numbers is a well defined
            notion.
            \begin{theorem}
                \label{thm:Diff_Theory_Complex_Inverse}
                If $z$ is a non-zero complex number, then there is
                a $z^{-1}$ such that $z\cdot{z}^{-1}=1$. The
                inverse of $z=x+iy$ is:
                \begin{equation}
                    z^{-1}=\frac{x-iy}{x^{2}+y^{2}}
                \end{equation}
            \end{theorem}
            \begin{proof}
                If $a+ib\ne{0}$, then $a^2+b^2\ne{0}$, so
                $\frac{a-ib}{a^2+b^2}$ is wel defined. But:
                \begin{equation}
                    (a+ib)\cdot \frac{a-ib}{a^2+b^2}
                    =\frac{(a+ib)(a-ib)}{a^2+b^2}
                    =\frac{a^2+b^2}{a^2+b^2}=1
                \end{equation}
                The uniqueness of inverses gives us our result
            \end{proof}
            The uniqueness of inverses can be found in any
            abstract algebra textbook.
            If $|z|$ is the modulus of $z$, and
            $\overline{z}$ is it's complex conjugate,
            $z^{-1}$ can be written as:
            \begin{equation}
                z^{-1}=\frac{\overline{z}}{|z|^{2}}
            \end{equation}
            Euler's Theorem is a crucial
            part of complex analysis, and allows one to define
            the \textit{polar representation} of a complex number.
            \begin{ftheorem}{Euler's Exponential Formula}
                            {Diff_Theory_Euler_Expo_Formula}
                If $\theta$ is a real number, then:
                \begin{equation}
                    \exp(i\theta)=\cos(\theta)+i\sin(\theta)
                \end{equation}
            \end{ftheorem}
            \begin{bproof}
                The definition of $\exp(x)$ is given below in
                Eqn.~\ref{def:Diff_Theory_Exp_Func}. Pluggin in
                $i\theta$ into this equation, we obtain
                Eqn.~\ref{def:Diff_Theory_Exp_Func_With_it}.
                \par
                \begin{subequations}
                    \begin{minipage}[b]{0.49\textwidth}
                    \centering
                    \begin{equation}
                        \label{def:Diff_Theory_Exp_Func}
                        \exp(x)=\sum_{n=0}^{\infty}
                        \frac{x^{n}}{n!}
                    \end{equation}
                \end{minipage}
                    \hfill
                    \begin{minipage}[b]{0.49\textwidth}
                    \centering
                    \begin{equation}
                        \label{def:Diff_Theory_Exp_Func_With_it}
                    \exp(i\theta)=\sum_{n=0}^{\infty}
                    i^{n}\frac{\theta^{n}}{n!}
                    \end{equation}
                \end{minipage}
                \end{subequations}
                \clearpage
                But $i^{n}$ cycles between $i,-1,-i,$
                and $1$. So
                we may split this sum into two parts,
                a real part
                and an imaginary part, to obtain:
                \begin{equation}
                    \sum_{n=0}^{\infty}\exp(i\theta)
                    =\sum_{n=0}^{\infty}(-1)^{n}
                    \frac{x^{2n}}{(2n)!}+
                    i\sum_{n=0}^{\infty}
                    (-1)^{n}\frac{x^{2n+1}}{(2n+1)!}
                \end{equation}
                But the left sum is the Taylor expansion for
                $\cos(\theta)$, and the right sum is the Taylor
                expansion for $i\sin(\theta)$.
                This completes the proof.
            \end{bproof}
            Euler's Theorem can also be proved by showing the two
            expressions satisfy the same initial value problem.
            A corollary of this is often hailed as the most
            beautiful result in mathematics.
            This is Euler's Identity:
            \begin{equation}
                e^{i\pi}+1=0
            \end{equation}
            We care about the fact that complex
            number can be written in polar form.
            \begin{theorem}
                If $z$ is a complex number, then there is a
                unique real number $r\geq{0}$ and a
                real number $\theta\in[0,2\pi)$ such that:
                \begin{equation}
                    z=r\exp(i\theta)
                \end{equation}
            \end{theorem}
            \begin{proof}
                Let $z=x+iy$. Define $r$ and $\theta$ as:
                \begin{subequations}
                    \begin{equation}
                        r=\sqrt{x^{2}+y^{2}}
                    \end{equation}
                    \begin{equation}
                        \theta=
                        \begin{cases}
                            \arctan\big(\frac{y}{x}\big),&x>0,y\geq{0}\\
                            \frac{\pi}{2}+\arctan\big(\frac{y}{|x|}\big),
                                &x<0,y\geq{0}\\
                            \pi+\arctan\big(\frac{y}{x}\big),
                            &x<0,y\leq{0}\\
                            \frac{3\pi}{2}+
                            \arctan\big(\frac{|y|}{x}\big),
                            &x<0, y\geq{0}\\
                            \frac{\pi}{2}\sgn(y),&x=0
                        \end{cases}
                    \end{equation}
                \end{subequations}
                Here $\sgn(y)$ is the sign of $y$.
                Euler's Theorem completes the proof.
                Uniqueness of $r$ comes from the fact that
                $|\exp(i\theta)|=1$, so if
                $z=r_{1}\exp(i\theta_{1})$
                and $z=r_{2}\exp(i\theta_{2})$, then
                $|r_{1}|=|r_{2}|$. But $r_{1}$ and $r_{2}$ are
                non-negative, and thus $r_{1}=r_{2}$.
            \end{proof}
            We define the $n^{th}$ root of a complex number to be:
            \begin{equation}
            \sqrt[n]{z}=
            \sqrt[n]{r}\exp\Big(\frac{i\theta}{n}\Big)
        \end{equation}
            This is well defined for all complex numbers since
            the $n^{th}$ root of a non-negative real number $r$
            is well defined, and $\exp(i\theta/n)$ is well
            defined for all real $\theta$. Thus we have avoided
            the messiness of square roots that occurs in
            the real world. By $\sqrt[n]{r}$, we still mean
            the principle positive root. So
            $\sqrt{4}=2$. As our first example:
            \begin{equation}
                \sqrt{i}=\frac{1+i}{\sqrt{2}}
            \end{equation}
        \subsection{Analytic Functions}
            In this text we wish to provide all of the necessary
            mathematics and derivations to rigorously lay the foundation
            for diffraction theory as applied to occultation
            observations. As such we will need various theorems
            to aid us in deriving key results. We take a brief
            moment to talk about what it means to be analytic,
            the Cauchy-Riemann Equations, and Green's Theorem.
            This will tie back later to prove some useful results about
            the \textit{Fresnel Integrals}.
            \begin{definition}
                An entire function is a function
                $f:\mathbb{C}\rightarrow\mathbb{C}$ such that
                for all $z_{0}\in\mathbb{C}$, the following limit
                exists:
                \begin{equation}
                    f'(z_{0})=\underset{z\rightarrow{z_{0}}}{\lim}
                    \frac{f(z)-f(z_{0})}{z-z_{0}}
                \end{equation}
            \end{definition}
            An entire function is simply a complex function that
            is \textit{differentiable} at every point in the
            complex plane. We can weaken this definition to include
            only some parts of the complex plane, and these are
            called \textit{holomorphic} functions.
            A function $f$ is analytic about the point $z_{0}$
            if it's Taylor Series converges for all $z$ in some
            nearby neighborhood of $z_{0}$:
            \begin{equation}
                f(z)=\sum_{n=0}^{\infty}\frac{f^{(n)}(z_{0})}{n!}
                (z-z_{0})^{n}
            \end{equation}
            The remarkable fact of entire functions
            is that they are automatically analytic.
            This is certainly not true for real valued functions.
            One only need consider the example
            $f(x)=x|x|$. The derivative of is
            $f'(x)=2|x|$, and this has no derivative at the
            origin. There is a set of equations that will be
            absolutely essential for later results, and we derive
            them now. These are the
            \textit{Cauchy-Riemann Equations}.
            The remarkable fact about the Cauchy-Riemann equations,
            which we will not go into, is that they are a
            necessary and \textit{sufficient}
            condition for analyticity. We will prove the easy part.
            That is, the analytic functions satisfy the
            Cauchy-Riemann equations.
            The curious should consult a textbook on
            complex analysis.
            \newpage
            \begin{ftheorem}{Cauchy-Riemann Theorem}
                            {Diff_Theory_Cauchy_Riemann}
                If $f:\mathbb{C}\rightarrow\mathbb{C}$ is an
                entire function:
                \begin{equation}
                    f(z)=u(x,y)+iv(x,y)
                \end{equation}
                Then:
                \par
                \begin{subequations}
                    \begin{minipage}{0.49\textwidth}
                        \begin{equation}
                            \frac{\partial{u}}{\partial{x}}=
                            \frac{\partial{v}}{\partial{y}}
                        \end{equation}
                    \end{minipage}
                    \hfill
                    \begin{minipage}{0.49\textwidth}
                        \begin{equation}
                            \frac{\partial{u}}{\partial{y}}=
                            -\frac{\partial{v}}{\partial{x}}
                        \end{equation}
                    \end{minipage}
                \end{subequations}
                \par\hfill\par
            \end{ftheorem}
            \begin{bproof}
                As $f$ is entire, it's complex derivative exists
                at every point in the complex plane.
                Let $z_{0}=x_{0}+iy_{0}$, and let
                $z=x+iy_{0}$. Taking the limit, we have:
                \begin{subequations}
                    \begin{align}
                        f'(z_{0})
                        &=\underset{x\rightarrow{x_{0}}}{\lim}
                        \frac{u(x,y_{0})+iv(x,y_{0})-
                              u(x_{0},y_{0})-iv(x_{0},y_{0})}
                             {x-x_{0}}\\
                        &=\underset{x\rightarrow{x_{0}}}{\lim}
                            \frac{\big(u(x,y_{0})-u(x_{0},y_{0})\big)
                            +i\big(v(x,y_{0})-iv(x_{0},y_{0})\big)}
                            {x-x_{0}}\\
                        &=\underset{x\rightarrow{x_{0}}}{\lim}
                        \Big(\frac{u(x,y_{0})-u(x_{0},y_{0})}
                                  {x-x_{0}}\Big)+
                        i\underset{x\rightarrow{x_{0}}}{\lim}   
                        \Big(\frac{v(x,y_{0})-v(x_{0},y_{0})}
                                  {x-x_{0}}\Big)
                    \end{align}
                \end{subequations}
                Using the definition of
                \textit{partial derivatives}, we obtain:
                \begin{equation}
                    \label{eqn:Cauchy_Riemann_x_Limit}
                    f'(z_{0})=
                    \frac{\partial{u}}{\partial{x}}+
                    i\frac{\partial{v}}{\partial{x}}
                \end{equation}
                Next we evaluate the limit along the path
                $z=x_{0}+iy$. Since the function is complex
                differentiable, any path as
                $z\rightarrow{z_{0}}$ will give the same value.
                Therefore:
                \begin{subequations}
                    \begin{align}
                        f'(z_{0})
                        &=\underset{y\rightarrow{y_{0}}}{\lim}
                        \frac{u(x_{0},y)+iv(x_{0},y)-
                              u(x_{0},y_{0})-iv(x_{0},y_{0})}
                             {i(y-y_{0})}\\
                        &=\underset{y\rightarrow{y_{0}}}{\lim}
                        \frac{\big(u(x_{0},y)-u(x_{0},y_{0})\big)+
                              i\big(v(x_{0},y)-iv(x_{0},y_{0})\big)}
                             {i(y-y_{0})}\\
                        &=\frac{1}{i}
                        \underset{y\rightarrow{y_{0}}}{\lim}
                        \Big(\frac{u(x_{0},y)-u(x_{0},y_{0})}
                                  {i(y-y_{0})}\Big)+
                        \underset{y\rightarrow{y_{0}}}{\lim}   
                        \Big(\frac{v(x_{0},y)-v(x_{0},y_{0})}
                                  {(y-y_{0})}\Big)
                    \end{align}
                \end{subequations}
                Recalling our result from
                Thm.~\ref{thm:Diff_Theory_Complex_Inverse}, the
                inverse of $1/i$ is $-i$. Again using the
                definition of partial derivatives:
                \begin{equation}
                    \label{eqn:Cauchy_Riemann_y_Limit}
                    f'(z_{0})=
                    -i\frac{\partial{u}}{\partial{y}}+
                    \frac{\partial{v}}{\partial{y}}
                \end{equation}
                Thus, equating
                Eqn.~\ref{eqn:Cauchy_Riemann_x_Limit} and
                Eqn.~\ref{eqn:Cauchy_Riemann_y_Limit}, we obtain:
                \begin{equation}
                    \frac{\partial{u}}{\partial{x}}+
                    i\frac{\partial{v}}{\partial{x}}=
                    \frac{\partial{v}}{\partial{y}}
                    -i\frac{\partial{u}}{\partial{y}}
                \end{equation}
                Comparing real and imaginary parts completes
                the proof.
            \end{bproof}
        \subsection{Contour Integrals}
            Next we introduce the idea of contour integrals.
            We start by defining \textit{Jordan Curves}.
            \begin{ldefinition}{Jordan Curve}
                A Jordan Curve in the Complex Plane is a
                continuous function
                $\Gamma:[0,1]\rightarrow\mathbb{C}$ such
                that $\Gamma(0)=\Gamma(1)$, and there are
                no values $0<x_{1}<x_{2}<1$ such that
                $\Gamma(x_{1})=\Gamma(x_{2})$.
            \end{ldefinition}
            A simple example of a Jordan curve is a circle. Jordan
            curves are \textit{closed}, meaning they start where
            they end, and do not self-intersect. A Figure-8 is
            thus \textbf{not} a Jordan curve, but an ellipse is.
            An example of a Jordan curve is given below in
            Fig.~\subref{fig:Diff_Theory_Ex_of_Smooth_Jordan_Curve}.
            Much the way the closed unit interval $[0,1]$ has an ordering
            on it, a Jordan curve has a direction associated with it. Given
            a Jordan curve $\Gamma(t)$, one may change directions by defining
            $\reflectbox{\ensuremath{\Gamma}}(t)=\Gamma(1-t)$.
            While this will plot out the same
            curve in the complex plane, the direction is different and thus
            it represents a different path. When evaluating contour
            integrals, the direction matters.
            \begin{figure}[H]
                \captionsetup{type=figure}
                \centering
                \begin{subfigure}[b]{0.49\textwidth}
                    \centering
                    \subimport{../../../tikz/}{Jordan_Curve_In_Complex_Plane}
                    \subcaption{Smooth Jordan Curve.}
                    \label{fig:Diff_Theory_Ex_of_Smooth_Jordan_Curve}
                \end{subfigure}
                \begin{subfigure}[b]{0.49\textwidth}
                    \centering
                    \subimport{../../../tikz/}{Jordan_Curve_in_Complex_Plane_Sector}
                    \subcaption{Example of a Sector.}
                    \label{fig:Diff_Theory_Ex_of_Jordan_Curve_Sector}
                \end{subfigure}
                \caption{Jordan Curves in the Complex Plane.}
                \label{fig:Diff_Theory_Ex_of_Jordan_Curve}
            \end{figure}
            For the sake of computation, we will stick to
            Jordan curves that are differentiable at all but
            finitely many points. A \textit{sector}, which is
            the region contained within an arc of a circle,
            is an example of a Jordan curve that is differentiable
            at all but three points. An example of a sector is
            shown in Fig.~\subref{fig:Diff_Theory_Ex_of_Jordan_Curve_Sector}.
            We prove Green's Theorem
            for such curves, particularly curves that can be
            broken into a \textit{top} part and a
            \textit{bottom} part. While we wish to prove and
            derive everything needed in the various computations
            for diffraction theory, some theorems are too
            difficult to include. We state the \textit{Jordan Curve Theorem},
            but do not prove it. The proof can be found in a
            textbook on advanced topology or algebraic topology.
            \begin{theorem}
                If $\Gamma:\mathbb{R}\rightarrow\mathbb{R}^{2}$
                is a Jordan curve, then $\Gamma$ separates the
                plane in to two disjoint parts:
                The interior, denoted
                $\Int(\Gamma)$, and the exterior. The interior
                is bounded, the exterior is unbounded, and
                $\Gamma$ is their common boundary.
            \end{theorem}
            A quick look at
            Fig.~\subref{fig:Diff_Theory_Ex_of_Smooth_Jordan_Curve} can
            convince one of the validity of this statement.
            We use the fact that a Jordan curve has an interior
            to prove Green's Theorem, which is useful for the
            evaluation of complex integrals. A student of
            electromagnetism will already understand the
            importance and usefulness of Green's Theorem.
            The Weak Green's Theorem applies to \textit{simple}
            regions.
            There are two types of simple regions: Horizontal and Vertical.
            \begin{definition}
                A vertically simple region is a subset $D$ of the plane
                $\mathbb{R}^{2}$ such that there are two functions
                $g_{1},g_{2}:[a,b]\rightarrow\mathbb{R}$ such that:
                \begin{equation}
                    D=\{(x,y):a\leq{x}\leq{b},g_{1}(x)\leq{y}\leq{g}_{2}(x)\}
                \end{equation}
            \end{definition}
            \begin{definition}
                A horizontally simple region is a subset $D$ of the plane
                $\mathbb{R}^{2}$ such that there are two functions
                $g_{1},g_{2}:[a,b]\rightarrow\mathbb{R}$ such that:
                \begin{equation}
                    D=\{(x,y):a\leq{y}\leq{b},g_{1}(y)\leq{x}\leq{g}_{2}(y)\}
                \end{equation}
            \end{definition}
            A vertically simple region is a subset of the plane bounded by
            two vertical lines, whereas a horizontally simple region is
            bounded by two horizontal lines.
            \begin{theorem}
                \label{thm:Diff_Theory_Greens_Theorem_Simple_t1_region}
                If $M:\mathbb{R}^{2}\rightarrow\mathbb{R}$ is a differentiable
                function and if $\Gamma$ is a Jordan curve such that
                $\Int(\Gamma)$ is vertically simple, then:
                \begin{equation}
                    \iint_{\Int(\Gamma)}
                        \frac{\partial{M}}{\partial{y}}\diff{A}
                    =-\oint_{\Gamma}M\diff{x}
                \end{equation}
            \end{theorem}
            \begin{proof}
                Since the interior of $\Gamma$ is a vertically simple region, there are
                two functions $g_{1},g_{2}:[a,b]\rightarrow\mathbb{R}$ such that:
                \begin{equation}
                    \Gamma=
                    \{(x,y):a\leq{x}\leq{b},g_{1}(x)\leq{y}\leq{g}_{2}(x)\}
                \end{equation}
                But then:
                \begin{subequations}
                    \begin{align}
                        \iint_{\Int(\Gamma)}
                            \frac{\partial{M}}{\partial{y}}\diff{A}
                        &=\int_{a}^{b}\int_{g_{1}(x)}^{g_{2}(x)}
                            \frac{\partial{M}}{\partial{y}}\diff{y}\diff{x}\\
                        &=\int_{a}^{b}
                            \Big(M\big(x,g_{2}(x)\big)
                            -M\big(x,g_{1}(x)\big)\Big)\diff{x}\\
                        \label{eqn:diff_theory_weak_type1_greens_theorem_eq1}
                        &=-\int_{a}^{b}
                            \Big(M\big(x,g_{1}(x)\big)
                            -M\big(x,g_{2}(x)\big)\Big)\diff{x}
                    \end{align}
                \end{subequations}
                But since $\Int(\Gamma)$ is simple, the path at
                $x=a$ is either a point or a vertical
                straight line. But then the integral along this
                portion with respect to $x$ is zero. Similarly for
                $x=b$, and therefore:
                \begin{equation}
                    \label{eqn:diff_theory_weak_type1_greens_theorem_eq2}
                    \oint_{\Gamma}M\diff{x}=
                    \int_{a}^{b}
                            \Big(M\big(x,g_{1}(x)\big)
                            -M\big(x,g_{2}(x)\big)\Big)\diff{x}
                \end{equation}
                Comparing Eqn.~\ref{eqn:diff_theory_weak_type1_greens_theorem_eq1}
                and Eqn.~\ref{eqn:diff_theory_weak_type1_greens_theorem_eq2}
                completes the proof.
            \end{proof}
            \begin{theorem}
                \label{thm:Diff_Theory_Greens_Theorem_Simple_t2_region}
                If $N:\mathbb{R}^{2}\rightarrow\mathbb{R}$ is a differentiable
                function and if $\Gamma$ is a Jordan curve such that
                $\Int(\Gamma)$ is horizontally simple, then:
                \begin{equation}
                    \iint_{\Int(\Gamma)}
                        \frac{\partial{M}}{\partial{x}}\diff{A}
                    =\oint_{\Gamma}N\diff{y}
                \end{equation}
            \end{theorem}
            \begin{proof}
                The proof is a mimicry of the proof for
                Thm.~\ref{thm:Diff_Theory_Greens_Theorem_Simple_t1_region},
                but since the orientation of the path changes since we are
                now integrating with respect to $y$, we pick up a minus sign
                in the contour integral.
            \end{proof}
            \begin{theorem}[Weak Green's Theorem]
                If $M:\mathbb{R}^{2}\rightarrow\mathbb{R}$ and
                $N:\mathbb{R}^{2}\rightarrow\mathbb{R}$ are
                differentiable functions, and if
                $\Gamma$ is a Jordan curve such that the
                interior of $\Gamma$ is vertically and horizontally
                simple (A rectangular region), then:
                \begin{equation}
                    \oint_{\Gamma}(M\diff{x}+N\diff{y})=
                    \iint_{\Int(\Gamma)}\Big(
                    \frac{\partial{N}}{\partial{x}}-
                    \frac{\partial{M}}{\partial{y}}\Big)\diff{A}
                \end{equation}
            \end{theorem}
            \begin{proof}
                Since $\Int(\Gamma)$ is both vertically and horizontally
                simple, we may sum the results from
                Thm.~\ref{thm:Diff_Theory_Greens_Theorem_Simple_t1_region}
                and Thm.~\ref{thm:Diff_Theory_Greens_Theorem_Simple_t2_region}
                to obtain the result.
            \end{proof}
            While this is not quite what we want, since most regions we wish
            to integrate over will not be simple, we can approximate the interior
            of a smooth Jordan curve arbitrarily well by a finite collection of
            simple regions. The full proof will get into the mechanics of this
            approximation, and show that in the \textit{limit} we obtain the
            result. We'll state Green's Theorem, but neglect the full proof.
            \begin{ftheorem}{Green's Theorem}
                            {Diff_Theory_Greens_Theorem}
                If $M:\mathbb{R}^{2}\rightarrow\mathbb{R}$ and
                $N:\mathbb{R}^{2}\rightarrow\mathbb{R}$ are
                differentiable functions, and if
                $\Gamma$ is a Jordan curve that is differentiable at
                all but finitely many points, then:
                \begin{equation}
                    \oint_{\Gamma}(M\diff{x}+N\diff{y})=
                    \iint_{\Int(\Gamma)}\Big(
                    \frac{\partial{N}}{\partial{x}}-
                    \frac{\partial{M}}{\partial{y}}\Big)\diff{A}
                \end{equation}
            \end{ftheorem}
            We now return to complex analysis, and prove one of the
            central theorems in the theory:
            Cauchy's Integral Theorem.
            \newpage
            \begin{ftheorem}{Cauchy's Integral Theorem}
                            {Diff_Theorem_Cauchy_Int_Theorem}
                If $f:\mathbb{C}\rightarrow\mathbb{C}$ is an
                entire function, and if
                $\Gamma:[0,1]\rightarrow\mathbb{C}$ is a
                Jordan curve differentiable at all but finitely
                many points, then:
                \begin{equation}
                    \oint_{\Gamma}f(z)\diff{z}=0
                \end{equation}
            \end{ftheorem}
            \begin{bproof}
                For let $f(z)=u(x,y)+iv(x,y)$. Then:
                \begin{subequations}
                    \begin{align}
                        \oint_{\Gamma}f(z)\diff{z}
                        &=\oint_{\Gamma}\big(u(x,y)+iv(x,y)\big)
                            \big(\diff{x}+i\diff{y}\big)\\
                        \nonumber&=\oint_{\Gamma}
                            \big(u(x,y)\diff{x}-v(x,y)\diff{y}\big)\\
                        &\hspace{2cm}+i\oint_{\Gamma}
                            \big(v(x,y)\diff{x}+u(x,y)\diff{y}\big)
                    \end{align}
                \end{subequations}
                As $f$ is entire, $u$ and $v$ are differentiable.
                Applying Green's Theorem, we obtain:
                \begin{equation}
                    \oint_{\Gamma}f(z)\diff{z}
                    =\iint_{\Int(\Gamma)}
                    \Big(\frac{\partial{u}}{\partial{y}}+
                         \frac{\partial{v}}{\partial{x}}\Big)
                         \diff{A}+
                    i\iint_{\Int(\Gamma)}
                    \Big(\frac{\partial{u}}{\partial{x}}-
                         \frac{\partial{v}}{\partial{y}}\Big)\diff{A}
                \end{equation}
                But since $f$ is entire, $u$ and $v$ satisfy
                the Cauchy-Riemann equations. That is:
                \par\hfill\par
                \begin{subequations}
                    \begin{minipage}{0.49\textwidth}
                        \begin{equation}
                            \frac{\partial{u}}{\partial{x}}-
                            \frac{\partial{v}}{\partial{y}}=0
                        \end{equation}
                    \end{minipage}
                    \hfill
                    \begin{minipage}{0.49\textwidth}
                        \begin{equation}
                            \frac{\partial{u}}{\partial{y}}+
                            \frac{\partial{v}}{\partial{x}}=0
                        \end{equation}
                    \end{minipage}
                \end{subequations}
                \par\hfill\par
                Thus the integrands of both double integrals are zero,
                and hence the integrals are zero. This completes the proof.
            \end{bproof}
            Finally we prove Jordan's Lemma. This is used in conjunction
            with Cauchy's Integral Theorem to provide a powerful means of
            computing the integrals of difficult functions. In particular, this
            is used to evaluate the limits of the \textit{Fresnel Integrals}.
            \begin{ftheorem}{Jordan's Lemma}{Diff_Theory_Jordans_Lemma}
                If blah yeah.
            \end{ftheorem}
    \section{Fourier Analysis}
        \subsection{Basic Notions}
        \subsection{The Fourier Transform}
            \begin{definition}
                The Spectrum of an integrable function
                $f:\mathbb{R}\rightarrow\mathbb{R}$ is the
                function $F$ defined by:
                \begin{equation}
                    F(\omega)=
                    \int_{-\infty}^{\infty}f(t)
                    \exp(-2\pi{i}\omega{t})\diff{t}
                \end{equation}
            \end{definition}
            The requirement that $f$ be integrable is to avoid
            strange issues in mathematics. For the sake of
            physical application, one may assume every function
            is integrable. Mathematically this is far from true,
            but oh well. For the sake of Fourier Analysis, when
            we say integrable we mean Lebesgue integrable. This
            simply means that:
            \begin{equation}
                \int_{-\infty}^{\infty}|f(x)|\diff{x}<\infty
            \end{equation}
            We now prove what is probably the most useful theorem
            in Fourier Analysis.
            \begin{theorem}
                If $f:\mathbb{R}\rightarrow\mathbb{R}$ is a
                continuous Lebesgue integrable function,
                and if its spectrum $F$ is also continuous
                and Lebesgue integrable, then:
                \begin{equation}
                    f(t)=\int_{-\infty}^{\infty}F(\omega)
                    \exp(2\pi{i}\omega{t})\diff{\omega}
                \end{equation}
            \end{theorem}
            A powerful application of this is
            Shannon's Sampling Theorem.
            \begin{theorem}[Shannon's Sampling Theorem]
                If $f(t)$ is a continuous
                Lebesgue integrable function such that
                its spectrum $F(\omega)$ is differentiable and zero
                outside the interval $[-W,W]$,
                then $f(t)$ is uniquely determined
                by the points $f(\frac{n}{2W})$, $n\in\mathbb{N}$.
            \end{theorem}
            \begin{proof}
                For let $F$ be the spectrum of $f$. That is:
                \begin{equation}
                    f(t)=\int_{-\infty}^{\infty}F(\omega)
                    \exp(-2\pi{i}\omega{t})\diff{\omega}
                \end{equation}
                But $F(\omega)=0$ for $|\omega|>W$. Thus we have:
                \begin{equation}
                    f(t)=\int_{-W}^{W}F(\omega)
                    \exp(-2\pi{i}\omega{t})\diff{\omega}
                \end{equation}
                Then for $n\in\mathbb{N}$, we have:
                \begin{equation}
                    f\big(\frac{n}{2W}\big)=\int_{-W}^{W}F(\omega)
                    \exp(-2\pi{i}\frac{n}{2W}\omega)\diff{\omega}
                \end{equation}
                But $F$ is differentiable, and thus it's Fourier
                series converges. That is:
                \begin{subequations}
                    \begin{align}
                        F(\omega)
                        &=\sum_{n=-\infty}^{\infty}e^{2\pi{i}n\omega}
                        \int_{W}^{W}F(\tau)
                        \exp(-2\pi{i}\frac{n}{2W}\tau)\diff{\tau}\\
                        &=\sum_{n=-\infty}^{\infty}f
                          \big(\frac{n}{2W}\big)e^{2\pi{i}n\omega}
                    \end{align}
                \end{subequations}
                Therefore $f(\frac{n}{2W})$, $n\in \mathbb{N}$
                uniquely determines $F(\omega)$. But the
                spectrum $F(\omega)$ uniquely determines
                $f(t)$. Therefore $f(t)$ is
                uniquely determined and:
                \begin{equation}
                    f(t)=\sum_{n=-\infty}^{\infty}\int_{-W}^{W}
                    f\big(\frac{n}{2W}\big)
                    \exp(2\pi{i}\omega(n+t))\diff{\omega}
                \end{equation}
            \end{proof}
        \subsection{Convolutions}
        \subsection{Sampling}
    \section{Numerical Analysis}
        \subsection{Power Series}
        \subsection{Asymptotic Expansions}
        \subsection{Stationary Phase Approximation}
            Suppose $g$ is an analytical function about
            the origin (i.e. it has a convergent MacLaurin series),
            and consider the integral:
            \begin{equation}
                I(k) = \int_{a}^{b}e^{ikg(x)}dx
            \end{equation}
            Suppose that there is a $c\in(a,b)$ such
            that $g'(c)=0$ and $g''(c)\ne 0$. Then:
            \begin{align}
                \nonumber I(k)&=e^{ikg(c)}\int_{a}^{b}e^{ik[g(x)-g(c)]}dx\\
                &=e^{ikg(c)}\int_{a}^{b}e^{ik[\frac{g''(c)}{2}(x-c)^{2}+\hdots]}dx
            \end{align}
            Higher terms are extremely oscillatory, and so we neglect them.
            \begin{remark}
                Note that higher terms can indeed cancel each
                other out, meaning these neglected terms may
                not be negligible. For example, if
                $g(x)=-sin(\pi x)$, then $\exp(ig(x))$ is never
                too oscillatory. However, so long as the interval
                $[a,b]$ is small enough, the approximation is still
                valid. The previously mentioned $g(x)$ is how
                one approximates the $J_{0}(x)$ Bessel function.
            \end{remark}
            Out integral then becomes:
            \begin{align}
                I(k)&\approx e^{ikg(c)}\int_{a}^{b}e^{ik\frac{g''(c)}{2}(x-c)^{2}}dx\\
                &\approx e^{ikg(c)}\int_{\infty}^{\infty}e^{ik\frac{g''(c)}{2}(x-c)^{2}}dx\\
                &=e^{ikg(c)}\sqrt{\frac{2\pi i}{kg''(c)}}
            \end{align}
            We can use this for our double integral,
            and make it a single integral. The first and
            second integrals of $\psi$ are nasty, however.
            \begin{equation}
                \frac{\partial \psi}
                     {\partial \phi}
                =kD\Big[\frac{2D\rho\cos(B)\sin(\phi)+2\rho\rho_{0}
                \sin(\phi-\phi_{0})}{2D^2\sqrt{1+2\cos(B)
                \frac{\rho_{0}\cos(\phi_{0})-\rho\cos(\phi)}{D}+
                \frac{\rho^{2}+\rho_{0}^{2}-
                2\rho\rho_{0}\cos(\phi-\phi_{0})}{D^2}}}-
                \frac{\rho\cos(B)\sin(\phi)}{D}\Big]
            \end{equation}
            The second derivative is equally bad.
            Solving for $\frac{\partial\psi}{\partial\phi}=0$
            must be done iteratively by successive approximations.
            A further approximation can be made as $\psi$
            is analytic in $\phi$. Let $\phi_{s}$ be
            the solution to $\frac{\partial\psi}{\partial\phi}$
            and let $\phi_{s_{n}}$ be a sequence such that
            $\phi_{s_{n}}\rightarrow\phi_{s}$.
        \subsection{Root Finding}
    \section{Partial Differential Equations}
        \subsection{Green's Functions}
        \subsection{Kirchoff's Integral Formula}
        \subsection{The Wave Equation}
        \subsection{The Helmholtz Equation}
    \section{Special Functions}
        There are many special functions that arise in diffraction
        theory. These are functions that can not be written in
        closed form via a combination of rational functions,
        trigonometric functions, logarithms, or exponentials.
        Usually such functions are defined as the solution to
        a particular differential equation, such as Bessel functions,
        or as the result of integrating a non-trivial function,
        such as Fresnel Integrals. Other times functions are defined
        as the inverse of a tricky algebraic equation, such that the
        Lambert $W$ function. We'll discuss these three functions,
        numerical calculations, and their applications.
        \subsection{The Fresnel Integrals}
            The Fresnel Sine and Cosine Integrals, which are
            usually denoted $S(x)$ and $C(x)$, respectively,
            occur naturally in the study of diffraction theory.
            By examining the \textit{Fresnel Kernel},
            and using a Taylor series approximation, one
            comes across the following integral:
            \begin{equation}
                F(x)=\int_{0}^{x}\exp(it^{2})\diff{t}
            \end{equation}
            Using Euler's Theorem, we can rewrite this as:
            \begin{equation}
                F(x)=\int_{0}^{x}\cos(t^{2})\diff{t}+i\int_{0}^{x}\sin(t^{2})\diff{t}
            \end{equation}
            The Fresnel Cosine and Sine Integrals are defined as the real and
            imaginary parts of this equation, respectively.
            \begin{ldefinition}{Fresnel Integrals}
                The Fresnel Sine and Fresnel Cosine, denoted
                $S(x)$ and $C(x)$, respectively, are real valued
                functions defined by:
                \par\hfill\par
                \vspace{-1ex}
                \begin{subequations}
                    \begin{minipage}{0.49\textwidth}
                        \begin{equation}
                            S(x)=\int_{0}^{x}
                            \sin(t^{2})\diff{t}
                        \end{equation}
                    \end{minipage}
                    \hfill
                    \begin{minipage}{0.49\textwidth}
                        \begin{equation}
                            C(x)=\int_{0}^{x}
                            \cos(t^{2})\diff{t}
                        \end{equation}
                    \end{minipage}
                \end{subequations}
            \end{ldefinition}
            Graph of the Fresnel Sine and Fresnel Cosine functions are
            shown in Fig.~\ref{fig:Diff_Theory_Graphs_of_Sinx2_and_Cosx2}.
            \begin{figure}[H]
                \captionsetup{type=figure}
                \centering
                \begin{subfigure}[b]{0.49\textwidth}
                    \centering
                    \resizebox{\textwidth}{!}{%
                        \subimport{../../../tikz/}{Fresnel_Cosine}
                    }
                    \subcaption{Graphs of $\cos(x^{2})$ and $C(x)$.}
                \end{subfigure}
                %\par\hfill\par
                \begin{subfigure}[b]{0.49\textwidth}
                    \centering
                    \resizebox{\textwidth}{!}{%
                        \subimport{../../../tikz/}{Fresnel_Sine}
                    }
                    \subcaption{Graphs of $\sin(x^{2})$ and $S(x)$.}
                \end{subfigure}
                \caption[Fresnel Integrals]
                    {Graphs of the Fresnel Sine and Cosine
                     integrals, along with their respective derivatives.}
                \label{fig:Diff_Theory_Graphs_of_Sinx2_and_Cosx2}
            \end{figure}
            We will be interested in functions
            of the form $\exp(i\psi)$ later on. The Fresnel
            Approximation uses the Taylor expansion of $\psi$
            up to the quadratic term, and hence we will see
            something of the form $\exp(i(a+bx+cx^2))$. From
            elementary algebra we can complete the square, and
            do a change of variables to obtain
            $\exp(i(u^{2}-d^{2}))$, where $d$ is some constant.
            We are interested in the integral of this across the
            entire real line. From Euler's Formula
            (Thm.~\ref{th:Diff_Theory_Euler_Expo_Formula}) we
            see that $\exp(ix^{2})=\cos(x^{2})+i\sin(x^{2})$.
            But $x^{2}$ grows rapidly,
            and thus $\sin(x^{2})$ and $\cos(x^{2})$ are two
            rapidly oscillating functions. The oscillation are
            so rapid that the areas cancel out, and hence
            $S(x)$ and $C(x)$ are well defined as
            $x\rightarrow\infty$. We will use Cauchy's Integral
            Theorem to evaluate the limits of these two functions.
            First, a result from Gauss.
            \begin{theorem}
                \begin{equation}
                    \int_{-\infty}^{\infty}\exp(-x^{2})\diff{x}
                    =\sqrt{\pi}
                \end{equation}
            \end{theorem}
            \begin{proof}
                Convergence can be shown, since for
                all $x\in\mathbb{R}$:
                \begin{equation}
                    0<\exp(-x^{2})\leq\frac{1}{1+x^2}
                \end{equation}
                And therefore:
                \begin{equation}
                    0\leq\int_{-\infty}^{\infty}
                    \exp(-x^{2})\diff{x}\leq
                    \int_{-\infty}^{\infty}
                    \frac{1}{1+x^2}\diff{x}
                    =\tan^{-1}(x)\big|_{-\infty}^{\infty}=\pi
                \end{equation}
                Define the following:
                \begin{equation}
                    \mathcal{I}=\int_{-\infty}^{\infty}
                    \exp(-x^{2})\diff{x}
                \end{equation}
                Squaring $\mathcal{I}$, we obtain:
                \begin{subequations}
                    \begin{align}
                        \mathcal{I}^{2}&=
                        \Big(\int_{-\infty}^{\infty}
                        \exp(-x^{2})\diff{x}\Big)
                        \Big(\int_{-\infty}^{\infty}
                        \exp(-y^{2})\diff{y}\Big)\\
                        &=\int_{-\infty}^{\infty}
                        \int_{-\infty}^{\infty}
                        \exp\big(-(x^2+y^2)\big)\diff{x}\diff{y}
                    \end{align}
                \end{subequations}
                Switching from Cartesian to
                Polar coordinates, we have:
                \begin{equation}
                    \mathcal{I}^{2}=
                    \int_{0}^{2\pi}\int_{0}^{\infty}
                    r\exp(-r^{2})\diff{r}\diff{\phi}=
                    2\pi \int_{0}^{\infty}
                    r\exp(-r^{2})\diff{r}
                \end{equation}
                This final integral can be computed from basic
                methods one would find in a Calculus textbook.
                Letting $u=r^{2}$, we have
                $\diff{u}=2r\diff{r}$,
                so the integral becomes:
                \begin{equation}
                    \mathcal{I}^{2}=
                    \pi\int_{0}^{\infty}\exp(-u)\diff{u}
                \end{equation}
                Hence, $\mathcal{I}^{2}=\pi$, and therefore
                $\mathcal{I}=\pm\sqrt{\pi}$.
                But $\mathcal{I} > 0$, and thus
                $\mathcal{I}=\sqrt{\pi}$.
            \end{proof}
            This result has many fundamental applications in
            probability theory and in statistics, where
            it is used to define the normal distribution.
            For us, we can use this to evaluate the limits of
            $S(x)$ and $C(x)$ as $x\rightarrow\infty$.
            First note, that since $\exp(-x^{2})$ is an even
            function, the integral on $[0,\infty)$ is half of
            that of the integral on the entire real line. That is:
            \begin{equation}
                \int_{0}^{\infty}\exp(-x^{2})\diff{x}
                =\frac{\sqrt{\pi}}{2}
            \end{equation}
            We now evaluate the complex version of this.
            \begin{theorem}
                \begin{equation}
                    \int_{0}^{\infty}\exp(ix^{2})\diff{x}
                    =\sqrt{\frac{\pi}{8}}(1+i)
                \end{equation}
            \end{theorem}
            \begin{proof}
                For let $C_{R}$ be the closed path in the complex plane
                defined by:
                \begin{equation}
                    C_{R}(t)=
                    \begin{cases}
                        3Rt,&0\leq{t}\leq\frac{1}{3}\\
                        R\exp\big(i\frac{3\pi}{4}(t-\frac{1}{3})\big),
                        &\frac{1}{3}<t<\frac{2}{3}\\
                        \frac{3R}{\sqrt{2}}(1+i)(1-t),
                        &\frac{2}{3}\leq{t}\leq{1}
                    \end{cases}
                \end{equation}
                Then, for all $R>0$, $C_{R}$ is a Jordan Curve in the
                complex differentiable at all but three points. Thus,
                by Cauchy's Theorem, as $\exp(iz^{2})$ is an entire
                function:
                \begin{equation}
                    \oint_{C_{R}}\exp(iz^{2})\diff{z}
                    =0
                \end{equation}
                But then:
                \begin{equation}
                    \begin{split}
                        \int_{0}^{R}\exp(ix^{2})\diff{x}+
                        \int_{\frac{1}{3}}^{\frac{2}{3}}&
                            \exp(iz(t)^{2})C_{R}'(t)\diff{t}\\
                        &+\frac{1+i}{\sqrt{2}}\int_{R}^{0}\exp(-x^{2})\diff{x}
                        =0
                    \end{split}
                \end{equation}
                But by Jordan's Lemma, this second integral tends to zero as
                $R\rightarrow\infty$. Therefore:
                \begin{align}
                    \int_{0}^{\infty}\exp(-ix^{2})\diff{x}
                    &=-\frac{1+i}{\sqrt{2}}\int_{\infty}^{0}\exp(-x^{2})\diff{x}\\
                    &=\frac{1+i}{\sqrt{2}}\int_{0}^{\infty}\exp(-x^{2})\diff{x}\\
                    &=\frac{1+i}{\sqrt{2}}\frac{\sqrt{\pi}}{2}\\
                    &=(1+i)\sqrt{\frac{\pi}{8}}
                \end{align}
            \end{proof}
            \begin{theorem}
                If $S$ and $C$ are the Fresnel Sine and Cosine integrals,
                respectively, then:
                \begin{align}
                    \underset{x\rightarrow\infty}{\lim}S(x)
                    &=\sqrt{\frac{\pi}{8}}\\
                    \underset{x\rightarrow\infty}{\lim}C(x)
                    &=\sqrt{\frac{\pi}{8}}
                \end{align}
            \end{theorem}
            \begin{proof}
                For:
                \begin{align}
                    \underset{x\rightarrow\infty}{\lim}
                        \big(C(x)+iS(x)\big)
                    &=\underset{x\rightarrow\infty}{\lim}
                        \int_{0}^{x}\exp(ix^{2})\diff{x}\\
                    &=\sqrt{\frac{\pi}{8}}(1+i)
                \end{align}
                Comparing real and imaginary parts complex the proof.
            \end{proof}
            \begin{figure}[H]
                \centering
                \captionsetup{type=figure}
                \begin{tikzpicture}[%
                    line width=0.4mm,
                    line cap=round,
                    >=Latex,
                    ->-/.style={%
                        decoration={%
                            markings,
                            mark=at position 0.55 with \arrow{>},
                        },
                        postaction={decorate}
                    }
                ]
                    \draw[->] (0, 0) to (3.2in, 0);
                    \draw[->] (0, 0) to (0, 3.2in);
                    \draw[->] (-0.5, 0) -- (3in, 0) node[above] {$\Re\{z\}$};
                    \draw[->] (0, -0.5) -- (0, 3in) node[right] {$\Im\{z\}$};
                    \draw (2in, 3pt) -- (2in, -3pt) node [below] {$R$};
                    \draw (3pt, 2in) -- (-3pt, 2in) node [left] {$R$};
                    \draw[->-, draw=blue] (0, 0) to (2in, 0);
                    \draw[->-, draw=blue] (2in, 0) arc (0:45:2in);
                    \draw[->-, draw=blue] (1.4141in, 1.4141in) to (0, 0);
                    \node at (0.8in, 1.3in) {$C_{R}(t)$};
                    \node at (2in, 2in) {\Large{$\mathbb{C}$}};
                \end{tikzpicture}
                \caption{Jordan Curve Used to Evaluate the Fresnel Integrals.}
                \label{fig:DIFF_THEORY_JORDAN_CURVE_FOR_FRESNEL_INTEGRALS}
            \end{figure}
            \par\hfill\par
            \begin{theorem}
                If $F$ is a positive real number and
                $f:\mathbb{R}\rightarrow\mathbb{C}$ is defined by:
                \begin{equation}
                    f(\rho)=
                        \exp\bigg(
                            i\frac{\pi}{2}
                            \Big(\frac{\rho}{F}\Big)^{2}
                        \bigg)
                \end{equation}
                Then:
                \begin{equation}
                    \mathcal{F}_{\xi}(f)
                    =(1+i)F\exp(\minus{2}\pi{i}F^{2}\xi^{2})
                \end{equation}
            \end{theorem}
            \begin{proof}
            For:
            \begin{align}
                \mathcal{F}_{\xi}(f)
                &=\int_{-\infty}^{\infty}
                    \exp\bigg(
                        i\frac{\pi}{2}
                        \Big(\frac{\rho}{F}\Big)^{2}
                    \bigg)
                    \exp(-2\pi{i}\rho\xi)\diff{\rho}\\
                &=\int_{-\infty}^{\infty}
                    \exp\bigg(
                        i\frac{\pi}{2}\Big(
                            \frac{\rho}{F}
                        \Big)^{2}-2\pi{i}\rho\xi
                    \bigg)
                    \diff{\rho}\\
                &=\int_{-\infty}^{\infty}
                    \exp\Big(
                        \frac{i\pi}{2F^2}
                        \big[\rho^2-4F^2\rho \xi\big]
                    \Big)\diff{\rho}
            \end{align}
            Completing the square, we get
            $(\rho-2F^{2}\xi)^{2}-4F^{4}\xi^{2}$.
            So, the integral becomes:
            \begin{equation}
                \begin{split}
                    \int_{-\infty}^{\infty}
                    \exp&\Big(
                        i\frac{\pi}{2F^2}
                        \big[\rho-2F^{2}\xi\big]^{2}
                    \Big)
                    \exp(-2\pi{i}F^{2}\xi^{2})\diff{\rho}\\
                    &=\exp(-2\pi{i}F^{2}\xi^{2})
                    \int_{-\infty}^{\infty}
                    \exp\Big(
                        i\frac{\pi}{2F^2}
                        \big[\rho-2F^{2}\xi\big]^{2}
                    \Big)\diff{\rho}
                \end{split}
            \end{equation}
            Let $u=\frac{\rho-2F^{2}\xi}{F}$, so then
            $F\diff{u}=\diff{\rho}$. We obtain:
            \begin{equation}
                \mathcal{F}_{\xi}(f)
                =F\exp(\minus{2}\pi{i}F^{2}\xi^{2})
                    \int_{-\infty}^{\infty}
                    \exp\Big(i\frac{\pi}{2}s^{2}\Big)\diff{s}
            \end{equation}
            But this integral is $1+i$, completing the proof.
            \end{proof}
            \begin{theorem}
            $\mathcal{F}(e^{-i\frac{\pi}{2}\big(\frac{\rho_0}{F}\big)^2}\big) = (1-i)Fe^{2\pi i F^2 \xi^2}$.
            \end{theorem}
            \begin{proof}
            For:
            \begin{align}
                \mathcal{F}_{\xi}(f)
                &=\int_{-\infty}^{\infty}
                    \exp\bigg[
                        \minus{i}\frac{\pi}{2}
                        \Big(\frac{\rho}{F}\Big)^2
                    \bigg]
                    \exp(\minus{2}\pi{i}\rho\xi)\diff{\rho}\\
                &=\int_{-\infty}^{\infty}
                    \exp\Big(
                        {\!}\minus{\!}\frac{i\pi}{2F^{2}}
                        \big[
                            {\rho}^{2}+4F^{2}\rho\xi
                        \big]\Big)\diff{\rho}\\
                &=\int_{-\infty}^{\infty}
                    \exp\Big(
                        {\!}\minus{\!}\frac{i\pi}{2F^{2}}
                        \big[
                            (\rho+2F^{2}\xi)^2-4F^{4}\xi^{2}
                        \big]
                    \Big)\diff{\rho}\\
                &=\exp(2\pi{i}F^{2}\xi^{2})\int_{-\infty}^{\infty}
                    \exp\Big(
                        {\!}\minus{\!}\frac{i\pi}{2F^2}
                        (\rho_0+2F^2\xi)
                    \Big)\diff{\rho}
            \end{align}
            Let $u = \frac{\rho_0 + 2F^2 \xi}{F}$, then
            $Fdu = d\rho_0$, so we have$Fe^{2\pi i F^2 \xi^2} \int_{-\infty}^{\infty} e^{-i\frac{\pi}{2}u^2}du$.
            Let $u = -is$, then $du = -ids$, and $u^2 = -s^2$. So
            we have $-i e^{2\pi i F^2 \xi^2}\int_{-\infty}^{\infty} e^{i\frac{\pi}{2}s^2}ds$.
            But this integral is $1+i$. So, we have
            $-iFe^{2\pi iF^2\xi^2}(1+i)=(1-i)Fe^{2\pi iF^2 \xi^2}$.
            \end{proof} 
            \begin{theorem}
                If $f:\mathbb{R}\rightarrow\mathbb{R}$ and
                $g:\mathbb{R}\rightarrow{R}$ are integrable,
                and if $f*g$ is the convolution of $f$ with
                respect to $g$:
                \begin{equation}
                    f*g=\int_{-\infty}^{\infty}
                        f(\tau)g(\tau-t)\diff{\tau}
                \end{equation}
                Then:
                \begin{equation}
                    \mathcal{F}_{\xi}\big(f*g\big)
                    =\mathcal{F}_{\xi}(f)\cdot\mathcal{F}_{\xi}(g)
                \end{equation}
            \end{theorem}
            \begin{proof}
            Let $\int_{-\infty}^{\infty} |f(t)|dt = \norm{f}_{1}$ and $\int_{-\infty}^{\infty} |g(t)|dt = \norm{g}_{1}$. Then:
            \begin{align*}
                \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}
                    |f(\tau)g(\tau-t)|\diff{\tau}\diff{t}
                &\leq\int_{-\infty}^{\infty}
                    |f(\tau)|\int_{-\infty}^{\infty}
                    |g(\tau-t)|\diff{\tau}\diff{t}\\
                &=\int_{\infty}^{\infty}
                    |f(x)|\norm{g}_{1}\diff{x}\\
                &=\norm{f}_{1}\norm{g}_{1}    
            \end{align*}
            Thus, $h(t)=f*g$ is such that
            $\int_{-\infty}^{\infty} |h(t)|dt < \infty$.
            Let $H(\xi) = \mathcal{F}(h)$. Then:
            \begin{align}
                H(\xi)&=
                    \int_{-\infty}^{\infty}
                    h(t)\exp(\minus{2}\pi{i}t\xi)\diff{t}\\
                &=\int_{-\infty}^{\infty}
                    \Bigg(
                        \int_{-\infty}^{\infty}
                        f(\tau)g(t-\tau)\diff{\tau}
                    \Bigg)\exp(\minus{2}\pi{i}t\xi)\diff{t}
            \end{align}
            But:
            \begin{equation}
                |e^{-2\pi{i}t\xi}f(\tau)g(t-\tau)|
                =|f(\tau)g(t-\tau)|
            \end{equation}
            But this is simply the integrand of $h$, and $h$
            is integrable. Thus, by Fubini's Theorem we may
            swap the integrals. Let $y=t-\tau$. Then:
            \begin{align}
                H(\xi)&=
                    \int_{-\infty}^{\infty}f(\tau)
                    \int_{-\infty}^{\infty}g(t-\tau)
                    e^{-2\pi{i}t\xi}\diff{t}\diff{\tau}\\
                &=\int_{-\infty}^{\infty}f(\tau)
                    e^{-2\pi{i}\tau\xi}\diff{\tau}
                    \int_{-\infty}^{\infty}g(y)
                    e^{-2\pi{i}y\xi}dy\\
                &=\mathcal{F}(f)\cdot\mathcal{F}(g)
                    \vphantom{\int_{-\infty}^{\infty}}
            \end{align}
            Therefore, etc.
            \end{proof}
            \begin{theorem}
            If $\hat{T}(\rho_0) = \frac{1-i}{2F}\int_{-\infty}^{\infty}T(\rho) e^{i\frac{\pi}{2}\big(\frac{\rho-\rho_0}{F}\big)^2}d\rho$, then $T(\rho) = \frac{1+i}{2F}\int_{-\infty}^{\infty}\hat{T}(\rho_0)e^{-i\frac{\pi}{2}\big(\frac{\rho-\rho_0}{F}\big)^2}d\rho_0$.
            \end{theorem}
            \begin{proof}
            For $\hat{T}(\rho) = \frac{1-i}{2F}(T* e^{i \frac{\pi}{2}\big(\frac{\rho}{F}\big)^2}\big)$. Therefore:
            \begin{align*}
            \mathcal{F}(\hat{T})&=\frac{1-i}{2F}\mathcal{F}(T)\cdot \mathcal{F}(e^{i\frac{\pi}{2}\big(\frac{\rho_0}{F}\big)^2})\\
            &=\frac{1-i}{2F}\mathcal{F}(T)\cdot(1+i)Fe^{-2\pi i F^2 \xi^2}\\
            &=\mathcal{F}(T)e^{-2\pi i F^2 \xi^2}
            \end{align*}
            But $e^{2\pi iF^{2}\xi^{2}}=\frac{1}{(1-i)F}\mathcal{F}\big(e^{-i\frac{\pi}{2}\big(\frac{\rho_0}{F}\big)^2}\big) = \frac{1+i}{2F} \mathcal{F}\big(e^{-i\frac{\pi}{2}\big(\frac{\rho_0}{F}\big)^2}$. Therefore:
            \begin{align*}
            \mathcal{F}(T)&=\frac{1+i}{2F}\mathcal{F}(\hat{T})\cdot\mathcal{F}\big(e^{-i\frac{\pi}{2}\big(\frac{\rho_0}{F}\big)^2}\big)\\
            &=\frac{1+i}{2F}\mathcal{F}(\hat{T}*e^{-i\frac{\pi}{2}\big(\frac{\rho_0}{F}\big)^2})\\
            &=\mathcal{F}\bigg(\frac{1+i}{2F}\int_{-\infty}^{\infty} \hat{T}(\rho_0)e^{-i\frac{\pi}{2}\big(\frac{\rho - \rho_0}{F}\big)^2} d\rho_0\bigg)
            \end{align*}
            Therefore, $T(\rho) = \frac{1+i}{2F}\int_{-\infty}^{\infty}\hat{T}(\rho_0)e^{-i\frac{\pi}{2}\big(\frac{\rho - \rho_0}{F}\big)^2}d\rho_0$.
            \end{proof}
            
            \begin{theorem}
            If $T,\psi \in L^2(\mathbb{R})$, and if $\hat{T}(\rho_0) = \int_{-\infty}^{\infty} T(\rho)e^{i\psi(\rho_0-\rho)}d\rho_0$, then $T(\rho) = \mathcal{F}^{-1}_{\rho}\big(\frac{\mathcal{F}(\hat{T})}{\mathcal{F}(e^{i\psi})}\big)$.
            \end{theorem}
            \begin{proof}
            For $\hat{T}(\rho_0) = T* e^{i\psi}$. But then $\mathcal{F}_{\xi}(\hat{T}) = \mathcal{F}_{\xi}\big(T* e^{i\psi}\big) = \mathcal{F}_{\xi}\big(T\big)\cdot \mathcal{F}_{\xi}\big(e^{i\psi}\big)$. So then $\mathcal{F}_{\xi}(T) = \frac{\mathcal{F}_{\xi}(\hat{T})}{\mathcal{F}_{\xi}(e^{i\psi})}$. Therefore $T(\rho) = \mathcal{F}^{-1}_{\rho}\big(\frac{\mathcal{F}_{\xi}(\hat{T})}{\mathcal{F}_{\xi}(e^{i\psi})}\big)$
            \end{proof}
            
        \subsection{Bessel Functions}
\end{document}