%------------------------------------------------------------------------------%
\documentclass{article}                                                        %
%------------------------------Preamble----------------------------------------%
\makeatletter                                                                  %
    \def\input@path{{../../}}                                                  %
\makeatother                                                                   %
\input{preamble.tex}                                                           %
%----------------------------Main Document-------------------------------------%
\begin{document}
    \title{Algebra}
    \author{Ryan Maguire}
    \date{\vspace{-5ex}}
    \maketitle
    \pagenumbering{roman}
    \tableofcontents
    \clearpage
    \pagenumbering{arabic}
    \section{HW I (Redo)}
        \begin{problem}
            Let $f(x)=x^{4}-1$ and $g(x)=3x^{2}+3x$. Find the quotient and
            remainder after dividing $f$ by $g$, the $\GCD$ and $f$ and $g$,
            and the expression of the $\GCD$ in terms of $af+bg$ with
            $a,b\in\mathbb{Q}[x]$. Use the Euclidean algorithm and B\'{e}zout's
            identity.
        \end{problem}
        \begin{solution}
            We first try to find the quotient $q$. Noting that $f$ is monic, and
            that the leading coefficient of $g$ is 3, we try
            $q_{0}=\frac{1}{3}x^{2}$. This gives:
            \begin{equation}
                q_{0}(x)g(x)=\frac{1}{3}x^{2}(3x^{2}+3x)
                    =x^{4}+x^{3}=(x^{4}-1)+(x^{3}+1)
                    =f(x)+(x^{3}+1)
            \end{equation}
            so we need to subtract off $x^{3}+1$. We try
            $q_{1}=\minus\frac{1}{3}x$ and obtain:
            \begin{equation}
                q_{1}(x)g(x)=\minus\frac{1}{3}x(3x^{2}+3x)
                    =\minus{x}^{3}-x^{2}
                    =(\minus{x}^{3}-1)+(\minus{x}^{2}+1)
            \end{equation}
            so the remainder is now $\minus{x}^{2}+1$. We try $\frac{1}{3}$, and
            get:
            \begin{equation}
                q_{2}(x)g(x)=\frac{1}{3}(3x^{2}+3x)
                    =x^{2}+x
                    =(x^{2}-1)+(x+1)
            \end{equation}
            and so we are left with remainder $\minus{x}-1$. That is:
            \begin{equation}
                x^{4}-1=\Big(\frac{1}{3}x^{2}-\frac{1}{3}x+\frac{1}{3}\Big)
                    (3x^{2}+3x)+(\minus{x}-1)
            \end{equation}
            We double check just to be safe:
            \begin{align*}
                \Big(\frac{1}{3}x^{2}-\frac{1}{3}x+\frac{1}{3}\Big)(3x^{2}+3x)
                    +(\minus{x}-1)
                    &=x^{4}-x^{3}+x^{2}+x^{3}-x^{2}+x+(\minus{x}-1)\\
                    &=x^{4}-1
            \end{align*}
            and we may rejoice. To find the $\GCD$, we apply the Euclidean
            algorithm and perform repeated division with remainder. We've done
            step one, now we need to divide $g$ by the remainder
            $\minus{x}-1$. We have:
            \begin{equation}
                g(x)=3x^{2}+3x=\minus{3}(\minus{x}-1)
            \end{equation}
            so the remainder is zero, and hence the GCD is $\minus{x}-1$. The
            coefficients for B'{e}zout identity are thus trivially computed,
            with $a=1$ and $b=\minus{q}$. That is:
            \begin{equation}
                (x^{4}-1)+
                    \Big(\minus\frac{1}{3}x^{2}+\frac{x}{3}-\frac{1}{3}\Big)
                    (3x^{2}+3x)=\minus{x}-1
            \end{equation}
        \end{solution}
        \begin{problem}
            Show that various polynomials are reducible or irreducible.
        \end{problem}
        \begin{solution}
            Since the only irreducible polynomials over $\mathbb{R}$ are linear
            and quadratic ones, $x^{4}+1$ is irreducible even though it has no
            roots. We look for complex roots, and then multiply conjugate pairs.
            The roots are $\pm\sqrt{\pm{i}}$, which corresponds to
            $(\pm{1}\pm{i})/\sqrt{2}$. Multiplying in conjugate pairs, we have:
            \begin{equation}
                x^{4}+1=\big(x-\frac{1+i}{\sqrt{2}}\big)
                        \big(x-\frac{1-i}{\sqrt{2}}\big)
                        \big(x-\frac{\minus{1}+i}{\sqrt{2}}\big)
                        \big(x-\frac{\minus{1}-i}{\sqrt{2}}\big)
            \end{equation}
            this simplifies to:
            \begin{equation}
                x^{4}+1=(x^{2}+\sqrt{2}x+1)(x^{2}-\sqrt{2}x+1)
            \end{equation}
            $x^{4}+1$ is irreducible over $\mathbb{Q}$. We prove this via the
            sliding technique and applying Eisenstein's criterion. We have:
            \begin{equation}
                f(x+1)=(x+1)^{4}+1
                =x^{4}+4x^{3}+6x^{2}+4x+2
            \end{equation}
            so 2 is a prime that divides the $a_{k}$ except for the leading one,
            and such that $2^{2}$ does not divide the last coefficient.
            Hence, by Eisenstein's criterion, $f(x+1)$ is irreducible. But if
            $f(x+1)$ is irreducible, then so is $f(x)$. Next up is
            $x^{7}+11x^{3}-33x+22$. Reducing modulo 2 is instantly noticed to be
            reducible since we can factor our an $x$. Three and five are
            trickier, but can be done. Finally in modulo 7 it is irreducible,
            and therefore it is irreducible in $\mathbb{Q}$. For
            $1+x+x^{2}+x^{3}+x^{4}$ we invoke the partial sum rule for the
            geometric series and shift this polynomial by 1. We have:
            \begin{equation}
                f(x+1)=\sum_{k=0}^{4}(x+1)^{k}=\frac{1-(x+1)^{5}}{\minus{x}}
                =x^{4}+5x^{3}+10x^{2}+10x+5
            \end{equation}
            which is irreducible by Eisenstein setting $p=5$. For the last one,
            $x^{7}-7x^{2}+3x+3$ we note that $x=1$ is a root since
            $1-7+3+3=\minus{6}+6=0$. Hence $x-1$ is a factor. Performing
            division we have:
            \begin{equation}
                x^{3}-7x^{2}+3x+3=(x-1)(x^{2}-6x+1)
            \end{equation}
            By the quadratic formula, the roots of $x^{2}-6x+1$ are irrational
            and hence this is the reduced form of $x^{3}-7x^{2}+3x+1$.
        \end{solution}
        \begin{problem}
            Find all monic irreducible polynomials of degree $\leq{3}$ in
            $\mathbb{Z}_{3}/3\mathbb{Z}$. Determine the number of irreducible
            polynomials of degree 4 as well.
        \end{problem}
        \begin{solution}
            Since we do not consider constants as irreducible, we move on to
            degree 1. In this case, since $f$ is required to be monic, we have
            that $f(x)=x+a$ is monic for any $a\in\mathbb{Z}/3\mathbb{Z}$. Hence
            $x$, $x+1$, and $x+2$ are all irreducible. For degree two, if
            $f$ is reducible then it must be the product of two degree
            polynomials of degree one: $f(x)=(x+a)(x+b)=x^{2}+(a+b)x+ab$ and so
            we must avoid such quadratics. In other words, $f$ must have no
            roots. If we write $f(x)=x^{2}+ax+b$, we require $b\ne{0}$ for
            otherwise 0 is a root of $f$. We also require $1+a+b\ne{0}$ and
            $1-a+b\ne{0}$. If $b=1$, then $2+a\ne{0}$ and $2-a\ne{0}$. The first
            equation means $a\ne{1}$ and the second implies $a\ne{2}$. Hence,
            $a=0$. That is, if $b=1$ we are forced to choose $f(x)=x^{2}+1$.
            If $b=2$, we have $1+a+2\ne{0}$, which implies $a\ne{0}$. The second
            equation gives us $\minus{a}\ne{0}$, but this is redundant, and
            hence the only constraint is the $a\ne{0}$. So
            $x^{2}+x+2$ and $x^{2}+2x+2$ are both irreducible polynomials.
            \begin{table}[H]
                \centering
                \captionsetup{type=table}
                \begin{tabular}{c|ccc}
                    &a=0&a=1&a=2\\
                    \hline
                    b=0&X&X&X\\
                    b=1&\checkmark&X&X\\
                    b=2&X&\checkmark&\checkmark
                \end{tabular}
                \caption{Monic Irreducible Quadratics in $\mathbb{Z}/3\mathbb{Z}$}
                \label{fig:Monic_Irreducible_Quadratics_Z3}
            \end{table}
            For degree three, if $f$ is reducible then either it splits into
            linear terms, or factors into a linear times an irreducible
            quadratic. In either scenario, if $f$ is reducible, then it has a
            root. So we begin by trying to avoid monic cubics with roots. If
            $f(x)=x^{3}+ax^{2}+bx+c$, then automatically we require $c\ne{0}$ or
            else 0 would be a root of $f$. We also require that $1+a+b+c\ne{0}$
            and $\minus{1}+a-b+c\ne{0}$. If $a=0$ then this reduces to
            $b+c\ne\minus{1}$ and $\minus{b}+c\ne{1}$. If we try $b=0$, we have
            $c\ne{0}$, $c\ne{1}$, and $c\ne\minus{1}$, which is impossible.
            So $a=0$ and $b=0$ are off the table. If we try $b=1$ we again have
            no legal choices for $c$, and so this column is crossed out as well.
            Finally, if $b=2$ we have $c\ne{0}$ as the only constraint, and
            $x^{3}+2x+1$ and $x^{2}+2x+2$ are irreducible. If $a=1$, we are left
            with $b+c\ne{1}$ and $\minus{b}+c\ne{0}$, in addition to the
            constraint that $c\ne{0}$. The second relation says $b\ne{c}$ and so
            we can cross out the diagonal in our table. But $b=0$ we simply
            have $c\ne{1}$, so $x^{3}+x^{2}+2$ does the trick. When $b=1$ we
            have $c\ne{1}$, leaving us with $x^{3}+x^{2}+x+2$. The only spot
            left on our table is $c=1$ and $b=2$, and this is indeed
            irreducible. In the final case with $a=2$, our equations are
            $1+2+b+c\ne{0}$ and $\minus{1}+2-b+c\ne{0}$, which reduces to
            $b+c\ne{0}$ and $\minus{b}+c\ne\minus{1}$. If $b=0$, we have
            $c\ne{0}$ and $c\ne\minus{1}$, and so we are stuck with $c=1$.
            Taking $b=1$ we have $c\ne{0}$ and $c\ne\minus{1}$, so $c=2$.
            Finally we conclude with $b=2$, which means $c\ne{1}$, and so $c=2$.
            \begin{table}[H]
                \centering
                \captionsetup{type=table}
                \begin{tabular}{c|ccc|c|ccc|c|ccc}
                    a=0&b=0&b=1&b=2&a=1&0&1&2&a=2&0&1&2\\
                    \hline
                    c=0&X&X&X&&X&X&X&&X&X&X\\
                    c=1&X&X&\checkmark&&X&X&\checkmark&&\checkmark&X&X\\
                    c=2&X&X&\checkmark&
                        &\checkmark&\checkmark&X&
                        &X&\checkmark&\checkmark
                \end{tabular}
                \caption{Monic Irreducible Cubics in $\mathbb{Z}/3\mathbb{Z}$}
                \label{fig:Monic_Irreducible_Cubics_Z3}
            \end{table}
            Hence we have a total of 8 irreducible monic cubic polynomials over
            $\mathbb{Z}/3\mathbb{Z}$. So in total there are 3 irreducible
            linear polynomials, 3 irreducible quadratics, and 8 irreducible
            cubics. To compute the number of irreducible quartics it suffices to
            compute the number or reducible quartics. There are a total of
            $3^{4}$ monic quartic polynomials $x^{4}+ax^{3}+bx^{2}+cx+d$ since
            there are 3 choices for each of the $a$, $b$, $c$, and $d$. If such
            a polynomial is reducible then it factors either as a product of
            linear terms, a product of an irreducible cubic and a linear term,
            a product of two linear terms and an irreducible quadratic, or the
            product of two irreducible quadratics.
        \end{solution}
    \section{HW II}
    \section{Rings}
        \begin{fdefinition}{Ring}{Ring}
            A ring is a set $R$ with two binary operations $+$ and $\cdot$,
            denoted $\ring{R}$, such that $\monoid[][+]{R}$ is a group,
            $\monoid[][\cdot]{R}$ is a monoid, and such that $\cdot$ distributes
            over $+$. That is, for all $a,b,c\in{R}$ it is true that:
            \begin{equation*}
                a\cdot(b+c)=(a\cdot{b})+(a\cdot{c})
            \end{equation*}
            The unital element of $\monoid[][+]{R}$ is denoted $0$ and the
            unital element of $\monoid[][\cdot]{R}$ is written as $1$. $+$ is
            called addition and $\cdot$ is called multiplication.
        \end{fdefinition}
        If we relax the requirement that $\monoid[][\cdot]{R}$ is a monoid and
        merely require it to be a semigroup (that is, there need not exist a
        multiplicative identity), then the resulting structure is called a
        \textit{rng}. Some authors reserve the word ring for the more general
        case of what we're calling a rng, and use the phrasing
        \textit{ring with identity} for what we're calling a ring. Commutative
        rings are rings $\ring{R}$ where $\monoid[][\cdot]{R}$ is an Abelian
        monoid. Rings (with identity) are automatically commutative in $+$. That
        is, $\monoid[][+]{R}$ is an Abelian group.
        \begin{theorem}
            \label{thm:Ring_Add_is_Commutative}%
            If $\ring{R}$ is a ring, then $\monoid[][+]{R}$ is an Abelian group.
        \end{theorem}
        \begin{proof}
            For suppose not. Then there exist $a,b\in{R}$ such that
            $a+b\ne{b}+a$. But $\ring{R}$ is a ring, and hence
            $\monoid[][\cdot]{R}$ is a monoid and thus there is an identity
            element $1\in{R}$ with respect to multiplication. But since $R$ is a
            ring, multiplication distributes over addition and therefore:
            \begin{align}
                (1+1)\cdot(a+b)
                    &=\big((1+1)\cdot{a}\big)+\big((1+1)\cdot{b}\big)
                    \tag{Left Distributive Law}\\
                    &=\big((1\cdot{a})+(1\cdot{a})\big)+
                        \big((1\cdot{b})+(1\cdot{b})\big)
                    \tag{Right Distributive Law}\\
                    &=(a+a)+(b+b)
                        \tag{Multiplicative Identity}\\
                    &=a+\big(a+(b+b)\big)
                        \tag{Associative Law}
            \end{align}
            But again by distributivity, we obtain:
            \begin{align}
                (1+1)\cdot(a+b)
                &=\big(1\cdot(a+b)\big)+\big(1\cdot(a+b)\big)
                \tag{Right Distributive Law}\\
                &=(a+b)+(a+b)
                    \tag{Multiplicative Identity}\\
                &=a+\big(b+(a+b)\big)
                    \tag{Associative Law}
            \end{align}
            By the transitivity of equality, we have:
            \begin{equation}
                a+\big(a+(b+b)\big)=a+\big(b+(a+b)\big)
            \end{equation}
            But $\monoid[][+]{R}$ is a group, and thus by the left cancellation
            law $a+(b+b)=b+(a+b)$. By associativity $a+(b+b)=(a+b)+b$ and
            $b+(a+b)=(b+a)+b$, and hence $(a+b)+b=(b+a)+b$. Thus by the right
            cancellation law, $a+b=b+a$, which is a contradiction. Therefore,
            $\monoid[][+]{R}$ is Abelian.
        \end{proof}
        \begin{example}
            If $R=\{0\}$, and if $+$ and $\cdot$ are the only binary operations
            one can define on $R$: $0\cdot{0}=0$ and $0+0=0$, then
            $\ring{R}$ is a ring. Since $\monoid[][+]{R}$ and
            $\monoid[][\cdot]{R}$ are simply the Abelian group $\mathbb{Z}_{1}$,
            we need only check the distributive law, but this holds trivially
            since $0\cdot(0+0)=0\cdot{0}=0$ and $0\cdot{0}+0\cdot{0}=0$. This is
            often called the \textit{zero ring}, but also the trivial ring.
        \end{example}
        \begin{example}
            There are other types of different trivial rng structures on any
            Abelian group $\monoid{G}$. Defined $\cdot:G\times{G}\rightarrow{G}$
            by $a\cdot{b}=e$ for all $a,b\in{G}$, where $e\in{G}$ is the unital
            element. This is a rng since $\cdot$ is associative, and hence
            $\monoid[][\cdot]{R}$ is a semigroup. Multiplication also
            distributes over $*$ trivially since:
            \vspace{-2.5ex}
            \twocolumneq{a\cdot(b*c)=e}{(a\cdot{b})*(a\cdot{c})=e*e=e}
            Thus, $\ring{R}$ is a rng. If $R$ has at least two elements then
            this cannot be a proper ring, since there can be no multiplicative
            identity.
        \end{example}
        \begin{example}
            The rational, real, and complex numbers, together with their usual
            arithmetic, form commutative rings (moreover, they form fields).
            That is, letting $+$ and $\cdot$ denote the familiar forms of
            addition and multiplication, respective, $\ring{\nspace[]}$ is a
            ring, and similarly for the other two.
        \end{example}
        \begin{example}
            The quintessential example of a ring is $\mathbb{Z}$ quipped with
            its usual arithmetic. That is, $\ring{\mathbb{Z}}$ is a ring.
            $\monoid[][+]{\mathbb{Z}}$ is an Abelian group and and
            $\monoid[][\cdot]{\mathbb{Z}}$ forms an Abelian monoid. Moreover,
            multiplication distributes over addition, and hence
            $\ring{\mathbb{Z}}$ is a commutative ring.
        \end{example}
        \begin{example}
            Endowing $\mathbb{Z}_{n}$ with it's modulo arithmetic structure,
            $\ring{\mathbb{Z}_{n}}$ also forms a ring.
        \end{example}
        An important class of rings comes from studying function spaces.
        \begin{theorem}
            \label{thm:Ring_of_Funcs_is_Ring}%
            If $X$ is a set, if $\ring[R]{R}$ is a ring, if
            $\funcspace[R]{X}$ is the set of all functions $f:X\rightarrow{R}$,
            if $+$ is the binary operation on $\funcspace[R]{X}$ defined by:
            \begin{equation}
                (f+g)(x)=f(x)+_{R}g(x)
            \end{equation}
            and if $\cdot$ is the binary operation defined by:
            \begin{equation}
                (f\cdot{g})(x)=f(x)\cdot_{R}g(x)
            \end{equation}
            then $\ring{\funcspace[R]{X}}$ is a ring.
        \end{theorem}
        \begin{example}
            The set of even integers $\mathbb{Z}_{e}$ with addition and
            multiplication forms a rng, but not a ring. The identity element of
            $\mathbb{Z}$ is 1, and 1 is not even.
        \end{example}
        \begin{theorem}
            \label{thm:Rng_Mult_by_Zero}%
            If $\ring{R}$ is a rng, if 0 is the unital element of
            $\monoid[][+]{R}$, and if $a\in{R}$, then $a\cdot{0}=0$ and
            $0\cdot{a}=0$.
        \end{theorem}
        \begin{proof}
            For since $\ring{R}$ is a rng, $\cdot$ distributes over addition,
            and hence both left and right distributes. And since 0 is the
            additive identity, we obtain:
            \begin{equation}
                a\cdot{0}=a\cdot(0+0)=(a\cdot{0})+(a\cdot{0})
            \end{equation}
            by the left cancellation law we have that $a\cdot{0}=0$. Similary:
            \begin{equation}
                0\cdot{a}=(0+0)\cdot{a}=(0\cdot{a})+(0\cdot{a})
            \end{equation}
            Again by the cancellation law, $0\cdot{a}=0$.
        \end{proof}
        \begin{theorem}
            \label{thm:Ring_with_0_Eq_1}%
            If $\ring{R}$ is a ring, if 0 is a the multiplicative identity, if
            1 is the additive identity, and if $0=1$, then $R=\{0\}$.
        \end{theorem}
        \begin{proof}
            For suppose not. Then there is an element $x\in{R}$ such that
            $x\ne{0}$. But 1 is the multiplicative identity, and hence
            $a=a\cdot{1}$. But $1=0$, and thus $a=a\cdot{0}$. But $a\cdot{0}=0$
            (Thm.~\ref{thm:Rng_Mult_by_Zero}), and thus by the transitivity of
            equality we have $a=0$, a contradiction.
        \end{proof}
        \begin{theorem}
            \label{thm:Rng_MinusA_B_EQ_Minus_AB}%
            If $\ring{R}$ is a rng, if $a,b\in{R}$, and if $\minus{a}\in{R}$ is
            the additive inverse of $a$, then:
            \begin{equation}
                (\minus{a})\cdot{b}=\minus(a\cdot{b})
            \end{equation}
        \end{theorem}
        \begin{proof}
            For:
            \begin{equation}
                a\cdot{b}+(\minus{a})\cdot{b}
                =\big(a+(\minus{a})\big)\cdot{b}
                =0\cdot{b}
                =0
            \end{equation}
            and thus $(\minus{a})\cdot{b}=\minus(a\cdot{b})$.
        \end{proof}
        \begin{theorem}
            \label{thm:Rng_MinusA_MinusB_EQ_AB}%
            If $\ring{R}$ is a rng, if $a,b\in{R}$, and if
            $\minus{a},\minus{b}\in{R}$ are their additive inverses,
            respectively, then:
            \begin{equation}
                (\minus{a})\cdot(\minus{b})=a\cdot{b}
            \end{equation}
        \end{theorem}
        \begin{proof}
            For by Thm.~\ref{thm:Rng_MinusA_B_EQ_Minus_AB},
            $(\minus{a})\cdot(\minus{b})=\minus\big(a\cdot(\minus{b})\big)$. But
            then:
            \begin{equation}
                a\cdot{b}-\big((\minus{a})\cdot(\minus{b})\big)
                =a\cdot{b}+(a\cdot(\minus{b})
                =a\cdot\big(b+(\minus{b})\big)
                =a\cdot{0}
                =0
            \end{equation}
            and hence $a\cdot{b}=(\minus{a})\cdot(\minus{b})$.
        \end{proof}
        \begin{theorem}
            \label{thm:Ring_Minus_1_Squared}%
            If $\ring{R}$ is a ring, if $1\in{R}$ is the multiplicative
            identity, and if $\minus{1}$ is the additive inverse of 1, then
            $(\minus{1})^{2}=1$.
        \end{theorem}
        \begin{proof}
            For $(\minus{1})^{2}=(\minus{1})\cdot(\minus{1})$ and by
            Thm.~\ref{thm:Rng_MinusA_MinusB_EQ_AB}
            $(\minus{1})\cdot(\minus{1})=1\cdot{1}$. But 1 is the multiplicative
            identity and thus $1\cdot{1}=1$.
        \end{proof}
        \begin{theorem}
            If $\ring{R}$ is a ring, if $r\in{R}$ has a multiplicative inverse,
            and if $\minus{r}$ is the additive inverse of $r$, then $\minus{r}$
            has a multiplicative inverse.
        \end{theorem}
        \begin{proof}
            For if $r$ has a multiplicative inverse, then there is an
            element $r^{\minus{1}}\in{R}$ such that $r\cdot{r}^{\minus{1}}=1$.
            But then:
            \begin{align}
                (\minus{r})\cdot(\minus{r}^{\minus{1}})&=r\cdot{r}^{\minus{1}}
                    \tag{Thm.~\ref{thm:Rng_MinusA_MinusB_EQ_AB}}\\
                &=1\tag{Multiplicative Inverse}
            \end{align}
            and thus $\minus{r}$ is invertible.
        \end{proof}
        \begin{fdefinition}{Zero Divisor}{Zero_Divisor}
            A zero divisor of a rng $\ring{R}$ is an element $a\in{R}$ such that
            $a\ne{0}$ and there exists a $b\in{R}$ such that $b\ne{0}$ and
            either $a\cdot{b}=0$ or $b\cdot{a}=0$.
        \end{fdefinition}
        \begin{example}
            The ring of integers contains no zero divisors. This follows from
            the work of Euclid. If $n\cdot{m}=0$, then either $n=0$ or $m=0$ and
            hence there can be no zero divisors.
        \end{example}
        \begin{example}
            Let $n\in\mathbb{N}^{+}$ be positive, and suppose
            $a\in\mathbb{Z}_{n}$ is such that $a$ divides $n$. That is, there is
            an $m\in\mathbb{N}$ such that $a\cdot{m}=n$. Then in the ring of
            integers modulo $n$,
            $(\mathbb{Z}/n\mathbb{Z},\tilde{+},\tilde{\cdot})$, we have that
            both $a$ and $m$ are zero divisors. That is, since $a\cdot{m}=n$,
            they are congruent to zero modulo $n$ and hence
            $a\cdot{m}\equiv{0}\mod{n}$. We can also see that if $p$ is prime
            then there are no zero-divisors. This is just a consequence of
            Thm.~\ref{thm:Invertible_Mod_n_iff_Relatively_Prime}. That is, if
            $p$ is prime, then
            $(\mathbb{Z}/p\mathbb{Z},\tilde{+},\tilde{\cdot})$ is a ring without
            zero divisors.
        \end{example}
        \begin{theorem}
            \label{thm:Group_of_Units_of_Ring_is_Group}%
            If $\ring{R}$ is a ring, and if $R^{\times}$ is the set:
            \begin{equation}
                R^{\times}=\big\{\,r\in{R}\;|\;
                    \exists_{r^{\minus{1}}\in{R}}
                    \big(r\cdot{r}^{\minus{1}}=1\big)\,\big\}
            \end{equation}
            then $\monoid[R^{\times}][\cdot]{R^{\times}}$ is a group.
        \end{theorem}
        \begin{proof}
            For if $a,b\in{R}$ are invertible, then $a\cdot{b}$ is. Moreover,
            the identity is invertible, and $\cdot$ is associative. Hence, we
            have a group.
        \end{proof}
        \begin{theorem}
            \label{thm:Ring_Zero_Divisor_Not_Invertible}%
            If $\ring{R}$ is a non-zero ring, and if $a\in{R}$ is a zero
            divisor, then $a$ is not invertible.
        \end{theorem}
        \begin{proof}
            For suppose not. Then there is a $b\in{R}$ such that $a\cdot{b}=1$.
            But since $a$ is a zero divisor, $a\ne{0}$ and there is a $c\in{R}$
            such that $c\ne{0}$ and either $a\cdot{c}=0$ or $c\cdot{a}=0$
            (Def.~\ref{def:Zero_Divisor}) $c\cdot{a}=0$. But then:
            \par
            \begin{minipage}[t]{0.49\textwidth}
                \begin{align*}
                    c&=c\cdot{1}\tag{Identity}\\
                    &=c\cdot(a\cdot{b})\tag{Inverse}\\
                    &=(c\cdot{a})\cdot{b}\tag{Associativity}
                \end{align*}
            \end{minipage}
            \hfill
            \begin{minipage}[t]{0.49\textwidth}
                \centering
                \begin{align*}
                    &=0\cdot{b}\tag{Hypothesis}\\
                    &=0\tag{Thm.~\ref{thm:Rng_Mult_by_Zero}}
                \end{align*}
            \end{minipage}
            \par\vspace{2.5ex}
            a contradiction. Similarly if $a\cdot{c}=0$.
        \end{proof}
        \begin{fdefinition}{Integral Domain}{Integral_Domain}
            An integral domain is a ring $\ring{R}$ such that for all $r\in{R}$
            it is true that $r$ is not a zero divisor.
        \end{fdefinition}
        \begin{example}
            By our previous discussion, $\ring{\mathbb{Z}}$ is an integral
            domain since it contains no zero divisors.
        \end{example}
        \begin{theorem}
            \label{thm:Left_Cancellation_Law_Int_Domain}%
            If $\ring{R}$ is an integral domain, if $a,b,c\in{R}$, and if
            $a\cdot{b}=a\cdot{c}$, then either $b=c$ or $a=0$.
        \end{theorem}
        \begin{proof}
            For suppose not. Then:
            \begin{align}
                a\cdot(b-c)=a\cdot{b}-a\cdot{c}=a\cdot{c}-a\cdot{c}=0
            \end{align}
            But $b\ne{c}$ and thus $b-c\ne{0}$. But then $a$ is a non-zero
            element of $R$ such that there exists a non-zero element $b-c$ with
            $a\cdot(b-c)=0$, and hence $a$ is a zero divisor
            (Def.~\ref{def:Zero_Divisor}). But $\ring{R}$ is an integral domain
            and hence there are no zero divisiors
            (Def.~\ref{def:Integral_Domain}), a contradiction.
        \end{proof}
        \begin{theorem}
            \label{thm:Int_Domain_AB_EQ_Zero_A_or_B_EQ_Zero}%
            If $\ring{R}$ is an integral domain, if $a,b\in{R}$, and if
            $a\cdot{b}=0$, then either $a=0$ or $b=0$.
        \end{theorem}
        \begin{proof}
            For if $\ring{R}$ is a ring, then $a\cdot{0}=0$
            (Thm.~\ref{thm:Rng_Mult_by_Zero}). But $a\cdot{b}=0$ by hypothesis,
            and hence by the transitivity of equality $a\cdot{b}=a\cdot{0}$.
            But if $\ring{R}$ is an integral domain, then by the cancellation
            law either $a=0$ or $b=0$
            (Thm.~\ref{thm:Left_Cancellation_Law_Int_Domain}).
        \end{proof}
        \begin{fdefinition}{Field}{Field}
            A field is a commutative ring $\ring{R}$ such that for all
            $r\in{R}\setminus\{0\}$ there exists a multiplicative inverse
            $r^{\minus{1}}\in{R}\setminus\{0\}$ and such that $0\ne{1}$.
        \end{fdefinition}
        \begin{example}
            Since by definition a division ring is a non-zero ring, fields are
            required to have at least two elements. The smallest field is thus
            $\ring{\mathbb{Z}_{2}}$ with modulo arithmetic.
        \end{example}
        \begin{theorem}
            \label{thm:Fields_are_Int_Domains}%
            If $\ring{F}$ is a field, then $\ring{F}$ is an integral domain.
        \end{theorem}
        \begin{proof}
            For suppose not. But if $\ring{F}$ is not an integral domain,
            then there is a zero divisor $r\in{F}$
            (Def.~\ref{def:Integral_Domain}). But if $r$ is a zero divisor, then
            $r\ne{0}$, and hence $r\in{R}\setminus\{0\}$. But $F$ is a field,
            and therefore $r$ is invertible (Def.~\ref{def:Field}). But
            zero divisors are not invertible
            (Thm.~\ref{thm:Ring_Zero_Divisor_Not_Invertible}), a contradiction.
        \end{proof}
        \begin{theorem}
            \label{thm:Int_Domain_Left_Mult_is_Inj}%
            If $\ring{R}$ is an integral domain, if $a\in{R}$ is non-zero, and
            if $f:R\rightarrow{R}$ is defined by $f(x)=a\cdot{x}$, then $f$
            is injective.
        \end{theorem}
        \begin{proof}
            For suppose not. Then there are elements $x,y\in{R}$ such that
            $x\ne{y}$ and $f(x)=f(y)$. But then $a\cdot{x}=a\cdot{y}$. But since
            $R$ is an integral domain, if $a\cdot{x}=a\cdot{y}$, then either
            $a=0$ or $x=y$ (Thm.~\ref{thm:Left_Cancellation_Law_Int_Domain}).
            But $a\ne{0}$ and $x\ne{y}$, a contradiction.
        \end{proof}
        \begin{theorem}
            \label{thm:Finite_Int_Domain_if_Field}%
            If $\ring{R}$ is an integral domain, if $R$ is finite, and if
            $0\ne{1}$, then $\ring{R}$ is a field.
        \end{theorem}
        \begin{proof}
            For suppose not. Then there is an $r\in{R}\setminus\{0\}$ such that
            $r$ has no multiplicative inverse. Let $f:R\rightarrow{R}$ be
            defined by $f(x)=r\cdot{x}$. But then $f$ is injective
            (Thm.~\ref{thm:Int_Domain_Left_Mult_is_Inj}). And since $R$ is
            finite, it is therefore surjective. But then there is an
            $r^{\minus{1}}\in{R}$ such that $f(r^{\minus{1}})=1$. But then
            $r\cdot{r}^{\minus{1}}=1$, a contradiction.
        \end{proof}
        \begin{fdefinition}{Subring}{Subring}
            A subring of a ring $\ring{R}$ is a subset $S\subseteq{R}$ such that
            $\monoid[S][+]{S}$ is a subgroup of $\monoid[][+]{R}$ and
            $\monoid[S][\cdot]{S}$ is a submonoid of $\monoid[][\cdot]{R}$.
        \end{fdefinition}
        \begin{example}
            If we take $\ring{\mathbb{C}}$ to be usual field structure on the
            complex numbers (which is therefore a ring), there are several
            familiar subrings. $\nspace[]$, $\mathbb{Q}$, and $\mathbb{Z}$ are
            all subrings.
        \end{example}
        \begin{theorem}
            \label{thm:Subring_of_Field_is_Int_Domain}%
            If $\ring{F}$ is a field, and if $R\subseteq{F}$ is a subring of
            $F$, then $\ring[R]{R}$ is an integral domain.
        \end{theorem}
        \begin{proof}
            For suppose not. Then there is a zero divisor $a\in{R}$
            (Def.~\ref{def:Integral_Domain}), and hence there is a non-zero
            element $b\in{R}$ such that either $a\cdot{b}=0$ or $b\cdot{a}=0$.
            But $R\subseteq{F}$, and hence $a,b\in{F}$. But if $\ring{F}$ is a
            field, then it is an integral domain
            (Thm.~\ref{thm:Fields_are_Int_Domains}). But if $\ring{F}$ is an
            integral domain, and if $a\cdot{b}=0$, then either $a=0$ or $b=0$
            (Thm.~\ref{thm:Int_Domain_AB_EQ_Zero_A_or_B_EQ_Zero}),
            a contradiction.
        \end{proof}
        \begin{example}
            If $n\subseteq\mathbb{N}$ is square free (that is, there is no
            $m\in\mathbb{N}$ such that $m^{2}=n$), then we can adjoin
            $\sqrt{n}$ to $\mathbb{Z}$ by considering the set:
            \begin{equation}
                \mathbb{Z}[\sqrt{n}]=\{\,a+b\sqrt{n}\;|\;a,b\in\mathbb{Z}\,\}
            \end{equation}
            This is a subring of $\nspace[]$. More precisely, it is a subring of
            $\mathbb{Q}(\sqrt{n})$. If $a,b,c,d\in\mathbb{Z}$, then:
            \begin{equation}
                (a+b\sqrt{n})\cdot(c+d\sqrt{n})=ac+bdn+(ad+bc)\sqrt{n}
            \end{equation}
            And hence we have that $\mathbb{Z}[\sqrt{n}]$ is closed under
            multiplication.
        \end{example}
        \begin{example}
            If $r\in\mathbb{Q}^{+}$ is not a square of another rational number,
            we can define $\mathbb{Q}(r)$ as follows:
            \begin{equation}
                \mathbb{Q}(r)=\{\,a+b\sqrt{r}\;|\;a,b\in\mathbb{Q}\,\}
            \end{equation}
            Since $\mathbb{Q}$ is a field, it may be reasonable to suspect that
            $\mathbb{Q}(\sqrt{r})$ is a field as well. First, we show that it is
            closed under addition:
            \begin{equation}
                (a+b\sqrt{r})+(c+d\sqrt{r})=(a+c)+(b+d)\sqrt{r}
            \end{equation}
            and similarly for multiplication:
            \begin{equation}
                (a+b\sqrt{r})\cdot(c+d\sqrt{r})=(ac+bdr)+(ad+bc)\sqrt{r}
            \end{equation}
            and hence $\mathbb{Q}(\sqrt{r})$ is a subring of $\nspace[]$. It is
            also a field. We need only show that multiplicative inverses for
            non-zero elements exist. Give $a+b\sqrt{r}$, define:
            \begin{equation}
                (a+b\sqrt{r})^{\minus{1}}=\frac{a}{a^{2}-b^{2}}-
                    \frac{b}{a^{2}-rb^{2}}\sqrt{r}
            \end{equation}
            this is well defined since $a^{2}\ne{r}b^{2}$, otherwise
            $r=a^{2}/b^{2}$ which contradicts the fact that $r$ is square free.
            Then $(a+b\sqrt{r})^{\minus{1}}$ is an element of
            $\mathbb{Q}(\sqrt{r})$ and moreover:
            \begin{subequations}
                \begin{align}
                    (a+b\sqrt{r})^{\minus{1}}\cdot(a+b\sqrt{r})
                    &=\Big(\frac{a}{a^{2}-rb^{2}}-
                        \frac{b}{a^{2}-rb^{2}}\sqrt{r}\Big)\cdot(a+b\sqrt{r})\\
                    &=\frac{1}{a^{2}-rb^{2}}\big(
                        (a-b\sqrt{r})\cdot(a+b\sqrt{r})\big)\\
                    &=\frac{a^{2}-rb^{2}}{a^{2}-rb^{2}}\\
                    &=1
                \end{align}
            \end{subequations}
            which shows that $a+b\sqrt{r}$ is invertible. Thus,
            $\mathbb{Q}(\sqrt{r})$ is a \textit{subfield} of $\nspace[]$.
        \end{example}
        \begin{example}
            The Gaussian integers are another common example of a subring of the
            complex numbers $\mathbb{Q}$. Consider the subring $\mathbb{Z}[i]$
            defined as follows:
            \begin{equation}
                \mathbb{Z}[i]=\{\,a+ib\;|\;a,b\in\mathbb{Z}\,\}
            \end{equation}
            this is a subring of $\mathbb{C}$ since:
            \begin{equation}
                (a+ib)+(c+id)=(a+c)+i(b+d)
            \end{equation}
            and:
            \begin{equation}
                (a+ib)\cdot(c+id)=ac-bd+i(ad+bc)
            \end{equation}
            hence, $\mathbb{Z}[i]\subseteq\mathbb{C}$ is a subring.
        \end{example}
    \section{Ideals}
        \begin{fdefinition}{Left Ideal}{Left_Ideal}
            An ideal in a ring $\ring{R}$ is a subset $I\subseteq{R}$ such that
            for all $a,b\in{I}$ it is true that $a+b\in{I}$ and for all
            $r\in{R}$ and $a\in{I}$ it is true that $r\cdot{a}\in{I}$.
        \end{fdefinition}
        \begin{example}
            The motivating example of an ideal is the even integers
            in $\ring{\mathbb{Z}}$. The sum of two even integers is again an
            even integer, and for any even element $2n$ and any $m\in\mathbb{Z}$
            we have $2n\cdot{m}=2\cdot(n\cdot{m})$, which is again even.
        \end{example}
        \begin{theorem}
            \label{thm:Left_Ideal_is_Additive_Subgroup}%
            If $\ring{R}$ is a ring, and if $I\subseteq{R}$ is a non-empty left
            ideal, then $I$ is a subgroup of $\monoid[][+]{R}$.
        \end{theorem}
        \begin{proof}
            For since $I$ is a left ideal, $I$ is closed under addition
            (Def.~\ref{def:Left_Ideal}). Moreover, since $I$ is non-empty there
            is an $x\in{I}$. But since $I$ is a left ideal, $0\cdot{x}\in{I}$
            (Def.~\ref{def:Left_Ideal}). But $0\cdot{x}=0$
            (Thm.~\ref{thm:Rng_Mult_by_Zero}) and hence $0\in{I}$. Lastly, if
            $x\in{I}$, thus $(\minus{1})\cdot{x}=\minus{x}\in{I}$ and hence
            $I$ is closed to additive inverses. Hence, $I$ is a subgroup of
            $\monoid[][+]{R}$.
        \end{proof}
        We similarly define what it means to be a right ideal.
        \begin{fdefinition}{Right Ideal}{Right_Ideal}
            A ring ideal of a ring $\ring{R}$ is a subset $I\subseteq{R}$ such
            that for all $a,b\in{I}$ it is true that $a+b\in{I}$ and for all
            $r\in{R}$ and $a\in{I}$ it is true that $a\cdot{r}\in{I}$.
        \end{fdefinition}
        That we have two different definitions may indicate that these are
        different objects (Left ideals vs. right ideals). This stems from the
        fact that ring multiplication need not be commutative.
        \begin{example}
            The set of all $n\times{n}$ matrices where the entire last row is
            all zeroes forms a right ideal in the ring of $n\times{n}$ matrices,
            but not a left ideal. Similarly, the set of all $n\times{n}$
            matrices with the last column set to zero is a left ideal, but not a
            right ideal.
        \end{example}
        To rid ourselves of such things, we often speak plainly of
        \textit{ideals}. We define this now.
        \begin{fdefinition}{Two Sided Ideal}{Two_Sided_Ideal}
            A two sided ideal in a ring $\ring{R}$ is a subset $I\subseteq{R}$
            such that $I$ is a left ideal and a right ideal in $R$.
        \end{fdefinition}
        \begin{example}
            In $\ring{\mathbb{Z}}$, the even integers form a two sided ideal.
            As discussed previously, they form a left sided ideal, and from
            the commutativity of integer multiplication they then necessarily
            form a right ideal.
        \end{example}
        \begin{theorem}
            \label{thm:One_Sided_Ideal_in_Comm_Ring_is_Two_Sided}%
            If $\ring{R}$ is a commutative ring, and if $I\subseteq{R}$ is
            either a left ideal or a right ideal, then it is a two sided ideal.
        \end{theorem}
        \begin{proof}
            For suppose not. Then either $I$ is a left ideal and not a two sided
            ideal, or $I$ is a right ideal and not two sided. But if $I$ is a
            left ideal and not two sided, then it is not a right ideal
            (Def.~\ref{def:Two_Sided_Ideal}) and hence there is an $a\in{I}$ and
            an $r\in{R}$ such that $a\cdot{r}\notin{I}$. But $R$ is commutative,
            so $a\cdot{r}=r\cdot{r}$. But $I$ is a left ideal and thus
            $a\cdot{r}\in{I}$ (Def.~\ref{def:Left_Ideal}), a contradiction.
            Similarly if $I$ is a right ideal.
        \end{proof}
        \begin{theorem}
            \label{thm:Ideals_of_Field}%
            If $\ring{F}$ is a commutative ring, then it is a field if and only
            if there only ideals are the zero ideal and the entire ring.
        \end{theorem}
        \begin{proof}
            For if $\ring{F}$ is a field then for all $a\in{F}$ such that
            $a\ne{0}$ there exists a multiplicative inverse
            $a^{\minus{1}}\in{F}$ such that $a\cdot{a}^{\minus{1}}=1$. But then
            if $I$ is a non-zero subring of $R$, there is a non-zero element
            $a\in{I}$, and thus $a\cdot{a}^{\minus{1}}\in{I}$, and
            therefore $1\in{I}$. But then for all $r\in{F}$, $r\cdot{1}\in{I}$,
            but $r\cdot{1}=r$. Hence, $I=R$. In the other direction, if
            the only ideals are the zero ideal and the entire ring, then for
            any non-zero $a\in{F}$ the ideal generated by $a$ must be the entire
            ring. Hence, there is a $b\in{I}$ such that $a\cdot{b}=1$, and so
            $F$ is a field.
        \end{proof}
        \begin{example}
            Fields: $\mathbb{Q}$, $\mathbb{R}$, $\mathbb{C}$, $\mathbb{Z}_{p}$
            with $p$ a prime.
        \end{example}
        \begin{fdefinition}{Prime Ideal}{Prime_Ideal}
            A prime ideal of a commutative ring $\ring{R}$ is a propert subset
            $I\subsetneq{R}$ such that $I$ is a two sided ideal ideal in $R$
            and for all $a,b\in{R}$ with $a\cdot{b}\in{I}$, it is true that
            either $a\in{I}$ or $b\in{I}$.
        \end{fdefinition}
        This is a generalization of Euclid's prime number lemma
        (Thm.~\ref{thm:Euclid_Prime_Number_Lemma}). More precisely, it is a
        generalization of the corrollary that follows
        (Thm.~\ref{thm:Prime_Div_AB_then_PdivA_or_PdivB}). We can use this
        connection to prove the following theorem.
        \begin{theorem}
            \label{thm:Prime_Ideals_of_Z}%
            If $\ring{\mathbb{Z}}$ is the usual ring structure on $\mathbb{Z}$,
            and if $p\in\mathbb{Z}$, then $p\mathbb{Z}$ is a prime ideal of
            $\mathbb{Z}$ if and only if $p$ is prime.
        \end{theorem}
        \begin{proof}
            For if $p\in\mathbb{N}$ is prime, and if $a,b\in\mathbb{Z}$ are such
            that $a\cdot{b}\in{p}\mathbb{Z}$, then there is an $m\in\mathbb{Z}$
            such that $a\cdot{b}=m\cdot{p}$. But then $p$ divides $a\cdot{b}$,
            and thus either $p$ divides $a$ or $p$ divides $b$
            (Thm.~\ref{thm:Prime_Div_AB_then_PdivA_or_PdivB}). But if $p$
            divides $a$, then $a\in{p}\mathbb{Z}$, and similarly if $p$ divides
            $b$. Hence, $p\mathbb{Z}$ is a prime ideal
            (Def.~\ref{def:Prime_Ideal}). In the other direction, if
            $p\mathbb{Z}$ is a prime ideal, suppose $p$ is not prime. Then there
            are integers $a,b\in\mathbb{Z}$, neither of which are units, such
            that $p=a\cdot{b}$. But by hypothesis $p\mathbb{Z}$ is a prime
            ideal, and hence if $a\cdot{b}\in{p}\mathbb{Z}$, then either
            $a\in{p}\mathbb{Z}$ or $b\in{p}\mathbb{Z}$. But if
            $a\in{p}\mathbb{Z}$, then there is an $m\in\mathbb{Z}$ such that
            $a=m\cdot{p}$. But $a\cdot{b}=p$, and hence $m\cdot{p}\cdot{b}=p$.
            But then $m\cdot{b}=1$, and thus $b$ is a unit, which is a
            contradition. Hence, $p$ is prime.
        \end{proof}
        We can use this to generate examples of prime ideals.
        \begin{example}
            The even integers $\mathbb{Z}_{e}$ form a prime ideal of
            $\mathbb{Z}$. This can be seen immediately once one notes that
            $\mathbb{Z}_{e}=2\mathbb{Z}$, and since $2$ is a prime number,
            by Thm.~\ref{thm:Prime_Ideals_of_Z} we see that $2\mathbb{Z}$ is a
            prime ideal.
        \end{example}
        A useful equivalent definition of prime ideals goes as follows:
        \begin{theorem}
            If $\ring{R}$ is a ring, and if $I\subsetneq{R}$ is a two sided
            ideal, then $I$ is a prime ideal if and only if for all
            $a,b\in{R}\setminus\{I\}$ it is true that
            $a\cdot{b}\in{R}\setminus\{I\}$.
        \end{theorem}
        \begin{proof}
            For if $I$ is a prime ideal, then for all $a,b\in{R}$ such that
            $a\cdot{b}\in{I}$, it is true that either $a\in{I}$ or $b\in{I}$.
            Suppose there exists $a,b\in{R}\setminus\{I\}$ such that
            $a\cdot{b}\notin{R}\setminus\{I\}$. But since $\cdot$ is a binary
            operation it is true that $a\cdot{b}\in{R}$, and hence if
            $a\cdot{b}\notin{R}\setminus{I}$, then $a\cdot{b}\in{I}$. But $I$ is
            a prime ideal, and thus if $a\cdot{b}\in{I}$ then either $a\in{I}$
            or $b\in{I}$, a contradiction since by hypothesis $a,b\notin{I}$.
            Therefore, $a\cdot{b}\in{R}\setminus\{I\}$. In the other direction,
            if $R\setminus\{I\}$ is multiplicatively closed, suppose $a,b\in{R}$
            are such that $a\cdot{b}\in{I}$. But $R\setminus\{I\}$ is
            multiplicatively closed, and thus if $a,b\in{R}\setminus\{I\}$, then
            $a\cdot{b}\in{R}\setminus\{I\}$, a contradiction, and thus either
            $a\in{I}$ or $b\in{I}$. Thus, $I$ is a prime ideal.
        \end{proof}
        \begin{theorem}
            \label{thm:homo_pre_image_of_ideal_is_ideal}%
            If $\ring[R]{R}$ and $\ring[S]{S}$ are rings, if
            $\phi:R\rightarrow{S}$ is a ring homomorphism, and if
            $I\subseteq{S}$ is a two sided ideal, then $\phi^{\minus{1}}[I]$ is
            a two sided ideal.
        \end{theorem}
        \begin{proof}
            Since $I$ is a two sided ideal, it is a left ideal
            (Def.~\ref{def:Two_Sided_Ideal}) and hence $I$ is a subgroup of
            $\monoid[S][+]{S}$
            (Thm.~\ref{thm:Left_Ideal_is_Additive_Subgroup}) and therefore
            $0_{S}\in{I}$. But $\phi$ is a ring homomorphism and therefore
            $\phi(0_{R})=0_{S}$. Hence, $\phi^{\minus{1}}[I]$ is non-empty. But
            if $a,b\in\phi^{\minus{1}}[I]$, then $\phi(a),\phi(b)\in{I}$. But
            again since $\phi$ is a ring homomorphism we have
            $\phi(a+_{R}b)=\phi(a)+_{S}\phi(b)$, and since $I$ is an ideal it is
            true that $\phi(a)+_{S}\phi(b)\in{I}$
            (Def.~\ref{def:Two_Sided_Ideal}). Therefore $\phi(a+_{R}b)\in{I}$,
            and thus $a+_{R}b\in\phi^{\minus{1}}[I]$. Hence
            $\phi^{\minus{1}}[I]$ is closed to addition. But if $r\in{R}$ and
            $a\in\phi^{\minus{1}}[I]$, then since $\phi$ is a ring homomorphism
            it is true that $\phi(r\cdot_{R}a)=\phi(r)\cdot_{S}\phi(a)$. But
            since $a\in\phi^{\minus{1}}[I]$ it is true that $\phi(a)\in{I}$, and
            since $I$ is an ideal in $S$ it is therefore true that
            $\phi(r)\cdot_{S}\phi(a)\in{I}$. Thus, $\phi(r\cdot_{R}a)\in{I}$,
            and hence $r\cdot_{R}a\in\phi^{\minus{1}}[I]$. Therefore,
            $\phi^{\minus{1}}[I]$ is an ideal in $R$.
        \end{proof}
        \begin{theorem}
            If $\ring[R]{R}$ and $\ring[S]{S}$ are commutative rings, if
            $I\subsetneq{S}$ is a prime ideal, and if $\phi:R\rightarrow{S}$ is
            a ring homomorphism, then $\phi^{\minus{1}}[I]$ is a prime ideal in
            $R$.
        \end{theorem}
        \begin{proof}
            For if $I$ is an ideal in $S$ and $\phi:R\rightarrow{S}$ is a ring
            homomorphism, then $\phi^{\minus{1}}[I]$ is an ideal in $R$
            (Thm.~\ref{thm:homo_pre_image_of_ideal_is_ideal}). Suppose
            $a,b\in{R}$ are such that $a\cdot_{R}b\in\phi^{\minus{1}}[I]$.
            But then $\phi(a\cdot_{R}b)\in{I}$, and since $\phi$ is a ring
            homomorphism it is therefore true that
            $\phi(a\cdot_{R}b)=\phi(a)\cdot_{S}\phi(b)$. But $I$ is a prime
            ideal, and if $\phi(a)\cdot_{S}\phi(b)\in{I}$, then either
            $\phi(a)\in{I}$ or $\phi(b)\in{I}$. But then either
            $a\in\phi^{\minus{1}}[I]$ or $b\in\phi^{\minus{1}}[I]$, and hence
            $\phi^{\minus{1}}[I]$ is a prime ideal.
        \end{proof}
        Given a ring $\ring{R}$, the existence of prime ideals follows directly
        from the existence of \textit{maximal} ideals. We first must define such
        things, and then prove that every maximal ideal is prime.
        \begin{fdefinition}{Maximal Ideal}{Maximal_Ideal}
            A maximal ideal in a ring $\ring{R}$ is an ideal $I\subsetneq{R}$
            such that for every ideal $J\subsetneq{R}$ such that
            $I\subseteq{J}$, it is true that $I=J$.
        \end{fdefinition}
        \begin{theorem}
            \label{thm:Proper_Ideal_Does_Not_Contain_1}%
            If $\ring{R}$ is an ideal, if $I\subseteq{R}$ is a proper ideal, and
            if $1\in{R}$ is the multiplicative idenity, then $1\notin{R}$.
        \end{theorem}
        \begin{proof}
            For suppose not. But if $1\in{R}$, then since $I$ is an ideal for
            all $r\in{R}$ it is true that $r\cdot{1}\in{R}$. But $1$ is the
            multiplicative identity, and hence $r\cdot{1}\in{I}$. But then for
            all $r\in{R}$ it is true that $r\in{I}$, and hence $r=I$, a
            contradiction since $I$ is a proper ideal. Thus, $1\notin{I}$.
        \end{proof}
        \begin{ftheorem}{Krull's Theorem}{Krulls_Theorem}
            If $\ring{R}$ is a non-zero commutative ring, then there is a
            maximal ideal $I\subseteq{R}$.
        \end{ftheorem}
        \begin{bproof}
            For let $J\subseteq\powset{R}$ be defined as follows:
            \begin{equation}
                J=\{\,I\in\powset{R}\;|\;I\textit{ is a proper ideal of }R\,\}
            \end{equation}
            But $R$ is non-zero, and hence $\{0\}$ is a proper ideal of $R$,
            and therefore $J$ is non-empty. Then if $\subseteq$ is the inclusion
            relation, then $(J,\subseteq)$ is a partially ordered set. Suppose
            $\Lambda\subseteq{J}$ is a chain. But then $\bigcup\Lambda\in{J}$.
            For if $x,y\in\bigcup\Lambda$, then there is a $I_{x}\in\Lambda$ and
            an $I_{y}\in\Lambda$ such that $x\in{I}_{x}$ and $y\in{I}_{y}$. But
            since $\Lambda$ is a chain, either $I_{x}\subseteq{I}_{y}$ or
            $I_{y}\subseteq{I}_{x}$. Suppose $I_{x}\subseteq{I}_{y}$. But then
            $x,y\in{I}_{y}$, and since $I_{y}\in{J}$ it is an ideal. But then
            $x+y\in{I}_{y}$, and similarly if $I_{y}\subseteq{I}_{x}$. Hence,
            $x+y\in\bigcup\Lambda$. Moreover, if $r\in{R}$ and
            $x\in\bigcup\Lambda$, then there is an $I\in\Lambda$ such that
            $x\in{I}$. But $I$ is an ideal, and therefore $r\cdot{x}\in{I}$.
            But then $r\cdot{x}\in\bigcup\Lambda$. Therefore, $\bigcup\Lambda$
            is an ideal. Moreover, since for all $I\in\Lambda$ it is true that
            $I\in{J}$, $I$ is thus a proper ideal. But if $I$ is a proper ideal,
            then $1\not\in{I}$ (Thm.~\ref{thm:Proper_Ideal_Does_Not_Contain_1}).
            But this is true of all $I\in\Lambda$, and hence
            $1\notin\bigcup\Lambda$, and therefore $\bigcup\Lambda$ is a proper
            ideal. Therefore, every chain of $(J,\subseteq)$ is bounded, and
            therefore there exists a maximal element $I\in{J}$. But then $I$
            is a proper ideal of $R$ that is not contained in any other proper
            ideals, and is therefore a maximal ideal
            (Def.~\ref{def:Maximal_Ideal}).
        \end{bproof}
        Zorn's lemma is indeed required for this proof. Krull's original proof
        invoked transfinite induction, a consequence of the well-ordering
        theorem, and indeed Krull's theorem implies the axiom of choice. Since
        the axiom of choice and Zorn's lemma are equivalent, any axiomatic
        system capable of proving Krull's theorem must contain, or be able to
        prove, the axiom of choice. Nevertheless, we can now prove that every
        non-zero ring contains a prime ideal.
        \begin{ltheorem}{Maximal Ideals are Prime}{Maximal_Ideals_are_Prime}
            If $\ring{R}$ is a ring, if $I\subseteq{R}$ is a maximal ideal, then
            $I$ is a prime ideal.
        \end{ltheorem}
        \begin{proof}
            For suppose not. Then there exists $a,b\in{R}$ such that
            $a,b\notin{I}$, but $a\cdot{b}\in{I}$ (Def.~\ref{def:Prime_Ideal}).
            But then neither $a$ nor $b$ are units, for otherwise since $I$ is
            an ideal, since $a\cdot{b}\in{I}$, it is then true that
            $a^{\minus{1}}\cdot{a}\cdot{b}\in{I}$, and thus $b\in{I}$, a
            contradiction. But then the ideal generated by $I\cup\{a\}$ is a
            proper ideal since it does not contain 1. But then
            $I\subsetneq(I\cup\{a\})$ since $a\notin{I}$, a contradiction since
            $I$ is maximal. Thus, $I$ is prime.
        \end{proof}
        \begin{theorem}
            \label{thm:Existence_of_Prime_Ideals}%
            If $\ring{R}$ is a non-zero ring, then there exists a prime ideal
            $I\subseteq{R}$.
        \end{theorem}
        \begin{proof}
            For by Krull's theorem, there exists a maximal ideal
            $I\subseteq{R}$ (Thm.~\ref{thm:Krulls_Theorem}). But maximal ideals
            are prime ideals (Thm.~\ref{thm:Maximal_Ideals_are_Prime}). Hence,
            $I$ is a prime ideal.
        \end{proof}
        We now return to fields for a moment to develop their structure.
        \begin{theorem}
            If $\ring[F]{F}$ and $\ring[K]{K}$ are fields, and if
            $\phi:F\rightarrow{K}$ is a field homomorphism, then it is
            injective.
        \end{theorem}
        \begin{proof}
            For $\phi^{\minus{1}}[\{0_{K}\}]$ is an ideal in $F$
            (Thm.~\ref{thm:homo_pre_image_of_ideal_is_ideal}), and since it
            doesn't contain 1 by the definition of a field homomorphism, it
            must be a proper ideal. But the only proper ideal of a field is the
            zero ideal (Thm.~\ref{thm:Ideals_of_Field}), and thus
            $\phi^{\minus{1}}[\{0_{K}\}]=\{0_{F}\}$. But then if
            $\phi(a)=\phi(b)$, then $\phi(a-b)=0_{K}$, and thus $a-b=0_{F}$.
            That is, $a=b$.
        \end{proof}
        \begin{theorem}
            If $\ring[F]{F}$ is a field, if $\ring{\mathbb{Z}}$ is the standard
            ring of integers, if $\ring{\mathbb{Q}}$ is the field of
            rational numbers, and if $f:\mathbb{Z}\rightarrow{F}$ is an
            injective ring homomorphism, then there exists a field homomorphism
            $\tilde{f}:\mathbb{Q}\rightarrow{F}$.
        \end{theorem}
        \begin{proof}
            For any $n\in\mathbb{N}^{+}$, we have:
            \begin{equation}
                f(n)=f\Big(\sum_{k\in\mathbb{Z}_{n}}1\Big)
                =\sum_{k\in\mathbb{Z}_{n}}f(1)
            \end{equation}
            and similarly for any negative integer. But $f$ is injective, and
            hence $f(1)\ne{0}_{R}$. Thus, since $\ring[F]{F}$ is a field, for
            all $m\in\mathbb{Z}\setminus\{0\}$ there is a unique $x\in{F}$ such
            that $f(m)\cdot_{F}x=1_{F}$. Label $x$ as $f(m)^{\minus{1}}$ and
            define $\tilde{f}$ as follows:
            \begin{equation}
                \tilde{f}\big((n,m)\big)=
                \begin{cases}
                    0,&n=0\\
                    f(n)\cdot_{F}f(m)^{\minus{1}},&\textrm{otherwise}
                \end{cases}
            \end{equation}
        \end{proof}
        \begin{definition}
            A field of characteristic zero is a field $\ring{F}$ such that there
            exists a field homomorphism $f:\mathbb{Q}\rightarrow{F}$.
        \end{definition}
        Equivalently one could say that adding $1_{F}$ to itself $n$ times never
        results in zero.
        \begin{example}
            $\mathbb{Q}$, $\mathbb{R}$, and $\mathbb{C}$ are fields of
            characteristic zero.
        \end{example}
        \begin{definition}
            The characteristic of a field $\ring{F}$ is the smallest non-zero
            $n\in\mathbb{N}$ such that:
            \begin{equation}
                \sum_{k\in\mathbb{Z}_{n}}1_{F}=0_{F}
            \end{equation}
        \end{definition}
        \begin{theorem}
            If $n\in\mathbb{N}^{+}$ is a non-negetive integer, and if
            $\ring{F}$ is a field of characteristic $n$, then $n$ is a prime
            number.
        \end{theorem}
        \begin{proof}
            For suppose not. Then there are integers $p,q\in\mathbb{N}^{+}$ such
            that $p,q<n$ and $p\cdot{q}=n$. Let $f:\mathbb{Z}\rightarrow{F}$ be
            defined by:
            \begin{equation}
                f(n)=
                \begin{cases}
                    0_{F},&n=0\\
                    \sum_{k\in\mathbb{Z}_{n}}1_{F},&n>0\\
                    \minus\sum_{k\in\mathbb{Z}_{|n|}}1_{F},&n<0
                \end{cases}
            \end{equation}
            This is a ring homomorphism, and hence
            $f(p\cdot{q})=f(p)\cdot{f}(q)$. But $f(n)=0$, and since all fields
            are integral domains, either $f(p)=0$ or $f(q)=0$. But then there
            exists a positive integer smaller than $n$ such that $f(p)=0$,
            a contradiction as $n$ is the characteristic of $F$. Hence, $n$ is
            a prime.
        \end{proof}
        \begin{ftheorem}{Binomial Theorem}{Binomial_Theorem}
            If $\ring{R}$ is a commutative ring, if $a,b\in{R}$, and if
            $n\in\mathbb{N}$, then:
            \begin{equation*}
                (a+b)^{n}=\sum_{k\in\mathbb{Z}_{n}}\binom{n}{k}a^{k}b^{n-k}
            \end{equation*}
            where $\binom{n}{k}x$ denotes the sum of $x$ with itself
            $\binom{n}{k}$ times.
        \end{ftheorem}
        \begin{theorem}
            If $p\in\mathbb{N}$ is a prime number, if $r\in\mathbb{N}$ is such
            that $1\leq{r}$ and $r\leq{p}^{n}-1$, then $p$ divides
            $\binom{p^{n}}{r}$.
        \end{theorem}
        \begin{theorem}
            If $\ring{F}$ is a field of characteristic $p\in\mathbb{N}^{+}$,
            if $n\in\mathbb{N}$, and if $a,b\in{F}$, then:
            \begin{equation}
                (a+b)^{p^{n}}=a^{p^{n}}+b^{p^{n}}
            \end{equation}
        \end{theorem}
        \begin{proof}
            Apply the binomial in combination with the previous theorem.
        \end{proof}
    \section{Polynomial Rings}
        \begin{fdefinition}{Polynomial Ring}
            The ring of polynomials over a ring $\ring[R]{R}$ is the set $R[x]$
            of all finitely supported sequences $a:\mathbb{N}\rightarrow{F}$
            with the following addition and multiplication:
            \begin{align}
                (a+b)_{n}&=a_{n}+_{F}b_{n}\\
                (ab)_{n}&=\sum_{k=0}^{n}a_{k}\cdot_{F}b_{n-k}
            \end{align}
        \end{fdefinition}
        This is very much mimicing polynomials. The sum rule says we simply add
        the coefficients of two polynomials, and the product is the Cauchy
        product of two polynomials. That is, we multiply
        $(a_{0}+a_{1}x+\dots+a_{n}x^{n})$ by $(b_{0}+b_{1}x+\dots+b_{n}x^{n})$
        and collect the coefficients of all terms with order $x^{k}$ and group
        them. The resulting coefficient is precisely this sum. We now show that,
        given a ring $\ring[R]{R}$, the polynomial ring $R[x]$ is indeed a ring.
        That is, $\monoid[][+]{R}$ is a group, $\monoid[][\cdot]{R}$ is a
        monoid, and $\cdot$ distributes over $+$.
        \begin{theorem}
            If $\ring[R]{R}$ is a ring, then $\ring{R[x]}$ is a ring.
        \end{theorem}
        \begin{proof}
            For let $e:\mathbb{N}\rightarrow{R}$ be the sequence defined by
            $e_{k}=0_{R}$ for all $k\in\mathbb{N}$. Then $e$ is finitely
            supported, and hence $e\in{R}[x]$, and moreover $e$ is a unital
            element with respect to $+$. For if $f\in{R}[x]$, then for all
            $n\in\mathbb{N}$ we have:
            \begin{equation}
                (e+f)(n)=e_{n}+_{R}f_{n}=0_{R}+f_{n}=f_{n}
            \end{equation}
            and hence $e+f=f$. Similarly, $f+e=f$, and therefore $e$ is a unital
            element. If $f\in{R}[x]$, let $\minus{f}$ be the sequence
            defined by $(\minus{f})(n)=\minus{1}_{R}\cdot(f(n))$. Since $f$ is
            finitely supported, $\minus{f}$ is also finitely supported, and
            hence an element of $R[x]$, and moreover for all $n\in\mathbb{N}$
            we have:
            \begin{subequations}
                \begin{align}
                    \big(f+(\minus{f})\big)(n)&=f(n)+_{R}(\minus{f})(n)\\
                        &=f(n)+_{R}(\minus{1}_{R})\cdot{f}(n)\\
                        &=f(n)\cdot_{R}\big(1+(\minus{1})\big)\\
                        &=f(n)\cdot_{R}0\\
                        &=0\tag{Thm.~\ref{thm:Rng_Mult_by_Zero}}
                \end{align}
            \end{subequations}
            That is, $f+(\minus{f})=e$, and hence $f$ has an additive inverse.
            Lastly, $+$ is associative, for if $f,g,h\in{R}[x]$, if
            $A=(f+g)+h$, and if $B=f+(g+h)$, then for all $n\in\mathbb{N}$ we
            obtain:
            \begin{subequations}
                \begin{align}
                    A(n)&=\big((f+g)+h\big)(n)\\
                        &=(f+g)(n)+_{r}h(n)\\
                        &=\big(f(n)+_{R}g(n)\big)+_{R}h(n)\\
                        &=f(n)+_{R}\big(g(n)+_{R}h(n)\big)\\
                        &=f(n)+_{R}(g+h)(n)\\
                        &=\big(f+(g+h)\big)(n)\\
                        &=B(n)
                \end{align}
            \end{subequations}
            and therefore $+$ is associative. Next, $\monoid[][\cdot]{R}$ is a
            monoid. For let $I:\mathbb{N}\rightarrow{R}$ be defined
            by $I_{0}=1$ and $I_{k}=0$ for all $k\in\mathbb{N}^{+}$. Then for
            all $f\in{R}[x]$, we have:
            \begin{equation}
                (f\cdot{I})(n)=\sum_{k=0}^{n}f(k)\cdot_{R}I(n-k)
                =f(n)
            \end{equation}
            since $I(n-k)=0$ for all $k\ne{0}$. Thus, $f\cdot{I}=f$, and
            similarly $I\cdot{f}=f$. Therefore $I$ is a multiplicative identity.
            Multiplication is also associative, for if $f,g,h\in{R}[x]$, and if
            $A=(f\cdot{g})\cdot{h}$ and $B=f\cdot(g\cdot{h})$, then:
            \begin{subequations}
                \begin{align}
                    A(n)=\big((f\cdot{g})\cdot{h}\big)(n)
                    &=\sum_{k=0}^{n}(f\cdot{g})(k)\cdot_{R}h(n-k)\\
                    &=\sum_{k=0}^{n}\Big(\sum_{j=0}^{n}f(j)\cdot_{R}g(n-j)\Big)
                        \cdot_{R}h(n-k)
                \end{align}
            \end{subequations}
        \end{proof}
        \begin{theorem}
            If $\ring{F}$ is a field, then $\ring{F[x]}$ is a commutative
            algebra over $F$.
        \end{theorem}
        There is a natural embedding of $F$ into $F[x]$ by looking at the
        subspace of all sequences $a:\mathbb{N}\rightarrow{F}$ such that
        $a_{k}=0$ for all $k>0$. That is, the only possible non-zero term is
        $a_{0}$.
        \begin{theorem}
            If $\ring[F]{F}$ is a field, if $\ring[R]{R}$ is a ring, if
            $F\subseteq{R}$ is a subring, and if $r\in{R}$, then there is a
            unique homomorphism $f:F[x]\rightarrow{R}$ such that for all
            $a\in{F}$, $f(a)=r$.
        \end{theorem}
        \begin{definition}
            The degree of a non-zero polynomial $a\in{F}[x]$ is the largest
            $n\in\mathbb{N}$ such that $a_{n}\ne{0}$. The degree of the zero
            polynomial is zero.
        \end{definition}
        Since $F[x]$ is the space of all finitely supported sequences, for any
        such $a\in{F}[x]$ there will eventually be an $N\in\mathbb{N}$ such that
        for all $n>N$ it is true that $a_{n}=0$. By the well-ordering all of the
        integers, there will be a least such element, and hence the above
        definition is well defined. There's a natural way of looking at
        $F[x]$ as a subset of $\funcspace[F]{F}$, the set of all functions from
        $F$ to itself. Given $a\in{F}[x]$ of degree $n\in\mathbb{N}$ we consider
        the function $f\in\funcspace[F]{F}$ defined by:
        \begin{equation}
            f(x)=\sum_{k\in\mathbb{Z}_{n+1}}a_{k}\cdot_{F}x^{k}
            =a_{0}+_{F}a_{1}\cdot_{k}x+_{F}a_{2}\cdot_{F}x^{2}+_{F}\dots
            +_{F}a_{n}\cdot_{F}x^{n}
        \end{equation}
        In more familiar language (dropping the subscripts and using
        concatenation to denote multiplication), we have:
        \begin{equation}
            f(x)=a_{0}+a_{1}x+\cdots+a_{n}x^{n}
        \end{equation}
        The sequence definition is good for rigor and solving theorems, since
        the Cauchy product allows one to easily manipulate expressions without
        worrying about the non-existent dummy variable $x$, whereas the function
        definition (as a subset of $\funcspace[F]{F}$) is good for intuition.
        \begin{definition}
            A monic polynomial of degree $n$ in a field $\ring{F}$ is a
            polynomial $a\in{F}[x]$ of degree $n\in\mathbb{N}$ such that
            $a_{n}=1$.
        \end{definition} 
        \begin{theorem}
            If $\ring{F}$ is a field, then $F[x]$ is a Euclidean domain.
        \end{theorem}
        \begin{theorem}
            If $\ring{F}$ is a field, if $\ring[I]{I}$ is an ideal in $F[x]$,
            and if $a\in{I}$ is of least degree, then $I=(a)$, where $(a)$ is
            the ideal generated by $a$.
        \end{theorem}
        \begin{proof}
            For given $b\in(a)$, there is an $r\in{F}[x]$ such that
            $b=r\cdot{a}$. But $I$ is an ideal, and $a\in{I}$, and hence
            $b\in{I}$. Thus, $(a)\subseteq{I}$. If $b\in{I}$, then by the
            division algorithm there are polynomials $p,q\in{F}[x]$ such that
            $b=aq+r$ where the degree of $r$ is strictly less than the degree of
            $a$. But then $r=b-aq$. And since $b\in{I}$ and $q\in{F}[x]$, it is
            true that $bq\in{I}$ since $I$ is an ideal. But if $a\in{I}$ and
            $bq\in{I}$, then $b-aq\in{I}$ since $I$ is an ideal. Thus,
            $r\in{I}$. But $r$ is a polynomial of degree strictly less than
            $a$, and $a$ is a non-zero polynomial of least degree in $I$.
            Therefore, $r=0$. But then $b=aq$, and hence $b\in(a)$. Thus,
            $I\subseteq(a)$. From the definition of equality, $I=(a)$.
        \end{proof}
        \begin{theorem}
            There exists a bijection between monic polynomials in $F[x]$ and
            the ideals of $F[x]$.
        \end{theorem}
        \begin{definition}
            A root of a polynomial $a\in{F}[x]$ of degree $n\in\mathbb{N}$ over
            a field $\ring{F}$ is an element $r\in{F}$ such that:
            \begin{equation}
                \sum_{k\in\mathbb{Z}_{n+1}}a_{k}\cdot_{F}x^{k}=0_{F}
            \end{equation}
        \end{definition}
        \begin{theorem}
            If $\ring{\mathbb{Q}}$ is the field of rational numbers, if
            $a\in\mathbb{Q}[x]$ is a polynomial of degree $n\in\mathbb{N}$,
            if $r\in\mathbb{Q}$ is a root of $a$, and if $p,q\in\mathbb{Q}$
            are such that $r=p/q$ and $\GCD(p,q)=1$, then $p$ divides $a_{0}$
            and $q$ divides $a_{n}$.
        \end{theorem}
        \begin{proof}
            For if $r$ is a root, then:
            \begin{equation}
                \sum_{k\in\mathbb{Z}_{n+1}}a_{k}\cdot{r}^{k}=0
            \end{equation}
            But $r=p/q$, and thus:
            \begin{equation}
                \sum_{k\in\mathbb{Z}_{n+1}}a_{k}\cdot\big(\frac{p}{q}\big)^{k}
                =0
            \end{equation}
            Simplifying, and multiplying both sides by $q^{k}$, we have:
            \begin{equation}
                \sum_{k\in\mathbb{Z}_{n+1}}a_{k}\cdot{p}^{k}q^{n-k}=0
            \end{equation}
            From this, $q$ divides $a_{n}p^{n}$. But $\GCD(p,q)=1$, and hence
            $q$ does not divide $p^{n}$. Thus, $q$ divides $a_{n}$. Similarly,
            $p$ divides $a_{0}$.
        \end{proof}
        \begin{example}
            Sticking with $\mathbb{Q}[x]$, the polynomial $f(x)=x^{3}-3x-1$ is
            irreducible over $\mathbb{Q}$. By the previous theorem, the only
            possible roots $p/q$ must be such that $p$ divides $\minus{1}$ and
            $q$ divides $1$. Hence, $p/q=\pm{1}$. But $f(1)=\minus{3}$ and
            $f(\minus{1})=1$, neither of which are zero. Hence, $f$ has no roots
            over $\mathbb{Q}$. Since it is a cubic, it must be irreducible.
        \end{example}
        \begin{ftheorem}{Gauss's Lemma for Polynomials}
                        {Gauss's Lemma for Polynomials}
            If $\ring{\mathbb{Z}}$ is the ring of integers, if
            $\ring{\mathbb{Q}}$ is the field of rational numbers, if
            $a\in{\mathbb{Z}}[x]$ is such that $a$ factors non-trivially in
            $\mathbb{Q}[x]$, then $a$ factors non-trivially in $\mathbb{Z}[x]$.
        \end{ftheorem}
        \begin{bproof}
            For if $b,c\in\mathbb{Q}[x]$ are such that $a=b\cdot{c}$, if
            $M,N\in\mathbb{N}$ are the products of the denominators of $b$
            and $c$, respectively, then $Ma,Nb\in\mathbb{Z}[x]$. But then
            $NMa\in\mathbb{Z}[x]$. By the fundamental theorem of arithmetic,
            there exists a prime $p\in\mathbb{N}$ such that $p$ divides $MN$.
            But then $Ma\cdot{N}b$ is the zero polynomial in $\mathbb{F}_{p}[x]$
            and since $\mathbb{F}_{p}$ is a field, this means that $p$ divides
            the coefficients of every element of either $Ma$ or $Nb$. Continuing
            we remove all of the prime factor of $MNa$ and obtain a
            factorization of $a$ in $\mathbb{Z}[x]$.
        \end{bproof}
        \begin{theorem}
            If $\ring{\mathbb{Z}}$ is the ring of integers, if
            $a\in\mathbb{Z}[x]$ is a monic polynomial of degree
            $n\in\mathbb{N}$, and if $b\in\mathbb{Q}[x]$ is a monic factor of
            $a$, then $b\in\mathbb{Z}[x]$.
        \end{theorem}
        \begin{proof}
            For if $b,c\in\mathbb{Q}[x]$ are such that $a=bc$, with $b$ a monic
            polynomial, then by the Cauchy product, since $a$ is monic, $c$
            must also be monic. Let $m$ and $n$ by the least integers such that
            $mb,nc\in\mathbb{Z}[x]$. If $p\in\mathbb{N}$ is a prime that divides
            $mn$, then it divides all of the coefficients of either $mb$ of $nc$
            and hence either $(m/p)b\in\mathbb{Z}[x]$ or
            $(n/p)c\in\mathbb{Z}[x]$, a contradiction since $m$ and $n$ are the
            least such integers with this property. Hence, $m=1$ and $n=1$.
        \end{proof}
        \begin{definition}
            An algebraic integer in $\mathbb{C}$ is a complex number
            $z\in\mathbb{C}$ such that $z$ is the root of a monic polynomial
            $a\in\mathbb{Z}[x]$.
        \end{definition}
        \begin{ftheorem}{Eisenstein's Criterion}{Eisenstein_Criterion}
            If $\ring{\mathbb{Z}}$ is the ring of integers, if
            $a\in\mathbb{Z}[x]$ is a polynomial in $\mathbb{Z}$ of degree
            $n\in\mathbb{N}$, if $p\in\mathbb{N}$ is a prime number such that
            $p$ does not divide $a_{n}$, $p^{2}$ does not divide $a_{0}$, and
            such that $p$ divides $a_{k}$ for all $k\in\mathbb{Z}_{n}$, then $a$
            is irreducible in $\mathbb{Q}[x]$.
        \end{ftheorem}
        \begin{bproof}
            For suppose $a$ factors in $\mathbb{Q}[x]$. But then it factors in
            $\mathbb{Z}[x]$. Suppose $b,c\in\mathbb{Z}[x]$ are non-trivial
            factors. By the Cauchy product, $a_{0}=b_{0}c_{0}$, and thus
            $p$ divides either $b_{0}$ or $c_{0}$. Suppose it divides $b_{0}$.
            But since $p^{2}$ does not divide $a_{0}$, $p$ does not divide
            $c_{0}$. But again by the Cauchy product,
            $a_{1}=b_{0}c_{1}+b_{1}c_{0}$. But $p$ divides $a_{1}$, and hence
            $p$ divides $b_{1}$. Continuing by induction on the Cauchy product,
            $p$ divides all of the $b_{k}$, contradicting that $p$ does not
            divide $a_{n}$.
        \end{bproof}
        It is important again to note that $\mathbb{Z}_{n}=\{0,1,\dots,n-1\}$,
        hence $\mathbb{Z}_{n}$ does not contain $n$. So we require $p$ to divide
        $a_{0},\dots,a_{n-1}$, but not $a_{n}$, and we require $p^{2}$ not to
        divide $a_{0}$. Eisenstein's criterion holds for any unique
        factorization domain.
        \begin{theorem}
            If $\ring{Z}$ is the ring of integers, if $a\in\mathbb{Z}[x]$ is a
            reducible polynomial in $\mathbb{Z}$ of degree $n\in\mathbb{N}$, if
            $p\in\mathbb{N}$ is a prime, and if $p$ does not divide $a_{n}$,
            then $\overline{a}\in\mathbb{F}_{p}[x]$ is reducible.
        \end{theorem}
        \begin{proof}
            For since $a\in\mathbb{Z}[x]$ is reducible, there exists
            $b,c\in\mathbb{Z}[x]$ such that $a=b\cdot{c}$. But since
            $p$ does not divide $a_{n}$, it does not divide
            $a_{0}b_{n}+a_{n}b_{0}$, and thus
            $\overline{a}=\overline{b}\cdot\overline{c}$ is a non-trivial
            factorization. Hence, $\overline{a}$ is reducible in
            $\mathbb{F}_{p}$.
        \end{proof}
        A more useful result is the contrapositive.
        \begin{theorem}
            If $\ring{\mathbb{Z}}$ is the ring of integers, if
            $p\in\mathbb{N}$ is a prime, if $\ring[p]{\mathbb{F}_{p}}$ is the
            field of integers modulo $p$, if $a\in\mathbb{Z}[x]$ is a polynomial
            of degree $n\in\mathbb{N}$ such that $p$ does not divide $a_{n}$,
            and if $\overline{a}\in\mathbb{F}_{p}[x]$ is irreducible, then
            $a$ is irreducible in $\mathbb{Z}[x]$.
        \end{theorem}
        The converse of this theorem does not hold. Indeed, there exist
        polynomials $a\in\mathbb{Z}[x]$ that are reducible in
        $\mathbb{F}_{p}[x]$ for every prime integer $p\in\mathbb{N}$, yet $a$
        is not reducible in $\mathbb{Z}[x]$.
        \begin{theorem}
            If $\ring{F}$ is a field, if $a\in{F}$, and if $f\in{F}[x]$, then
            there exists a polynomial $q\in{F}[x]$ such that, for all
            $x\in{F}$ it is true that:
            \begin{equation}
                f(x)=q(x)\cdot(x-a)+f(a)
            \end{equation}
        \end{theorem}
        \begin{proof}
            For by the division algorithm, there exists polynomials
            $q,r\in{F}[x]$ such that $f(x)=q(x)\cdot(x-a)+r(x)$ and $r$ has
            degree less than $x-a$. But the degree of $x-a$ is 1, and hence
            $r$ is a constant (has degree zero). Moreover:
            \begin{equation}
                f(a)=q(a)\cdot(a-a)+r(a)=q(a)\cdot{0}+r(a)=0+r(a)=r(a)
            \end{equation}
            Therefore $r(x)=f(a)$. Thus, for all $x\in{F}$,
            $f(x)=q(x)\cdot(x-a)+f(a)$.
        \end{proof}
        \begin{theorem}
            If $\ring{F}$ is a field, if $f\in{F}[x]$ is a polynomial, if
            $a\in{F}$, and if $x-a$ divides $f(x)$, then $f(a)=0$.
        \end{theorem}
        \begin{proof}
            For by the previous theorem, there is a polynomial $q\in{F}[x]$
            such that $f(x)=q(x)\cdot(x-a)+f(a)$. But if $f(a)=0$, then
            $f(x)=q(x)\cdot(x-a)$, and hence $x-a$ divides $f$. In the other
            direction, if $x-a$ divides $f(x)$, then there is a $q\in{F}[x]$
            such that $f(x)=q(x)\cdot(x-a)$. But then $f(a)=q(a)\cdot(a-a)$,
            and thus $f(a)=0$.
        \end{proof}
        \begin{theorem}
            If $\ring{F}$ is a field, if $f\in{F}[x]$ is a non-zero polynomial
            in $F$ of degree $n\in\mathbb{N}$, the there are at most $n$ roots.
        \end{theorem}
        \begin{proof}
            For by induction. If $f\in{F}[x]$ is a polynomial of degree 1, then
            $f(x)=ax+b$ for some $a,b\in{F}$. But if $\alpha$ is a root, then
            $f(x)=q(x)\cdot(x-\alpha)$. But since $f$ is degree 1, and since
            $x-\alpha$ is degree 1, $q$ must be of degree 0, and hence a
            constant. But $f$ is non-zero, and hence $\alpha\ne{0}$. Thus
            $f(x)=0$ if and only if $x=\alpha$. Suppose the proposition is true
            for $n\in\mathbb{N}$, and let $f\in{F}[x]$ be a non-zero polynomial
            of degree $n+1$. Suppose $f$ has more than $n+1$ roots. But if
            $\alpha$ is a root, then $x-\alpha$ divides $f$. Hence
            $f(x)=q(x)\cdot(x-\alpha)$, where $q$ is a polynomial of degree less
            than $f$. But by hypothesis $q$ then has at most $n$ roots. And any
            root of $q$ is a root of $f$, and hence $f$ has at most $n+1$ roots,
            a contradiction. Thus, $f$ has at most $n+1$ roots.
        \end{proof}
        \begin{theorem}
            If $\monoid{G}$ is a finite Abelian group, if $m\in\mathbb{N}$ is
            such that $m$ divides $\cardinality{G}$, then there are at most
            $m$ elements in $G$ whose order divides $m$ if and only if $G$ is
            cyclic.
        \end{theorem}
        \begin{proof}
            For if $G$ is cycle, and if $d$ divides $\cardinality{G}$, let
            $G_{d}=\{x\in{G}\;|\;x^{d}=1\}$. If $G_{d}$ is empty, then
            $\cardinality{G_{d}}<m$. If not then there is a $y\in{G}_{d}$.
            But then $\innerprod{y}\subseteq{G}_{d}$. For if
            $x\in\innerprod{y}$, then $x=y^{k}$ for some $k\in\mathbb{Z}_{n}$.
            But then $x^{d}=(y^{k})^{d}=y^{kd}=1$, and hence $x\in{G}_{d}$.
            Therefore, $\innerprod{y}\subseteq{G}_{d}$. Now let $x\in{G}_{d}$
            Since $\monoid{G}$ is cyclic there exists $z\in{G}$ such that
            $\innerprod{z}=G$. But then there exists
            $k_{1},k_{2}\in\mathbb{Z}_{n}$ such that $z^{k_{1}}=y$ and
            $z^{k_{2}}=x$. But then $y^{k_{2}-k_{1}}=x$, and therefore
            $x\in\innerprod{y}$. Thus, $\innerprod{y}=G_{d}$. But
            $\innerprod{y}$ has $d$ elements, and hence there are at most $d$
            elements of order $d$. Going the other way, suppose that if $d$
            divides $n$ then there are at most $d$ elements of order $d$.
            Since $\monoid{G}$ is finite and Abelian, it is the product of
            finite cyclic groups $\monoid[][+]{\mathbb{Z}_{{p_{k}}^{n_{k}}}}$.
            Suppose two of the primes $p_{k}$ are equal. But then there are at
            least $p_{k}^{2}$ elements of order $p_{k}$, and $p_{k}$ divides the
            order of the group, a contradiction. Thus, all of the primes are
            distinct. By the direct product of coprime cyclic groups is a cyclic
            group. Thus, $\monoid{G}$ is cyclic.
        \end{proof}
        \begin{theorem}
            If $\ring{F}$ is a field, if $\monoid[][\cdot]{F\setminus\{0\}}$ is
            the multiplicative group of $F$, then it is cyclic.
        \end{theorem}
        \begin{proof}
            Since $\ring{F}$ is a field, $\monoid[][\cdot]{F\setminus\{0\}}$ is
            Abelian. Suppose $d\in\mathbb{N}$ divides the cardinality of
            $F\setminus\{0\}$ and let $f\in{F}[x]$ be the polynomial
            $f(x)=x^{d}-1$. Then by the previous theorem there are at most
            $d$ roots. But then there are at most $d$ elements of
            $F\setminus\{0\}$ such that $x^{d}=1$, and thus by the previous
            theorem $\monoid[][\cdot]{F\setminus\{0\}}$ is cyclic.
        \end{proof}
        With this we now return to the claim that there are irreducible
        polynomials in $\mathbb{Q}[x]$ that are reducible if $\mathbb{Z}_{p}[x]$
        for every prime $p\in\mathbb{N}$.
        \begin{theorem}
            If $\monoid{G}$ is a cyclic group, if $a,b\in{G}$ do not have
            square roots, then $a*b$ has a square root.
        \end{theorem}
        \begin{proof}
            For since $\monoid{G}$ is cyclic there is an $x\in{G}$ such that
            $\innerprod{x}=G$. But then if $a,b\in{G}$, there exists
            $n,m\in\mathbb{N}$ such that $x^{n}=a$ and $x^{m}=b$. But $a$ and
            $b$ do not have square roots, and hence $n$ and $m$ are odd. But
            then $a*b=x^{n}*x^{m}=x^{n+m}$, and $n+m$ is even. Hence,
            $x^{n+m}$ has a square root, and thus $a*b$ has a square root.
        \end{proof}
        \begin{example}
            The polynomial $f(x)=x^{4}-10x^{2}+1$ is irreducible in
            $\mathbb{Z}[x]$, yet it is reducible in $\mathbb{F}_{p}[x]$ for
            every prime $p\in\mathbb{N}$. If $p\in\mathbb{N}$ is such that
            2 has a square root in $\mathbb{Z}_{p}$, then we can factor this
            to obtain:
            \begin{equation}
                (x^{2}-2\sqrt{2}x-1)(x^{2}+2\sqrt{2}x-1)
                =x^{4}-10x^{2}-1
            \end{equation}
            And hence $f$ is reducible over all such $\mathbb{Z}_{p}[x]$. If $p$
            is such that 3 has a square root, then:
            \begin{equation}
                (x^{2}-2\sqrt{3}x-1)(x^{2}+2\sqrt{2}x+1)
                =x^{4}-10x^{2}+1
            \end{equation}
            For all other such $p$, neither 2 nor 3 are square roots, and hence
            by the previous theorem their product $2\cdot{3}=6$ does have a
            square root. But then:
            \begin{equation}
                x^{4}-10x^{2}+1=
                    \big(x^{2}-(5+2\sqrt{6})\big)\big(x^{2}-(5-2\sqrt{6})\big)
            \end{equation}
        \end{example}
        There is a stronger result that for every non-prime
        $n\in\mathbb{N}^{+}$ there is a polynomial $f\in\mathbb{Z}[x]$ of degree
        $n$ such that $f$ is irreducible over $\mathbb{Z}[x]$ but reducible
        over $\mathbb{Z}_{p}[x]$ for every prime $p$.
        (See Brandl, R. American Mathematical Monthly, 1986).
        \begin{definition}
            An extension field of a field $\ring[F]{F}$ is a field $\ring[K]{K}$
            such that $\ring[F]{F}$ is a subfield of $\ring[K]{K}$.
        \end{definition}
        Thus, a field extension is to fields as supersets are to sets. Unlike
        set theory where one considers subsets most of the time, most of field
        theory is obsessed with field extensions.
        \begin{theorem}
            If $\ring[F]{F}$ and $\ring[K]{K}$ are fields, and if $K$ is an
            extension field of $F$, then $\ring[K]{K}$ is a vector field
            over $\ring[F]{F}$.
        \end{theorem}
        \begin{definition}
            The degree of a field extension $\ring[K]{K}$ of a given field
            $\ring[F]{F}$ is the dimension of the vector space
            $\ring[K]{K}$ over $\ring[F]{F}$. This is denoted $[K:F]$.
        \end{definition}
        \begin{definition}
            A finite extension of a field $\ring[F]{F}$ is an extension field
            $\ring[K]{K}$ of $F$ such that $[K:F]\in\mathbb{N}$.
        \end{definition}
        \begin{example}
            The field of complex numbers $\mathbb{C}$ is a field extension over
            $\mathbb{R}$. Moreover, it is a finite extension since $\mathbb{C}$
            has the bases $\{1,i\}$. Thus, $[\mathbb{C}:\mathbb{R}]=2$
        \end{example}
        \begin{example}
            The field $\mathbb{R}$ is also a field extension over the rational
            numbers $\mathbb{Q}$. However, unlike $[\mathbb{C}:\mathbb{R}]$,
            which is finite, $[\mathbb{R}:\mathbb{Q}]=\cardinality{\mathbb{R}}$.
            This can be shown since any finite extension of $\mathbb{Q}$ must
            be countable, and any countable extension must have cardinality
            $\cardinality{\mathbb{Q}\times\mathbb{Q}}$, but this is equal to
            $\cardinality{\mathbb{N}}$, and $\mathbb{R}$ is uncountable. Hence,
            at the very least, $[\mathbb{Q}:\mathbb{R}]$ is uncountable, and if
            we assume the continuum hypothesis then the cardinality must then
            be equal to the cardinality of $\mathbb{R}$ (for it is certinaly not
            greater).
        \end{example}
        \begin{example}
            The Gaussian numbers are a subfield of $\mathbb{C}$ defined as
            follows:
            \begin{equation}
                \mathbb{Q}(i)=\{\,a+ib\in\mathbb{C}\;|\;a,b\in\mathbb{Q}\,\}
            \end{equation}
            We can then see that $[\mathbb{Q}:\mathbb{Q}(i)]=2$ since this
            vector field has $\{1,i\}$ as a basis.
        \end{example}
        \begin{example}
            While $[\mathbb{Q}:\mathbb{R}]$ is uncountable, there are countably
            infinite extension fields. Given any field $\ring{F}$, the field of
            rational functions $F(x)$ has the countable basis $x^{n}$ for all
            $n\in\mathbb{N}$. The subspace of polynomials $F[x]$, while not a
            field, is still a vector space over $F$ and has the same basis.
        \end{example}
        \begin{theorem}
            If $\ring[F]{F}$ is a field, if $\ring[K]{K}$ is a finite field
            extension of $F$, and $\ring[L]{L}$ is a finite field extension of
            $K$, then $L$ is a finite field extension of $F$ and:
            \begin{equation}
                [L:F]=[L:K][K:F]
            \end{equation}
        \end{theorem}
        \begin{proof}
            For if $\ring[K]{K}$ is a finite field extension of $\ring[F]{F}$,
            then it is a finite dimensional vector space over $F$ and thus there
            is an $m\in\mathbb{N}$ and a finite sequence
            $a:\mathbb{Z}_{m}\rightarrow{K}$ such that $a[\mathbb{Z}_{m}]$ is a
            basis for $K$ over $F$. And similarly if $\ring[L]{L}$ is a finite
            field extension over $\ring[K]{K}$ then there is an $n\in\mathbb{N}$
            and a finite sequence $b:\mathbb{Z}_{n}\rightarrow{L}$ such that
            $b[\mathbb{Z}_{n}]$ is a basis for $L$ over $K$. Define
            $A:\mathbb{Z}_{m}\times\mathbb{Z}_{n}\rightarrow{L}$ by
            $A(i,j)=a_{i}\cdot{b}_{j}$ for all
            $(i,j)\in\mathbb{Z}_{m}\times\mathbb{Z}_{n}$. Let
            $f:\mathbb{Z}_{m\cdot{n}}\rightarrow%
             \mathbb{Z}_{m}\times\mathbb{Z}_{n}$ be a bijection and define
            $e:\mathbb{Z}_{n\cdot{m}}\rightarrow{L}$ by $e=A\circ{f}$. Suppose
            $e[\mathbb{Z}_{n\cdot{m}}]$ does not span all of $L$ over $F$. Then
            there exists $x\in{L}$ such that for every sequence
            $c:\mathbb{Z}_{n\cdot{m}}\rightarrow{F}$, $\alpha$ is not the
            sum over $c_{i}\cdot{e}_{i}$. But $b[\mathbb{Z}_{n}]$ forms a basis
            of $L$ over $K$, and thus there is a sequence
            $\beta:\mathbb{Z}_{n}\rightarrow{K}$ such that:
            \begin{equation}
                \alpha=\sum_{k\in\mathbb{Z}_{n}}\beta_{k}\cdot_{L}{b}_{k}
            \end{equation}
            But for each $k$ it is true that $\beta_{k}\in{K}$, and since
            $a[\mathbb{Z}_{m}]$ is a basis for $K$ over $F$, there is a sequence
            $\gamma_{k}:\mathbb{Z}_{m}\rightarrow{F}$ such that:
            \begin{equation}
                \beta_{k}=\sum_{i\in\mathbb{Z}_{m}}\gamma_{i}\cdot_{K}a_{i}
            \end{equation}
            Let $c:\mathbb{Z}_{m}\times\mathbb{Z}_{n}\rightarrow{F}$ be defined
            by $c(i,j)=\gamma_{i}$ and let
            $d:\mathbb{Z}_{m\cdot{n}}\rightarrow{F}$ be defined by
            $d=c\circ{f}$. But then:
            \begin{align}
                \alpha&=\sum_{j\in\mathbb{Z}_{n}}\beta_{j}\cdot_{L}b_{j}\\
                &=\sum_{j\in\mathbb{Z}_{n}}\Big(
                    \sum_{i\in\mathbb{Z}_{m}}\gamma_{i}\cdot_{K}a_{i}\Big)
                    \cdot_{L}b_{j}\\
                &=\sum_{j\in\mathbb{Z}_{n}}\sum_{i\in\mathbb{Z}_{m}}
                    \Big(\gamma_{k}\cdot_{k}\big(a_{i}\cdot_{L}b_{j}\big)\Big)\\
                &=\sum_{j\in\mathbb{Z}_{n}}\sum_{i\in\mathbb{Z}_{m}}
                    c(i,j)\cdot_{K}A(i,j)\\
                &=\sum_{k\in\mathbb{Z}_{n\cdot{m}}}
                    \big((c\cdot_{K}A)\circ{f}\big)(k)\\
                &=\sum_{k\in\mathbb{Z}_{n\cdot{m}}}d_{k}\cdot_{K}e_{k}
            \end{align}
            A contradiction. Hence, $e[\mathbb{Z}_{n\cdot{m}}]$ does span
            $L$. Moreover, it is linearly independent. For if
            $d:\mathbb{Z}_{m\cdot{n}}\rightarrow{F}$ is a sequence such that:
            \begin{equation}
                \sum_{k\in\mathbb{Z}_{m\cdot{n}}}d_{k}\cdot_{K}e_{k}=0
            \end{equation}
            The composing with $f^{\minus{1}}$ such that:
            \begin{equation}
                \sum_{j\in\mathbb{Z}_{n}}\sum_{i\in\mathbb{Z}_{n}}
                    d_{ij}a_{i}b_{j}=0
            \end{equation}
            From the linear independence of the $a_{i}$ and the $b_{j}$,
            all of the $d_{ij}$ are zero. Hence, by the basis theorem,
            $[L:K]=n\cdot{m}$.
        \end{proof}
        Given a field $F$ and a monic irreducible polynomial of positive degree
        $f\in{F}[x]$, we can always obtain a field extension for $F$ by
        considering the quotient $F[x]/(f)$, where $(f)$ is the ideal generated
        by $f$.
        \begin{example}
            Consider the field of real numbers $\ring{\mathbb{R}}$ and let
            $f(x)=x^{2}+1$. This is irreducible over $\mathbb{R}$ since it is
            a quadratic with no roots, as one can determine via the quadratic
            formula. If we consider $\mathbb{R}[x]/(x^{2}+1)$, we obtain the
            following arithmetic:
            \begin{equation}
                (a+bx)+(c+dx)=(a+c)+(b+d)x
            \end{equation}
            Multiplication is carried out as follows:
            \begin{equation}
                (a+bx)(c+dx)=ac+(bc+ad)x+bdx^{2}
            \end{equation}
            But in the quotient we have $x^{2}+1=0$, and hence
            $x^{2}=\minus{1}$. Thus, we can simplify this to:
            \begin{equation}
                (a+bx)(c+dx)=(ac-bd)+(ad+bc)x
            \end{equation}
            And this is precisely the multiplicative structure on $\mathbb{C}$.
            Thus the field $\mathbb{R}[x]/(x^{2}+1)$ is isomorphic to the
            complex numbers $\mathbb{C}$ under the mapping
            $\phi(1)=1$ and $\phi(x)=i$.
        \end{example}
        Such fields are called stem fields. That is, stem fields are fields of
        the form $F[x]/(f)$. The intersection of subrings is a subring. The
        subring generated by a subset $S\subseteq{R}$ in a ring $\ring{R}$ is
        the intersection of all subrings containing $S$. Given a ring
        $\ring{R}$ and a subring $\ring{S}$, and given any subset
        $A\subseteq{R}$, the subring generated by $S$ over $A$ is the
        intersection of all subrings of $R$ that contains $A\cup{S}$.
        \begin{example}
            Letting $\mathbb{C}$ and $\mathbb{R}$ have their usual field
            structures, and taking $A\subseteq\mathbb{C}$ to be $A=\{i\}$, the
            subring generated by $\mathbb{R}$ over $A$ is simply the entire
            complex plane $\mathbb{C}$. We can write this as
            $\mathbb{C}=\mathbb{R}[i]$ or
            $\mathbb{C}=\mathbb{R}[\sqrt{\minus{1}}]$
        \end{example}
        The subring generated by a subset is equal to the set of all possible
        linear combintations of elements in the subset. Rigorously, we look at
        all finite sequence $a:\mathbb{Z}_{n}\rightarrow{F}$ and
        $b:\mathbb{Z}_{n}\rightarrow\mathcal{P}(S)$ such that $b_{k}$ is finite
        for all $k\in\mathbb{Z}_{n}$, and we form the sums:
        \begin{equation}
            z=\sum_{k\in\mathbb{Z}_{n}}\Big(
                a_{k}\prod_{\alpha\in{b}_{k}}\alpha
            \Big)
        \end{equation}
        \begin{theorem}
            If $\ring{R}$ is an integral domain, if $\ring[F]{F}$ is a subring
            of $R$ such that $\ring[F]{F}$ is also a field, and if $R$ is a
            finite dimensional vector space over $F$, then $\ring{R}$ is a
            field.
        \end{theorem}
        \begin{proof}
            For if not then there is a non-zero element $a\in{R}$ with no
            inverse element. But the function $f:R\rightarrow{R}$ defined by
            $f(x)=a\cdot{x}$ is injective. For if $f(x)=f(y)$, then:
            \begin{equation}
                a\cdot{x}-a\cdot{y}=a\cdot(x-y)=0
            \end{equation}
            But $\ring{R}$ is an integral domain and thus either $a=0$ or
            $x-y=0$. But by hypothesis $a$ is non-zero, and hence $x-y=0$.
            Thus, $f$ is injective. But it is also linear since:
            \begin{equation}
                f(x+y)=a\cdot(x+y)=a\cdot{x}+a\cdot{y}=f(x)+f(y)
            \end{equation}
            And thus $f$ is a linear map from a finite dimensional vector space
            to itself and is therefore surjective. But then there exists
            $x\in{R}$ such that $f(x)=1$. But then $a\cdot{x}=1$, a
            contradiction. Hence, $\ring{R}$ is a field.
        \end{proof}
        \begin{theorem}
            If $\ring{F}$ is a field, if $\ring{K}$ is a finite extension field,
            and if $\ring{S}$ is a subring of $K$ that contains $F$, then it is
            a field.
        \end{theorem}
        \begin{theorem}
            For such a ring will be an integral domain, and will also be a
            finite dimensional vector space over $F$, and hence will be a field
            by the previous theorem.
        \end{theorem}
        The intersection of subfields is a field. We can similarly define field
        generated by subset. Given the ring generated by $S$, $F[S]$, the field
        generated by $S$ $F(S)$ is also called the field of fractions of $F[S]$.
        This is because it is obtained by considering all fractions of elements
        in $F[S]$. For example, in the polynomial ring $F[x]$, the field that
        this generates is the field of rational functions $F(x)$.
        \begin{example}
            The ring $\mathbb{Q}[\pi]$ is a subset of $\mathbb{R}$ that is all
            linear combinations of powers of $\pi$ with rational coefficients.
            The field generated by this $\mathbb{Q}(\pi)$ is the set of all
            $f(\pi)/g(\pi)$ where $f,g$ are rational polynomials evaluated at
            $\pi$, and $g$ is non-zero.
        \end{example}
        \begin{definition}
            A simple extension of a field $\ring{F}$ is an extension field
            $\ring{K}$ such that there exists $\alpha\in{K}$ such that
            $K=F(\alpha)$.
        \end{definition}
        \begin{definition}
            The composite of two subfields $\ring{F_{1}}$ and $\ring{F_{2}}$
            of a field $\ring{F}$ is the field generated by $F_{1}\cup{F}_{2}$.
        \end{definition}
        \begin{theorem}
            If $\ring{F}$ is a field, if $\ring{F_{1}}$ and $\ring{F_{2}}$ are
            subfields, and if $K$ is the composite of $F_{1}$ and $F_{2}$,
            then $K$ is the field generated by $F_{1}$ over $F_{2}$, and $K$ is
            the field generated by $F_{2}$ over $F_{1}$.
        \end{theorem}
        Given a field $\ring{F}$ and an extension field $\ring{K}$, for any
        element $\alpha\in{E}$ we can define the homomorphism
        $\phi:F[x]\rightarrow{E}$ be $\phi(f)=f(\alpha)$. If the kernel of this
        is zero, then we say $\alpha$ is transcendental. It then turns out that
        $\phi:F[x]\rightarrow{F}[\alpha]$ is an isomorphism and this extends
        to an isomorphism $\tilde{\phi}:F(x)\rightarrow{F}(\alpha)$. In the
        latter case, where the kernel is non-zero, we call $\alpha$ algebraic.
        \begin{definition}
            An algebraic element of a field extension $\ring{K}$ over a field
            $\ring{F}$ is an element $\alpha\in{K}$ such that there exists a
            polynomial $f\in{F}[x]$ such that $f(\alpha)=0$.
        \end{definition}
        \begin{definition}
            A minimal polynomial of an algebraic element $\alpha$ of a field
            extension $\ring{K}$ over a field $\ring{F}$ is a monic irreducible
            polynomial such that $f(\alpha)=0$.
        \end{definition}
        Minimal polynomials are unique. We can define
        $\phi:F[x]/(f)\rightarrow{F}[\alpha]$ by
        $\phi(\overline{f})=f(\alpha)$. Since $F[x]/(f)$ is a field, $F[\alpha]$
        is as well. Therefore $F[\alpha]=F(\alpha)$, and $F[\alpha]$ is a stem
        field.
        \begin{definition}
            An algebraic extension field of a field $\ring{F}$ is an extension
            field $\ring{K}$ such that for all $\alpha\in{K}$ it is true that
            $\alpha$ is algebraic in $F$.
        \end{definition}
        \begin{example}
            The set of algebraic numbers in $\mathbb{R}$ are an algebraic
            extension over $\mathbb{Q}$. The complex numbers are an algebraic
            extension over $\mathbb{R}$. The real numbers are not an algebraic
            extension over $\mathbb{Q}$ however, since $\pi$ is transcendental.
            Indeed, the set of all algebraic numbers in $\mathbb{R}$ form a
            countable subset, and so most real numbers are transcendental.
        \end{example}
        \begin{theorem}
            If $\ring{F}$ is a field, and if $\ring{K}$ is an extension field,
            then $K$ is a finite extension if and only if it is algebraic and
            finitely generated.
        \end{theorem}
        \begin{ftheorem}{Liouville's Transcendental Theorem}
                        {Liouvilles_Transcendental_Theorem}
            If $a\in\mathbb{N}$, if $a\geq{2}$, and if $s\in\mathbb{R}$ is
            defined by:
            \begin{equation}
                s=\sum_{k\in\mathbb{N}}\frac{1}{a^{n!}}
            \end{equation}
            then $s$ is transcendental.
        \end{ftheorem}
        \begin{bproof}
            For suppose not. Then $s$ is algebraic and thus there is a
            $d\in\mathbb{N}$ and a minimal polyomial $f\in\mathbb{Q}[x]$ of
            degree $d$ such that $f(s)=0$. But then $\mathbb{Q}[s]$ is a
            $d$ dimensional vector space over $\mathbb{Q}$. Let $N\in\mathbb{N}$
            be such that $N\cdot{f}\in\mathbb{Z}[x]$ and let $S_{n}$ be the
            $n^{th}$ partial sums:
            \begin{equation}
                S_{n}=\sum_{k\in\mathbb{Z}_{n}}\frac{1}{a^{k!}}
            \end{equation}
            Thus, $|S_{n}-s|\rightarrow{0}$. If $s$ is rational, then the
            minimal polynomial has degree one and hence $f(x)=x-s$. Otherwise,
            since $f$ is irreducible and of degree greater than one, $f$ has no
            rational roots. Moreover, for all $n\in\mathbb{N}$, $S_{n}\ne{s}$
            and hence $f(S_{n})\ne{0}$. Moreover, by induction
            $S_{n}\in\mathbb{Q}$ for all $n\in\mathbb{N}$, and
            $(a^{n!})^{d}DS_{n}$ is an integer, and therefore:
            \begin{equation}
                |(a^{n!})^{d}DS_{n}|\geq{1}
            \end{equation}
            From the fundamental theorem of algebra, $f$ splits into it's roots
            and so:
            \begin{equation}
                f(x)=\prod_{k\in\mathbb{Z}_{d}}(x-\alpha_{k})
            \end{equation}
            Where the $\alpha_{k}$ are the complex roots of $f$. Let
            $M_{1}=\maximum{|\alpha_{k}|\;:\;k\in\mathbb{Z}_{d}\setminus\{0\}}$.
            That is, $M$ is the largest modulus of all the roots, neglecting the
            zeroth term. Let $M=\maximum{M_{1},1}$. Then:
            \begin{equation}
                |f(S_{n})|=\prod_{k\in\mathbb{Z}_{d}}|S_{n}-\alpha_{k}|
                    \leq|S_{n}-\alpha_{0}|(S_{n}+M)^{d-1}
            \end{equation}
            But we can simplify this further, since:
            \begin{equation}
                |S_{n}-\alpha_{1}|=\sum_{k=n+1}^{\infty}\frac{1}{a^{k!}}
                \leq\frac{1}{a^{(k+1)}!}\sum_{k\in\mathbb{N}}\frac{1}{a^{k}}
                =\frac{a}{a-1}\frac{1}{a^{(n+1)!}}
            \end{equation}
            Piecing this together, we obtain:
            \begin{equation}
                |f(S_{n})|\leq\frac{2}{2^{(n+1)!}}(S_{n}+M)^{d-1}
            \end{equation}
            And this convergences to zero. Therefore:
            \begin{equation}
                |(a^{n!})^{d}DS_{n}|\leq\frac{a}{a-1}
                    \frac{a^{d\cdot{n!}}}{a^{(n+1)!}}(S_{n}+M)^{d-1}
            \end{equation}
            Which thus convergences to zero, contradicting that this is always
            greater than one.
        \end{bproof}
        \begin{definition}
            A splitting polynomial in a field $\ring{F}$ is a polynomial
            $f\in{F}[x]$ of degree $n\in\mathbb{N}$ such that there exists
            finite sequences $a,b:\mathbb{Z}_{n}\rightarrow{F}$ such that:
            \begin{equation}
                f(x)=\prod_{k\in\mathbb{Z}_{n}}(a_{k}x-b_{k})
            \end{equation}
        \end{definition}
        \begin{theorem}
            If $\ring{F}$ is a field, then every non-constant polynomial
            $f\in{F}[x]$ is a splitting polynomial if and only if every
            non-constant polynomial has at least one root in $F$.
        \end{theorem}
        \begin{proof}
            If every non-constant polynomial $f\in{F}[x]$ splits, then given
            such an $f$ of degree $n\in\mathbb{N}$ there are sequences
            $a,b:\mathbb{Z}_{n}\rightarrow{F}$ such that:
            \begin{equation}
                f(x)=\prod_{k\in\mathbb{Z}_{n}}(a_{k}x-b_{k})
            \end{equation}
            But then $b^{k}/a_{k}$ is a root for all $k$, and hence there is at
            least one root. Going the other way, let $f\in{F}[x]$ be a
            non-constant polynomial. Then there is a root $\alpha$ and hence
            $f(x)=(x-\alpha)g(x)$. Then either $g$ is a constant or a
            non-constant polynomial. In the latter case it has a root $\beta$
            and so $f(x)=(x-\alpha)g(x)=(x-\alpha)(x-\beta)h(x)$. Continuing
            inductively we find that $f$ is a splitting polynomial.
        \end{proof}
        \begin{theorem}
            If $\ring{F}$ is a field, then every non-constant polynomial
            $f\in{F}[x]$ has at least one root in $F$ if and only if every
            irreducible polynomial in $F[x]$ has degree 1.
        \end{theorem}
        \begin{proof}
            For if $f\in{F}[x]$ is a non-constant polynomial, then either
            the degree of $f$ is 1 or it is greater than 1. But if the degree
            of $f$ s 1, then $f(x)=ax+b$ and thus $f$ has a root. If the degree
            of $f$ is greater than 1, then it is reducible and hence there
            are polynomials $g,h\in{F}[x]$ of positive degree such that
            $f=gh$. Continuing inductively we eventually factor $f$ down to the
            product of degree 1 polynomials, and thus $f$ has a root. Going the
            other way, if $f\in{F}[x]$ is non-constant and irreducible then it
            has a root, and thus $f(x)=ax+b$. Thus, the only irreducible
            polynomials have degree 1.
        \end{proof}
        \begin{theorem}
            If $\ring{F}$ is a field, then every irreducible polynomial
            $f\in{F}[x]$ has degree 1 if and only if for every finite field
            extension $\ring{K}$ over $F$, $K=F$.
        \end{theorem}
        \begin{proof}
            For if $\ring{K}$ is a finite field extension over $F$, then the
            minimal polynomial $f\in{F}[x]$ for any element $\alpha\in{K}$ has
            degree 1 and hence $f(x)=x-\alpha$. But then $\alpha\in{F}$, and
            thus $F=K$. In the reverse direction, if $f\in{F}[x]$ is an
            irreducible polynomial, then $F[x]/(f)$ is a finite extension field
            of $F$. But by hypothesis, $F[x]/(f)=F$, and since the degree of
            $f$ is equal to $[F[x]/(f):F]=1$, $f$ is a degree 1 polynomial.
        \end{proof}
        \begin{definition}
            An algebraically closed field is a field $\ring{F}$ such that for
            all non-constant polynomials $f\in{F}[x]$, there exists a root
            $\alpha\in{F}$ of $f$.
        \end{definition}
        \begin{definition}
            An algebraic closure of a field $\ring{F}$ is an extension field
            $\ring{K}$ of $F$ such that $K$ is algebraically closed.
        \end{definition}
        \begin{example}
            By the fundamental theorem of algebra, $\mathbb{C}$ is algebraically
            closed. It is therefore an algebraic closure of $\mathbb{R}$. To
            note that $\mathbb{R}$ is not algebraically closed one need only
            consider the polynomial $f(x)=x^{2}+1$.
        \end{example}
        \begin{theorem}
            If $\ring{F}$ is a field, if $\ring{K}$ is an algebraic extension
            field of $F$, and if for every polynomial $f\in{F}[x]$ it is true
            that $f$ is a splitting polynomial in $K[x]$, then $\ring{K}$ is
            algebraically closued.
        \end{theorem}
        \begin{proof}
            For if $f\in{K}[x]$ is a non-constant polynomial then there is a
            finite extension field $\ring{L}$ of $K$ such that $f\in{L}[x]$ has
            a root $\alpha\in{L}$. But then by the tower law, $L$ is a finite
            extension field over $F$. Thus $\alpha$ is algebraic over $F$ and
            so there is a polynomial $g\in{F}[x]$ such that $g(\alpha)=0$. But
            by hypothesis, $g$ splits in $K$ and so the roots of $g$ lie in
            $K$. Thus, $\alpha\in{K}$. Hence, $K$ is algebraically closed.
        \end{proof}
        \begin{theorem}
            If $\ring{K}$ is an extension field of a field $\ring{F}$ and if
            $\mathbb{A}_{F}$ is the set:
            \begin{equation}
                \mathbb{A}_{F}=\{\,\alpha\in\Omega\;|\;
                    \alpha\textrm{ is algebraic over }F\}
            \end{equation}
            Then $\ring{\mathbb{A}_{F}}$ is a field.
        \end{theorem}
        \begin{proof}
            For if $\alpha,\beta\in{K}$ are algebraic over $F$, then
            $F[\alpha,\beta]$ is a field of finite degree over $F$. Thus every
            element of $F[\alpha,\beta]$ is algebraic over $F$, and hence
            $\alpha+\beta$, $\alpha-\beta$, $\alpha\cdot\beta$, and
            $\alpha/\beta$ are all algebraic. Therefore, $\mathbb{A}_{F}$ is a
            field.
        \end{proof}
        \begin{definition}
            The algebraic closure of a field $\ring{F}$ with respect to an
            extension field $\ring{K}$ is the subfield $\ring{\mathbb{A}_{F}}$
            defined by:
            \begin{equation}
                \mathbb{A}_{F}=\{\,\alpha\in\Omega\;|\;
                \alpha\textrm{ is algebraic over }F\}
            \end{equation}
        \end{definition}
        By the previous theorem, the algebraic of a field $\ring{F}$ with
        respect to any field extension $\ring{K}$ is a subfield, and hence this
        is well defined.
        \begin{theorem}
            If $\ring{F}$ is a field, if $\ring{K}$ is an algebraically closed
            field extension of $F$, and if $\mathbb{A}_{F}$ is the algebraic
            closure of $F$ with respect to $K$, then $\mathbb{A}_{F}$ is an
            algebraic closure of $F$.
        \end{theorem}
        \begin{proof}
            Since $\mathbb{A}_{F}$ is algebraic over $F$, and since every
            polynomial $f\in{F}[x]$ splits in $\mathbb{A}_{F}[x]$, we thus have
            that, since $K$ is algebraically closed, the $f$ has a root in
            $\mathbb{A}_{F}$. Hence, $\mathbb{A}_{F}$ is an algebraic closure
            of $F$.
        \end{proof}
        Combining this with the fundamental theorem of algebra we see that
        every subfield of $\mathbb{C}$ has an algebraic closure.
        \begin{example}
            Let $\mathbb{Q}$ denote the standard field of rational numbers, and
            let $\mathbb{Q}[\sqrt{2},\sqrt{3}]$ be the field extension obtained
            by appending $\sqrt{2}$ and $\sqrt{3}$. The degree of
            $[\mathbb{Q}[\sqrt{2},\sqrt{3}]:\mathbb{Q}]$ is four, and to prove
            this we use the tower law. Firstly, note that
            $[\mathbb{Q}[\sqrt{2}]:\mathbb{Q}]=2$ for $\{1,\sqrt{2}\}$ form a
            linear independent basis of $\mathbb{Q}[\sqrt{2}]$ over
            $\mathbb{Q}$. Next,
            $[\mathbb{Q}[\sqrt{2},\sqrt{3}]:\mathbb{Q}[\sqrt{2}]]=2$. For
            $\{1,\sqrt{3}\}$ certainly forms a basis, but moreover it is
            linearly independent. For suppose not and suppose we have:
            \begin{equation}
                a+b\sqrt{3}=0\quad\quad
                a,b\in\mathbb{Q}[\sqrt{2}]
            \end{equation}
            Then we have that $\sqrt{3}=\minus{a}/b$ with
            $a,b\in\mathbb{Q}[\sqrt{2}]$. With this we may write:
            \begin{equation}
                \sqrt{3}=\minus\frac{a_{1}+a_{2}\sqrt{2}}{b_{1}+b_{2}\sqrt{2}}
            \end{equation}
            Squaring both sides yields:
            \begin{equation}
                a_{1}^{2}+2a_{1}a_{2}\sqrt{2}+2a_{2}^{2}=
                3b_{1}^{2}+6b_{1}b_{2}\sqrt{2}+6b_{2}^{2})
            \end{equation}
            If we collect the $\sqrt{2}$ terms, we obtain:
            \begin{equation}
                \sqrt{2}(2a_{1}a_{2}-6b_{1}b_{2})=
                3b_{1}^{2}-a_{1}^{2}+6b_{2}^{2}-2a_{2}^{2}
            \end{equation}
            Now if $2a_{1}a_{2}-6b_{1}b_{2}\ne{0}$, then we may divide through
            by this showing that $\sqrt{2}$ is a rational number, which is a
            contradiction. Thus, $2a_{1}a_{2}-6b_{1}b_{2}=0$ and hence
            $a_{1}a_{2}=3b_{1}b_{2}$. But then:
            \begin{align}
                a_{1}+a_{2}\sqrt{2}+b_{1}\sqrt{3}+b_{2}\sqrt{6}&=0\\
                \Rightarrow
                a_{1}^{2}+3b_{1}b_{2}\sqrt{2}
                    +a_{1}b_{1}\sqrt{3}+a_{1}b_{2}\sqrt{6}&=0\\
                \Rightarrow
            \end{align}
        \end{example}
        \begin{definition}
            An $F$ homomorphism from an extension field $\ring{E}$ over a field
            $\ring{F}$ to an extension field $\ring{E'}$  over $F$ is a field
            homomorphism $\phi:E\rightarrow{E}'$ such that
            $\phi|_{F}=\identity{F}$.
        \end{definition}
        \begin{theorem}
            If $\ring{F}$ is a field, if $\ring{E}$ is a simply field extension
            of $F$, if $\ring{K}$ is a field extension of $F$, and if
            $\varphi:E\rightarrow{K}$ is an $F$ homomorphism, if $\alpha\in{E}$
            is such that $E=F(\alpha)$, if $\alpha$ is transcendental over $F$,
            then $\varphi(\alpha)$ is transcendental over $F$ and the function
            $f:\Homomorphisms{E}{K}\rightarrow{E}\setminus\mathbb{A}_{F}$
            defined by $f(\varphi)=\varphi(\alpha)$ is a bijection.
        \end{theorem}
        \begin{example}
            Take the polynomial ring $\mathbb{C}[x]$. The prime ideals on this
            space are $(z-a)$ for some $a\in\mathbb{C}$. The topology is then
            $\{(x-a)\;|\;a\in\mathbb{C}\}\cup\{(0)\}$. The weirdness about this
            is that $(0)$ is dense in this space.
        \end{example}
        \begin{theorem}
            If $\ring[F]{F}$ is a field, if $\ring[K]{K}$ is a field extension,
            and if $\alpha\in{K}$, then $F[\alpha]$ is a field if and only if
            $\alpha$ is algebraic. 
        \end{theorem}
        \begin{theorem}
        \end{theorem}
        \begin{fdefinition}{Irreducible Elements}{Irreducible_Element}
            An irreducible element of a ring $\ring{R}$ is an element
            $x\in{R}$ such that for all $a,b\in{R}$ such that $a\cdot{b}=x$, it
            is true that either $a$ is a unit or $b$ is a unit.
        \end{fdefinition}
        \begin{fdefinition}{Prime Element}{Prime_Element}
            A prime element of a ring $\ring{R}$ is an element $p\in{R}$
            such that for all $a,b\in{R}$ such that $p$ divides $a\cdot{b}$,
            then either $p$ divides $a$ or $p$ divides $b$.
        \end{fdefinition}
        \begin{fdefinition}{Unique Factorization Domain}
                           {Unique_Factorization_Domain}
            An integral domain is a ring $\ring{R}$ such that for all $r\in{R}$
            there exists finitely many irreducible elements $a_{i}$ such that
            $\prod{a}_{i}=r$ and such that for any other sequence of irreducible
            elements $b_{k}$ such that $\prod{b}_{k}=r$, there exists units
            $u_{i}$ such that $a_{i}=u_{i}\cdot{b}_{i}$.
        \end{fdefinition}
        \begin{example}
            The fundamental theorem of arithmetic states the $\mathbb{Z}$ is a
            UFD.
        \end{example}
        \begin{theorem}
            If $F$ is a field, then $F[x]$ is a UFD.
        \end{theorem}
        \begin{proof}
            Any principal ideal domain is a unique factorization domain.
        \end{proof}
        \begin{example}
            There are non-unique factorization domains. For example,
            $\mathbb{Z}[\sqrt{\minus{5}}]$ since
            $6=2\cdot{3}=(1+\sqrt{\minus{5}})(1-\sqrt{\minus{5}})$.
        \end{example}
        The converse of the previous example is not true.
        \begin{theorem}
            $\mathbb{Z}[x]$ is a UFD.
        \end{theorem}
        However, $\mathbb{Z}[x]$ is not a PID since $(2,x)$ is not principal.
        \subsection{Roots and Irreducibility}
            \begin{definition}
                An element $\alpha\in{F}$ is a root of a polynomial $f(x)$ if
                $f(x)=0$.
            \end{definition}
            \begin{theorem}
                $\alpha\in{F}$ is a root of $f\in{F}[x]$ if and only if
                $(x-\alpha)$ divides $f$.
            \end{theorem}
            \begin{proof}
                One was is clear, if $(x-\alpha)|f$, then $f(x)=(x-\alpha)r(x)$
                for some $r\in{F}[x]$. But then $f(\alpha)=0\cdot{r}(\alpha)=0$,
                and thus $\alpha$ is a root. In the other direction, use the
                division algorithm. Write $f(x)=(x-\alpha)q(x)+r(x)$. But $r(x)$
                must have degree less than $x-\alpha$, and is therefor a
                constant. But then $f(\alpha)=r=0$, so $r=0$. Hence,
                $f(x)=(x-\alpha)q(x)$, so $x-\alpha$ divides $f$.
            \end{proof}
            \begin{theorem}
                If $f\in{F}[x]$ is a polynomial of degree $n$, then there are at
                most $n$ roots.
            \end{theorem}
            \begin{proof}
                By induction. If there are no roots, we are done. If not, write
                $f(x)=(x-\alpha)q(x)$. Then $q$ is a polynomial of degree $n-1$,
                and by the induction hypothesis has at most $n-1$ roots. Thus,
                there are at most $n$ roots.
            \end{proof}
            \begin{example}
                $x^{2}-1\in\mathbb{Z}_{8}[x]$ has 4 roots. That is, 1, 3, 5, 7
                are all roots. This does not contradict the previous theorem
                since $\mathbb{Z}_{8}$ is not a field.
            \end{example}
            \begin{ftheorem}{Fundamental Theorem of Algebra}
                            {Fundamental_Theorem_of_Algebra}
                If $f:\mathbb{C}\rightarrow\mathbb{C}$ is a non-constant
                polynomial, then there exists an $\alpha\in\mathbb{C}$ such that
                $f(\alpha)=0$.
            \end{ftheorem}
            \begin{theorem}
                Any linear polynomial $ax+b\in{F}[x]$ is irreducible.
            \end{theorem}
            \begin{theorem}
                If $f\in{F}[x]$ is irreducible and $\textrm{deg}(g)\geq{2}$,
                then $f$ has no roots. Moreover, the converse holds if
                $\textrm{deg}(f)=2$ or 3.
            \end{theorem}
            \begin{example}
                The previous theorem is very special for degree 2 and 3. Let
                $f(x)=x^{2}+x+1$ where $f\in\mathbb{F}_{p}[x]$, $p$ a prime.
                For the case of $p=2$ have seen that this is a irreducible. For
                $p=3$ we have that 1 is a root: $1^{2}+1+1=3\cong{0}$ in
                $\mathbb{F}_{3}$. For $p=5$, this is once again irreducible,
                but not in $\mathbb{F}_{7}$.
            \end{example}
            \begin{example}
                Consider $x^{4}+x^{2}+1\in\mathbb{F}_{2}[x]$. This has no roots,
                since 0 and 1 both map to 1, but it is reducible since
                $x^{4}+x^{2}+1=(x^{2}+x+1)^{2}$ in $\mathbb{F}_{2}[x]$. That is,
                the so called \textit{freshman's dream} is true in this case, and
                we can distribute the power. Thus we have a reducible polynomial
                with no roots. Note, however, that the degree is not 2 or 3.
            \end{example}
            Next, we present Gauss's Lemma. Note that being irreducible over
            $\mathbb{Z}$ is stronger than being irreducible over $\mathbb{Z}$.
            Consider $f(x)=2x$. While this is linear in $\mathbb{Q}$, and since
            $\mathbb{Q}$ is a field, $f$ is irreducible in $\mathbb{Q}[x]$. However
            when viewed in $\mathbb{Z}[x]$, we have $2x=2\cdot{x}$, neither of
            which are units, and hence $f$ is reducible in $\mathbb{Z}[x]$.
            \begin{theorem}
                If $f\in\mathbb{Z}[x]$ is irreducible, then it is irreducible in
                $\mathbb{Q}[x]$.
            \end{theorem}
        \subsection{Reduction Modulo a Prime}
            Consider the projection map
            $\pi:\mathbb{Z}[x]\rightarrow\mathbf{F}_{p}[x]$, reduction mod $p$
            of the coefficients:
            \begin{equation}
                f(x)=\sum{a}_{k}x^{k}\longrightarrow
                    \overline{f}(x)\sum\overline{a}_{k}x^{k}
            \end{equation}
            The mapping $\pi$ is a ring homomorphism.
            \begin{example}
                $x^{3}+21x+31\mapsto{x}^{3}+x+1$ in $\mathbb{F}_{2}[x]$.
            \end{example}
            \begin{theorem}
                If $f\in\mathbb{Z}[x]$, if $p$ is prime, and if $p\not|a_{n}$,
                and if $\pi(f)\in\mathbb{F}_{p}[x]$ is irreducible, then
                $f\in\mathbb{Z}[x]$ is irreducible.
            \end{theorem}
            \begin{proof}
                We prove by the contrapositive. If $f(x)=g(x)h(x)$, then
                $\overline{f}(x)=\overline{g}(x)\overline{h}(x)$ since $\pi$ is
                a homomorphism. But $\overline{g}$ and $\overline{h}$ are not
                units since $p$ does not $a_{n}$, and thus the degree of the
                reduction is equal to the degree of the original polynomial.
                That is, $\textrm{deg}(f)=\textrm{deg}(\overline{f})$. But also
                $\textrm{deg}(g)=\textrm{deg}(\overline{g})$ and similarly for
                $h$ since:
                \begin{equation}
                    \textrm{deg}(\overline{g})+\textrm{deg}(\overline{h})
                    =\textrm{deg}(g)+\textrm{deg}(h)=\textrm{deg}(f)
                \end{equation}
                Since $p$ does not divide the leading coefficients of either
                $g$ or $h$, the degrees of $\overline{g}$ and $\overline{h}$
                remain the same, and hence are not units. Thus, $\overline{f}$
                is reducible.
            \end{proof}
            \begin{example}
                Consider $x^{3}+21x+31\in\mathbb{Z}[x]$. In $\mathbb{F}_{3}[x]$
                we have $x^{3}+x+1$, which is indeed irreducible, and hence
                the original polynomial $x^{3}+21x+31$ is irreducible.
            \end{example}
            \begin{example}
                The converse is not true. There are polynomials that are
                reducible for all $p$, yet in $\mathbb{Z}[x]$ it is irreducible.
                Consider $(2x+1)(x+1)=2x^{2}+3x+1$. This is reducible, since
                we have the factorization, however in $\mathbb{F}_{2}[x]$ this
                is simply $x+1$, which is irreducible. However, 2 divides 2 so
                the theorem does not apply.
            \end{example}
            \begin{example}
                In $\mathbb{Z}[x]$, consider $x^{2}+x+1$ which we know is
                irreducible since it is irreducible in $\mathbb{R}[x]$
                (the roots are complex). In $\mathbb{F}_{2}[x]$ we can check and
                see that there are no roots, and thus $x^{2}+x+1$ is irreducible
                by Gauss' lemma. However, in $\mathbb{F}_{3}[x]$ it is reducible
                since $x^{2}+x+1=(x-1)^{2}$. That is, 1 is a root of $x^{2}+x+1$
                in $\mathbb{F}_{2}[x]$ with multiplicity 2.
            \end{example}
            \begin{ftheorem}{Eisenstein's Criterion}{Eisensteins_Criterion}
                If $p$ is prime, if $f\in\mathbb{Z}[x]$, if $p$ does not divide
                $a-{n}$, if $p$ divides $a_{k}$ for all $k<n$, and if
                $p^{2}$ does not divide $a_{0}$, then $f$ is irreducible.
            \end{ftheorem}
            \begin{bproof}
                We prove by the contradiction. Suppose $f$ is reducible.
                If $p$ divides all of the $a_{i}$, then in the reduction map
                $\pi:\mathbb{Z}[x]\rightarrow\mathbb{F}_{p}[x]$, all of the
                $a_{i}$ map to zero. So we have:
                \begin{equation}
                    \overline{a}_{n}x^{n}=\overline{f}(x)
                    =\overline{g}(x)\overline{h}(x)
                \end{equation}
                But since $\mathbb{Z}[x]$ is a UFD, both $\overline{g}$ and
                $\overline{h}$ are monomials, $\overline{g}(x)=g_{0}x^{i}$ and
                $\overline{h}(x)=h_{0}x^{j}$. If $i,j<n$, then $i,j>0$, and so
                both constant terms must be divisible by $p$. Thus
                $g(x)h(x)$ has a constant term divisible by $p^{2}$, a
                contradiction.
            \end{bproof}
            \begin{example}
                Let $f(x)=x^{4}+22x^{2}+33x+44$. By Eisenstein, with $p=11$, we
                have that this is irreducible.
            \end{example}
        \subsection{Review of Previous Lecture}
            If $\mathbf{F}$ is a field, then $\zeta\in\mathbf{F}$ is called a
            root of unity of there is some $n\geq{1}$ such that $\zeta^{n}=1$.
            \begin{example}
                In the field $\mathbb{R}$, the roots of unity of 1 and
                $\minus{1}$. The number 1 is a first root of unity, whereas
                $\minus{1}$ is a second root of unity.
            \end{example}
            \begin{example}
                In the complex numbers $\mathbb{C}$, there is an $n^{th}$ root
                of unity for all $n\in\mathbb{N}^{+}$. Let
                $\zeta=\exp(2\pi{i}/n)$. The roots of unity are scattered along
                the unit circle.
            \end{example}
            A root of unity is some element $\zeta\in\mathbb{F}$ such that
            $\zeta$ is a root of the polynomial $f(x)=x^{n}-1$.
            \begin{example}
                Let $p$ be a prime integer, and consider the roots of
                $f(x)=x^{p}-1$. There is always a root since 1 satisfies this
                criterion, and hence we can factor this and obtain:
                \begin{equation}
                    f(x)=x^{p}-1=(x-1)(x^{p-1}+x^{p-2}+\dots+x+1)
                \end{equation}
                This latter polynomial $x^{p-1}+x^{p-1}+\dots+x+1$ is
                irreducible over $\mathbb{Q}$.
            \end{example}
            \begin{theorem}
                If $p$ is prime and $f(x)=x^{p-1}+x^{p-2}+\dots+x+1$,
                $f\in\mathbb{R}[x]$, then $f$ is irreducible over $\mathbb{Q}$.
            \end{theorem}
            \begin{proof}
                For:
                \begin{equation}
                    xf(x+1)=(x+1)^{p}-1
                \end{equation}
                by the binomial theorem we have:
                \begin{equation}
                    xf(x+1)=(x+1)^{p}-1=
                    \minus{1}+\sum_{k=0}^{p}\binom{p}{k}x^{k}
                    =\sum_{k=1}^{p}\binom{p}{k}x^{k}
                \end{equation}
                Thus, simplifying, we have:
                \begin{equation}
                    f(x+1)=\sum_{k=1}^{p}\binom{p}{k}x^{k-1}
                \end{equation}
                Thus by the Eisenstein criterion, $f(x+1)$ is irreducible over
                $\mathbb{Q}$. But if $f(x+1)$ is irreducible over $\mathbb{Q}$,
                then $f(x)$ is as well.
            \end{proof}
            The converse of the statement before is not true. If the translation
            of a polynomial is reducible, it need not mean the original
            polynomial was reducible. For let $f(x)\in\mathbb{F}_{2}[x]$ be
            defined by $f(x)=x^{2}+x+1$. Plugging in $x^{2}$ we get
            $f(x)^{2}=x^{4}+x^{2}+1=(x^{2}+x+1)^{2}$, which is reducible.
    \section{Gauss's Lemma}
        \begin{definition}
            A polynomial $f\in\mathbb{Z}[x]$ is called primitive if
            $\textrm{GCD}(a_{0},\dots,a_{n})=1$. Equivalently, for all primes
            $p$ there is an $i$ such that $p\not|a_{i}$. That is, $p$ does not
            divide $a_{i}$.
        \end{definition}
        \begin{example}
            Let $f(x)=10x^{3}+5x^{2}+2x+23$. This is a primitive polynomial.
            If $p$ divides all of the $a_{i}$, it must divide 2, but 2 is the
            only prime that divides 2, and hence $p=2$. But 5 and 23 are odd,
            and hence 2 does not divide them. So, $f$ is primitive.
        \end{example}
        \begin{ltheorem}{Gauss's Lemma}{Gauss_Lemma}
            FOr any $f\in\mathbb{Q}[x]$ there is a unique $c\in\mathbb{Q}$ and
            a unique primitive polynomial $g\in\mathbb{Z}[x]$ such that
            $f(x)=c\cdot{g}(x)$.
        \end{ltheorem}
        \begin{proof}
            Existence is straight forward. Clear out the denominators of $f$,
            let $c$ be the common factor, and we're done. Suppose
            $cg(x)=c'g'(x)$. If $c$ and $c'$ are not integers, multiply by their
            denominators and thus we may assume $c$ and $c'$ are integers. Then
            we have:
            \begin{equation}
                cg(x)=c'(b_{0}+b_{1}x+\dots+b_{n}x^{n})
            \end{equation}
            And hence:
            \begin{equation}
                c=\textrm{GCD}(c'b_{0},\dots,c'b_{n})
                 =c\textrm{GCD}((b_{0},\dots,b_{n}))
            \end{equation}
            But $g'$ is trivial, so the greatest common denominator is 1. Hence,
            $c=c'$. Also, $g=g'$.
        \end{proof}
        \begin{ltheorem}{Gauss' Lemma Version 2}{Gauss_Lemma_2}
            If $g\in\mathbb{Z}[x]$ is primitive and $f\in\mathbb{Z}[x]$, and if
            $g|f$ in $\mathbb{Q}[x]$, then $g|f$ in $\mathbb{Z}[x]$.
        \end{ltheorem}
        \begin{proof}
            For if $g|f$, then $f=gh$ with $h\in\mathbb{Q}[x]$. But by Gauss'
            lemma there is a unique $c\in\mathbb{Q}$ such that
            $h(x)=ch_{0}(x)$, where $h_{0}\in\mathbb{Z}[x]$ is primitive. So
            $f(x)=cg(x)h_{0}(x)$. But then $g(x)h_{0}(x)$ is primitive, and
            hence $c$ is an integer. Thus, $h\in\mathbb{Z}[x]$. That is, since
            the product of primitive is primitive, the numerator of $c$ is equal
            to the denominator of $c$ times the GCD of the coefficients of $f$.
            But this GCD is a positive integer, and hence the numerator divides
            it, and hence $c$ is an integer itself.
        \end{proof}
        \begin{ltheorem}{Gauss' Lemma V3}{Gauss_Lemma_3}
            If $f\in\mathbb{Z}[x]$ and if $g,h\in\mathbb{Q}[x]$ are such that
            $f=gh$, then $f=g_{0}h_{0}$.
        \end{ltheorem}
    \section{Field Extensions}
        \begin{ftheorem}{Tower Law}{Tower_Law}
            If $\mathbb{F}$, $\mathbb{K}$, and $\mathbb{L}$ are fields, if
            $\mathbb{K}$ is a field extension of $\mathbb{F}$, if $\mathbb{L}$
            is a field extension over $\mathbb{K}$, then $\mathbb{L}/\mathbb{F}$
            is a finite dimensional vector space if and only if
            $\mathbb{L}/\mathbb{K}$ and $\mathbb{K}/\mathbb{F}$ is finite.
            Moreover:
            \begin{equation*}
                [\mathbb{L}:\mathbb{F}]=
                [\mathbb{L}:\mathbb{K}][\mathbb{K}:\mathbb{F}]
            \end{equation*}
        \end{ftheorem}
        Suppose $K/F$ is a field extension. That is, $F\subseteq{K}$ and $F$ is
        a subfield of $K$. Let $X\subseteq{K}$ be any non-empty subset. We now
        define the adjoinment of $X$ to $F$.
        \begin{definition}
            The field extension generated by a subfield $F$ of a field $K$ by a
            subset $X\subseteq{K}$ is the subfield:
            \begin{equation}
                F(X)=
                \bigcap_{\overset{F\subseteq{L}\subseteq{K}}{X\subseteq{L}}}L
            \end{equation}
            Where $L$ is a subfield.
        \end{definition}
        $F(X)$ is non-empty since $K$ is in the intersection, and the
        intersection of subfields is again a subfield, so $F(X)$ is a field.
        If $X=\{a_{0},a_{1},\dots\}$, we often write $F(X)=F(a_{0},a_{1},\dots)$
        and if $X$ if finite we call $F(X)/F$ finitely generated. If $X$ is a
        single element, $F(\alpha)$ is called a simple extension. A field
        extension is simple if it can be written as a simple extension.
        \begin{example}
            $\mathbb{R}/\mathbb{Q}$ is not finitely generated. A simple
            cardinality argument works here, for if it were countably generated
            then since $\mathbb{Q}$ is countable, $\mathbb{R}$ would again be
            countable, which is false.
        \end{example}
        \begin{example}
            $\mathbb{Q}(\sqrt{2},\sqrt{3},\sqrt{5},\sqrt{7},\dots)$ is not
            finitely generated. We can show this by building a chain of
            subfields. First consider $\mathbb{Q}(\sqrt{2})/\mathbb{Q}$. This
            is a field extension of degree two. One can see this since
            $\sqrt{2}$ is not rational. The next field extension
            $\mathbb{Q}(\sqrt{2},\sqrt{3})/\mathbb{Q}(\sqrt{2})$ is also an
            extension of degree 2. For suppose not and suppose:
            \begin{equation}
                \sqrt{3}=a+b\sqrt{2}
            \end{equation}
            Squaring, we obtain:
            \begin{equation}
                3=a^{2}+2b^{2}+2ab\sqrt{2}
            \end{equation}
            Now since $\sqrt{2}$ is irrational we must conclude that $ab=0$.
            Thus either $a=0$ or $b=0$. f $b=0$, then $\sqrt{3}$ is an integer,
            which is false. If $a=0$ then $3=2b^{2}$, but $3$ is prime, a
            contradiction. Thus $\sqrt{3}$ is not contained in
            $\mathbb{Q}(\sqrt{3})$.
        \end{example}
        \begin{theorem}
            If $F$ is a field, if $K$ is a field extension, if $\alpha\in{K}$,
            then:
            \begin{equation}
                F(\alpha)=\big\{\frac{f(\alpha)}{g(\alpha)}\;|\;
                    f,g\in{F}[x],g(\alpha)\ne{0}\big\}
            \end{equation}
        \end{theorem}
        \begin{theorem}
            A field extension of a field $F$ is a field extension $K$ of $F$
            is an injective homomorphism $\iota:F\rightarrow{K}$.
        \end{theorem}
        \begin{definition}
            A morphism of field extensions $K/F$ and $K'/F$ is a homomorphism
            $\varphi:K\rightarrow{K}'$ such that the injective homomorphisms
            $\iota$ and $\iota'$ commute with $\varphi$. That is,
            $\iota'=\varphi\circ\iota$.
        \end{definition}
        \begin{example}
            $\mathbb{C}$ and $\mathbb{R}[x]/(x^{2}+1)$ are isomorphic field
            extensions of $\mathbb{R}$.
            $\varphi\mathbb{R}[x]\rightarrow\mathbb{R}$ mapping
            $f\mapsto{f}(i)$ is surjective since $a+ib=a(i)+b(i)^{4}$. Thus this
            is a surjective $\mathbb{R}$ algebra homomorphism. The kernel is
            the ideal generated by $x^{2}+1$. Then by the first isomorphism
            theorem,
            $\tilde{\varphi}:\mathbb{R}[x]/(x^{2}+1)\rightarrow\mathbb{C}$ is an
            isomorphism. Hence, these are isomorphic.
        \end{example}
    \section{Minimal Polynomial}
        If $F\subseteq{K}$ is a subfield, if $\alpha\in{K}$, then there exists
        an $F$ algebra homomorphism $\varphi_{\alpha}:F[x]\rightarrow{K}$.
        Then $\textrm{Ker}(\varphi_{\alpha})\subseteq{F}[x]$. If
        $\varphi_{\alpha}$ is injective, then the kernel is 0, and this is
        true if and only if $f(\alpha)\ne{0}$ for all $f\in{F}[x]$. That is,
        $\alpha$ is \textit{transcendental} over $F$. On the other hand, if
        $\varphi_{\alpha}$ is not injective then
        $\textrm{Ker}(\varphi_{\alpha})=(m_{\alpha}/F(x))$ where
        $m_{\alpha}/F(x)$ is a monic polynomial in $F[x]$, and this is called
        the minimal polynomial of $\alpha$ over $F$. In this case we say that
        $\alpha$ is algebraic over $F$.
        \begin{theorem}
            If $K/F$ is a field extension, $\alpha\in{K}$ is algebraic over $F$,
            if $f\in{F}[x]$ with $f(\alpha)=0$, then
            $m_{\alpha/F}(x)|f$, hence $m_{\alpha}/F(x)$ is the unique monic
            polynomial over $F$ of minimal degree with $\alpha$ as a root.
            Moreover, $m_{\alpha}/F(x)$ is irreducible over $F$ and is the
            unique monic irreducible polynomial over $F$ with $\alpha$ as a
            root.
        \end{theorem}
        \begin{example}
            Let $d\in\mathbb{Z}$ be a non-square. Then the minimal polynomial of
            $d$ is $m_{\sqrt{d}}/\mathbb{Q}(x)=x^{2}-d$.
        \end{example}
        \begin{example}
            Let $\alpha=\sqrt{2}+\sqrt{3}$. What is the minimal polynomial of
            $\alpha$ over $\mathbb{Q}$? Well we first try to find a polynomial
            that has $\alpha$ as a root. Squaring, we have:
            \begin{equation}
                \alpha^{2}=2+2\sqrt{6}+3=5+2\sqrt{6}
            \end{equation}
            Bringing the 5 over and squaring:
            \begin{equation}
                (\alpha^{2}-5)^{2}=24=\alpha^{4}-10\alpha^{2}+25
            \end{equation}
            Thus letting $f(x)=x^{4}-10x+1$, we obtain a polynomial with
            $\alpha$ as a root.
        \end{example}
        \begin{theorem}
            If $\alpha\in{K}$ is algebraic over $F$, then
            $F[x]/(m_{\alpha}/F(x))$ is isomorphic to $F[\alpha]$, and
            $F[\alpha]=F(\alpha)$.
        \end{theorem}
        Since $F(\alpha)\subseteq{F}[\alpha]$ because the first part,
        $F[x]$ is a field since $m_{\alpha}/F(x)$ is irreducible, so
        $F[x]/(m_{\alpha}/F(x))$ is a field. Hence
        $F(\alpha)\subseteq{F}[\alpha]$ since $F(\alpha)$ is minimal field
        containing $\alpha$. GIven $\alpha$ algebraic over $F$,
        $\alpha^{\minus{1}}$ is a polynomial in $\alpha$. There exists an
        algorithm demonstrating this using Bezout's identity.
        \begin{definition}
            A field extension $K/F$ is algebraic if for all $\alpha\in{K}$,
            $\alpha$ is algebraic over $F$.
        \end{definition}
        \begin{theorem}
            $K/F$ finite if and only if $K/F$ is algebraic and finitely
            generated.
        \end{theorem}
    \section{Review of Previous Lectures}
        A field extension $K/F$ is algebraic if every element $\alpha\in{K}$ is
        algebraic over $F$.
        \begin{theorem}
            A field extension $K/F$ is finite if and only if $K/F$ is algebraic
            and finitely generated.
        \end{theorem}
        \begin{proof}
            For if $K/F$ is finite, then $K$ is a finite dimensional vector
            space over $F$. Thus, if $\alpha\in{K}$, then the set
            $\{1,\alpha,\alpha,\dots\}$ is linearly dependent. That is, there
            exists $a_{0},\dots,a_{n}\in{F}$ such that
            $a_{0}+\dots+a-{n}\alpha^{n}=0$. Let $\alpha_{1},\dots,\alpha_{m}$
            be an $F$ basis for $K$. Then $K=F(\alpha_{1},\dots,\alpha_{n})$,
            and so $K$ is finitely generated. The converse is trickier. Since
            $K$ is finitely generated, $K=F(\alpha_{1},\dots,\alpha_{n})$. But
            since $K$ is algebraic over $F$, $\alpha_{i}$ is algebraic in $F$.
            Thus we build a tower:
            \begin{equation}
                F\rightarrow{F}(\alpha_{1})\rightarrow{F}(\alpha_{1},\alpha_{2})
                    \rightarrow\dots\rightarrow{F}(\alpha_{1},\dots,\alpha_{n})
                    =K
            \end{equation}
            By the generalized tower law, $K$ is finite over $F$.
        \end{proof}
        $K/F$ is transcendental (that is, not algebraic) implies that $K/F$ is
        of infinite degree. Given $\alpha\in{K}$ transcendental over $F$, then
        $\varphi_{\alpha}:F[x]\rightarrow{F}[\alpha]\subseteq{F}(\alpha)$.
        \begin{theorem}
            If $K/L$ is algebraic and $L/F$ is algebraic, then $K/F$ is
            algebraic.
        \end{theorem}
        \begin{proof}
            For let $\alpha\in{K}$. We want to show that $\alpha$ is algebraic
            over $F$. This is equivalent to the claim that $F(\alpha)$ is finite
            over $F$. But $\alpha$ is algebraic over $L$, and hence there is a
            minimal polynomial
            $m_{\alpha}/L(x)=a_{0}+\dots+a_{n-1}x^{n-1}+x^{n}\in{L}[x]$. Since
            $L/F$ is algebraic, $a_{i}$ is algebraic over $F$.
        \end{proof}
        The converse is true as well.
    \section{Compass and Straight Edge}
        Consider some subset $S\subseteq\mathbb{C}$, equipped with some rules:
        \begin{itemize}
            \item Given two points $P,Q$ there is a line through $P$ and $Q$.
            \item Given $P,Q$, there is a circle $C(P,|Q-P|)$ centering at $P$
                  with radius $|P-Q|$.
        \end{itemize}
        Any point that is the intersection of any of the lines and circles is
        said to be constructible by compass and straightedge.
        \begin{example}
            Bisecting a line is possible. Bisecting an angle is possible. Can
            you trisect an angle? What about double a cube?
        \end{example}
        We'll define inductively some sets $P_{n}$, $L_{n}$, and $C_{n}$.
        $P_{0}=\{0,1\}\subseteq\mathbb{C}$, $L_{0},C_{0}=\emptyset$. If
        $L_{n}$ has been constructed, let $L_{n+1}$ be the set of all lines
        through all points in $P_{n}$. If $C_{n}$ has been constructed, let
        $C_{n+1}$ be the set of all circles about all points in $P_{n}$ with
        radii all of the distances $|z_{1}-z_{2}|$ for points
        $z_{1},z_{2}\in{P}_{n}$. Then $P_{n}$ is finite for all $n\in\mathbb{N}$
        and thus the union over all $P_{n}$ is at most countable. This
        cardinality argument shows that there are inconstructible numbers.
        Moreover, $P=\bigcup{P}_{n}$ is a subfield of $\mathbb{C}$. To see this
        we must show that $P$ is closed to addition, multiplication, and
        inverses.
        \begin{theorem}
            The set of constructible numbers is a subfield of $\mathbb{C}$.
        \end{theorem}
        \begin{proof}
            It suffices to show that $P\cap\mathbb{R}$ is a subfield. For let
            $a,b\in{P}\cap\mathbb{R}$. We need to show that $a+b$, $a\cdot{b}$,
            and $a/b$ are constructible. Given the length $a$ and the length
            $b$, we can translate the length $b$ to start at $a$, given us the
            point $a+b$, which will have length $a+b$. For $ab$ we construct
            two triangles, one with length 1 and the other with length $a$.
            We build a triangle on the first with a length $b$, and a similar
            triangle on the second length which will have length $ab$ by
            similiarity. Lastly, do the same triangle with $b$ on the inside to
            get $a/b$. Draw some pictures later.
        \end{proof}
        \begin{theorem}
            $\mathbb{Q}\subseteq{P}$.
        \end{theorem}
        \begin{proof}
            Since $1\in{P}$, every integer multiple of 1 is also contained in
            $P$ since we can add 1 to itself $n$ times. Thus $n/m$ is contained
            in $P$, so $\mathbb{Q}\subseteq{P}$.
        \end{proof}
        \begin{theorem}
            $P$ is closed to square roots.
        \end{theorem}
        \begin{proof}
            Draw a circle of diameter $a+1$. Do that fancy circle.
        \end{proof}
        Let $Q^{py}$ be the intersection of all subfields $K\subseteq\mathbb{C}$
        such that for all $z\in{K}$ it is true that $\sqrt{z}\in{K}$. This is
        the smallest subfield of $\mathbb{C}$ in which one can always take
        square roots. It's called the Pythagorean closure of $\mathbb{Q}$. From
        the previous theorem, $Q^{py}\subseteq{P}$ is a subfield.
        \begin{theorem}
            $\mathbb{Q}^{py}=P$.
        \end{theorem}
        \begin{proof}
            For let $z\in{P}$. It suffices to show that
            $P_{n}\subseteq\mathbb{Q}^{n}$ for all $n$, since the
            $\bigcup{P}_{n}=P\subseteq\mathbb{Q}^{py}$. We prove this by
            induction. The base case is true since $P_{0}=\{0,1\}$ and this is a
            subset of $\mathbb{Q}^{py}$. Suppose $P_{n}\subseteq\mathbb{Q}^{py}$
            and recall that $P_{n+1}$ is defined as all $z\in\mathbb{C}$ such
            that $z\in{L}\cap{L}'$, or $z\in{L}\cap{C}$, or $z\in{C}\cap{C}'$,
            where $L$, $L'$, $C$, and $C'$ are lines and circles through points
            in $P_{n}$. Case 1, $z\in{L}\cap{L}'$ Then there are four points
            $z_{1},z_{2},z_{3},z_{4}$ such that:
            \begin{equation}
                z=z_{1}\alpha+(1-\alpha)z_{2}=z_{3}\beta+(1-\beta)z_{4}
            \end{equation}
            We can solve this and note that $P_{n}$ is closed under complex
            conjugation. Thus, in case 1 we have that $z$ is contained in
            $\mathbb{Q}^{py}$. Here we have:
            \begin{equation}
                z=z_{1}\alpha+(1-\alpha)z_{2}=z_{3}+r\exp(i\theta)
            \end{equation}
            This be true as well. The final case is two circles:
            \begin{equation}
                z=z_{1}+r_{1}\exp(i\theta_{1})=z_{2}+r_{2}\exp(i\theta_{2})
            \end{equation}
            Which also be like it is.
        \end{proof}
        \begin{theorem}
            $\mathbb{Q}^{py}$ is algebraic over $\mathbb{Q}$.
        \end{theorem}
        We can build the Pythagorean closure from $\mathbb{Q}$ by considering
        $\sqrt{\mathbb{Q}}$, all numbers of the for $a+b\sqrt{c}$ with
        $a,b,c\in\mathbb{Q}$, and then $\sqrt{\sqrt{\mathbb{Q}}}$, and so on.
        \begin{theorem}
            $z\in\mathbb{Q}^{py}$ if and only if there is a tower of extensions
            $K_{0},\dots,K_{n}$ such that $\mathbb{Q}=K_{0}$ and
            $K_{n}=\mathbb{Q}[z]$ where $K_{j+1}$ has degree 2 over $K_{j}$.
        \end{theorem}
        \begin{theorem}
            If $\alpha\in\mathbb{Q}^{py}$, then
            $[\mathbb{Q}[\alpha]:\mathbb{Q}]=2^{n}$ for some $n\in\mathbb{N}$.
        \end{theorem}
        \begin{proof}
            Apply the tower law to the previous theorem.
        \end{proof}
        From this we can do all of the impossibility proofs of various
        constructions.
        \begin{example}
            It is impossible to double the volume of a cube with lengths 1. The
            minimal polynomial of $\sqrt[3]{2}$ is $x^{3}-2$ by Eisenstein. Thus
            the degree of the field extension
            $[\mathbb{Q}[\sqrt[3]{2}]:\mathbb{Q}]=3$ which is not a power of 2.
            It is also impossible to trisect angles. For let $\theta=2\pi/3$.
            Trisecting this is equivalent to constructed $2\pi/9$. The minimal
            polynomial of this is $x^{3}-\frac{3}{4}x+\frac{1}{8}$, and thus
            the degree of the extension is again 3, which is not a power of 2.
        \end{example}
    \section{Review}
        If $f\in{F}[x]$ is irreducible, then $f$ has no roots. The converse is
        false. If $f\in{F}[x]$ has no roots, it may still be reducible. This
        holds for any field if $f$ has degree 2 or 3.
        \begin{example}
            If $f(x)=x^{4}+1$, $f\in\mathbb{R}[x]$, then $f$ is reducible. For
            we have:
            \begin{equation}
                x^{4}+1=(x^{2}-\sqrt{x}x+1)(x^{2}+\sqrt{2}x+1)
            \end{equation}
            We can now complete this since $x^{2}-\sqrt{2}x+1$ and
            $x^{2}+\sqrt{2}x+1$ have no real roots by the quadratic formula,
            and hence these are irreducible. Thus, $x^{4}+1$ is reducible and
            factors as above.
        \end{example}
        \begin{example}
            There is only one irreducible quadratic polynomial over
            $\mathbb{F}_{2}[x]$ and that is $x^{2}+x+1$. Using this, is the
            polynomial $x^{4}+x^{2}+x+1$ irreducible in $\mathbb{F}_{2}[x]$?
            Since this has no roots, we know that it has no linear factors, and
            thus if it is reducible it must be the product of irreducible
            quadratics. But there is only one irreducible quadratic, so we can
            use this to check. We have:
            \begin{equation}
                (x^{2}+x+1)^{2}=
                x^{4}+2x^{3}+x^{2}+2x+1=x^{4}+x^{2}+1
            \end{equation}
            Since in $\mathbb{F}_{2}$ we have that $2=0$. But this is not equal
            to the original polynomial $x^{4}+x^{3}+x^{2}+x+1$, and hence
            this is indeed irreducible.
        \end{example}
        If $\alpha\in\mathbb{C}$ is constructible, and if
        $K_{i}$ is a tower of field extensions each of degree 2 over the
        previous one, then $[\mathbb{Q}(\alpha):\mathbb{Q}]=2^{n}$.
        \begin{theorem}
            A regular $n$ gon is constructible if and only if the complex number
            $\exp(2\pi{i}/n)$ is constructible.
        \end{theorem}
        If $p$ is a prime number, then the minimal polynomial of
        $\exp(i2\pi{i}/n)$ is just $x^{p-1}+\dots+1$. Thus
        $[\mathbb{Q}(\zeta_{p}):\mathbb{Q}]=p-1$. Thus we need $p-1$ to be a
        power of 2.
        \begin{theorem}
            If $p-1$ is not a power of 2 then we can not construct a regular
            $p$ gone.
        \end{theorem}
        \begin{theorem}
            If $2^{k}+1$ is a prime, then $k=2^{n}$ for some $n$.
        \end{theorem}
        There are only 5 Fermat primes, $p=3,5,17,257,65537$. An unsolved
        problem (as of 2020) is whether or not there are more such primes, or
        are there infinitely many primes? 
    \section{Splitting Fields}
        If $f\in{F}[x]$, we have given a construction of $K/F$ in which $f$ has
        a root $\alpha\in{K}$. That is, $(x-\alpha)$ divides $f$ in$ K[x]$. For
        if $f$ splits completely (factors into a product of linear polynomials
        over $f$), then there is nothing to prove. If not, let $g(x)$ be an
        irreducible factor of $f$ and define $K=F[x]/(g)$, where $(g)$ is the
        ideal generated by $g$ in $F[x]$. Then $g(x)$ has a root $\overline{x}$,
        $K=F(\alpha)$, so $(x-\alpha)$ divides $g$, which divides $f$, so we're
        done.
        \begin{example}
            Let $f(x)=x^{3}-2$, $f\in\mathbb{Q}[x]$. This has three roots,
            $\sqrt[3]{2}$, $\sqrt[3]{2}\omega$, and
            $\sqrt[3]{2}\overline{\omega}$, where $\omega$ is a complex cubed
            root of unity, and $\overline{\omega}$ is its complex conjugate.
            Thus $\mathbb{Q}[x]/(x^{3}-2)$ is isomorphic to
            $\mathbb{Q}(\sqrt[3]{2})$.
        \end{example}
        \begin{definition}
            An extension $K/F$ is a splitting field of $f\in{F}[x]$ if
            $f$ splits completely in $K[x]$ and does not split in any proper
            subfield.
        \end{definition}
        \begin{theorem}
            Splitting fields exist.
        \end{theorem}
        \begin{proof}
            Proof by induction on the degree of $n$. If it is true for $n$,
            write $f=(x-\alpha)g$ for some $\alpha$ in $F[x]/(h)$. Then
            $g$ is of degree $n$ or less, and hence has a splitting field
            $K$. Adjoing $\alpha$ to $K$. Let $E$ be the intersection of all
            subfields of $E$ containing this $K$ adjoing $\alpha$.
        \end{proof}
        \subsection{Review from Previous Lecture}
            If $F$ is a field, and if $f\in{F}[x]$, then an extension field
            $K$ over $F$ is a splitting field if $f\in{K}[x]$ is a splitting
            polynomial. That is, there are linear factors
            $a_{k}x+b_{k}$, with $a_{k},b_{k}\in{K}$, such that:
            \begin{equation}
                f(x)=\prod_{k\in\mathbb{Z}_{n}}(a_{k}x+b_{k})
            \end{equation}
            Splitting fields exists since given $f$ we can find an irreducible
            factor $g$, and thus $F[x]/(g)$ will be a field extension of $F$.
            Letting $\alpha_{1}=\overline{x}$, the equivalence class of $x$,
            this will be a root of $g$ in $F[x]/(g)$, and hence a root of
            $f$ in $G$. The extension field will have degree less than or equal
            to $n$. Continuing on inductively, obtaining $\alpha_{k}$, we find
            that $f\in{F}[\alpha_{1},\dots,\alpha_{n}]$ splits completely and
            that this field extension has at most degree $n!$. If $f$ was
            irreducible to begin with then $f$ is a minimal polynomial up to
            scalar multiplication, and hence the degree of $F[\alpha_{1}]$ over
            $F$ will be $n$. From the tower law, $n$ will divide the degree of
            $F[\alpha_{1},\dots,\alpha_{n}]$.
            \begin{example}
                Let $f(x)=x^{4}+4$. This factors as:
                \begin{equation}
                    x^{4}+4(x^{2}-2x+2)(x^{2}+2x+2)
                \end{equation}
                We can invoke the quadratic formula to find the roots of these
                two over $\mathbb{C}$:
                \twocolumneq{
                    \alpha_{1}=\frac{2\pm\sqrt{\minus{4}}}{2}
                              =1\pm{i}
                }
                {
                    \alpha_{2}=\frac{\minus{2}\pm\sqrt{\minus{4}}}{2}
                              =\minus{1}\pm{i}
                }
                So the splitting field is $K=\mathbb{Q}(\pm{1}\pm{i})$, and this
                is just $\mathbb{Q}(i)$. Thus, $[K:\mathbb{Q}]=2$.
            \end{example}
            \begin{theorem}
                If $F$, $F'$ are fields, if $\varphi:F\rightarrow{F}'$ is
                a field isomorphism, and if $f\in{F}[x]$, if
                $f'(x)(\varphi\circ{f})(x)\in{F}'[x]$, if $K$ is a splitting
                field for $f$, and if $K'$ is a splitting field for
                $f'$, then there is an isomorphism $\sigma:K\rightarrow{K}'$
                such that for all $c\in{F}$, $\sigma(c)=\varphi(c)$.
            \end{theorem}
            \begin{proof}
                We prove by induction on the degree of $f$. Since $\varphi$ is
                an isomorphism, $f'$ and $f$ have the same degree. Thus if
                $f\in{F}[x]$ and $f'\in{F}'[x]$ are degree one polynomials, then
                they already split and hence if we let $\sigma=\varphi$, then
                we have shown that the splitting fields are isomorphic. Assume
                the claim is true for $n\in\mathbb{N}$. We can assume that $f$
                has an irreducible factor $g$ of degree greater than 1. Then
                $g$ has a root in $K$, label it $\alpha$, and let
                $g'=\varphi\circ{g}$. Then $g'|f'$ and $g'$ is irreducible
                over $F'$. So let $\alpha$ be a root of $g'$ in $K'$. Then
                $F[x]/(g)\simeq{F}[\alpha]\simeq{F}'[\alpha']\simeq{F}'[x]/(g')$
                and thus we can extend this isomorphism one step up. So now we
                have:
                \twocolumneq{f(x)=(x-\alpha)f_{1}(x)}
                            {f'(x)=(x-\alpha')f_{1}'(x)}
                $f_{1}$ splits in $K$ and thus we need to show that $K$ is
                minimal. Suppose $K/L/F_{1}$. Then since $f_{1}$ splits in $L$,
                $F$ splits in $L$ as well, contradicting the minimality of $K$
                over $F$, and hence $K$ is minimal over $F_{1}$. Thus, $K$ is
                a splitting field for $f_{1}$ over $F_{1}$. Thus by induction
                there is an isomorphism $\sigma:K\rightarrow{K}'$ with the
                desired property over $F_{1}$ and $F_{1}'$. But $F_{1}$ and
                $F_{1}'$ are extensions over $F$ and $F'$, so we're done.
            \end{proof}
    \section{Separability}
            Let $F$ be a field, $f\in{F}[x]$ be a polynomial of degree $n$, and
            $K$ the splitting field of $f$ over $F$. Then:
            \begin{equation}
                f(x)=a_{n}\prod_{k\in\mathbb{Z}_{r}}(x-\alpha_{k})^{m_{k}}
            \end{equation}
            where the $\alpha_{k}$ are distinct roots, and the $m_{k}$ are all
            positive integers.
            \begin{definition}
                A simply root of a polynomial $f$ over a field $F$ is an element
                $\alpha\in{F}$ such that $\alpha$ is a root of $f$ and
                $(x-\alpha)^{2}$ does not divide $f$. Otherwise $\alpha$ is
                called a multiple root.
            \end{definition}
            \begin{example}
                In $\mathbb{Q}[x]$ we can consider $x^{2}-1$. This splits as
                $(x-1)(x+1)$ and so we see that the roots are $\pm{1}$. However
                neither $(x-1)^{2}$ nor $(x+1)^{2}$ divides $f$, and hence both
                roots are simple. If we consider $x^{2}+2x+1$ then we know this
                factors as $(x+1)^{2}$ and so the only root is $\minus{1}$ and
                it is a multiple root with multiplicity 2.
            \end{example}
            \begin{definition}
                A separable polynomial is one such that every rot is simple.
                Otherwise, $f$ is inseparable.
            \end{definition}
            Over $\mathbb{C}$ we can use calculus to determine separability.
            Let $\alpha\in\mathbb{C}$ be a root of $f(x)\in\mathbb{C}[x]$ and
            suppose:
            \begin{equation}
                f(x)=(x-\alpha)^{m}g(x)
            \end{equation}
            with $m\geq{1}$. If we look at the derivative of $f$, we obtain:
            \begin{equation}
                \dot{f}(x)=m(x-\alpha)^{m-1}g(x)+(x-\alpha)^{m}\dot{g}(x)
            \end{equation}
            where $g(\alpha)\ne{0}$. Then there are two possibilities:
            \begin{equation}
                \dot{f}(\alpha)=
                \begin{cases}
                    0,&m\geq{2}\\
                    g(\alpha),&m=1
                \end{cases}
            \end{equation}
            Since $g(\alpha)$ is non-zero we see that $\alpha$ is a simply root
            if and only if the derivative of $f$ evaluated at $\alpha$ is
            non-zero. The idea is to generalize such notions to general fields
            where we may not have the structure to perform calculus. We thus
            define a derivation on $F[x]$.
            \begin{definition}
                A derivation on an algebra $\mathscr{A}$ over a field $F$ is a
                function $D:\mathscr{A}\rightarrow\mathscr{A}$ such that:
                \begin{align}
                    D(af+bg)&=aD(f)+bD(g)\\
                    D(x^{n})&=nx^{n-1}\\
                    D(f\circ{g})&=Df(g(x))\cdot{D}g(x)\\
                    D(fg)&=D(f)g+fD(g)
                \end{align}
            \end{definition}
            \begin{theorem}
                Linearity, Liebnizean, and $D(x)=1$ imply the other properties.
            \end{theorem}
            \begin{example}
                Let $p\in\mathbb{N}$ be a prime, and consider the field
                $\ring{\mathbb{Z}_{p}}$ with the usual arithmetic modulo $p$.
                Let $t\in\mathbb{Z}_{p}$ be some fixed constant, and consider
                the polynomial $f\in\mathbb{Z}_{p}[x]$ defined by
                $f(x)=x^{p}-t$. Then $f$ is irreducible, regardless of choice of
                $t$ by applying Eisenstein's criterion to the field
                $\mathbb{Z}_{p}[t]\subseteq\mathbb{Z}_{p}$. Let
                $\mathbb{Z}_{p}(\sqrt[p]{t})=\mathbb{Z}_{p}[x]/(x^{p}-t)$. Then:
                \begin{equation}
                    f(x)=x^{p}-t=x^{p}-(\sqrt[p]{t})^{p}-(x-\sqrt[p]{t})^{p}
                \end{equation}
                and thus $f$ has root $\sqrt[p]{t}$ with multiplicity $p$. Note
                that we can distribute the power only because $\mathbb{Z}_{p}$
                has characteristic $p$, and hence $(a+b)^{p}=a^{p}+b^{p}$,
                a consequence of the binomial theorem.
            \end{example}
            \begin{theorem}
                If $\ring{F}$ is a field, $f\in{F}[x]$ is a nonconstant, then
                $f$ is separable if and only if $f$ and $Df$ are relatively
                prime.
            \end{theorem}
            \begin{proof}
                First note that the greatest common denominator is independent
                of the base field for two polynomials $f$ and $g$. Thus, suppose
                $K$ is a splitting field for $f$ and $\alpha$ is a root. Then
                $f$ being separable implies that:
                \begin{align}
                    f(x)&=(x-\alpha)g(x)\\
                    \Rightarrow{Df}(x)&=g(x)+(x-\alpha)Dg(x)\\
                    \Rightarrow(Df)(\alpha)&=g(\alpha)
                \end{align}
                But $g(\alpha)\ne{0}$, and hence $Df(\alpha)\ne{0}$. In the
                other direction, let $\alpha\in{K}$ be a multiple root of $f$ so
                that $f(x)=(x-\alpha)^{2}h(x)$ with $h\in{K}[x]$. But then:
                \begin{align}
                    f(x)&=(x-\alpha)^{2}h(x)\\
                    \Rightarrow{Df}(x)&=2(x-\alpha)h(x)+(x-\alpha)^{2}Dh(x)
                    \Rightarrow{Df}(\alpha)&=0)
                \end{align}
                So $\alpha$ is also a root of $Df$ in $K$.
                If $f\in{F}[x]$ and $K/F$ is an extension field with
                $\alpha\in{K}$ such that $f(\alpha)=0$, then $m_{\alpha/F}(x)$
                divides $f(x)$. So $m_{\alpha/F}(x)$ divides the greatest
                common denominator of $f$ and $Df$, and thus $f$ and $Df$ are
                relatively prime.
            \end{proof}
            \begin{example}
                Let $F$ be a field, and let $f(x)=x^{n}-a$ for some $a\in{F}$.
                Using the previous theorem we can determine when $f$ is
                separable over $F$. Suppose $a\ne{0}$. Then we have
                $Df(x)=nx^{n-1}$, and thus we have two cases: $n=0$ and
                $n\ne{0}$. In the first case we have that the characteristic of
                $F$ then divides $n$, and hence $f$ is not separable. In the
                latter, $n\ne{0}$ and thus the characteristic does not divide
                $n$, and since the GCD of $nx^{n-1}$ and $x^{n}-a$ is 1,
                for otherwise $0$ would be a root of $f$ but it is not, and thus
                $f$ is separable.
            \end{example}
            \begin{theorem}
                If $f\in{F}[x]$ is irreducible, then $f$ is separable if and
                only if $Df$ is non-zero.
            \end{theorem}
            \begin{theorem}
                If $F$ is a field with characteristic zero, then any irreducible
                polynomial is separable. Hence $f\in{F}[x]$ is separable if and
                only if $f$ is the product of distinct irreducibles.
            \end{theorem}
            \begin{theorem}
                If $F$ is a field with characteristic $p>0$ then any irreducible
                $f(x)=g(x^{p^{r}})$ for $g\in{F}[x]$ is irreducible, separable,
                and uniquely determined for some $r\geq{1}$.
            \end{theorem}
            \begin{definition}
                The Frobenius map on a field of characteristic $p\in\mathbb{N}$
                is the function $\phi:F\rightarrow{F}$ defined by
                $\phi(x)=x^{p}$.
            \end{definition}
            \begin{definition}
                A perfect field is a field $F$ such that every irreducible
                polynomial is separable.
            \end{definition}
            \begin{example}
                Every field of characteristic zero is perfect by the previous
                theorems.
            \end{example}
            Do there exists fields of characteristic $p>0$ that are perfect?
            The answer is yes, and this can be categorized by the following
            theorem.
            \begin{theorem}
                If $\ring{F}$ is a field, then $F$ is perfect if and only if
                either $F$ has characteristic zero, or if the Frobenius
                map $\phi:F\rightarrow{F}$ is surjective.
            \end{theorem}
    \section{Subfields and Automorphisms}
        If $K/F$ is a finite field extension, $A=\autgroup[F]{K}$ the
        automorphism group, $H\subseteq{A}$ a subgroup, the fixed field of $H$
        is the set $K^{H}$ defined by all $\alpha\in{K}$ such that for all
        $\sigma\in{H}$ it is true that $\sigma(\alpha)=\alpha$.
        \begin{theorem}
            If $\ring{F}$ is a field, if $\ring{K}$ is a field extension of $F$,
            if $\autgroup[F]{K}$ is the automorphism group of $K$ over $F$,
            and if $H\subseteq\autgroup[F]{K}$ is a subgroup, then the fixed
            field $K^{H}$ is a subfield of $K$.
        \end{theorem}
        \begin{proof}
            For if $\alpha,\beta\in{K}^{H}$, then:
            \begin{equation}
                \sigma(\alpha+\beta)=\sigma(\alpha)+\sigma(\beta)
                    =\alpha+\beta
            \end{equation}
            and thus $\alpha+\beta\in{K}^{H}$. Similarly:
            \begin{equation}
                \sigma(\alpha\cdot\beta)=\sigma(\alpha)\cdot\sigma(\beta)
                    =\alpha\cdot\beta
            \end{equation}
            and therefore $\alpha\cdot\beta\in{K}^{H}$. Lastly:
            \begin{equation}
                \sigma(\alpha^{\minus{1}})=\sigma(\alpha)^{\minus{1}}
                    =\alpha^{\minus{1}}
            \end{equation}
            and hence $\alpha^{\minus{1}}\in{K}^{H}$.
        \end{proof}
        Note that since $\sigma\in\autgroup[F]{K}$, for all $x\in{F}$ it is true
        that $\sigma(x)=x$. Hence, $F\subseteq{K}^{H}$.
        \begin{theorem}
            If $\ring{F}$ is a field, if $\ring{L}$ is a field extension of
            $F$, and if $\ring{K}$ is a field extension of $L$, then
            $\autgroup[L]{K}$ is a subgroup of $\autgroup[F]{K}$.
        \end{theorem}
        \begin{theorem}
            If $\ring[F]{F}$ is a field, if $\ring[K]{K}$ is a field extension
            of $F$, if $H$ is a subgroup of the automorphism group
            $\autgroup[F]{K}$, and if $K^{H}$ is the fixed field of $H$,
            then $H$ is a subgroup of $\autgroup[K^{H}]{K}$.
        \end{theorem}
        \begin{theorem}
            If $\ring[F]{F}$ is a field, if $\ring[L]{L}$ is an extension field
            of $F$, if $\ring[K]{K}$ is an extension field of $L$, if
            $\autgroup[L]{K}$ is the automorphism group, and if
            $K^{\autgroup[L]{K}}$ is the fixed field, then
            $L\subseteq{K}^{\autgroup[L]{K}}$.
        \end{theorem}
        \begin{theorem}
            $K/L_{1}/L_{2}/F$ implies that $\autgroup[L_{2}]{K}$ is a subgroup
            of $\autgroup[L_{1}]{K}$.
        \end{theorem}
        \begin{theorem}
            If $H_{1}\subseteq{H}_{2}\subseteq\autgroup[F]{K}$, then
            $K^{H_{2}}\subseteq{K}^{H_{1}}$.
        \end{theorem}
        \begin{definition}
            A Galois extension of a field $\ring[F]{F}$ is a field $\ring[K]{K}$
            such that $\cardinality{\autgroup[F]{K})}=[K:F]$.
        \end{definition}
    \section{More on Symmetric Polynomials}
        Given the symmetric group $S_{n}$, this acts on $F[x_{1},\dots,x_{n}]$
        by permuting the variables. The symmetric polynomials are those that are
        invariant under permutations of variables. This is a ring.
        If $K/F$ is a finite extension field, then the number of automorphisms
        is bounded by the degree. That is,
        $\cardinality{\autgroup[F]{K}}\leq[K:F]$. Let $K/F$ and $L/F$ are
        extensions, where $K/F$ is finite (there is no constraint on $L/F$). We
        look at $\Homomorphisms[F]{K}{L}$ which is the set of all homomorphisms
        $\varphi:K\rightarrow{L}$ that fix $F$. Then $\Homomorphisms[F]{K}{L}$
        is finite. For if $K/F$ is finite, then it is finitely generated:
        $K=F(\alpha_{1},\dots,\alpha_{n})$, and thus any $\varphi$ is determined
        by where it maps the $\alpha_{i}$, and the $\varphi(\alpha_{i})$ are
        roots of the minimal polynomial $m_{\alpha_{i}/F}$. So there are
        finitely many cuch choices for $\varphi$. If we consider the vector
        space homomorphisms $\Homomorphisms[F(VS)]{K}{L}$ of all linear
        transformations $\varphi:K\rightarrow{L}$ that fix $F$, then there are
        infinitely many elements. If $L/F$ is also finite, then the dimension
        of this space is also finite since it's the space of $m\times{n}$
        matrices where $m=[K:F]$ and $n=[L:F]$.
        \begin{theorem}
            $\Homomorphisms[F]{K}{L}\subseteq\Homomorphisms[F(VS)]{K}{L}$.
        \end{theorem}
        \begin{proof}
            Asuume $\Homomorphisms[F]{K}{L}$ is dependent. So there exists
            elements $a_{j}\in{L}$, not all of which are zero, and function
            $\varphi_{j}\in\Homomorphisms[F]{K}{L}$ such that:
            \begin{equation}
                \sum_{i=1}^{m}a_{j}\varphi_{j}=0
            \end{equation}
        \end{proof}
        \begin{theorem}
            Letting $L=K$, we have $\autgroup[F]{K}$ has less than $[K:F]$
            elements.
        \end{theorem}
        Heading back to the before time:
        \begin{theorem}
            If $\ring{K}$ is a field, if $H\subseteq\autgroup{K}$ is a finite
            subgroup of the automorphism group of $K$, and if $K^{H}$ is the
            fixed field of $H$, then $K/K^{H}$ is a finite field extension and
            $[K:K^{H}]=\cardinality{H}$.
        \end{theorem}
        Let $K/F$ be a finite Galois extension, and let $G\in\autgroup[F]{G}$.
        There is a bijection between the subgroups of $H$ and the subextensions
        of $L$ such that $K/L/F$. We map $H\mapsto{K}^{H}$, the fixed field
        of $H$, and we map $L$ to $\autgroup[L]{K}$. Moreover,
        $H=\autgroup[K^{H}]{K}$. Next we need to prove that for any extension
        field $L$ of $F$ such that $K$ is an extension of $L$, we must show
        that $L=K^{\autgroup[L]{K}}$. That is, $L$ is equal to the fixed field
        generated by the automorphism group $\autgroup[L]{K}$.
        \begin{theorem}
            If $\ring[F]{F}$ is a field, if $\ring[L]{L}$ is a field
            extension of $F$, is $\ring[K]{K}$ is a field extension of $L$,
            and if $K$ is a Galois extension of $F$, then $K$ is a Galois
            extension of $L$.
        \end{theorem}
        \begin{proof}
            For let $X=\Homomorphisms[F]{L}{K}$. Then for $\iota\in{X}$,
            $\iota:L\rightarrow{K}$ is an inclusion mapping. Note that
            $G$ acts on $\autgroup[F]{K}$ by $g\cdot\varphi=g\circ\varphi$
            for all $g\in\autgroup[F]{K}$ and $\varphi\in{X}$. Then the
            stabilizer $G_{\iota}$ of $\iota$, which is the set of all
            $g\in{G}$ such that $g\cdot\iota=\iota$. But then
            $G_{\iota}=\autgroup[L]{K}$ since $\iota$ fixes $L$. But since $K$
            is a Galois extension of $F$,
            $[K:F]=\cardinality{\autgroup[F]{G}}$. But by the orbit stabilizer
            theorem:
            \begin{equation}
                \cardinality{\autgroup[F]{G}}
                =\cardinality{\autgroup[F]{K}_{\iota}}\cdot
                    \cardinality{\autgroup[F]{K}\cdot\iota}
            \end{equation}
            where $G\cdot\iota=\{g\cdot\iota|g\in\autgroup[F]{K}\}$. But
            $\autgroup[F]{K}_{\iota}=\autgroup[L]{K}$, and hence:
            \begin{equation}
                [K:F]=\cardinality{\autgroup[L]{K}}\cdot
                    \cardinality{\autgroup[F]{K}\cdot\iota}
            \end{equation}
            But $G\cdot\iota\subseteq{X}$, and hence:
            \begin{equation}
                [K:F]=\cardinality{\autgroup[L]{K}}\cdot\cardinality{X}
                \leq[K:L]\cdot[L:F]=[K:F]
            \end{equation}
            and hence these inequalities are actually equalities. Thus,
            $\autgroup[L]{K}=[K:L]$.
        \end{proof}
        \begin{theorem}
            If $\ring[F]{F}$ is a field, and if $\ring[K]{K}$ is a Galois
            extension of $F$, then $F=K^{\autgroup[F]{K}}$, where
            $K^{\autgroup[F]{G}}$ is the fixed field of $\autgroup[F]{K}$.
        \end{theorem}
        \begin{proof}
            For let $E=K^{\autgroup[F]{G}}$. We know that $F\subseteq{E}$.
            But since $K$ is a Galois extension of $F$, it is true that
            $[K:F]=\cardinality{\autgroup[F]{K}}$. But any
            $\varphi:K\rightarrow{K}$ that fixed $F$ also fixed $E$, and hence
            $\autgroup[F]{G}=\autgroup[E]{G}$, and therefore:
            \begin{equation}
                [K:F]=\cardinality{\autgroup[F]{K}}
                    =\cardinality{\autgroup[E]{K}}
                    \leq[K:E]=[K:F]/[E:F]
            \end{equation}
            and thus $[E:F]=1$. But if $E$ has degree 1 over $F$, then
            $E$ is equal to $F$. Thus, $F=K^{\autgroup[F]{K}}$.
        \end{proof}
        Some known properties of the bijection given by the fundamental theorem
        of Galois theory: If $H_{1}\subseteq{H}_{2}$, then
        $K^{H_{2}}\subseteq{K}^{H_{1}}$. That is, the bijection is inclusion
        reversing. If $H$ is a subgroup of $\autgroup[F]{K}$, then
        the index $[G:H]=[K^{H}:F]$. In other words, if $L$ is the
        subextension corresponding to $H$, then the index of $G$ over $H$ is
        equal to the degree of $L$ over $F$.
        \begin{theorem}
            If $K/L/F$ is a subextension, if $K/F$ is a finite Galois extension,
            if $G=\autgroup[F]{K}$, and if $H=\autgroup[L]{K}$, then
            $L/F$ is Galois if and only if $H$ is a normal subgroup of $G$.
        \end{theorem}
        \begin{proof}
            Let $X=\Homomorphisms[F]{L}{K}$, $\iota\in{X}$. $G$ acts on $X$ and
            $G_{\iota}=\autgroup[L]{K}=H$. If $L/F$ is Galois, then
            $\cardinality{\autgroup[F]{L}}=[L:F]$ and this is equal to
            $\cardinality{X}$. The mapping $\autgroup[F]{L}\rightarrow{X}$
            defined by $\tau\mapsto\iota\circ\tau$ is injective since $\iota$
            is injective, and hence this is also surjective since
            $\cardinality{\autgroup[F]{L}}=\cardinality{X}$, and these are
            finite sets. So given any $\sigma\in\autgroup[F]{K}$,
            $\sigma|_{L}:L\rightarrow{K}$, so $\sigma_{L}\in{X}$ and hence
            there is a $\tau\in\autgroup[F]{L}$ such that
            $\sigma|_{L}=\iota\circ\tau$. Hence $\sigma(L)=\iota(\tau(L))=L$.
        \end{proof}
        The fundamental theorem of Galois theory states that if $K/F$ is a
        finite Galois extension, then there is a bijection between the subgroups
        of $\autgroup[F]{K}$ and subextensions $K/L/F$. We map
        $H\mapsto{K}^{H}$, the fixed field of $H$, and we map
        $L\mapsto\autgroup[L]{K}$.
        \begin{theorem}
            If $K/F$ is a finite Galois extension, $K/L/F$ a subextension, and
            if $H=\autgroup[L]{K}$, then the following are equivalent:
            \begin{itemize}
                \item $L/F$ is Galois.
                \item $\sigma(L)=L$ for all $\sigma\in\autgroup[F]{K}$
                \item $H$ is a normal subgroup of $\autgroup[F]{K}$.
            \end{itemize}
        \end{theorem}
        \begin{theorem}
            If $L/F$ and $K/F$ are field extensions, and if $K/F$ is finite,
            then $\cardinality{\Homomorphisms[F]{K}{L}}\leq[L:F]$.
        \end{theorem}
        \begin{theorem}
            Given a Galois correspondence, if the subgroup $H$ corresponds to
            the subextension $L$, and if $\sigma\in\autgroup[F]{K}$, then
            $\sigma{H}\sigma^{\minus{1}}$ corresponds to $\sigma(L)$.
        \end{theorem}
        \begin{example}
            $\mathbb{Q}(\sqrt[3]{2},\omega)$.
        \end{example}
        \begin{example}
            $\mathbb{Q}(\exp(2\pi{i}/7))$. The minimal polynomial is
            $1+x+\dots+x^{5}+x^{6}$. Roots of minimal polynomial are 
        \end{example}
    \section{Normal Extension}
        \begin{definition}
            A normal extension of a field $\ring[F]{F}$ is a field extension
            $\ring[K]{K}$ of $F$ such that for all $f\in{F}[x]$ such that $f$
            has a root in $K$, it is true that $f$ splits over $K$.
        \end{definition}
        \begin{theorem}
            The minimal polynomial $m_{\alpha/F}(x)$ of any $\alpha\in{K}$
            splits in $K$. Equivalently, given any irreducible polynomial
            $f(x)$ with coefficients in $F$, either $f$ is irreducible over $K$
            or splits over $K$.
        \end{theorem}
        \begin{theorem}
            If $K/F$ is finite, and normal, then $K$ is the splitting field of
            a polynomial $f\in{F}[x]$. The converse is true as well.
        \end{theorem}
        Show that if $a_{1},\dots,a_{n}\in{F}$, with $F$ a field of
        characteristic not equal to 2 such that no product $a_{i}\cdot{a}_{j}$
        is a square. Prove $K=F(\sqrt{a_{1}},\dots,\sqrt{a_{n}})$ has degree
        $2^{n}$ over $F$. We prove by induction on $n$. The base case of $n=1$
        is true. We use the tower law and the hypothesis that
        $F(\sqrt{a_{1}},\dots,\sqrt{a_{n-1}})$ has degree $2^{n-1}$ and show
        that $F(\sqrt{a_{1}},\dots,\sqrt{a_{n}})$ has degree 2 over
        $F(\sqrt{a_{1}},\dots,\sqrt{a_{n-1}})$. We just need to show that
        $\sqrt{a_{n}}$ is not in $F(\sqrt{a_{1}},\dots,\sqrt{a_{n-1}})$.
        An algebraic extension $K/F$ is normal if for every $f\in{F}[x]$ with a
        root $\alpha\in{K}$, then $f$ splits over $K$.
        \begin{theorem}
            If $K/F$ is a finite, then $K$ is normal if and only if $K$ is the
            splitting field of some $f\in{F}[x]$.
        \end{theorem}
        \begin{proof}
            For if $K=F(\alpha_{1},\dots,\alpha_{n})$, and if $f$ is the
            minimum polynomial $f=\prod_{i}m_{\alpha_{i}/F}$, then $K$ is the
            splitting field of $f$. Now, suppose $K$ is the splitting field of
            $f\in{F}[x]$ where $f$ has degree $n$. Let $\alpha\in{K}$ and $g$
            the minimal polynomial of $\alpha$ over $F$. We want to prove that
            $g$ splits in $K$. Let $M$ be the splitting field of $g$ over $K$
            and let $\beta\in{M}$ be any root of $g$.
        \end{proof}
        \begin{definition}
            The normal closure of an algebraic field extension $K/F$ is an
            extension $N/K$ such that $N/F$ is normal and minimal with this
            proprty.
        \end{definition}
        \begin{theorem}
            If $K/F$ is finite, then a normal closure $N/K$ exists and is unique
            up to $F$ isomorphism.
        \end{theorem}
        \begin{proof}
            Let $K=F(\alpha_{1},\dots,\alpha_{n})$ and $f$ the product over the
            minimal polynomials. Let $N$ be the splitting field of $f$ over $F$.
        \end{proof}
        \begin{definition}
            A separable extension of a field $\ring[F]{F}$ is an algebraic field
            extension $\ring[K]{K}$ of $F$ such that for all $\alpha\in{K}$ and
            $m_{\alpha/F}(x)\in{F}[x]$ it is true that $m_{\alpha/F}$ is
            separable.
        \end{definition}
        \begin{ftheorem}{Separable Field Extensions Theorem}{}
            If $K/F$ is finite, then it is Galois if and only if it is separble
            and normal.
        \end{ftheorem}
    \section{Radical Stuff}
        \begin{theorem}
            If $K/F$ is a radical Galois extension, then $\autgroup{F}{K}$
            is solvable.
        \end{theorem}
        \begin{proof}
            If $K$ is a radical field extension, then
            $K=F(\alpha_{1},\dots,\alpha_{n})$ with
            $\alpha_{i}^{n_{i}}\in{F}(\alpha_{i},\dots,\alpha_{i-1})$. We may
            assume that all of the $n_{i}$ are prime or equal to 1. For let
            $p$ be a prime that divides $n_{m}$. So then
            $n_{m}=p^{r}k$, where $p$ does not divide $k$. But then:
            \begin{equation}
                F(\alpha_{1},\dots,\alpha_{m})
                    =F(\alpha_{1},\dots,a_{m}^{k},a_{m}^{p^{r}})
            \end{equation}
            Since $\alpha_{m}^{k}$ and $\alpha_{m}^{p^{r}}$ can be generated
            by $\alpha_{m}$, the right side is a subset of the left. By Bezout's
            identity there exists $a,b$ such that $ak+bp^{r}=1$, since
            $k$ and $p^{r}$ are coprime. But then:
            \begin{equation}
                \alpha_{m}=(\alpha_{m}^{k})^{a}(\alpha_{m}^{p^{r}})^{b}
            \end{equation}
            and hence we have equality. Some stuff, and now we prove the theorem
            by induction on the length $m$ of elements $\alpha_{i}$. Since $K$
            is a Galois extension, it is normal, and hence $\alpha_{1}$ has
            another conjugate $\beta$, that is, a root of the minimal polynomial
            $m_{\alpha_{1}/F}(x)$, in $K$, call it $\beta$. But then
            $\alpha_{1}^{n_{1}}\in{F}$ implies that $\beta^{n_{1}}\in{F}$, and
            then $(\alpha_{1}/\beta)^{n_{1}}=1$.
        \end{proof}
    \section{Quartic Polynomials}
        Suppose $G\subseteq{S}_{4}$ is a transitive subgroup. The possibilities
        are the alternating group $A_{4}$, the dihedral group
        $D_{8}=\langle(12)(34),(1234)\rangle$, the Klein 4 group, denoted
        $V_{4}=\langle{1},(12)(34),(13)(24),(14)(23)\rangle$, and the cyclic
        group $\mathbb{Z}_{4}=\langle{(1234)}\rangle$. The only normal subgroups
        of $S_{4}$ are $A_{4}$, $D_{8}$, $V_{4}$, and $S_{4}$ itself. Moreover,
        $V_{4}$ is a normal subgroup of $A_{4}$. If the discrimant is a square
        in $F$, then $G$ is a subgroup of $A_{4}$ and hence either $G=V_{4}$ or
        $G=A_{4}$. If $f(x)=x^{4}+ax^{2}+b$, then $G$ is a subgroup of $D_{8}$
        and hence $G$ is isomorphic to the trivial group,
        $\mathbb{Z}/2\mathbb{Z}$, $\mathbb{Z}/4\mathbb{Z}$,
        $\mathbb{Z}_{2}\times\mathbb{Z}_{2}$, or $D_{8}$. Since $f$ is
        irreducible we can conclude that $G$ is isomorphic to either
        $\mathbb{Z}/4\mathbb{Z}$, $\mathbb{Z}_{2}\times\mathbb{Z}_{2}$, or
        $D_{8}$. Let $\alpha_{1},\alpha_{2},\alpha_{3},\alpha_{4}$ be roots of
        $f(x)$ and defined $\beta_{i}$ as follows:
        \begin{subequations}
            \begin{align}
                \beta_{1}&=\alpha_{1}\alpha_{4}+\alpha_{2}\alpha_{3}\\
                \beta_{2}&=\alpha_{1}\alpha_{3}+\alpha_{2}\alpha_{4}\\
                \beta_{3}&=\alpha_{1}\alpha_{2}+\alpha_{3}\alpha_{4}
            \end{align}
        \end{subequations}
        Then the Galois group $G$ of $f$ acts on
        $\{\beta_{1},\beta_{2},\beta_{3}\}$, so we get the following diagram:
        \begin{figure}[H]
            \centering
            \captionsetup{type=figure}
            \begin{tikzpicture}[>=Latex]
                \node (G)  at (0,  0) {$G$};
                \node (3)  at (2,  0) {$S_{3}$};
                \node (4)  at (0, -2) {$S_{4}$};
                \node (3a) at (2, -2) {$S_{3}$};

                \draw[->] (G) to (3);
                \draw[->] (G) to (4);
                \draw[->] (4) to (3a);
                \draw[->] (3) to (3a);
            \end{tikzpicture}
            \caption{Lagrange Discriminant}
        \end{figure}
        the kernel of the induced homomorphism $\varphi:S_{3}\rightarrow{S}_{4}$
        is the Klein 4 group $V_{4}$. If $G=S_{4}$ then $g$ is irreducible.
        So, in summary, if $f\in{F}[x]$ is an irreducible quartic, if
        $g\in{F}[x]$ is it's cubic resolvant:
        \begin{equation}
            g(x)=(x-\beta_{1})(x-\beta_{2})(x-\beta_{3})
        \end{equation}
        then we have the following table of facts:
        \begin{table}[H]
            \centering
            \captionsetup{type=table}
            \begin{tabular}{l|l|c}
                $\Delta(f)$&g(x)&$\autgroup[F]{K}$\\
                \hline
                Not a square&Irreducible&$S_{4}$\\
                Square&Irreducible&$A_{4}$\\
                Not a square&Has one root&$\mathbb{Z}_{4}$ or $D_{8}$\\
                Square&Splits&$\mathbb{Z}_{2}\times\mathbb{Z}_{2}$
            \end{tabular}
            \caption{Classifying Quartics}
        \end{table}
    \section{Stuff}
        If $\langle\cdot|\cdot\rangle$ is a symmetric bilinear form, it is
        represented by its Gram matrix relative to a bsis $\mathscr{B}$ of the
        finite dimensional vector space $\mathscr{B}$. We have:
        \begin{equation}
            \langle{v}|w\rangle
            =[V]_{\mathscr{B}}^{T}G_{\mathscr{B}}[W]\mathscr{B}
        \end{equation}
        Let $\langle\cdot|\cdot\rangle:V\times{V}\rightarrow{k}$ be bilinear,
        let $\mathscr{B}$ and $\mathscr{C}$ be bases of $V$, and let
        $G_{\mathscr{B}}$ and $G_{\mathscr{C}}$ be the corresponding Gram
        matrices. Let $x,y\in{V}$. Then:
        \begin{subequations}
            \begin{align}
                [x]_{\mathscr{B}}^{T}G_{\mathscr{B}}[y]_{\mathscr{B}}
                &=\langle{x}|y\rangle\\
                &=[x]_{\mathscr{C}}^{T}G_{\mathscr{C}}[Y]_{\mathscr{C}}\\
                &=[\textrm{Id}(x)]_{\mathscr{C}}^{T}G_{\mathscr{C}}
                    [\textrm{Id}(y)]_{\mathscr{C}}\\
                &=\Big([\textrm{Id}]_{\mathscr{C}}^{\mathscr{B}}
                    [x]_{\mathscr{B}}\Big)^{T}G_{\mathscr{C}}
                    \Big([\textrm{Id}_{\mathscr{C}}^{\mathscr{B}}]
                        [y]_{\mathscr{B}}\Big)
            \end{align}
        \end{subequations}
        From this, we obtain the formula for the Gram matrix:
        \begin{equation}
            G_{\mathscr{B}}=P^{T}G_{\mathscr{C}}P
        \end{equation}
        \begin{fdefinition}{Congruent Matrices}{Congruent_Matrices}
            Congruent matrices are matrices $A,B\in{M}_{n}(k)$ such that there
            is an invertible matrice $P\in{GL}_{n}(k)$ such that:
            \begin{equation}
                B=P^{T}AP
            \end{equation}
        \end{fdefinition}
    \section{Orthogonal Transformations}
        \begin{ltheorem}{Sylvester's Theorem}{Sylvesters_Theorem}
            If $V$ is a finite dimensional vector space over $\mathbb{R}$ and if
            $\langle\cdot|\cdot\rangle$ is a symmetric bilinear form, then there
            exists a basis $\mathscr{B}$ of $V$ such that:
            \begin{equation}
                G_{\mathscr{B}}=
                \begin{bmatrix*}[r]
                    1&\dots&0&0&\dots&0&0&\dots&0\\
                    \vdots&\ddots&\vdots&\vdots&\ddots
                        &\vdots&\vdots&\ddots&\vdots\\
                    0&\dots&1&0&\dots&0&0&\dots&0\\
                    0&\dots&0&\minus{1}&\dots&0&0&\dots&0\\
                    \vdots&\ddots&\vdots&\vdots
                        &\ddots&\vdots&\vdots&\ddots&\vdots\\
                    0&\dots&0&0&\dots&\minus{1}&0&\dots&0\\
                    0&\dots&0&0&\dots&0&0&\dots&0\\
                    \vdots&\ddots&\vdots&\vdots&\ddots
                        &\vdots&\vdots&\ddots&\vdots\\
                    0&\dots&0&0&\dots&0&0&\dots&0\\
                \end{bmatrix*}
            \end{equation}
            Where there are $r$ 1's, $s$ negative 1's, and $t$ zeros, and
            $r$, $s$, and $t$ are uniquely determined.
        \end{ltheorem}
        Let $(V,\langle\cdot|\cdot\rangle)$ be a nondegenerate bilinear space.
        Then a linear map $T:V\rightarrow{V}$ is orthogonal if or a linear
        isometry if:
        \begin{equation}
            \langle{T}(v)|T(w)\rangle=\langle{v}|w\rangle
        \end{equation}
        Let $\mathscr{B}$ be a basis of $V$ and let $G_{\mathscr{B}}$ be the
        Gram matrix of $\langle\cdot|\cdot\rangle$. Let $A$ be the representing
        matrix of $T$ over the basis $\mathscr{B}$. Then:
        \begin{equation}
            \langle{v}|w\rangle
            =\langle{T}(v)|T(w)\rangle
            =[T(v)]_{\mathscr{B}}^{T}G_{\mathscr{B}}[T(w)]_{\mathscr{B}}
            =[v]_{\mathscr{B}}^{T}A^{T}G_{\mathscr{B}}A[w]_{\mathscr{B}}
        \end{equation}
        From this we can compute what the Gram matrix is:
        \begin{equation}
            G_{\mathscr{B}}=A^{T}G_{\mathscr{B}}A
        \end{equation}
        If $A$ represents an orthogonal transformation, then $A$ must satisfy
        this equation. In the special case of when $\mathscr{B}$ is orthonormal,
        then $G_{\mathscr{B}}$ is simply the identity matrix and thus we have
        that $A^{T}A=I$, or $A^{T}=A^{\minus{1}}$.
        \par\hfill\par
        A nondegenerate skew symmetric bilinear form on a $2n$ dimensional real
        vector space is called a symplectic form. The Gram matrix is:
        \begin{equation}
            J=
            \begin{bmatrix*}[r]
                0&I_{n}\\
                \minus{I}_{n}&0
            \end{bmatrix*}
        \end{equation}
        A transformation $A$ such that $A^{T}JA=J$ is called a symplectic or
        a canonical transformation.
        \par\hfill\par
        If $\langle\cdot|\cdot\rangle$ is the Lorentz form on $\mathbb{R}^{4}$,
        then an orthogonal transformation is called a Lorentz transformation.
    \section{Sesquilinear Geometry}
        Let $V$ and $W$ be $\mathbb{R}$ inner product spaces. That is, $V$ and
        $W$ are equipped with a symmetric bilinear form
        $\langle\cdot|\cdot\rangle_{V}$ and $\langle\cdot|\cdot\rangle_{W}$ that
        are positive-definite:
        \begin{equation}
            \langle{v}|v\rangle\geq{0}
        \end{equation}
        With equality if and only if $v=0$. Note that positive definite implies
        nondegenerate since $\langle{v}|v\rangle>0$ for nonzero $v$. Let
        $T:V\rightarrow{W}$ be a linear map. Then $T$ induces
        $T^{*}:W^{*}\rightarrow{V}^{*}$ by something.
        Let $V$ be a finite dimension $\mathbb{R}$ vector space. An inner
        product on $V$ is a bilinear form that is symmetric and
        positive-definite. Euclidean geometry is derived from the inner product.
        This induces a norm and the notion of angle:
        \begin{equation}
            \norm{v}=\sqrt{\langle{v}|v\rangle}
            \quad\quad
            \theta=\cos^{\minus{1}}\Big(
                \frac{\langle{v}|w\rangle}{\norm{v}\norm{w}}\Big)
        \end{equation}
        Cauchy-Schwartz comes out of this. Let $V$ be a vector space over
        $\mathbb{C}$, say $V=\mathbb{C}^{n}$. If we define:
        \begin{equation}
            \langle{v}|w\rangle=\sum_{k=1}^{n}v_{k}w_{k}
        \end{equation}
        We lose positive-definiteness since $(1,2i)\cdot(1,2i)=\minus{3}$,
        in $\mathbb{C}^{2}$, for example. We define the dot product on
        $\mathbb{C}^{n}$ by:
        \begin{equation}
            \langle{z}|w\rangle=\sum_{k=1}^{n}\overline{z}_{k}\overline{w}_{k}
        \end{equation}
        That is, $\langle{v}|w\rangle=\overline{v}\cdot{w}$. From this we have
        that $\langle{z}|\cdot\rangle$ is linear on $\mathbb{C}$. However,
        looking at $\langle\cdot|w\rangle$, we have that it is $\mathbb{R}$
        linear but only conjugate linear over $\mathbb{C}$. This gives rise to
        the notion of a sesquilinear product.
        \begin{fdefinition}{Sesquilinear Product}{Sesquilinear_Product}
            A sesquilinear product on a vector space $V$ over $\mathbb{C}$ is a
            function $\langle\cdot|\cdot\rangle:V\times{V}\rightarrow\mathbb{C}$
            such that $\langle{z}|\cdot\rangle$ is $\mathbb{C}$ linear and
            $\langle\cdot|w\rangle$ is $\mathbb{R}$ linear and $\mathbb{C}$
            conjugate linear.
        \end{fdefinition}
        With this we can define a Hermitian form on a $\mathbb{C}$ vector space
        $V$. Note that a sesquilinear form is conjugate symmetric. That is,
        $\langle{w}|z\rangle=\overline{\langle{z}|w\rangle}$.
        \begin{fdefinition}{Hermitian Form}{Hermitian_Form}
            A Hermitian form on a $\mathbb{C}$ vector space $V$ is a function
            $\langle\cdot|\cdot\rangle:V\times{V}\rightarrow\mathbb{C}$ that is
            sesquilinear and conjugate symmetric.
        \end{fdefinition}
        \begin{fdefinition}{Hermitian Inner Product Space}
                           {Hermitian_Inner_Product_Space}
            A Hermitian inner product space is a vector space $V$ over
            $\mathbb{C}$ with a Hermitian inner product.
        \end{fdefinition}
        Let $\mathcal{B}$ be a basis of a Hermitian inner product space $V$ and
        let $G_{\mathcal{B}}=[g_{ij}]_{ij}$ where
        $g_{ij}=\langle{e}_{i}|e_{j}\rangle$. Then for $v,w\in{V}$, we have:
        \begin{equation}
            v=\sum_{k=1}^{n}a_{k}e_{k}
            \quad\quad
            w=\sum_{k=1}^{n}b_{k}e_{k}
        \end{equation}
        And moreover:
        \begin{equation}
            \langle{v}|w\rangle=
            \langle\sum_{j=1}^{n}a_{j}e_{j}|\sum_{k=1}^{n}b_{k}e_{k}\rangle
            =\sum_{k=1}^{n}\sum_{j=1}^{n}\overline{a}_{j}b_{k}
                \langle{e_{j}}|e_{k}\rangle
            =\sum_{j=1}^{n}\sum_{k=1}^{n}\overline{a}_{j}g_{jk}b_{k}
        \end{equation}
        So we have:
        \begin{equation}
            \langle{v}|w\rangle
            =\overline{[v]_{\mathcal{B}}^{T}}G_{\mathcal{B}}[w]_{\mathcal{B}}
        \end{equation}
        This gives rise to the definition of a Hermitian transpose.
        \begin{fdefinition}{Hermitian Transpose}{Hermitian_Transpose}
            The Hermitian transpose of a matrix $A$ is the matrix:
            \begin{equation*}
                A^{H}=\overline{A^{T}}
            \end{equation*}
        \end{fdefinition}
    \section{Unitary Transformations}
        \begin{fdefinition}{Unitary Transformations}{Unitary_Transformations}
            A unitary transformation on a vector space $V$ over $\mathbb{C}$ is
            a linear function $T:V\rightarrow{V}$ such that:
            \begin{equation*}
                \langle{T}(v)|T(w)\rangle=\langle{v|w}\rangle
            \end{equation*}
        \end{fdefinition}
        Let $V$ and $W$ be Hermitian inner product spaces and let
        $T:V\rightarrow{W}$ be $\mathbb{C}$ linear. $T$ induces
        $T^{*}:W^{*}\rightarrow{V}^{*}$ by $T^{*}(\psi)=\psi\circ{T}$.
        \begin{fdefinition}{Self-Adjoint Hermitian Operator}
                           {Self-Adjoint_Hermitian_Operator}
            A self-adjoint operator on a Hermitian inner product space $V$ is a
            linear function $T:\mathbb{C}\rightarrow\mathbb{C}$ such that
            $T=T^{H}$.
        \end{fdefinition}
        \begin{fdefinition}{Normal Hermitian Operator}
                           {Normal_Hermitian_Operator}
            A function $T:\mathbb{C}\rightarrow\mathbb{C}$ such that
            $TT^{H}=T^{H}T$
        \end{fdefinition}
    \section{Spectral Theorem}
        \begin{theorem}
            If $V$ is a finite dimensional vector space over $\mathbb{C}$, if
            $T:V\rightarrow{V}$ is a linear map, then $T$ has an eigenvalue.
        \end{theorem}
        \begin{proof}
            For the characteristic polynomial is non-constant and thus by the
            fundamental theorem of algebra there exists a root.
        \end{proof}
        \begin{theorem}
            If $V$ is a finite dimensional Hermitian inner product space and if
            $T:V\rightarrow{V}$ is a Hermitian operator, then the eigenvalues of
            $T$ are real and if $v$ is an eigenvector of $\lambda$ and if $w$ is
            a $\mu$ eigenvector for two different eignvalues $\lambda\ne\mu$,
            then $v$ and $w$ are orthogonal.
        \end{theorem}
        \begin{proof}
            For let $\lambda$ be an eigenvalue and let $v$ be a non-zero
            eigenvector for $\lambda$. Then $T(v)=\lambda{v}$. But $T$ is
            Hermitian, and therefore:
            \begin{equation}
                \langle{T}^{H}(v)|v\rangle
                =\langle{v}|T(v)\rangle
                =\langle{v}|\lambda{v}\rangle
                =\lambda\rangle{v}|v\rangle
            \end{equation}
            But also:
            \begin{equation}
                \langle{T}^{H}(v)|v\rangle
                =\langle{T}(v)|v\rangle
                =\langle{\lambda}v|v\rangle
                =\overline{\lambda}\langle{v}|v\rangle
            \end{equation}
            And therefore, since $\langle{v}|v\rangle\ne{0}$, we have
            $\lambda=\overline{\lambda}$, and therefore $\lambda$ is real. For
            the second part, we have:
            \begin{equation}
                \langle{v}|T(w)\rangle
                =\langle{v}|\mu{w}\rangle
                =\mu\langle{v}|w\rangle
                =\langle{T}^{H}(v)|w\rangle
                =\langle{T}(v)|w\rangle
                =\langle\lambda{v}|w\rangle
                =\overline{\lambda}\langle{v}|w\rangle
            \end{equation}
            But we just proved that $\lambda$ is real, and thus
            $\overline{\lambda}=\lambda$. Moreover $\lambda\ne\mu$, and thus
            for equality to occur we must have $\langle{v}|w\rangle=0$.
        \end{proof}
        \begin{theorem}
            If $V$ is a finite dimensional Hermitian inner product space, if
            $T:V\rightarrow{V}$ is linear, and if $W\subseteq{V}$ is a $T$
            invariant subspace (that is, $T(W)\subseteq{W}$), then
            $W^{\perp}$ is $T^{H}$ invariant.
        \end{theorem}
        \begin{proof}
            For let $x\in{W}^{\perp}$ and let $w\in{W}$. Then:
            \begin{equation}
                \langle{T}^{H}(x)|w\rangle
                =\langle{x}|T(w)\rangle=0
            \end{equation}
            And thus $T(x)\in{W}^{\perp}$. Therefore,
            $T(W^{\perp})\subseteq{W}^{\perp}$.
        \end{proof}
        \begin{ltheorem}{Unitary Triangulation Theorem}
                        {Unitary_Triangulation_Theorem}
            If $V$ is a finite dimensional Hermitian inner product space over
            $V$ and if $T:V\rightarrow{V}$ is a linear operator, then there
            exists an orthonormal basis $\mathscr{B}$ of $V$ such that
            $[T]_{\mathscr{B}}^{\mathscr{B}}$ is upper triangular.
        \end{ltheorem}
        \begin{proof}
            For consider $T^{H}:V\rightarrow{V}$. It has an eigenvalue
            $\lambda$. Let $v$ be a $\lambda$ eigenvector of $T^{H}$ and let
            $W=\mathbb{C}\cdot{v}=\textrm{Span}\{zv:z\in\mathbb{C}\}$. Since
            $v$ is an eigenvector of $T^{H}$ we have that $W$ is a $T^{H}$
            invariant subspace so therefore $W^{\perp}$ is invariant under
            $(T^{H})^{H}=T$. That is, $W^{\perp}$ is a $T$ invariant subspace.
            Since $W$ is non-zero, $W^{\perp}$ has dimension less than $V$ and
            thus by induction there is an orthonormal basis of $W^{\perp}$ such
            that $[T_{W^{\perp}}]_{\mathscr{B}'}^{\mathscr{B}'}$ is upper
            triangular. Extending $\mathscr{B}$ from $\mathscr{B}'$ gives an
            orthonormal basis such that $[T]_{\mathscr{B}}^{\mathscr{B}}$ is
            upper triangular.
        \end{proof}
        \begin{theorem}
            If $A\in{M}_{n}(\mathbb{C})$ then there is a unitary matric
            $P\in{U}(n)$ such that $PAP^{\minus{1}}$ is upper triangular.
        \end{theorem}
        \begin{ftheorem}{Spectral Theorem for Hermitian Operators}
                        {Spectral_Theorem_for_Hermitian_Operators}
            If $V$ is a finite dimensional Hermitian inner product space and if
            $T:V\rightarrow{V}$ is a Hermitian operator then there exists an
            orthonormal basis $\mathscr{B}$ of $V$ such that
            $[T]_{\mathscr{B}}^{\mathscr{B}}$ is diagonal.
        \end{ftheorem}
        \begin{proof}
            For the representing matrix of $T$ is upper triangular, and thus the
            representing matrix for $T^{H}$ is lower triangular. But $T=T^{H}$
            and therefore the represeting matrix of $T$ is both upper and lower
            triangular, and therefore it is diagonal.
        \end{proof}
        The theorem holds for normal operators as well. For Hermitian we see
        that the eigenvalues are real. For skew-Hermitian ($T=\minus{T}^{H}$)
        the eigenvalues are purely imaginary, and lastly for a unitary $T$ the
        eigenvalues are unit modulus.
        \begin{ftheorem}{Real Spectral Theorem}
            If $V$ is a finite dimensional inner product space, if
            $T:V\rightarrow{V}$ is a self-adjoint operator then there is an
            orthonormal basis of $V$ such that the representing matrix is
            diagonal.
        \end{ftheorem}
        \begin{proof}
            Let $A$ be the representing matrix of $T$ relative to the standard
            basis of $\mathbb{R}^{n}$. Then $A=A^{T}$ and thus $A$ defines a
            linear map $A:\mathbb{C}^{n}\rightarrow\mathbb{C}^{n}$ by mapping
            $A(v)=Av$ as a matrix operatoion. Then $A$ is a Hermitian operator
            and therefore $A$ has real eigenvalues. Let $v\in\mathbb{C}^{n}$ be
            a complex eigenvector for $\lambda$. Let $v=x+iy$ for
            $x,y\in\mathbb{R}^{n}$. Then since $Av=\lambda{v}$ we have:
            \begin{equation}
                Ax+iAy=\lambda{x}+i\lambda{y}
            \end{equation}
            But $A$ is real, as are $x$ and $y$, and thus we have:
            \begin{equation}
                Ax=\lambda{x}
                \quad\quad
                Ay=\lambda{y}
            \end{equation}
            But since $v$ is non-zero, at least one of $x$ or $y$ is non-zero.
            Thus $A$ has a real eigenvector, $x$ or $y$. Let $u$ be a unit
            real eigenvector and let $W$ be the real span of $u$. Then $W$ is
            $T$ invariant and therefore $W^{\perp}$ is $T^{H}$ invariant, but
            $T=T^{H}$. We complete the proof by induction.
        \end{proof}
    \section{Tensor Products}
        Let $R$ be a ring nd let $M$ be a right $R$ module and let $N$ be a
        left $R$ module. We seek a universal product $M\times{N}$ into
        $M\otimes_{R}N$. Examples of products: Dot products, cross products,
        bilinear forms, matrix multiplication. All of these are Biadditive.
        Matrices also have the \textit{balanced} property:
        \begin{equation}
            A(cB)=(Ac)B
        \end{equation}
        If the underlying ring is not commutative, we may not have perfect
        bilinearity and so we replace this requirement with the balanced
        property. So we seek a function
        $\beta:M\times{N}\rightarrow{M}\otimes_{R}N$ such that:
        \begin{align}
            \beta(a+b,c)&=\beta(a,b)+\beta(b,c)\tag{Left Additivity}\\
            \beta(a,b+c)&=\beta(a,b)+\beta(a,c)\tag{Right Additivity}\\
            \beta(ar,b)&=\beta(a,rb)\tag{Balanced}
        \end{align}
        Universal means that given any Abelian group $A$ and any biadditive
        $R$ balanced map $\mu:M\times{N}\rightarrow{A}$ there is a unique
        $\mathbb{Z}$ module homomorphism
        $\tilde{\mu}:M\otimes_{R}N\rightarrow{A}$ such that the diagram
        commutes. Since if it exists it is unique up to unique isomorphism, we
        need only construct such a thing. Long complicated construction to
        follow. By construction, $M\otimes_{R}N$ is generated by $m\otimes{n}$
        for all $m\in{M}$, $n\in{N}$. As a warning, not every element of
        $M\otimes_{R}N$ need be decomposable. It's folly to try to define a map
        $M\times{N}\rightarrow{A}$ by $m\otimes{n}\mapsto{f}(m,n)$ unless
        $f(m,n)$ is biadditive and $R$ balanced.
        \begin{theorem}
            If $N$ is a left $R$ module, then $R\otimes_{R}N$ is isomorphic to
            $N$ as a $\mathbb{Z}$ module.
        \end{theorem}
        \begin{proof}
            For let $\mu:R\times{N}\rightarrow{N}$ by defined by
            $\mu(r,n)=r\cdot{n}$. Then $\mu$ is biaddiative and balanced.
        \end{proof}
        The idea is to use scalar multiplication $R\times{N}\rightarrow{N}$ to
        construct such a module. The by the universal mapping property there
        exists a unique $\mathbb{Z}$ module homomorphism
        $\tilde{\mu}:R\otimes_{R}N\rightarrow{N}$ such that the diagram
        commutes. Define $\tilde{\mu}$ by:
        \begin{equation}
            \tilde{\mu}(r\otimes{n})=r\cdot{n}
        \end{equation}
        Define $j:N\rightarrow{R}\otimes_{R}N$ by $j(n)=1\otimes{n}$.
        This is a $\mathbb{Z}$ module since the tensor product is biaddiative
        and balanced. Moreover, $\tilde{\mu}$ and $j$ are inverses of each
        other.
        \begin{theorem}
            If $R$ is a ring, if $I$ is an ideal of $R$, and if $N$ is a left
            $R$ module, then:
            \begin{equation}
                R/I\otimes_{R}N\simeq{N}/IN
            \end{equation}
        \end{theorem}
        \begin{proof}
            For define $\mu:R/I\times{N}\rightarrow{N}/IN$ by:
            \begin{equation}
                \mu(\overline{r},n)=\mu(r+I,n)=rn+IN=\overline{rn}
            \end{equation}
            For $n\in{N}$ let $f_{N}:R\rightarrow{N}/IN$ be defined by
            $f_{n}(r)=rn+IN=\overline{rn}$. Then $f_{n}$ is a map of left
            $R$ modules and therefore induces a map $\overline{f}_{n}$ from
            $R/I$ to $N/IN$ by $\overline{r}\mapsto\overline{rn}$. By the
            universal mapping property there is a $\mathbb{Z}$ linear map
            $\tilde{\mu}$ such that the diagram commutes. So we have that
            $\tilde{\mu}(\overline{r}\otimes{n})=\overline{rn}$. We now want a
            map $N/IN\rightarrow{R}/I\otimes_{R}N$. Let $j$ be defined by
            $j(n)=\overline{1}\otimes{n}$. Then $j$ vanishes on generators of
            $IN$ and therefore induces $\overline{j}$ such that
            $\overline{n}\mapsto\overline{1}\otimes{n}$. Thus we now have two
            maps that are well defined and now we need only check that they are
            inverses of each other. And it is so. So we are done.
        \end{proof}
        \begin{theorem}
            Given a sequence of modules over $R$,
            $N'\rightarrow{N}\rightarrow{N}''\rightarrow{0}$,
            and suppose that for all left $R$ modules $Y$ the
            sequence:
            \begin{equation}
                0\rightarrow\textrm{Hom}_{R}(N'',Y)
                \rightarrow\textrm{Hom}_{R}(N,Y)
                \rightarrow\textrm{Hom}_{R}(N',Y)
            \end{equation}
            is exact, then the original sequence is exact.
        \end{theorem}
        \begin{proof}
            For let $Y=\textrm{coker}(v)=N''/v(N)$. Then since:
            \begin{equation}
                0\rightarrow\textrm{Hom}_{R}(N'',Y)
                \rightarrow\textrm{Hom}_{R}(N,Y)
                \rightarrow\textrm{Hom}_{R}(N',Y)
            \end{equation}
            is exact, we have that the canonical projection $\pi$ gets mapped
            to $v^{*}(\pi)$. But $v^{*}(\pi)=\pi\circ{v}$, and this is zero
            since $n\mapsto{v}(n)$ which maps to $0$ by $\pi$. Thus $v^{*}$ is
            surjective and injective. On the other side, let $Y=N''$. Then
            $\textrm{id}_{N''}$ mapts to $v$ under $v^{*}$, and thus
            $v$ maps to $0$ under $u^{*}$ since the sequence is exact, and
            thus $u\circ{v}=0$. Thus $\textrm{im}(u)\subseteq\textrm{ker}(u)$.
        \end{proof}
        An algebra over a commutative ring $k$ is a left $k$ module with a ring
        structure such that the ring multiplication is compatible with scalar
        multiplication:
        \begin{equation}
            (\lambda\star{a})\cdot{b}=\lambda\star(a\cdot{b})
                =a\cdot(\lambda\star{b})
        \end{equation}
        An equivalent definition is a ring $A$ with a ring homomorphism
        $\varphi:k\rightarrow{Z}(A)$. We can also define an algebra in terms of
        the tensor product. An algebra over $k$ is a $k$ module $A$ with a
        homomorphism $\mu:A\otimes{A}\rightarrow{A}$ and a module homomorphism
        $\eta:k\rightarrow{A}$ such that some diagram commutes.
    \section{Bonus Stuff}
        Given a ring homomorphism $\varphi:R\rightarrow{S}$, there is a map
        $\varphi^{\#}:\textrm{Spec}(S)\rightarrow\textrm{Spec}(R)$, where
        $\textrm{Spec}(X)$ is the set of all prime ideals on $X$.
        Zariski topology. Define, for all $r\in{R}$:
        \begin{equation}
            X_{r}=\{p\in\textrm{Spec{R}}\;|\;r\in{p}\}
        \end{equation}
        The topology is $\tau=\{X_{r}\;|\;r\in{R}\}$.
    \section{Notes from Jacobson Volume I}
        \subsection{Sets}
            Fibers of a function $f:X\rightarrow{Y}$ partition $X$.
            If $X$ is a set, $R$ an equivalence relation $X/R$ the quotient set,
            and if $f:X\rightarrow{Y}$ is a function such that for all $y\in{Y}$
            the fiber $f^{\minus{1}}[\{y\}]$ is the union of elements of the
            quotient set $X/R$, then there is an induced map
            $\tilde{f}:X/R\rightarrow{Y}$ where $[x]\mapsto{f}(x)$. That is,
            the equivalence class $[x]$ in $X/R$ is mapped to the image of
            the representative. This is well defined by our hypothesis about the
            fibers of $f$. It's useful when $f^{\minus{1}}[\{y\}]$ correspnds to
            precisely one element of $X/R$.
            \begin{example}
                Projection of $\nspace[2]$ onto the $x$ axis. The fibers are
                vertical lines.
            \end{example}
            Peano's axioms for $\mathbb{N}$. Principle of induction. Addition,
            associativity of addition, commutativity. Same with multiplication.
            Distributive law. Order: $a\leq{b}$ if there exists $x$ such that
            $a+x=b$. This is a total order by the Peano axioms. Trichotomy.
            Well ordering principle.
            \begin{theorem}
                If $a+c\leq{b}+c$, then $a\leq{b}$.
            \end{theorem}
            \begin{theorem}
                If $a\leq{b}$, then $a+c\leq{b}+c$.
            \end{theorem}
            \begin{theorem}
                If $c\in\mathbb{N}^{+}$, and if $ac\leq{b}c$, then $a\leq{c}$.
            \end{theorem}
            \begin{theorem}
                If $a\leq{b}$, then $ac\leq{b}c$.
            \end{theorem}
            Define $\mathbb{Z}$ to be $\mathbb{N}\times\mathbb{N}/R$ where $R$
            is the relation $(a,b)R(c,d)$ if and only if $a+d=b+c$. That is,
            equivalence classes are straight lines through the lattice
            $\mathbb{N}\times\mathbb{N}$ with slope 1. We take zero to be the
            equivalence class of $(x,x)$. This acts as zero, since
            $(a,b)+(x,x)=(a+x,b+x)$ and this is in the same equivalence class as
            $(a,b)$. The additive inverse of $(a,b)$ is $(b,a)$ since
            $(a,b)+(b,a)=(a+b,b+a)=(a+b,a+b)$ which is in the equivalence class
            $(x,x)$. We have an order $(a,b)>(c,d)$ if $a+d>b+c$. This is well
            defined on the equivalence classes of $\mathbb{N}\times\mathbb{N}$.
            \begin{theorem}
                If $z>0$, then $x>y$ if and only if $xz>yz$.
            \end{theorem}
            \begin{theorem}
                If $x<y$, then $\minus{y}<\minus{x}$.
            \end{theorem}
            \begin{proof}
                For if $x\in\mathbb{Z}$, then there are $a,b\in\mathbb{N}$ such
                that $x=[(a,b)]$ and similarly $y=[(c,d)]$. But if
                $x<y$, then $a+d<b+c$. But $\minus{x}=[(b,a)]$ and
                $\minus{y}=[(d,c)]$, and if $a+d<b+c$, then $d+a<c+b$, and hence
                $[(d,c)]<[(b,a)]$ and hence $\minus{y}<\minus{x}$.
            \end{proof}
            We can match $\mathbb{N}$ directly to the non-negative elements of
            $\mathbb{Z}$. That is, the equivalence classes of
            $\mathbb{N}\times\mathbb{N}/R$. Given this function $f$, we have
            $f(n+m)=f(n)+f(m)$, $f(nm)=f(n)f(m)$, and $f(n)>f(m)$ if and only if
            $n>m$. $\mathbb{Z}$ has least and greatest upper bound properties.
            Absolute value function: $\mathbb{Z}\rightarrow\mathbb{N}$ defined
            by $|x|=x$ if $x\geq{0}$ and $|x|=\minus{x}$.
            \begin{theorem}
                $|xy|=|x||y|$, $|x+y|\leq|x|+|y|$.
            \end{theorem}
            \begin{theorem}
                $0\leq{n}^{2}$
            \end{theorem}
            Euclid's division algorithm.
        \subsection{Groups and Semigroups}
            Cayley table of semigroup. Cayley table of $S_{2}$, $S_{3}$.
            Generalized associative law, proof by induction. Power law
            $a^{m}a^{n}=a^{m+n}$. If Abelian, any permutation of the product is
            equal. If Abelian, $(ab)^{n}=a^{n}b^{n}$. Uniqueness of identity,
            inverses. Inverse of inverse is self. Inverse of product. Groups.
            \begin{example}
                The roots of unity $z^{n}-1$ form a subgroup of $\mathbb{C}$
                with multiplication.
            \end{example}
            \begin{example}
                Rotations about a point $(x,y)\in\nspace[2]$ defined by:
                \begin{equation}
                    \begin{pmatrix}
                        x'\\
                        y'
                    \end{pmatrix}=
                    \begin{pmatrix*}[r]
                        \cos(\theta)&\minus\sin(\theta)\\
                        \sin(\theta)&\cos(\theta)
                    \end{pmatrix*}
                    \begin{pmatrix}
                        x\\
                        y
                    \end{pmatrix}
                \end{equation}
                form a group under rotation by an angle $\theta$. $\theta=0$ is
                the identity, and given a rotation $\theta$, the rotation
                $\minus\theta$ in the reverse direction serves as inverse.
            \end{example}
            \begin{example}
                Define $*$ on $\nspace[2]$ (excluding $(0,\cdot)$) by
                $(a,b)*(c,d)=(ac,bc+d)$. This is a group. The element $(1,0)$
                serves as identity since:
                \begin{align}
                    (1,0)*(c,d)&=(1\cdot{c},0\cdot{c}+d)=(c,d)\\
                    (a,b)*(1,0)&=(a\cdot{1},b\cdot{1}+0)=(a,b)
                \end{align}
                Lastly, given $(a,b)$, the inverse is $(1/a,\minus{b}/a)$ since:
                \begin{equation}
                    \big(a,b\big)*\big(\frac{1}{a},\minus\frac{b}{a}\big)
                        =\big(a\cdot\frac{1}{a},\frac{b}{a}-\frac{b}{a}\big)
                        =(1,0)
                \end{equation}
                and similarly for left multiplication. Hence, this is a group.
            \end{example}
            Only idempotent element of a group is the identity $x^{2}=x$ by
            cancellation law. Right identity with right inverses in a semigroup
            is a group.
            \begin{theorem}
                If $\monoid{G}$ is a semigroup with the Latin square property,
                then $\monoid{G}$ is a group.
            \end{theorem}
            \begin{proof}
                For by the Latin square property there is a right identity since
                we may solve $ax=a$. Let $e$ be such a right identity. But then
                there are right inverses since we may solve $ax=e$. Hence,
                $\monoid{G}$ is a semigroup with right identities and right
                inverses, and is therefore a group.
            \end{proof}
            \begin{theorem}
                If $G$ is a set, if $*$ is a cancellative operation on $G$,
                if $a\in{G}$, and if $f:G\rightarrow{G}$ is defined by
                $f(x)=a*x$, then $f$ is injective.
            \end{theorem}
            \begin{proof}
                For suppose not. Then there are $x_{1},x_{2}\in{G}$ such that
                $x_{1}\ne{x}_{2}$ and $f(x_{1})=f(x_{2})$. And thus
                $a*x_{1}=a*x_{2}$. But $*$ is cancellative and hence
                $x_{1}=x_{2}$, a contradiction.
            \end{proof}
            \begin{theorem}
                If $\monoid{G}$ is a finite cancellative semigroup, then it is
                a group.
            \end{theorem}
            \begin{proof}
                For let $a\in{G}$ and define $f:G\rightarrow{G}$ by
                $f(x)=a*x$. Then $f$ is injective, and since $G$ is finite it is
                surjective. Hence there is an $e\in{G}$ such that $a*e=a$. But
                then for all $b\in{G}$ we have $a*b=(a*e)*b=a*(e*b)$ and since
                $*$ is cancellative we have $e*b=b$, and therefore $e$ is a
                left identity. Given $b\in{G}$ the function $g:G\rightarrow{G}$
                defined by $g(x)=x*b$ is surjective, and hence there is a
                $c\in{G}$ such that $g(c)=e$ and thus $c*b=e$. Thus there are
                left inverses, and so $\monoid{G}$ is a group.
            \end{proof}
            Subsemigroup, subgroup.
            \begin{theorem}
                If $\monoid{G}$ is a monoid, and if $H\subseteq{G}$ is the set
                of all invertible elements of $G$, then $\monoid{H}$ is a
                subgroup.
            \end{theorem}
            \begin{proof}
                It suffices to show that $a*b$ is contained in $H$ for all
                $a,b\in{H}$. But if $a$ and $b$ are invertible, then $a*b$ is
                invertible since
                $(a*b)^{\minus{1}}=b^{\minus{1}}*a^{\minus{1}}$. Similarly, the
                identity is contained in $H$ since $e*e=e$ so $e$ is invertible.
            \end{proof}
            \begin{theorem}
                $H$ is a subgroup of $G$ if and only if for all $a,b\in{H}$,
                $a*b^{\minus{1}}\in{H}$.
            \end{theorem}
            \begin{proof}
                If $H$ is a subgroup, then it is closed to inverses and it is
                closed to multiplication, so $a*b^{\minus{1}}\in{H}$. If this
                equation holds, then the identity is in $H$ since
                $a*a^{\minus{1}}\in{H}$. Hence it is closed to inverses since
                $b^{\minus{1}}=e*b^{\minus{1}}\in{H}$. Thus it is closed to
                multiplication since $a*b=a*(b^{\minus{1}})^{\minus{1}}\in{H}$.
            \end{proof}
            \begin{theorem}
                If $\monoid{G}$ is a finite group, and if $H\subseteq{G}$ is a
                subsemigroup, then $H$ is a subgroup of $G$.
            \end{theorem}
            \begin{theorem}
                The intersection of subgroups is a subgroup.
            \end{theorem}
            \begin{theorem}
                If $\monoid{G}$ is a group, if $a\in{G}$, and if $H$ is the set
                of all elements of $G$ that commute with $a$, then $H$ is a
                subgroup of $G$.
            \end{theorem}
            Def isomorphism, example of $\exp$ and $\ln$. Isomorphism is an
            equivalence relation.
            \begin{theorem}
                If $\monoid[G]{G}$ and $\monoid[H]{H}$ are groups, and if
                $\varphi:G\rightarrow{G}$ is an isomorphism, then
                $\varphi(e_{G})=e_{H}$.
            \end{theorem}
            \begin{proof}
                For we have:
                \begin{equation}
                    \varphi(a)=\varphi(a*_{G}e_{G})
                              =\varphi(a)*_{H}\varphi(e_{G})
                \end{equation}
                Hence by the cancellation law, $\varphi(e_{G})=e_{H}$.
            \end{proof}
            \begin{theorem}
                If $\monoid[G]{G}$ and $\monoid[H]{H}$ are groups, and if
                $\varphi:G\rightarrow{G}$ is an isomorphism, then
                $\varphi(a^{\minus{1}})=\varphi(a)^{\minus{1}}$.
            \end{theorem}
            \begin{proof}
                For:
                \begin{equation}
                    \varphi(a)*_{H}\varphi(a^{\minus{1}})
                        =\varphi(a*_{G}a^{\minus{1}})
                        =\varphi(e_{G})
                        =e_{H}
                \end{equation}
                and since inverses are unique, we have
                $\varphi(a^{\minus{1}})=\varphi(a)^{\minus{1}}$.
            \end{proof}
            \begin{example}
                $f:\nspace[]\rightarrow\nsphere[1]$ defined by
                $f(\theta)=\exp(i\theta)$ is a homomorphism but not an
                isomorphism since it is not bijective. $2\pi{n}$ gets mapped to
                $(1,0)$ for all $n\in\mathbb{Z}$.
            \end{example}
            Symmetric groups.
            \begin{example}
                Let $\alpha,\beta:\mathbb{Z}_{5}\rightarrow\mathbb{Z}_{5}$ be
                defined as follows:
                \twocolumneq{%
                    \alpha=
                    \begin{pmatrix}
                        0&1&2&3&4\\
                        1&2&0&4&3
                    \end{pmatrix}
                }{%
                    \beta=
                    \begin{pmatrix}
                        0&1&2&3&4\\
                        0&2&3&4&1
                    \end{pmatrix}
                }
                Let's compute $\alpha\circ\beta$ and $\beta\circ\alpha$.
                Performing $\alpha\circ\beta$, we compute $\beta$ first, and
                hence $0\mapsto{0}$, and then $0\mapsto{1}$ under $\alpha$.
                We do the same for the other columns:
                \begin{equation}
                    \alpha\circ\beta=
                    \begin{pmatrix}
                        0&1&2&3&4\\
                        0&2&3&4&1\\
                    \end{pmatrix}
                    \begin{pmatrix}
                        0&1&2&3&4\\
                        1&2&0&4&3
                    \end{pmatrix}
                    =
                    \begin{pmatrix}
                        0&1&2&3&4\\
                        1&0&4&3&2
                    \end{pmatrix}
                \end{equation}
                and similarly we have $\beta\circ\alpha$:
                \begin{equation}
                    \beta\circ\alpha=
                    \begin{pmatrix}
                        0&1&2&3&4\\
                        1&2&0&4&3
                    \end{pmatrix}
                    \begin{pmatrix}
                        0&1&2&3&4\\
                        0&2&3&4&1
                    \end{pmatrix}
                    =
                    \begin{pmatrix}
                        0&1&2&3&4\\
                        2&3&0&1&4
                    \end{pmatrix}
                \end{equation}
            \end{example}
            \begin{example}
                Suppose we have $\identity{\mathbb{Z}_{3}}$ and the two
                functions $\alpha,\beta:\mathbb{Z}_{3}\rightarrow\mathbb{Z}_{3}$
                by:
                \twocolumneq{%
                    \alpha=
                    \begin{pmatrix}
                        0&1&2\\
                        1&2&0
                    \end{pmatrix}
                }{%
                    \beta=
                    \begin{pmatrix}
                        0&1&2\\
                        2&0&1
                    \end{pmatrix}
                }
                Then $(\identity{\mathbb{Z}_{3}},\alpha,\beta)$ form a group
                under function composition. Since composition is associative and
                we're including the identity, we need only show that $\circ$ is
                a valid binary operation on this set and that we have inverses.
                But $\alpha$ and $\beta$ are inverses of each other. This group
                is actually just $\mathbb{Z}/3\mathbb{Z}$.
            \end{example}
            \begin{example}
                The functions $f:\nspace[]\rightarrow\nspace[]$ defined by
                $f(x)=ax+b$ with $a\ne{0}$ is a group under composition. The
                identity is contained in this, setting $a=1$ and $b=0$. Moreover
                if $f=ax+b$, then $g=x/a-b/a$ serves as inverse. This group is
                isomorphic to the group on $\nspace[2]$ minus $(0,\cdot)$ with
                the operation $(a,b)*(c,d)=(ac,bc+d)$.
            \end{example}
            \begin{ftheorem}{Cayley's Theorem}{Cayleys_Theorem}
                If $\monoid{G}$ is a group, then there is a subgroup
                $H\subset{S}_{G}$ of the symmetric group
                $\monoid[][\circ]{S_{G}}$ such that $\monoid{G}$ is isomorphic
                to $(H,\circ|_{H})$.
            \end{ftheorem}
            \begin{bproof}
                For let $H\subseteq{S}_{G}$ be defined by:
                \begin{equation}
                    H=\{\,f\in{S}_{G}\;|\;
                        \exists_{a\in{G}}\big(f(x)=a*x\big)\,\}
                \end{equation}
                Let $\varphi:G\rightarrow{H}$ be defined by
                $\varphi(a)=f$ where $f:G\rightarrow{G}$ is the function defined
                by $f(x)=a*x$. This is an isomorphism.
            \end{bproof}
            \begin{theorem}
                If $\monoid{G}$ is a finite group, then there is an
                $n\in\mathbb{N}$ such that $G$ is isomorphic to a subgroup of
                $S_{n}$.
            \end{theorem}
            We can form a different subgroup of $S_{G}$ that is isomorphic to
            $G$ by considering functions of the form $f(x)=x*a$. That is, by
            multiplying $x$ on the left rather than the right. If we label this
            group $H_{L}$, and the other group $H_{R}$, it turns out all
            elements of $H_{L}$ commute with all elements of $H_{R}$, and vice
            versa. The converse holds, if $f$ commutes with all left
            multiplications, then $f$ is a right multiplying, and if $f$
            commutes with all right multiplications, then $f$ is a left
            multiplication.
            \subsubsection{Cyclic Groups}
                Group generated by set (Intersection of all subgroups
                containing set). This is also the set of all finite
                multiplications of elements in $M$, combined with inverses.
                Cyclic group is group generated by a single element.
                \begin{example}
                    $\mathbb{Z}$ is cycle with generator 1, and the set of roots
                    of unity in $\mathbb{C}$ are cyclic with generator
                    $\exp(2\pi{i}/n)$.
                \end{example}
                \begin{theorem}
                    If $\monoid{G}$ is cyclic, if $a\in{G}$, and if
                    $f:\mathbb{Z}\rightarrow{G}$ is defined by $f(n)=a^{n}$ is
                    a homomorphism.
                \end{theorem}
                If $\monoid{G}$ is cyclic generated by $a$, then there either
                there is a least $n\in\mathbb{N}^{+}$ such that $a^{n}$ is a
                unital element, or $a^{n}$ is distinct for all
                $n\in\mathbb{N}^{+}$. Using the Euclidean division algorithm we
                have $a^{m}=a^{qn+r}=a^{qn}a^{r}=a^{r}$. Thus, every cyclic
                group is either $\mathbb{Z}$ or $\mathbb{Z}/n\mathbb{Z}$ for
                some $n\in\mathbb{N}^{+}$. The subgroups of a cyclic group
                generated by $a$ are the subgroups generated by $a^{k}$ for some
                $k\in\mathbb{N}$. If the group is finite, the $k$ the divide
                the order of $a$ give all of the subgroups. That is, if
                $G$ is cyclic of order $r$, and if $d$ is the number of divisors
                of $r$, then there are $d$ subgroups of $G$.
                \begin{example}
                    Consider $\monoid[][+]{\mathbb{Z}/12\mathbb{Z}}$. There are
                    12 elements in this group and it is cyclic and generated by
                    the equivalence class $[1]$. For every divisor of 12 we get
                    a different subgroup. The divisors are 1, 2, 3, 4, 6, and
                    12. These subgroups correspond to $\mathbb{Z}_{12}$,
                    $\mathbb{Z}_{6}$, $\mathbb{Z}_{4}$, $\mathbb{Z}_{3}$,
                    $\mathbb{Z}_{2}$, and $\mathbb{Z}_{1}$, respectively.
                \end{example}
                \begin{theorem}
                    If $\monoid{G}$ is cyclid or order $n\in\mathbb{N}^{+}$, and
                    if $\varphi$ is the Euler totient function, then there are
                    $\varphi(n)$ elements of $G$ that generated $G$.
                \end{theorem}
                A cycle in $\mathbb{Z}_{n}$ is a function
                $f:\mathbb{Z}_{n}\rightarrow\mathbb{Z}_{n}$ such that
                $f(k)=k+1$ for all $k<m$, $f(m)=0$, and $f(k)=k$ for all $k>m$.
                A transposition is a function $f:X\rightarrow{X}$ such that
                there exists $x,y\in{X}$ such that $f(x)=y$ and $f(y)=x$, and
                for all other $z\in{X}$ we have $f(z)=z$. Every cycle can be
                written as product of transpositions. The number of products is
                either always even or always odd, no cycle can be written as
                both an even product and an odd product. The inverse of even is
                even, as is the product of even, and hence we have the subgroup
                of even transpositions. This is the alternating group.
                \begin{example}
                    There are $4!=12$ elements in $S_{4}$. We can write these
                    out as cycles. There are $4!/2=6$ even cycles.
                \end{example}
                \begin{theorem}
                    If $n\geq{3}$, then any element of $A_{n}$ is the product of
                    three cycles $(abc)$.
                \end{theorem}
            \subsubsection{Coset Decomposition}
                If $X$ is a set, and if $G\subseteq{S}_{X}$ is a subgroup of the
                permutation group on $X$, and if $R$ is the relation
                $xRy$ if and only if there is an $f\in{G}$ such that
                $y=f(x)$. Then $R$ is an equivalence relation. It is reflexive
                since the identity mapping is contained in $G$ since $G$ is a
                group. Also by this fact the inverse mapping $f^{\minus{1}}$ is
                contained in $G$ and hence $R$ is symmetric. Moreover, since the
                composition of permutations is a permutation, if $xRy$ and
                $yRz$, then $y=f(x)$ and $z=g(y)$, and hence $z=g(f(x))$ so
                $xRz$. $G$ is transitive in $X$ if for all $x,y\in{X}$ it is
                true that $xRy$. The equivalence classes of $X/R$ partition $X$
                and these are called transitivity sets of $X$ with respect to
                $G$. If $H\subseteq{G}$ is a subgroup, and if $H_{R}$ is the
                elements of $G$ that are right multiplications of elements in
                $H$. Then $H_{R}$ is a subgroup of $G_{R}$, the group of all
                right multiplications. Thus $H_{R}$ acts on $G$. We can write
                $x\equiv{y}$ if there is an $h\in{H}_{R}$ such that $y=xh$. In
                other words, $x^{\minus{1}}y\in{H}_{R}$. This gives the right
                coset of $x$ relative to $H$. We define the product of subsets
                of a group as follows:
                \begin{equation}
                    AB=\{\,a*b\in{G}\;|\;a\in{A}\textrm{ and }b\in{B}\,\}
                \end{equation}
                Coset of $x\in{G}$ with respect to $H$ is then just $xH$.
                \begin{theorem}
                    If $\monoid{G}$ is a group, if $H\subseteq{G}$ is a
                    subgroup, if $x,y\in{G}$, then either $xH=yH$ or
                    $xH\cap{y}H$.
                \end{theorem}
                \begin{proof}
                    For suppose $xH\ne{y}H$ and let $z\in{x}H\cap{y}H$. Then
                    there is an $h_{1}\in{H}$ such that $z=xh_{1}$ and
                    $h_{2}\in{H}$ such that $z=yh_{2}$. But then
                    $xh_{1}=yh_{2}$ and thus $x=yh_{2}h_{1}^{\minus{1}}$, and
                    so $x\in{y}H$, and hence $xH\subseteq{y}H$. And similarly,
                    $yH\subseteq{x}H$ and thus $xH=yH$, a contradiction. Thus,
                    $xH\cap{y}H=\emptyset$.
                \end{proof}
                \begin{example}
                    Give $n\in\mathbb{N}^{+}$, and the subgroup of $\mathbb{Z}$
                    generated by $n$, the cosets of this subgroup are the
                    sets:
                    \begin{subequations}
                        \begin{align}
                            0+\langle{n}\rangle
                                &=\{\,0,\,\pm{n},\,\pm{2}n,\,\dots\,\}\\
                            1+\langle{n}\rangle
                                &=\{\,1,\,1\pm{n},\,1\pm{2}n,\,\dots\,\}\\
                            2+\langle{n}\rangle
                                &=\{\,2,\,2\pm{n},\,2\pm{3}n,\,\dots\,\}\\
                            &\hdots\\
                            n-1+\langle{n}\rangle
                            &=\{\,n-1,\,n-1\pm{n},\,n-1\pm{3}n,\,\dots\,\}
                        \end{align}
                    \end{subequations}
                \end{example}
                \begin{theorem}
                    If $\monoid{G}$ is a group, if $x,y\in{G}$, and if
                    $H\subseteq{G}$ is a subgroup, then
                    $\cardinality{xH}=\cardinality{yH}$. That is, there is a
                    bijection $f:xH\rightarrow{y}H$.
                \end{theorem}
                \begin{proof}
                    Define $f$ by $f(a)=x^{\minus{1}}ya$.
                \end{proof}
                Since $eH$ is a right coset of $H$, every coset of $H$ has the
                same cardinality of $H$. Left cosets exist. The left coset
                $Hx$ is uniquely determined by $x^{\minus{1}}H$. Hence, the set
                of all left and right cosets have the same cardinal number. This
                is the index of $H$ in $G$. This gives rise to Lagrange's
                theorem.
                \begin{ftheorem}{Lagrange's Theorem}{Lagranges_Theorem}
                    If $\monoid{G}$ is a finite group, and if $H\subseteq{G}$
                    is a subgroup, then $\cardinality{H}$ divides
                    $\cardinality{G}$.
                \end{ftheorem}
                \begin{bproof}
                    For $\{\,xH\;|\;x\in{G}\,\}$ partitions $G$, and each
                    element has cardinality $\cardinality{H}$. If there are $r$
                    such distinct elements of this set, then $G$ thus has
                    $r\cdot\cardinality{H}$ elements.
                \end{bproof}
                \begin{theorem}
                    If $A_{n}$ is the alternating group on $n$ elements, then
                    $\cardinality{A_{n}}=n!/2$.
                \end{theorem}
                \begin{theorem}
                    If $\monoid{G}$ is a finite group, if $n=\cardinality{G}$,
                    and if $x\in{G}$, then $x^{n}=e$.
                \end{theorem}
                \begin{proof}
                    For let $m$ be the order of $x$. Then by Lagrange's theorem
                    we have $mr=n$, and hence
                    $x^{n}=x^{mr}=(x^{m})^{r}=e^{r}=e$.
                \end{proof}
                \begin{example}
                    Consider the subgroup $H\subseteq{S}_{3}$ defined by
                    $H=\{e,\,(1\;2)\}$. That is, the subgroup defined by the
                    identity and the transposition $(1\;2)$. This is a subgroup
                    since $(1\;2)(1\;2)=e$ and hence $H$ is closed under
                    multiplication and inverses. Coset decomposition of $S_{3}$
                    by $H$ is obtained by multiplying $H$ by all elements of
                    $S_{3}$ and obtaining equivalence classed. Since
                    $e$ and $(1\;2)$ are contained in $H$, we can skip these.
                    The next one is $(1\;3)$. Multiplying by $e$ gives $(1\;3)$
                    back, and multiplying by $(1\;2)$ gives us
                    $(1\;3)(1\;2)=(1\;2\;3)$, so have:
                    \begin{equation}
                        (1\;3)H=\{\,(1\;3),\,(1\;2\;3)\,\}
                    \end{equation}
                    Next, $(2\;3)$ and we have $(2\;3)(1\;3)=(2\;3\;1)$, hence:
                    \begin{equation}
                        (2\;3)H=\{\,(2\;3),\,(2\;3\;1)\,\}
                    \end{equation}
                    Two left, $(1\;2\;3)=(2\;3\;1)=(3\;1\;2)$ and
                    $(1\;3\;2)=(3\;2\;1)=(2\;1\;3)$.
                    \begin{subequations}
                        \begin{align}
                            (1\;2\;3)H&=\{\,(1\;2\;3),\,(1\;3)\}\\
                            (1\;3\;2)H&=\{\,(1\;3\;2),\,(2\;3)\,\}
                        \end{align}
                    \end{subequations}
                    Since $(1\;3\;2)$ and $(2\;3\;1)$ are the same cycle, we see
                    that these two new computations are the same as previous
                    ones.
                \end{example}
                \begin{theorem}
                    If $\monoid{G}$ is a group, if $H_{1},H_{2}\subseteq{G}$ are
                    subgroups, then $x(H_{1}\cap{H}_{2})=(xH_{1})\cap(xH_{2})$.
                \end{theorem}
                \begin{ftheorem}{Poincare's Index Theorem}{Poincare_Index}
                    If $\monoid{G}$ is a group, if $H_{1},H_{2}\subseteq{G}$ are
                    subgroups, and if $H_{1}$ and $H_{2}$ have finite index
                    in $G$, then $H_{1}\cap{H}_{2}$ has finite index in $G$.
                \end{ftheorem}
            \subsubsection{Normal Subgroup}
                Given a group $\monoid{G}$ with a subgroup $H\subseteq{G}$, it
                is useful to know whether or not $H$ is \textit{closed} under
                multiplication modulo $H$. That is, given $x,y\in{G}$, and
                $[x],[y]\in{G}/H$, we wish to determine with $[x*y]=[x]*[y]$.
                This says that if $a\in{x}H$ and $b\in{y}H$, then $a*b\in{x}yH$.
                We can further write this by saying that
                $(xH)(yH)\subseteq{x}*H$, which is further equivalent to
                $HxH\subseteq{x}H$, and here is the property we wish to
                capitalize on.
                \begin{theorem}
                    If $\monoid{G}$ is a group, and if $H\subseteq{G}$ is a
                    subgroup, then $H^{2}=H$.
                \end{theorem}
                \begin{fdefinition}{Normal Subgroup}{Normal_Subgroup}
                    A normal subgroup of a group $\monoid{G}$ is a subgroup
                    $H\subseteq{G}$ such that for all $g\in{G}$ and for all
                    $h\in{H}$ it is true that $g^{\minus{1}}hg\in{H}$.
                \end{fdefinition}
                If $H$ is normal, then left and right cosets are identical.
                The subsets of a normal subgroup are closed under set
                multiplication, and hence $G/H$ forms a group under set
                multiplication. This is the quotient group.
                \begin{theorem}
                    If $\monoid{G}$ is a group, and if $H\subseteq{G}$ is a
                    subgroup of index 2, then $H$ is a normal subgroup.
                \end{theorem}
                \begin{proof}
                    Since $H$ has index 2 there are two distinct cosets. Since
                    $eH=H$ is one of the cosets, suppose $xH$ is the other.
                    Let $g\in{G}$ and $h\in{H}$. Since $xH$ is a distinct coset,
                    and since cosets are disjoint, we have that $x\notin{H}$.
                    But $H\cup{x}H=G$, and hence either $g^{\minus{1}}hg\in{H}$
                    or $g^{\minus{1}}hg\in{x}H$. Suppose the latter. But then
                    there is a $y\in{H}$ such that $xy=g^{\minus{1}}hg$.
                    But then $x=(g^{\minus{1}}hg)y^{\minus{1}}$. But $H$ is a
                    normal subgroup and therefore $y^{\minus{1}}\in{H}$. IDK.
                \end{proof}
            \subsubsection{Homomorphisms}
                Def homomorphism.
                \begin{example}
                    If we have a normal subgroup $N$ of a group $\monoid{G}$ and
                    map $x\in{G}$ to the coset $xN$ in the quotient group
                    $G/N$, then this gives rise to a homomorphism.
                \end{example}
                Homomorphisms need not be injective nor surjective.
                \begin{theorem}
                    If $\monoid[G]{G}$ and $\monoid[H]{H}$ are groups, and if
                    $\varphi:G\rightarrow{H}$ is a homomorphism, then
                    the image $\varphi[G]$ is a subgroup of $H$.
                \end{theorem}
                \begin{theorem}
                    If $\monoid[G]{G}$ and $\monoid[H]{H}$ are groups, and if
                    $\varphi:G\rightarrow{H}$ is a homomorphism, then $\varphi$
                    is an isomorphism if and only if it is surjective and
                    $\varphi^{\minus{1}}[e_{H}]=\{e_{G}\}$.
                \end{theorem}
                \begin{proof}
                    One direction is clear since is $\varphi$ is an isomorphism,
                    then it is a bijection. In the other direction, suppose
                    $\varphi^{\minus{1}}[\{e_{H}\}]=\{e_{G}\}$. For if it is not
                    bijective, then it is not injective, and hence there are
                    $a,b\in{G}$ such that $\varphi(a)=\varphi(b)$ and $a\ne{b}$.
                    But then:
                    \begin{equation}
                        e_{H}=\varphi(a)*_{H}\varphi(b)^{\minus{1}}
                             =\varphi(a*_{G}b^{\minus{1}})
                    \end{equation}
                    thus $a*_{G}b^{\minus{1}}\in\varphi^{\minus{1}}[\{e_{H}\}]$,
                    and so $a*_{G}b^{\minus{1}}=e_{G}$. But then $a=b$, a
                    contradiction. Thus, $\varphi$ is bijective.
                \end{proof}
                \begin{theorem}
                    If $\monoid[G]{G}$ and $\monoid[H]{H}$ are groups, if
                    $\varphi:G\rightarrow{H}$ is a homomorphism, then
                    $\varphi^{\minus{1}}[\{e_{H}\}]$ is a normal subgroup of
                    $G$.
                \end{theorem}
                \begin{proof}
                    For let $N=\varphi^{\minus{1}}[\{e_{H}\}]$. Then
                    $e_{G}\in{N}$ since $\varphi(e_{G})=e_{H}$. $N$ is closed
                    under products since
                    $\varphi(a*_{G}b)=\varphi(a)*_{H}\varphi(b)=e_{H}$. Next,
                    since $e_{H}^{\minus{1}}=e_{H}$ we have that $N$ is closed
                    to inverses. So $N$ is a subgroup. It is normal for if
                    $g\in{G}$ and $x\in{N}$, then:
                    \begin{subequations}
                        \begin{align}
                            \varphi(g^{\minus{1}}*_{G}x*_{G}g)
                            &=\varphi(g^{\minus{1}})*_{H}
                                \varphi(x)*_{H}\varphi(g)\\
                            &=\varphi(g^{\minus{1}})*e_{H}*\varphi(g)\\
                            &=\varphi(g^{\minus{1}})*\varphi(g)\\
                            &=e_{H}
                        \end{align}
                    \end{subequations}
                    hence $g^{\minus{1}}*x*g\in{N}$.
                \end{proof}
                \begin{example}
                    The function $f:\nsphere[1]\rightarrow\nsphere[1]$ defined
                    by $f(\theta)=\exp(ki\theta)$ is a homomorphism. The
                    kernel is all $\theta$ such that $k\theta=2\pi{n}$ for some
                    $n\in\mathbb{Z}$.
                \end{example}
                \begin{theorem}
                    The composition of homomorphisms is a homomorphism.
                \end{theorem}
                If $G$ is a group, $N$ a normal subgroup, $\varphi$ a
                homomorphism from $G/N$ to a group $H$, and if $\pi$ is the
                canonical quotient mapping, the $\varphi\circ\pi$ is a
                homomorphism and the kernel contains $N$. The converse is true.
                If $G$ is a group, and if $\varphi:G\rightarrow{H}$ is a
                homomorphism, then given a normal subgroup $N\subseteq{G}$ that
                is contained in the kernel of $\varphi$ we have that for any two
                $a,b\in{G}$ that are contained in the same coset of $N$, since
                $N$ is normal there is an $h\in{N}$ such that $a=bh$. But then
                $\varphi(a)=\varphi(bh)=\varphi(b)\varphi(h)$, and
                $\varphi(h)=e_{H}$, so $\varphi(a)=\varphi(b)$. That is, there
                is now a well defined map $\tilde{\varphi}:G/N\rightarrow{H}$
                that maps the coset $aN$ to $\varphi(a)$. Moreover,
                $\tilde{\varphi}$ is a homomorphism. If
                $\tilde{\varphi}(aN)=e_{H}$, then $\varphi(a)=e_{H}$ by
                definition. Moreover, if $\varphi(a)=e_{H}$ then
                $\tilde{\varphi}(aH)=e_{H}$, again by definition. Thus the
                kernel of $\tilde{\varphi}$ is cosets $aN$ where $a$ is in the
                kernel of $\varphi$. $\tilde{\varphi}$ is thus injective if and
                only if the kernel of $\varphi$ is equal to $N$.
                \begin{theorem}
                    If $\monoid[G]{G}$ and $\monoid[H]{H}$ are groups, if
                    $\varphi:G\rightarrow{H}$ is a homomorphism, if
                    $N\subseteq{G}$ is a normal subgroup, if
                    $N\subseteq\varphi^{\minus{1}}[\{e_{H}\}]$, then there is a
                    homomorphism $\tilde{\varphi}:G/N\rightarrow{H}$ such that
                    $\varphi=\tilde{\varphi}\circ\pi$, where
                    $\pi:G\rightarrow{G}/N$ is the canonical quotient mapping.
                    $\tilde{\varphi}$ is an isomorphism if and only if
                    $N=\varphi^{\minus{1}}[\{e_{H}\}]$.
                \end{theorem}
                \begin{ftheorem}{Fundamental Theorem of Homomorphisms}
                                {Fundamental_Theorem_of_Homomorphisms}
                    If $\monoid{G}$ is a group, if $N\subseteq{G}$ is a normal
                    subgroup, then there is a homomorphism
                    $\varphi:G\rightarrow{G}$ such that
                    $N=\varphi^{\minus{1}}[\{e\}]$.
                \end{ftheorem}
                \begin{example}
                    $\mathbb{R}/\mathbb{Z}$ is isomorphic to $\nsphere[1]$.
                \end{example}
                \begin{example}
                    Given a cyclic group generated by $a$, then function
                    $f:\mathbb{Z}\rightarrow{G}$ defined by $f(n)=a^{n}$ is a
                    surjective homomorphism. Hence $G$ is isomorphic to
                    $\mathbb{Z}/N$, where $N$ is the kernel of $f$. This also
                    shows that any two cyclic groups of the same cardinality
                    are isomorphic.
                \end{example}
                Endomorphism is homomorphism $\varphi:G\rightarrow{G}$. That is,
                a homomorphism from a group to itself. Automorphism is a
                bijective endomorphism. That is, an isomorphism
                $\varphi:G\rightarrow{G}$. Comp of endo is endo, comp of auto is
                auto. Endomorphisms form subsemigroup of group of functions
                $f:G\rightarrow{G}$. Moreover, this has identity so it is a
                monoid. Automorphisms, since bijective, have inverses and hence
                form a group. Moreover, this is the group of all invertible
                elements of the group of functions $f:G\rightarrow{G}$.
                \begin{theorem}
                    If $\monoid{G}$ is a group, if $g\in{G}$, and if
                    $\varphi:G\rightarrow{G}$ is defined by
                    $\varphi(x)=g^{\minus{1}}*x*g$, then $\varphi$ is an
                    automorphism.
                \end{theorem}
                This is called the inner automorphism defined by $a$. The set
                of such automorphisms forms a normal subgroup of the group of
                automorphisms. The quotient group is thus called the group of
                outer automorphisms. The kernel of the function
                $\varphi:G\rightarrow\autgroup{G}$ defined by
                $a\mapsto{f}$, where $f(x)=a^{\minus{1}}*x*a$, is called the
                center. This is the set of all elements $x\in{G}$ such that
                $x*y=y*x$ for all $y\in{G}$. The group of inner automorphisms is
                isomorphic to the quotient group of $G$ by its center.
                $a\mapsto{a}^{\minus{1}}$ is an automorphism if and only if
                $G$ is Abelian. $a\mapsto{a}^{n}$ is an endomorphism.
                \begin{example}
                    If $G$ is a finite cyclic group and $\cardinality{G}=n$,
                    then there are $\varphi(n)$ automorphisms of $G$, where
                    $\varphi$ is the Euler totient function. To see this note
                    that a generator of $G$ must map to a generator if the
                    function is to be a generator. The generators are precisely
                    the elements that have order $n$ in $G$, and correspond to
                    coprime elements of $\mathbb{Z}_{n}$ with respect to $n$.
                \end{example}
            \subsubsection{Conjugate Classes}
                The inner automorphisms determine conjugacy classes of $G$.
                $x$ and $y$ are conjugate if there is a $g\in{G}$ such that
                $x=g^{\minus{1}}yg$. The conjugacy class of $x$ is equal to
                $\{x\}$ if and only if $x$ is in the center of $G$ since then
                $g^{\minus{1}}xg=xg^{\minus{1}}g=xe=x$.
                \begin{example}
                    Conjugacy classes of $S_{n}$ correspond to the partitions of
                    the integer $n$. This shows for $n>2$ that $S_{n}$ is not
                    Abelian since the center is simply the idenity.
                \end{example}
        \subsection{Rings, Integral Domains, and Fields}
            Def rng, ring. Examples: $\mathbb{Z}$, $\mathbb{Q}$,
            $\nspace[]$, $\mathbb{C}$, $\mathbb{Q}[\sqrt{2}]$,
            $\mathbb{Z}[\sqrt{2}]$, $\mathbb{Z}[i]$,
            $\mathbb{Z}/n\mathbb{Z}$, $\funcspace{\nspace[]}$ with function
            addition and composition. $n(a+b)=na+nb$ for
            $n\in\mathbb{Z}$ and $a,b\in{R}$, also $(nm)a=n(ma)$.
            Generalized distributive law:
            \begin{equation}
                \Big(\sum_{j\in\mathbb{Z}_{m}}a_{j}\Big)
                \Big(\sum_{k\in\mathbb{Z}_{n}}b_{k}\Big)
                =\sum_{j\in\mathbb{Z}_{m}}\sum_{k\in\mathbb{Z}_{n}}a_{j}b_{k}
            \end{equation}
            Mult by zero, commutativity of $\minus{1}$, $n(ab)=(na)b=a(nb)$.
            Binomial theorem. If $\ring{R}$ is a rng, and if $\cdot$ contains a
            left cancellative element: $ae=be$ implies $a=b$, then $+$ is
            commutative. Def int domain. Ring of continuous functions is not an
            integral domain. Integral domain has cancellation. Converse is true.
            Division ring is ring $\ring{R}$ where $(R\setminus\{0\},\cdot)$ is
            a group. That is, non-zero elements have inverse elements. Field is
            commutative division ring. Division ring is integral domain. The
            converse need not hold, $\mathbb{Z}$ is an example. Division rings
            satisfy the Latin square property for non-zero elements:
            $a\cdot{x}=b$ has a unique solution for $a\ne{0}$. Mult group of
            ring. If $a$ is invertible, $\minus{a}$ is too. Finite integral
            domain is a division ring. If $\ring{R}$ is an integral domain,
            if $x\ne{0}$, and if $x^{2}=x$, then $x=1$. If $\ring{R}$ is int
            dom, $z^{n}=0$, then $z=0$. Rng with left identity has a right
            identity (It's a ring). If $\ring{R}$ is a ring, if $x\in{R}$ has
            multiple right inverses, then it has a left zero divisor and is not
            invertible.
            \begin{ftheorem}{Kaplansky's Theorem}{Kaplanskys_Theorem}
                If $\ring{R}$ is a ring, if and $x\in{R}$ has more then one
                right inverse, then it has infinitely many.
            \end{ftheorem}
            \subsubsection{Quasi Regularity}
                If $\ring{R}$ is a ring, and if $a$ has a right inverse $b$,
                then suppose $a=1-x$ and $b=1-y$. Then we have:
                \begin{equation}
                    1=a\cdot{b}=(1-x)\cdot(1-y)=1-x-y+xy
                \end{equation}
                and hence by cancellation we have $y+x=xy$, or in other words
                $x+y-xy=0$. Here's a condition of multiplicative inverse that
                does not rely on the notion of a unital element 1.
                \begin{fdefinition}{Right Quasi Regular}{Right_Quasi_Regular}
                    A right quasi regular element of a rng $\ring{R}$ is an
                    element $x\in{R}$ such that there exists a $y\in{R}$ with
                    $x+y-xy=0$.
                \end{fdefinition}
                If $\ring{R}$ is an actual ring (rng with identity), then given
                a quasi regular element $r\in{R}$, $1-r$ is invertible.
                \begin{theorem}
                    If $\ring{R}$ is a ring, and if $r\in{R}$ is quasi regular,
                    then $1-r$ is invertible with respect to $\cdot$.
                \end{theorem}
                \begin{proof}
                    For since $r$ is quasi regular, there is an $s\in{R}$ such
                    that $r+s=rs$. But then:
                    \begin{equation}
                        (1-r)(1-s)=1-r-s+rs=1-rs+rs=1
                    \end{equation}
                    and hence $1-r$ is invertible.
                \end{proof}
                This gives rise to the definition of the circle composition of a
                rng.
                \begin{fdefinition}{Circle Composition of Rng}
                                   {Circle_Comp_of_Rng}
                    The circle composition of a rng $\ring{R}$ is the binary
                    operation $\circ$ on $R$ defined by:
                    \begin{equation*}
                        a\circ{b}=a+b-a\cdot{b}
                    \end{equation*}
                \end{fdefinition}
                \begin{theorem}
                    If $\ring{R}$ is a rng, if $\circ$ is the circle composition
                    of $R$, then $\monoid[][\circ]{R}$ is a monoid.
                \end{theorem}
                \begin{proof}
                    For $\circ$ is associative since:
                    \begin{subequations}
                        \begin{align}
                            a\circ(b\circ{c})&=a\circ\big(b+c-bc)\\
                            &=a+(b+c-bc)-a(b+c-bc)\\
                            &=a+b+c-bc-ab-ac+abc\\
                            &=(a+b-ab)+c-(a+b-ab)c\\
                            &=(a+b-ab)\circ{c}\\
                            &=(a\circ{b})\circ{c}
                        \end{align}
                    \end{subequations}
                    Lastly, 0 is a unital element since:
                    \begin{equation}
                        0\circ{a}=0+a-0\cdot{a}=0+a-0=a
                    \end{equation}
                    and:
                    \begin{equation}
                        a\circ{0}=a+0-a\cdot{0}=a-0=a
                    \end{equation}
                    and therefore $\monoid[][\circ]{R}$ is a monoid.
                \end{proof}
                \begin{theorem}
                    If $\ring{R}$ is a rng, if $\circ$ is the circle
                    composition, and if $r,s\in{R}$ are right quasi regular,
                    then $r\circ{s}$ is right quasi regular.
                \end{theorem}
                \begin{proof}
                    For if $r$ and $s$ are right quasi regular, then there
                    exists $a,b\in{R}$ such that $r+a-ra=0$ and $s+b-sb=0$
                    (Def.~\ref{def:Right_Quasi_Regular}). That is,
                    $r\circ{a}=0$ and $s\circ{b}=0$
                    (Def.~\ref{def:Circle_Comp_of_Rng}). But then:
                    \begin{equation}
                        (r\circ{s})\circ(b\circ{a})=r\circ(s\circ{b})\circ{a}
                            =r\circ{0}\circ{a}=r\circ{a}=0
                    \end{equation}
                    hence, $r\circ{s}$ is quasi regular.
                \end{proof}
                This pieces all together to give the following:
                \begin{theorem}
                    If $\ring{R}$ is a rng, if $H\subseteq{R}$ is the set of all
                    quasi regular elements of $R$, and if $\circ$ is the
                    circle composition of $R$, then $\monoid[][\circ]{H}$ is a
                    group.
                \end{theorem}
                \begin{proof}
                    For $\circ$ is indeed a binary operation on $H$, and it is
                    associative and there exists an identity. Lastly, since $H$
                    is the set of all quasi regular elements, for all $x\in{H}$
                    there is a $y\in{H}$ such that $x\circ{y}=0$. Hence,
                    $\monoid[][\circ]{H}$ is a group.
                \end{proof}
                \begin{theorem}
                    If $\ring{R}$ is a ring, if $\circ$ is the circle
                    composition of $R$, and if $\varphi:R\rightarrow{R}$ is
                    defined by:
                    \begin{equation}
                        \varphi(r)=1-r
                    \end{equation}
                    then $\varphi$ is a monoid isomorphism between
                    $\monoid[][\circ]{R}$ and $\monoid[][\cdot]{R}$.
                \end{theorem}
                \begin{proof}
                    It is bijective, for if $1-r_{1}=1-r_{2}$, then by the
                    cancellation laws we obtain $r_{1}=r_{2}$. It is surjective,
                    for let $x=1-y$. The $\varphi(x)=1-(1-y)=y$. Lastly, it is
                    a homomorphism since:
                    \begin{subequations}
                        \begin{align}
                            \varphi(a\circ{b})
                                &=\varphi(a+b-a\cdot{b})\\
                                &=1-a-b+a\cdot{b}\\
                                &=(1-a)\cdot(1-b)\\
                                &=\varphi(a)\cdot\varphi(b)
                        \end{align}
                    \end{subequations}
                \end{proof}
                \begin{theorem}
                    If $\ring{R}$ is a ring, if $H$ is the set of all quasi
                    regular elements of $R$, if $\circ$ is the circle
                    composition of $R$, and if $R^{\times}$ is the set of all
                    invertible elements of $R$ with respect to $\cdot$, then
                    $\monoid[][\circ]{H}$ is group isomorphic to
                    $\monoid[][\cdot]{R^{\times}}$.
                \end{theorem}
                \begin{proof}
                    Use $\varphi(r)=1-r$.
                \end{proof}
                \begin{theorem}
                    If $\ring{R}$ is a rng, if $x\in{R}$ is idempotent, and if
                    $e$ is right quasi regular, then $x=0$.
                \end{theorem}
                \begin{proof}
                    For $x\circ{x}=x+x-x^{2}=x+x-x=x$, and hence
                    $x\circ{x}=x$. But $x$ is right quasi regular, and hence
                    there is a $y\in{R}$ such that $x\circ{y}=0$
                    (Def.~\ref{def:Right_Quasi_Regular}). But then
                    \begin{equation}
                        0=x+y-xy=x(x+y-xy)=x^{2}+xy-x^{2}y=x+xy-xy=x
                    \end{equation}
                    and thus $x=0$.
                \end{proof}
                Nilpotent elements of a rng are quasi regular. If we had a unit
                we could write:
                \begin{equation}
                    (1-x)\sum_{k=0}^{N}x^{k}=1-x^{N+1}=1
                \end{equation}
                since $x$ is nilpotent. Hence, removing the 1 from the equation,
                we can now guess what the quasi regular inverse will be.
                \begin{theorem}
                    If $\ring{R}$ is a ring, and if $r\in{R}$ is nilpotent, then
                    $r$ is right quasi regular.
                \end{theorem}
                \begin{proof}
                    For if $r$ is nilpotent, there is an $n\in\mathbb{N}$ such
                    that $r^{n}=0$. But then:
                    \begin{equation}
                        x\circ\sum_{k=0}^{N}\minus{x}^{k}=0
                    \end{equation}
                    and hence $x$ is quasi regular.
                \end{proof}
                \begin{ftheorem}{Kaplansky's Division Ring Theorem}
                    If $\ring{R}$ is a rng, then it is a division ring (with
                    idenity) if and only if for all but one element of $R$ is
                    right quasi regular.
                \end{ftheorem}
                \begin{bproof}
                    For $e\circ{a}=e$. For if not, then it is quasi regular
                    and hence there is a $b$ such that $(e\circ{a})\circ{b}=0$,
                    but then $e\circ(a\circ{b})=0$, a contradiction since $e$ is
                    not quasi regular. Hence $a+e-ae=e$ for all $a\in{R}$,
                    and therefore $a=ae$. Thus, $e$ is a right identity. Since
                    every element other than zero can be written as $e-a$ for
                    some quasi regular element $a$, it follows that every
                    non-zero element is invertible.
                \end{bproof}
            \subsubsection{Rings of Matrices}
                $n\times{n}$ matrices over a ring $\ring{R}$ form a ring. Even
                if $R$ is commutative, the ring of matrices need not be. If
                $n>0$ then there will be zero divisors in the ring of matrices
                (even if $R$ is an integral domain). For example,
                $\mathbb{Z}$ is a commutative ring (with identity), but:
                \begin{equation}
                    \begin{bmatrix}
                        1&1\\
                        0&1
                    \end{bmatrix}
                    \begin{bmatrix}
                        1&2\\
                        0&3
                    \end{bmatrix}
                    =
                    \begin{bmatrix}
                        1&5\\
                        0&3
                    \end{bmatrix}
                    \ne
                    \begin{bmatrix}
                        1&3\\
                        0&3
                    \end{bmatrix}
                    =
                    \begin{bmatrix}
                        1&2\\
                        0&3
                    \end{bmatrix}
                    \begin{bmatrix}
                        1&1\\
                        0&1
                    \end{bmatrix}
                \end{equation}
                If we have a ring with $0\ne{1}$, and $n>1$, consider the
                following:
                \begin{equation}
                    \begin{bmatrix}
                        1&0\\
                        0&0
                    \end{bmatrix}
                    \begin{bmatrix}
                        0&1\\
                        0&1
                    \end{bmatrix}
                    =
                    \begin{bmatrix}
                        0&1\\
                        0&0
                    \end{bmatrix}
                    \ne
                    \begin{bmatrix}
                        0&0\\
                        0&0
                    \end{bmatrix}
                    =
                    \begin{bmatrix}
                        0&1\\
                        0&1
                    \end{bmatrix}
                    \begin{bmatrix}
                        1&0\\
                        0&0
                    \end{bmatrix}
                \end{equation}
                Hence, regardless if $\ring{R}$ is a commutative ring, there
                will be non-commuting matrices for $n>1$. In general, consider:
                \begin{equation}
                    \begin{bmatrix}
                        1&1&\hdots&1&1\\
                        0&0&\hdots&0&0\\
                        \vdots&\vdots&\ddots&\vdots&\vdots\\
                        0&0&\hdots&0&0
                    \end{bmatrix}
                    \begin{bmatrix}
                        1&0&\hdots&0&0\\
                        0&0&\hdots&0&0\\
                        \vdots&\vdots&\ddots&\vdots&\vdots\\
                        0&0&\hdots&0&0
                    \end{bmatrix}
                    =
                    \begin{bmatrix}
                        1&0&\hdots&0&0\\
                        0&0&\hdots&0&0\\
                        \vdots&\vdots&\ddots&\vdots&\vdots\\
                        0&0&\hdots&0&0
                    \end{bmatrix}
                \end{equation}
                whereas in the other direction we obtain:
                \begin{equation}
                    \begin{bmatrix}
                        1&0&\hdots&0&0\\
                        0&0&\hdots&0&0\\
                        \vdots&\vdots&\ddots&\vdots&\vdots\\
                        0&0&\hdots&0&0
                    \end{bmatrix}
                    \begin{bmatrix}
                        1&1&\hdots&1&1\\
                        0&0&\hdots&0&0\\
                        \vdots&\vdots&\ddots&\vdots&\vdots\\
                        0&0&\hdots&0&0
                    \end{bmatrix}
                    =
                    \begin{bmatrix}
                        1&1&\hdots&1&1\\
                        0&0&\hdots&0&0\\
                        \vdots&\vdots&\ddots&\vdots&\vdots\\
                        0&0&\hdots&0&0
                    \end{bmatrix}
                \end{equation}
                and these are not equal. We also see from this how to obtain
                zero divisors. If $n>1$ and if we remove the 1 from the first
                slow we obtain:
                \begin{equation}
                    \begin{bmatrix}
                        0&1&\hdots&1&1\\
                        0&0&\hdots&0&0\\
                        \vdots&\vdots&\ddots&\vdots&\vdots\\
                        0&0&\hdots&0&0
                    \end{bmatrix}
                    \begin{bmatrix}
                        1&0&\hdots&0&0\\
                        0&0&\hdots&0&0\\
                        \vdots&\vdots&\ddots&\vdots&\vdots\\
                        0&0&\hdots&0&0
                    \end{bmatrix}
                    =
                    \begin{bmatrix}
                        0&0&\hdots&0&0\\
                        0&0&\hdots&0&0\\
                        \vdots&\vdots&\ddots&\vdots&\vdots\\
                        0&0&\hdots&0&0
                    \end{bmatrix}
                \end{equation}
                \begin{theorem}
                    If $A$ is a matrix, if $A_{ij}$ is the cofactor, then:
                    \begin{equation}
                        \sum_{k=1}^{n}a_{ik}A_{jk}=
                        \begin{cases}
                            \det(A),&i=j\\
                            0,&i\ne{j}
                        \end{cases}
                    \end{equation}
                \end{theorem}
                Hence, by this we have:
                \begin{equation}
                    A\adj(A)=\det(A)I=\adj(A)A
                \end{equation}
                Hence if $\det(A)$ is an invertible element of the ring $R$,
                then $A$ is an invertible matrix with inverse
                $\adj(A)/\det(A)$.
                \begin{theorem}
                    If $\ring{R}$ is a ring, and if $A$ is a matrix in $R$,
                    then $A$ is invertible if and only if $\det(A)$ is
                    invertible in $R$.
                \end{theorem}
                \begin{theorem}
                    If $\ring{F}$ is a field, and if $A$ is a matrix in $F$,
                    then $A$ is invertible if and only if $\det(A)\ne{0}$.
                \end{theorem}
                \begin{example}
                    Consider the following matrix:
                    \begin{equation}
                        A=
                        \begin{bmatrix*}[r]
                            1&4&1\\
                            0&1&\minus{1}\\
                            \minus{3}&\minus{6}&\minus{8}
                        \end{bmatrix*}
                    \end{equation}
                    the determinant is:
                    \begin{subequations}
                        \begin{align}
                            \det(A)&=\det\Big(
                                \begin{bmatrix*}[r]
                                    1&\minus{1}\\
                                    \minus{6}&\minus{8}
                                \end{bmatrix*}
                            \Big)-
                            4\det\Big(
                                \begin{bmatrix*}[r]
                                    0&\minus{1}\\
                                    \minus{3}&\minus{8}
                                \end{bmatrix*}
                            \Big)
                            +\det\Big(
                                \begin{bmatrix*}[r]
                                    0&1\\
                                    \minus{3}&\minus{6}
                                \end{bmatrix*}
                            \Big)\\
                            &=(\minus{8}-6)-4(0-3)+(0+3)\\
                            &=\minus{14}+12+3\\
                            &=1
                        \end{align}
                    \end{subequations}
                    since 1 is a unit in $\mathbb{Z}$, there is an inverse
                    matrix with elements in $\mathbb{Z}$.
                \end{example}
                \begin{theorem}
                    If $\ring{R}$ is a commutative ring, if $A$ is a right
                    invertible matrix, then $A$ is left invertible.
                \end{theorem}
            \subsubsection{Quaternions}
                The quaternions give a division ring structure on $\nspace[4]$.
                We can define it as a subset of the $2\times{2}$ matrices over
                the field $\mathbb{C}$. We consider all matrices of the form:
                \begin{equation}
                    \begin{bmatrix*}[r]
                        z&w\\
                        \minus\overline{w}&\overline{z}
                    \end{bmatrix*}
                    =
                    \begin{bmatrix*}[r]
                        a+bi&c+di\\
                        \minus{c}+di&a-bi
                    \end{bmatrix*}
                \end{equation}
                This is closed under addition since:
                \begin{subequations}
                    \begin{align}
                        \begin{bmatrix}
                            z_{1}&w_{1}\\[1.2ex]
                            \minus\overline{w}_{1}&\overline{z}_{1}
                        \end{bmatrix}
                        +
                        \begin{bmatrix}
                            z_{2}&w_{2}\\[1.2ex]
                            \minus\overline{w}_{2}&\overline{z}_{2}
                        \end{bmatrix}
                        &=
                        \begin{bmatrix}
                            z_{1}+z_{2}&w_{1}+w_{2}\\[1.2ex]
                            \minus\overline{w}_{1}-\overline{w}_{2}
                            &\overline{z}_{1}+\overline{z}_{2}
                        \end{bmatrix}\\
                        &=
                        \begin{bmatrix}
                            (z_{1}+z_{2})&(w_{1}+w_{2})\\[1.2ex]
                            \minus\overline{(w_{1}+w_{2})}
                                &\overline{(z_{1}+z_{2})}
                        \end{bmatrix}
                    \end{align}
                \end{subequations}
                It is closed to products as well:
                \begin{subequations}
                    \begin{align}
                        \begin{bmatrix}
                            z_{1}&w_{1}\\[1.2ex]
                            \minus\overline{w}_{1}&\overline{z}_{1}
                        \end{bmatrix}
                        \begin{bmatrix}
                            z_{2}&w_{2}\\[1.2ex]
                            \minus\overline{w}_{2}&\overline{z}_{2}
                        \end{bmatrix}
                        &=
                        \begin{bmatrix}
                            z_{1}z_{2}-w_{1}\overline{w}_{2}&
                            z_{1}w_{2}+w_{1}\overline{z}_{2}\\[1.2ex]
                            \minus\overline{w}_{1}z_{2}-
                                \overline{z}_{1}\overline{w}_{2}&
                            \minus\overline{w}_{1}w_{2}+
                                \overline{z}_{1}\overline{z}_{2}
                        \end{bmatrix}\\
                        &=
                        \begin{bmatrix}
                            (z_{1}z_{2}-w_{1}\overline{w}_{2})&
                            (z_{1}w_{2}+w_{1}\overline{z}_{2})\\[1.2ex]
                            \minus\overline{(z_{1}w_{1}+w_{1}\overline{z}_{2})}&
                            \overline{(z_{1}z_{2}-w_{1}\overline{w}_{2})}
                        \end{bmatrix}
                    \end{align}
                \end{subequations}
                which is of the form of quaternions with
                $u=z_{1}z_{2}-w_{1}\overline{w}_{2}$ and
                $v=z_{1}w_{2}+w_{1}\overline{z}_{2}$. This subring of
                $\mathbb{C}$ is called the Quaternions. Elements have the
                following determinant:
                \begin{equation}
                    \det\Big(
                        \begin{bmatrix}
                            z&w\\
                            \minus\overline{w}&\overline{z}
                        \end{bmatrix}
                    \Big)
                    =z\overline{z}+w\overline{w}
                    =a^{2}+b^{2}+c^{2}+d^{2}
                \end{equation}
                hence if $A$ is a non-zero quaternion, then it is invertible.
                Thus the quaternions form a division ring over $\nspace[]$. The
                inverse element is:
                \begin{equation}
                    \begin{bmatrix}
                        z&w\\
                        \minus\overline{w}&\overline{z}
                    \end{bmatrix}^{\minus{1}}
                    =\frac{1}{a^{2}+b^{2}+c^{2}+d^{2}}
                    \begin{bmatrix}
                        \overline{z}&\minus{w}\\
                        \overline{w}&z
                    \end{bmatrix}
                \end{equation}
                The quaternions contain an isomorphic copy of $\nspace[]$ by
                considering $w=0$ and $z=\alpha$ where $\alpha$ is a real
                number. That is, we have the following matrices:
                \begin{equation}
                    A=
                    \begin{bmatrix}
                        \alpha&0\\
                        0&\alpha
                    \end{bmatrix}
                    =\alpha
                    \begin{bmatrix}
                        1&0\\
                        0&1
                    \end{bmatrix}
                    =\alpha{I}
                \end{equation}
                These form a 4 dimensional vector space over $\nspace[]$ as well
                with the following basis elements:
                \par
                \begin{subequations}
                    \begin{minipage}[b]{0.49\textwidth}
                        \centering
                        \begin{align}
                            1&=
                            \begin{bmatrix*}[r]
                                1&0\\
                                0&\phantom{\minus}1
                            \end{bmatrix*}\\
                            i&=
                            \begin{bmatrix*}[r]
                                i&\phantom{\minus}0\\
                                0&\minus{i}
                            \end{bmatrix*}
                        \end{align}
                    \end{minipage}
                    \hfill
                    \begin{minipage}[b]{0.49\textwidth}
                        \centering
                        \begin{align}
                            j&=
                            \begin{bmatrix*}[r]
                                0&1\\
                                \minus{1}&0
                            \end{bmatrix*}\\
                            k&=
                            \begin{bmatrix*}[r]
                                \phantom{\minus}0&i\\
                                i&0
                            \end{bmatrix*}
                        \end{align}
                    \end{minipage}
                \end{subequations}
                \par\vspace{2.5ex}
                These follow the famous equations written down by William
                Rowan Hamilton:
                \begin{equation}
                    i^{2}+j^{2}+k^{2}=ijk=\minus{1}
                \end{equation}
                We can also conclude the following:
                \par
                \begin{subequations}
                    \begin{minipage}[b]{0.32\textwidth}
                        \centering
                        \begin{equation}
                            ij=\minus{ji}=k
                        \end{equation}
                    \end{minipage}
                    \hfill
                    \begin{minipage}[b]{0.32\textwidth}
                        \centering
                        \begin{equation}
                            jk=\minus{kj}=i
                        \end{equation}
                    \end{minipage}
                    \hfill
                    \begin{minipage}[b]{0.32\textwidth}
                        \centering
                        \begin{equation}
                            ki=\minus{ik}=j
                        \end{equation}
                    \end{minipage}
                \end{subequations}
                Thus the quaternions are not a commutative ring. That is, unlike
                the complex numbers which form a field over $\nspace[]$, the
                quaternions do not. They form a non-commutative division ring.
                The norm of a quaternion is the determinant of its matrix
                representation. The trace is double the real part. Trace and
                norm satisfy $a^{2}-\Tr(A)a+\det(A)=0$. Rational quaternions
                form division subring of the quaternions. Norm is multiplicative
                since determinant is multiplicative.
            \subsubsection{Subrings and Ideals}
                Intersection of subrings is subring. Subring generated by
                subset (Intersection of all subrings containing).
                If $\ring{R}$ is a ring, if $S\subseteq{R}$, if $\overline{S}$
                is the subring generated by $S$, and if $C(S)$ is the set of
                all elements in $R$ that commute with all elements in $S$,
                then $C(S)$ is a subring, $C(S)=C(\overline{S})$, and
                $S\subseteq{C}(C(S))$. If $S_{1}\subseteq{S}_{2}$, then
                $C(S_{2})\subseteq{C}(S_{1})$ and
                $C(C(C(S)))=C(S)$. $C(R)$ is called the center of $R$.
                \begin{example}
                    The center of the $n\times{n}$ matrix ring over a ring
                    $\ring{R}$ is the set of matrices of the form $\alpha{I}$,
                    where $\alpha\in{R}$ and $I$ is the identity matrix.
                \end{example}
                Given any additive subgroup $H\subseteq{R}$ of the Abelian
                group $\monoid[][+]{R}$, $H$ is necessarily a normal subgroup.
                If we were to look at the quotient group $R/H$ we can ask the
                question if $a$ and $a'$ are in the same equivalence class, and
                if $b$ and $b'$ are in the same equivalence class, what do we
                need to guarentee that $a\cdot{b}$ and $a'\cdot{b}'$ are in the
                same equivalence class? Since $[a]=[a']$ there is an $x\in{H}$
                such that $a'=a+x$, and similarly a $y\in{H}$ with $b'=b+y$.
                Multiplying, we obtain:
                \begin{equation}
                    a'\cdot{b}'=(a+x)\cdot(b+y)
                        =a\cdot{b}+a\cdot{y}+b\cdot{x}+x\cdot{y}
                \end{equation}
                For $a\cdot{b}$ and $a'\cdot{b}'$ to be in the same equivalence
                class we require that $a\cdot{y}+b\cdot{x}+x\cdot{y}$ be
                contained in $H$. Since we want this to be true for all elements
                of $H$, we can set $b=b'$, and hence $y=0$. This implies that
                $b\cdot{x}\in{H}$. Similarly, $a\cdot{y}\in{H}$. Hence, we have
                the requirement of an ideal: $I$ is closed under addition, and
                for all $a\in{I}$ and $r\in{R}$ we have $a\cdot{r}\in{I}$ and
                $r\cdot{a}\in{I}$. If we take a ring $\ring{R}$ and mod out by
                an ideal, then we can define coset multiplication consistently
                by write $(a+I)\cdot(b+I)=a\cdot{b}+I$. This forms a ring,
                called the quotient ring. If $\ring{R}$ is commutative, then
                so is $R/I$. The coset of the identity is the identity. Not
                everything is preserved, the quotient of an integral domain
                need not be an integral domain.
                \begin{example}
                    The set $nR$ for some $n\in\mathbb{N}$ is an ideal. For
                    $na+nb=n(a+b)\in{n}R$, and $r\cdot(na)=n(r\cdot{a})$, as
                    well as $(na)\cdot{r}=n(a\cdot{r})$. Hence, this is an
                    ideal. The set of all elements in $R$ where $na=0$ is also
                    an ideal. For $na+nb=0+0$, so $na+nb$ is contained in this.
                    $r\cdot(na)=r\cdot{0}=0$, and also $(na)\cdot{r}=0$.
                \end{example}
                \begin{example}
                    If $n\in\mathbb{N}$, then $n\mathbb{Z}$ is an ideal in
                    $\mathbb{Z}$. For $na+nb=n(a+b)$, and also $a(nb)=n(ab)$.
                    Suppose $m\in\mathbb{N}^{+}$ is composite, $m=a\cdot{b}$.
                    Recall that $\mathbb{Z}$ is a commutative ring that is also
                    an integral domain, by Euclid. That is, if
                    $a,b\in\mathbb{Z}$ and if $a\cdot{b}=0$, then either $a=0$
                    or $b=0$. Now we look at $\mathbb{Z}/m\mathbb{Z}$. Since $m$
                    is composite, we see that $[a]\cdot[b]=[a\cdot{b}]=[m]=[0]$,
                    and thus $\mathbb{Z}/m\mathbb{Z}$ is not an integral domain.
                    In the case when $m$ is prime, we have proven alread that
                    $\mathbb{Z}/m\mathbb{Z}$ is a field, and is hence an
                    integral domain. We have also proven that the number of
                    invertible elements in $\mathbb{Z}/m\mathbb{Z}$ is
                    $\varphi(m)$, where $\varphi$ is the Euler totient function.
                    If $m$ is prime we have $\varphi(m)=m-1$, which just states
                    that everything except for 0 is invertible.
                \end{example}
                Using Lagrange's theorem, if $\monoid{G}$ is a finite group with
                $n$ elements, then $a^{n}$ is the unital element for all
                $a\in{G}$. For there is a least $m\in\mathbb{N}^{+}$ such that
                $a^{m}=e$, and $m\leq{n}$. The set of powers of $a$ forms a
                subgroup, and hence the cardinality of this subgroup divides
                $n$. That is, $m$ divides $n$: $n=mk$. But the
                $a^{n}=a^{mk}=(a^{m})^{k}=e^{k}=e$. Looking at
                $\mathbb{Z}/m\mathbb{Z}$, if $k\in\mathbb{Z}$ and
                $\GCD(k,m)=1$, $k^{\varphi(m)}\equiv{1}$ in
                $\mathbb{Z}/m\mathbb{Z}$. For $\varphi(m)$ is the order of the
                group of units in $\mathbb{Z}/m\mathbb{Z}$, and since
                $\GCD(m,k)=1$, the equivalence class of $k$ is a unit. The
                corrollary if this is Fermat's Little Theorem:
                \begin{theorem}
                    If $p\in\mathbb{N}^{+}$ is prime, and if $a\ne{0}$, then
                    $a^{p-1}\equiv{1}\mod{p}$.
                \end{theorem}
                \begin{theorem}
                    If $\ring{R}$ is a finite division ring with $n$ elements,
                    then $a^{n}=a$ for all $a\in{R}$.
                \end{theorem}
                \begin{proof}
                    If $\ring{R}$ is a division ring, then $R\setminus\{0\}$ is
                    a group under multiplication. Hence this follows from the
                    fact that this result holds for groups.
                \end{proof}
            \subsubsection{Ring Homomorphisms}
                \begin{example}
                    $\mathbb{C}$ is isomorphic to the set of $2\times{2}$
                    matrices over $\nspace[]$ defined by:
                    \begin{equation}
                        A=
                        \begin{bmatrix}
                            \alpha&\beta\\
                            \minus\beta&\alpha
                        \end{bmatrix}
                    \end{equation}
                    with $\alpha,\beta\in\nspace[]$. We map
                    $\alpha+i\beta$ to this matrix. Then 1 maps to the identity,
                    0 maps to 0, and addition is preserved. Lastly:
                    \begin{subequations}
                        \begin{align}
                            \varphi\big((a+ib)(c+id)\big)
                            &=\varphi\big((ac-bd)+i(ad+bc)\big)\\
                            &=
                            \begin{bmatrix}
                                ac-bd&ad+bc\\
                                \minus(ad+bc)&ac-bd
                            \end{bmatrix}\\
                            &=
                            \begin{bmatrix*}[r]
                                a&b\\
                                \minus{b}&a
                            \end{bmatrix*}
                            \begin{bmatrix*}[r]
                                c&d\\
                                \minus{d}&c
                            \end{bmatrix*}\\
                            &=\varphi(a+ib)\varphi(c+id)
                        \end{align}
                    \end{subequations}
                \end{example}
                \begin{example}
                    The function $\varphi:\mathbb{C}\rightarrow\mathbb{C}$
                    defined by $\varphi(a+ib)=a-ib$ is an automorphism. This is
                    just conjugation, and
                    $\overline{z+w}=\overline{z}+\overline{w}$ and
                    $\overline{zw}=\overline{z}\overline{w}$. Morever, this is
                    bijective and $\overline{\overline{z}}=z$.
                \end{example}
                \begin{example}
                    There's an isomorphism from the quaternions into a subset of
                    $4\times{4}$ matrices over $\nspace[]$ defined by:
                    \begin{equation}
                        \varphi(a+ib+jc+kd)=
                        \begin{bmatrix*}[r]
                            a&b&c&d\\
                            \minus{b}&a&\minus{d}&c\\
                            \minus{c}&d&a&\minus{b}\\
                            \minus{d}&\minus{c}&b&a
                        \end{bmatrix*}
                    \end{equation}
                \end{example}
                \begin{theorem}
                    If $\ring[R]{R}$ and $\ring[S]{S}$ are rings, and if
                    $\varphi:R\rightarrow{S}$ is a ring homomorphism, then
                    $\varphi[R]$ is a subring of $S$.
                \end{theorem}
                \begin{theorem}
                    If $\ring[R]{R}$ and $\ring[S]{S}$ are rings, and if
                    $\varphi:R\rightarrow{S}$ is a ring homomorphism, then
                    $\varphi^{\minus{1}}[\{0_{S}\}]$ is an ideal in $R$. That
                    is, the kernel of $\varphi$ is an ideal.
                \end{theorem}
                The canonical projection map from $R$ to $R/I$ is a ring
                homomorphism. If $\varphi:R\rightarrow{S}$ is a ring
                homomorphism with kernel $K$ and if $I\subseteq{K}$ is an ideal,
                then there is an induced homomorphism
                $\tilde{\varphi}:R/I\rightarrow{S}$ defined by
                $\tilde{\varphi}(a+I)=\varphi(a)$. This is an additive
                homomorphism, and moreover since $I$ is contained in the kernel
                we have:
                \begin{equation}
                    \tilde{\varphi}\big((a+I)(b+I)\big)=\tilde{\varphi}(ab+I)
                    =\varphi(ab)=\varphi(a)\varphi(b)
                    =\tilde{\varphi}(a+I)\tilde{\varphi}(b+I)
                \end{equation}
                \begin{theorem}
                    If $\ring[R]{R}$ and $\ring[S]{S}$ are rings, if
                    $\varphi:R\rightarrow{S}$ is a ring homomorphism, and if
                    $I\subseteq\varphi^{\minus{1}}[\{0_{S}\}]$, and if $I$ is an
                    ideal in $R$, then there is a homomorphism
                    $\tilde{\varphi}:R/I\rightarrow{S}$ such that
                    $\varphi=\tilde{\varphi}\circ\pi$, where
                    $\pi:R\rightarrow{R}/I$ is the canonical projection map.
                    $\tilde{\varphi}$ is an isomorphism if and only if
                    $I=\varphi^{\minus{1}}[\{0_{S}\}]$.
                \end{theorem}
                \begin{ftheorem}{Fundamental Theorem of Ring Homomorphisms}
                                {Fundamental_Theorem_of_Ring_Homomorphism}
                    If $\ring{R}$ is a ring, if $I\subseteq{R}$ is an ideal,
                    then there is a homomorphism $\varphi:R\rightarrow{R}$ such
                    that $\varphi[R]=I$.
                \end{ftheorem}
                \begin{fdefinition}{Simple Ring}{Simple_Ring}
                    A simple ring is a ring $\ring{R}$ such that the only two
                    sided ideals are the 0 ideal and all of $R$.
                \end{fdefinition}
                \begin{theorem}
                    If $\ring{R}$ is a commutative ring, then $R$ is a field if
                    and only if $\ring{R}$ is a simple ring.
                \end{theorem}
                \begin{proof}
                    If $\ring{R}$ is a field, and $I\subseteq{R}$ is a field,
                    then either $I=\{0\}$ or not. If not, then there is an
                    $a\in{I}$ such that $a\ne{0}$. But $R$ is a field and hence
                    there is an $a^{\minus{1}}\in{R}$ such that
                    $a\cdot{a}^{\minus{1}}=1$. But $I$ is an ideal and therefore
                    $a\cdot{a}^{\minus{1}}\in{I}$, and thus $1\in{I}$. But then
                    for all $r\in{R}$, $r=r\cdot{1}$ and this is contained in
                    $I$ since $I$ is an ideal. Hence, the only two sided ideals
                    are the zero ideal and all of $R$. In the other direction,
                    if $\ring{R}$ is a commutative ring and if $r\in{R}$ is
                    non-zero, then the ideal generated by $r$ must be all of
                    $I$. But then there is a $b\in{I}$ such that $r\cdot{b}=1$
                    and thus $r$ is invertible. Hence, $\ring{R}$ is a field.
                \end{proof}
                \begin{theorem}
                    If $\ring{R}$ is a simple ring, and if
                    $\varphi:R\rightarrow{R}$ is a simple ring, then either
                    $\varphi[R]=\{0\}$ or $\varphi[R]=R$.
                \end{theorem}
                \begin{proof}
                    For by the fundamental theorem of ring homomorphisms,
                    $\varphi[R]$ is an ideal in $R$. But since $R$ is simple
                    the only ideals are $\{0\}$ and $R$. Hence, either
                    $\varphi[R]=\{0\}$ or $\varphi[R]=R$.
                \end{proof}
                One application of this is that if $\ring{R}$ is a ring
                generated by the identity $e\in{R}$, then either $R$ is
                isomorphic to $\mathbb{Z}$, or there is an $n\in\mathbb{N}^{+}$
                such that $R$ is isomorphic to $\mathbb{Z}/n\mathbb{Z}$. For we
                can place a homomorphic image of $\mathbb{Z}$ into $R$ by
                $\varphi(n)=ne$.
                \begin{theorem}
                    If $m,n\in\mathbb{N}^{+}$, if $m$ divides $n$, then
                    $n\mathbb{Z}$ is a subring of $m\mathbb{Z}$.
                \end{theorem}
                \begin{theorem}
                    If $m,n\in\mathbb{N}^{+}$, and if $m$ divides $n$, then
                    $m\mathbb{Z}/n\mathbb{Z}$ is an ideal in
                    $\mathbb{Z}/n\mathbb{Z}$ and
                    $(\mathbb{Z}/n\mathbb{Z})/(m\mathbb{Z}/n\mathbb{Z})$ is
                    isomorphic to $\mathbb{Z}/n\mathbb{Z}$.
                \end{theorem}
                \begin{theorem}
                    If $\ring[R]{R}$ and $\ring[S]{S}$ are rings, if
                    $n\in\mathbb{N}^{+}$, if $\varphi:R\rightarrow{S}$ is a
                    ring homomorphism, and if
                    $\tilde{\varphi}:\matspace{R}\rightarrow\matspace{S}$ is
                    defined by $(a_{ij})\mapsto\big(\varphi(a_{ij})\big)$,
                    then $\tilde{\varphi}$ is a ring homomorphism.
                \end{theorem}
                \begin{proof}
                    For:
                    \begin{subequations}
                        \begin{align}
                            \tilde{\varphi}\big((a_{ij})+(b_{ij})\big)
                            &=\tilde{\varphi}\big((a_{ij}+b_{ij})\big)\\
                            &=\big(\varphi(a_{ij}+b_{ij})\big)\\
                            &=\big(\varphi(a_{ij})+\varphi(b_{ij})\big)\\
                            &=\big(\varphi(a_{ij})\big)
                                +\big(\varphi(b_{ij})\big)\\
                            &=\tilde{\varphi}\big((a_{ij})\big)+
                                \tilde{\varphi}\big((b_{ij})\big)
                        \end{align}
                    \end{subequations}
                    and hence $\tilde{\varphi}$ is additive. Moreover it is
                    multiplicative since:
                    \par
                    \begin{subequations}
                        \begin{minipage}[t]{0.55\textwidth}
                            \centering
                            \begin{align}
                                \tilde{\varphi}\big((a_{ij})(b_{ij})\big)
                                &=\tilde{\varphi}\Big(
                                    \big(\sum_{k\in\mathbb{Z}_{n}}
                                        a_{ik}b_{kj}\big)
                                \Big)\\
                                &=\Big(\varphi\big(
                                    \sum_{k\in\mathbb{Z}_{n}}a_{ij}b_{kj}\big)
                                    \Big)\\
                                &=\Big(\sum_{k\in\mathbb{Z}_{n}}
                                    \varphi\big(a_{ij}b_{kj}\big)\Big)
                            \end{align}
                        \end{minipage}
                        \hfill
                        \begin{minipage}[t]{0.44\textwidth}
                            \centering
                            \begin{align}
                                &=\Big(\sum_{k\in\mathbb{Z}_{n}}
                                    \varphi(a_{ik})\varphi(b_{kj})\Big)\\
                                &=\big(\varphi\big(a_{ij}\big)\big)
                                    \big(\varphi\big(b_{ij}\big)\big)
                                    \vphantom{\Big(\sum_{k\in\mathbb{Z}_{n}}}\\
                                &=\tilde{\varphi}\big((a_{ij})\big)
                                    \tilde{\varphi}\big((b_{ij})\big)
                                    \vphantom{\Big(\sum_{k\in\mathbb{Z}_{n}}}
                            \end{align}
                        \end{minipage}
                    \end{subequations}
                    Lastly, since $\varphi$ is a ring homomorphism,
                    $\varphi(1_{R})=1_{S}$, and hence $I_{R}$ maps to $I_{S}$.
                    Thus, $\tilde{\varphi}$ is a ring homomorphism.
                \end{proof}
                \begin{theorem}
                    If $\ring{R}$ is a ring, if $\varphi:R\rightarrow{R}$ is a
                    ring homomorphism, and if $A\subseteq{R}$ is the set of all
                    fixed elements of $\varphi$, then $A$ is a subring of $R$.
                \end{theorem}
                \begin{theorem}
                    The only ring homomorphisms
                    $\varphi:\mathbb{Z}\rightarrow\mathbb{Z}$ is the identity.
                \end{theorem}
                \begin{proof}
                    For by induction on $n\in\mathbb{N}^{+}$ we have
                    $\varphi(1)=1$, and $\varphi(n+1)=\varphi(n)+\varphi(1)=n+1$
                    and similarly for the negatives.
                \end{proof}
                \begin{theorem}
                    If $A$ is a set, if $\ring{R}$ is a ring, and if
                    $\varphi:A\rightarrow{R}$ is a bijection, then the binary
                    operations $\oplus$ and $\otimes$ on $A$ defined by
                    $a\oplus{b}=f^{\minus{1}}\big(f(a)+f(b)\big)$ and
                    $a\otimes{b}=f^{\minus{1}}\big(f(a)\cdot{f}(b)\big)$ makes
                    $(A,\oplus,\otimes)$ a ring such that $f$ is an isomorphism.
                \end{theorem}
                \begin{theorem}
                    If $\ring{R}$ is a ring, and if $\oplus$ and $\otimes$ are
                    the binary operations defined by:
                    \twocolumneq{a\oplus{b}=a+b-1}
                                {a\otimes{b}=a+b-a\cdot{b}}
                    Then $(R,\oplus,\otimes)$ is a ring.
                \end{theorem}
            \subsubsection{Anti-Isomorphism}
                The conjugate of the quaternion $a+ib+jc+kd$ is $a-ib-jc-kd$.
                The inverse is given by $q^{\minus{1}}=\overline{q}/\norm{q}$.
                Conjugation gives rise to the notion of a ring anti-isomorphism.
                While conjugation preserves addition:
                $\overline{a+b}=\overline{a}+\overline{b}$, it reverses
                multiplication: $\overline{ab}=\overline{b}\overline{a}$. This
                issue does not arise in the complex numbers since multiplication
                is commutative in this ring. The quaternions are not a
                commutative ring, and therein lies the problem.
                \begin{fdefinition}{Ring Anti-Isomorphism}
                                   {Ring Anti-Isomorphism}
                    A ring anti-isomorphism from a ring $\ring[R]{R}$ to a ring
                    $\ring[S]{S}$ is a bijection $\varphi:R\rightarrow{S}$ such
                    that for all $a,b\in{R}$ it is true that:
                    \par
                    \begin{minipage}[b]{0.49\textwidth}
                        \centering
                        \begin{equation*}
                            \varphi(a+_{R}b)=\varphi(a)+_{S}\varphi(b)
                        \end{equation*}
                    \end{minipage}
                    \hfill
                    \begin{minipage}[b]{0.49\textwidth}
                        \centering
                        \begin{equation*}
                            \varphi(a\cdot_{R}b)=\varphi(b)\cdot_{R}\varphi(b)
                        \end{equation*}
                    \end{minipage}
                \end{fdefinition}
                \begin{theorem}
                    If $\ring{R}$ is a commutative ring, if $n\in\mathbb{N}^{+}$
                    is a positive integer, and if
                    $T:\matspace{R}\rightarrow\matspace{R}$ is the transpose
                    operator, then $T$ is an anti-isomorphism.
                \end{theorem}
                \begin{theorem}
                    If $\ring{R}$ is a ring, and if $\times$ is the binary
                    operation on $R$ defined by $a\times{b}=b\cdot{a}$, then
                    $(R,+,\times)$ is a ring that is anti-isomorphic to
                    $\ring{R}$.
                \end{theorem}
                \begin{theorem}
                    If $\ring[R]{R}$ and $\ring[S]{S}$ are rngs, if
                    $\varphi:R\rightarrow{S}$ is an anti-isomorphism, and if
                    $R$ has a left identity, then $S$ has a right identiy.
                \end{theorem}
                If we relax the definition to groups, we see that every group is
                anti-isomorphic to itself: define $\varphi(g)=g^{\minus{1}}$.
                \begin{theorem}
                    The set of anti-automorphisms form a group. So does the set
                    of automorphisms.
                \end{theorem}
                \begin{theorem}
                    If $\ring[R]{R}$ and $\ring[S]{S}$ are rings, if
                    $n\in\mathbb{N}^{+}$ is a positive integer, if
                    $T:\matspace{R}\rightarrow\matspace{S}$ is the transpose
                    operator, and if $\varphi:R\rightarrow{S}$ is an
                    anti-isomorphism, then
                    $\tilde{\varphi}:\matspace{R}\rightarrow\matspace{S}$
                    defined by:
                    \begin{equation}
                        \tilde{\varphi}\big((a_{ij})\big)
                            =T\big((\varphi(a_{ij})\big))
                    \end{equation}
                    is an anti isomorphism.
                \end{theorem}
                \begin{ftheorem}{Hua's Theorem}{Huas_Theorem}
                    If $\ring[R]{R}$ and $\ring[S]{S}$ are rings, if
                    $\varphi:R\rightarrow{S}$ is a function such that for all
                    $a,b\in{R}$ it is true that
                    $\varphi(a+_{R}b)=\varphi(a)+_{S}\varphi(b)$ and either
                    $\varphi(a\cdot_{R}b)=\varphi(a)\cdot_{S}\varphi(b)$ or
                    $\varphi(a\cdot_{R}b)=\varphi(b)\cdot_{S}\varphi(b)$, then
                    either $\varphi$ is a homomorphism and an anti-homomorphism.
                \end{ftheorem}
                \begin{bproof}
                    For suppose not. Then there exists $a,b\in{R}$ and
                    $c,d\in{R}$ such that
                    $\varphi(a\cdot_{R}b)\ne\varphi(a)\cdot_{S}\varphi(b)$ and
                    $\varphi(c\cdot_{R}d)\ne\varphi(b)\cdot_{S}\varphi(b)$. But
                    then by hypothesis,
                    $\varphi(a\cdot_{R}b)=\varphi(b)\cdot_{S}\varphi(a)$ and
                    $\varphi(c\cdot_{R}d)=\varphi(c)\cdot_{S}\varphi(d)$.
                \end{bproof}
                If $1$ has order $m$, then so does every other element since
                $ma=m(1a)=(m1)a=0a=0$. If a maximum such $m$ exists for all
                $a\in{R}$ then $m$ is called the characteristic of the ring.
                If no such max exists, we say the ring has characteristic zero.
                \begin{theorem}
                    If $\ring{R}$ is an integral domain, then either $R$ has
                    characteristic zero, or there is an integer $m\in{R}$ such
                    that for all $r\in{R}$, $mr=0$ and $m$ is the least such
                    value for all $r$.
                \end{theorem}
                \begin{theorem}
                    The characteristic of an integral domain is prime.
                \end{theorem}
                \begin{proof}
                    For suppose not and suppose $ma=0$ with $m=pq$ and
                    $a\ne{0}$. Since $R$ is an integral domain $a^{2}\ne{0}$,
                    and $ma^{2}={0}$. But then $(pa)(qa)=0$, contradicting that
                    $pa\ne{0}$ and $qa\ne{0}$.
                \end{proof}
                \begin{theorem}
                    If $\ring{R}$ is an integral domain of characteristic zero,
                    then all non-zero elements have infinite order.
                \end{theorem}
                \begin{proof}
                    For suppose $a$ has finite order $m$. Then
                    $ma=m(1a)=(m1)a=0$. But $a\ne{0}$, and hence $m1=0$, but
                    1 has infinite order.
                \end{proof}
                \begin{theorem}
                    If $\ring{R}$ is a simple ring of characteristic zero, then
                    every non-zero element has infinite order.
                \end{theorem}
                \begin{proof}
                    For suppose not, and let $a\in{R}$ have finite order $m$.
                    Then the subring generated by $a$ is an ideal that is not
                    the entirety of $R$ nor is it the zero ideal, contradicting
                    that $R$ is simple.
                \end{proof}
                \begin{theorem}
                    If $\ring{R}$ is a simple ring of characteristic $m$, then
                    $m$ is prime.
                \end{theorem}
                \begin{proof}
                    For suppose $a$ has characteristic $m$ that is not prime,
                    $m=pq$. Then $a^{p}$ forms a proper ideal that is non-zero.
                \end{proof}
                \begin{theorem}
                    If $\monoid{G}$ is an Abelian group, and if
                    $A,B\subseteq{G}$ are subgroups, then the group generated
                    by $A\cup{B}$ is equal to $AB$.
                \end{theorem}
                \begin{proof}
                    Since $A$ and $B$ are subgroups of an Abelian group, they
                    are normal, and hence $AB$ is a subgroup.
                \end{proof}
                The subgroup generated by the union of an arbitrary collection
                of subgroups of an Abelian group is the set of all finite sums
                of elements of the collection. The subgroup generated by the
                product of two subgroups (In a ring now) is the collection of
                all finite sums $a_{k}b_{k}$ with $a_{k}\in{A}$ and
                $b_{k}\in{B}$. The associative and distributive laws hold for
                this form of set multiplication: $A(BC)=(AB)C$ and
                $A(B+C)=AB+AC$. From this, $A^{k}$ is the set of finite sums of
                the form $a_{1}a_{2}\dots{a}_{k-1}a_{k}$.
                \begin{theorem}
                    If $\ring{R}$ is a ring, if $A\subseteq{R}$ is a subgroup of
                    $\monoid[][+]{R}$, then $A$ is a subring if and only if
                    $A^{2}\subseteq{A}$.
                \end{theorem}
                \begin{theorem}
                    If $\ring{R}$ is a ring, if $A\subseteq{R}$ is a subgroup of
                    $\monoid[][+]{R}$, then $A$ is a two sided ideal if and
                    only if $AR\subseteq{A}$ and $RA\subseteq{A}$.
                \end{theorem}
                Def left ideal, right ideal. $Rb$ is a left ideal,
                $aR$ is a right ideal, for all $a,b\in{R}$.
                \begin{theorem}
                    $\ring{R}$ is a division ring if and only if it contains no
                    proper left or right ideals.
                \end{theorem}
                A principal ideal is an ideal generated by a single element. If
                $\ring{R}$ is a rng (no identity), we need more than just
                $rb$ to characterize the ideal generated by $b$, but rather
                need $nb+rb$ for all $r\in{R}$ and $n\in\mathbb{Z}$. This
                ensures that the ideal generated by $b$ actually contains $b$ in
                it (Set $n=1$ and $x=0$). For a ring (with identity)
                $(b)=bR$, and for a rng, $bR\subseteq(b)$.
                \begin{theorem}
                    If $\ring{S}$ is a ring, if $L$ is a right sided ideal,
                    and if $R$ is a right sided ideal, then $LR$ is a two sided
                    ideal in $S$.
                \end{theorem}
                \begin{theorem}
                    If $\ring{R}$ is a ring, if $A\subseteq{R}$ is a subgroup of
                    $\monoid[][+]{R}$, and if $L\subseteq{R}$ is a left ideal,
                    then $LA$ is a left ideal.
                \end{theorem}
                \begin{theorem}
                    If $\ring{R}$ is a ring, then $R^{n}$ is an ideal in $R$.
                \end{theorem}
            \subsubsection{Ring of Endomorphisms of Commutative Group}
                If $\monoid{G}$ is an Abelian group, the set of all group
                endomorphisms $\varphi:G\rightarrow{G}$ can be given a ring
                structure. We define function addition pointwise. Given
                $\alpha,\beta\in\Endomorphisms{R}$, we define $\alpha+\beta$
                to be the function:
                \begin{equation}
                    (\alpha+\beta)(r)=\alpha(r)*\beta(r)
                \end{equation}
                \begin{theorem}
                    If $\monoid{G}$ is an Abelian group, if
                    $\alpha,\beta\in\Endomorphisms{R}$ are group endomorphisms,
                    and if $+$ denotes function addition, then
                    $\alpha+\beta\in\Endomorphisms{R}$.
                \end{theorem}
                \begin{proof}
                    For:
                    \begin{subequations}
                        \begin{align}
                            (\alpha+\beta)(a*b)
                                &=\alpha(a*b)*\beta(a*b)\\
                                &=\big(\alpha(a)*\alpha(b)\big)
                                    *\big(\beta(a)*\beta(b)\big)\\
                                &=\alpha(a)*\big(\alpha(a)*\beta(b)\big)
                                    *\beta(b)\\
                                &=\alpha*\big(\beta(b)*\alpha(a)\big)*\beta(b)\\
                                &=\big(\alpha(a)*\beta(a)\big)*
                                    \big(\alpha(b)*\beta(b)\big)\\
                                &=(\alpha+\beta)(a)*(\alpha+\beta)(b)
                        \end{align}
                        Hence, $(\alpha+\beta)(a*b)=(\alpha+\beta)(a)*(\alpha+\beta)(b)$
                        and so $\alpha+\beta$ is a homomorphism from $G$ to
                        itself, and therefore an endomorphism.
                    \end{subequations}
                \end{proof}
                \begin{theorem}
                    If $\monoid{G}$ is an Abelian group, and if $+$ denotes
                    function addition in $G$, then
                    $\monoid[][+]{\Endomorphisms{R}}$ is an Abelian group.
                \end{theorem}
                \begin{proof}
                    For let $E:G\rightarrow{G}$ be the zero
                    endomorphism: $\tilde{0}(r)=e$ for all $g\in{G}$. Then if
                    $\alpha\in\Endomorphisms{G}$, for all $g\in{G}$ we have:
                    \begin{equation}
                        (\alpha+E)(r)=\alpha(r)*E(r)=\alpha(r)*e=\alpha(r)
                    \end{equation}
                    and hence $\alpha+E=\alpha$. If
                    $\alpha\in\Endomorphisms{R}$, let $\minus{\alpha}$ be the
                    endomorphism $(\minus\alpha)(g)=\alpha(g)^{\minus{1}}$
                    where $\alpha(g)^{\minus{1}}$ is the unique inverse element
                    of $\alpha(r)$ in the group $\monoid{G}$. Then for all
                    $g\in{G}$ we have:
                    \begin{equation}
                        \big(\alpha+(\minus{\alpha})\big)(g)
                        =\alpha(r)*(\minus\alpha)(r)
                        =\alpha(r)*\alpha(r)^{\minus{1}}
                        =e
                    \end{equation}
                    and therefore $\alpha+(\minus{\alpha})=\tilde{0}$. Addition
                    of functions is associative, since:
                    \begin{subequations}
                        \begin{align}
                            \big(\alpha+(\beta+\gamma)\big)(r)
                            &=\alpha(r)*(\beta+\gamma)(r)\\
                            &=\alpha*(\beta(r)*\gamma(r))\\
                            &=(\alpha*\beta(r))*\gamma(r)\\
                            &=(\alpha+\beta)(r)*\gamma(r)\\
                            &=\big((\alpha+\beta)+\gamma\big)(r)
                        \end{align}
                    \end{subequations}
                    and thus $\monoid[][+]{\Endomorphisms{G}}$ is a group. It is
                    Abelian since $\monoid{G}$ is an Abelian group, and hence
                    for all $\alpha,\beta\in\Endomorphisms{G}$ we have:
                    \begin{equation}
                        (\alpha+\beta)(g)=\alpha(g)*\beta(g)
                            =\beta(g)*\alpha(g)
                            =(\beta+\alpha)(g)
                    \end{equation}
                    and hence function addition is commutative.
                \end{proof}
                To show that $\Endomorphisms{G}$ has a ring structure simply
                requires us to show that the distributive laws hold.
                \begin{theorem}
                    If $\monoid{G}$ is an Abelian group, and if $+$ is function
                    addition, then $(\Endomorphisms{R},+,\circ)$ is a ring.
                \end{theorem}
                \begin{proof}
                    We know that $\identity{G}$ is the identity element of
                    $\Endomorphisms{G}$, and that
                    $\monoid[][+]{\Endomorphisms{G}}$ is an Abelian group.
                    Moreover, $\circ$ is associative. All that's needed is to
                    show that $\circ$ distributives over $+$. But:
                    \begin{subequations}
                        \begin{align}
                            (\alpha\circ(\beta+\gamma))(g)
                            &=\alpha\big((\beta+\gamma)(g)\big)\\
                            &=\alpha\big(\beta(g)*\gamma(g)\big)\\
                            &=\alpha\big(\beta(g)\big)*
                                \alpha\big(\gamma(g)\big)\\
                            &=(\alpha\circ\beta)(g)*(\alpha\circ\gamma)(g)\\
                            &=(\alpha\circ\beta+\alpha\circ\gamma)(g)
                        \end{align}
                    \end{subequations}
                    similarly composition left distributes.
                \end{proof}
                We call this the ring of endomorphisms of $G$. These rings play
                a similar role as the groups of permutations do in group theory.
                \begin{example}
                    Consider the Abelian group $\monoid[][+]{\mathbb{Z}}$. From
                    the previous theorem, we know that
                    $(\Endomorphisms{\mathbb{Z}},+,\circ)$ is a ring, where we
                    have doubly used the symbol $+$ out of laziness. Any
                    endomorphism is uniquely determined by what it does to
                    $1$, since $\varphi(n)=\varphi(n\cdot{1})=n\cdot\varphi(1)$.
                    Moreover, to every $n\in\mathbb{Z}$ there is an endomorphism
                    $\varphi(1)=n$. This puts $\Endomorphisms{\mathbb{Z}}$ into
                    a bijection with $\mathbb{Z}$, and moreover the ring
                    structure induced on the endomorphims is precisely the
                    familiar ring structure $\ring{\mathbb{Z}}$.
                \end{example}
                \begin{example}
                    Given $n\in\mathbb{N}^{+}$, the hyper lattice
                    $\mathbb{Z}^{n}$ can be given an Abelian group structure by
                    considering pointwise addition. That is, if
                    $a,b\in\mathbb{Z}^{n}$, $a+b$ is the element such that
                    $(a+b)_{k}=a_{k}+b_{k}$ for all $k\in\mathbb{Z}_{n}$. By
                    letting $e_{k}$ be the element that is 1 in the $k^{th}$
                    entry and zero everywhere else, we see that any
                    endomorphism $\alpha\in\Endomorphisms{\mathbb{Z}^{n}}$ is
                    uniquely determined by how it acts on the $e_{k}$. Moreover,
                    every element of $\mathbb{Z}^{n}$ gives rise to an
                    endomorphism by mapping
                    $e=(e_{0},\dots,e_{n-1})$ to $m=(m_{0},\dots,m_{n-1})$.
                    If $\alpha\in\Endomorphisms{\mathbb{Z}^{n}}$, let
                    $a_{k}\in\mathbb{Z}^{n}$ be the image of $\alpha(e_{k})$. We
                    can then represent $\alpha$ uniquely by these $a_{k}$,
                    forming an $n\times{n}$ matrix over $\mathbb{Z}$:
                    \begin{subequations}
                        \begin{align}
                            A&=
                            \begin{bmatrix}
                                \alpha(e_{0})_{0}&\alpha(e_{1})_{0}
                                    &\hdots&\alpha(e_{n-1})_{0}\\
                                \alpha(e_{0})_{1}&\alpha(e_{1})_{1}
                                    &\hdots&\alpha(e_{n-1})_{1}\\
                                \vdots&\vdots&\ddots&\vdots\\
                                \alpha(e_{0})_{n-1}&\alpha(e_{1})_{n-1}
                                    &\hdots&\alpha(e_{n-1})_{n-1}
                            \end{bmatrix}\\[1ex]
                            &=
                            \begin{bmatrix}
                                a_{(0,0)}&a_{(0,1)}&\hdots&a_{(0,n-1)}\\
                                a_{(1,0)}&a_{(1,1)}&\hdots&a_{(1,n-1)}\\
                                \vdots&\vdots&\ddots&\vdots\\
                                a_{(n-1,0)}&a_{(n-1,1)}&\hdots&a_{(n-1,n-1)}
                            \end{bmatrix}
                        \end{align}
                    \end{subequations}
                    This shows the the matrix ring of $n\times{n}$ matrices over
                    $\mathbb{Z}$ is isomorphic to the endomorphism ring of
                    the Abelian group $\monoid[][+]{\mathbb{Z}^{n}}$.
                \end{example}
                The above example shows that we can determine the group of
                automorphisms of $\monoid[][+]{\mathbb{Z}^{n}}$ by finding the
                invertible elements of $\matspace{\mathbb{Z}}$. The subgroup of
                invertible elements of a ring $\ring[R]{R}$ map to the
                invertible elements of a ring $\ring[S]{S}$ under isomorphism.
                \begin{example}
                    If $n\in\mathbb{N}^{+}$, we can compute the ring of
                    endomorphims of the Abelian group
                    $\monoid[][+]{\mathbb{Z}/n\mathbb{Z}}$ in a similar manner
                    as with $\mathbb{Z}$. If
                    $\alpha\in\Endomorphisms{\mathbb{Z}/n\mathbb{Z}}$, then
                    $\varphi(k)=\varphi(k\cdot{1})=k\cdot\varphi(1)$, and hence
                    $\varphi$ is uniquely determined by what it does to 1. Also,
                    if $k\in\mathbb{Z}/n\mathbb{Z}$, then $\varphi(1)=k$ defines
                    an endomorphism since:
                    \begin{equation}
                        \varphi(a+b)=(a+b)\varphi(1)
                            =(a+b)k
                            =ak+bk
                            =\varphi(a)+\varphi(b)
                    \end{equation}
                    we see that function composition corresponds to
                    multiplication, and thus the ring of endomorphisms on
                    $\mathbb{Z}/n\mathbb{Z}$ is isomorphic to the usual ring
                    structure on $\mathbb{Z}/n\mathbb{Z}$.
                \end{example}
                If $\ring{R}$ is a rng, we can define $f:R\rightarrow{R}$ by
                $f(x)=x\cdot{r}$ for some fixed $r\in{R}$. This is an
                endomorphism of the Abelian group $\monoid[][+]{R}$ by the
                distributive law. We can also get a homomorphism
                $\varphi:R\rightarrow\Endomorphisms{R}$ by
                $r\mapsto{f}$ with $f(x)=x\cdot{r}$. The image of this is thus
                a subring of the ring of endomorphims. The kernel of $\varphi$
                is all elements $r\in{R}$ such that $x\cdot{r}=0$ for all
                $x\in{R}$. This is called the right annihilator of $R$. If $R$
                has identity, then $r=1\cdot{r}=0$, and hence $r=0$ so the
                kernel is $\{0\}$ and $\varphi$ is an isomorphism.
                \begin{theorem}
                    If $\ring[R]{R}$ is a ring, then there is an Abelian group
                    $\monoid{G}$ such that $\ring[R]{R}$ is isomorphic to the
                    endomorphism group $(\Endomorphisms{G},+,\circ)$.
                \end{theorem}
                If we do the same thing with left multiplication we get
                anti-homomorphisms of $R$ to $\Endomorphisms{R}$. The image of
                this anti-isomorphism is thus a subring, and if $R$ has identity
                then the kernel is just $0$ and hence this is an
                anti-isomorphism.
                \begin{theorem}
                    If $\ring{R}$ is a ring (with identity) and if
                    $f:R\rightarrow{R}$ commutes with all left multiplications,
                    then $f$ is a right multiplication.
                \end{theorem}
        \subsection{Extensions of Rings and Fields}
            Some rings lack certain features that are rather desirable. Indeed,
            rngs lack identity and hence it would be nice if we could convert
            these into rings. The integers $\mathbb{Z}$ lack the latin square
            property: $ax=b$ may have no solutions. It would be convenient to
            extend rngs to rings, and furthermore to extend rings to objects
            with inverses. In the case of $\mathbb{Z}$, this is done by
            constructing $\mathbb{Q}$. The method of construction extends to
            arbitrary commutative integral domains.
            \subsubsection{Embedding Rngs into Rings}
                \begin{fdefinition}{Embedding of a Rng}{Embedding_of_Rng}
                    An embedding of a rng $\ring[R]{R}$ into a ring
                    $\ring[S]{S}$ is an injective rng homomorphism
                    $\varphi:R\rightarrow{S}$ such that $R$ is isomorphic to its
                    image $\varphi[R]$.
                \end{fdefinition}
                \begin{ftheorem}{Embedding of Rngs into Rings}
                                {Embedding of Rngs into Rings}
                    If $\ring[R]{R}$ is a rng, then there exists a ring
                    $\ring[S]{S}$ such that $R$ may be embedded into $S$.
                \end{ftheorem}
                \begin{bproof}
                    For let $S=\mathbb{Z}\times{R}$ and define $+_{S}$ and
                    $\cdot_{S}$ as follows:
                    \begin{align}
                        (m,a)+_{S}(n,b)&=(m+n,a+b)\\
                        (m,a)\cdot_{S}(n,b)&=(mn,na+mb+ab)
                    \end{align}
                    This is a ring with identity $I=(1,0_{R})$. Define
                    $\varphi:R\rightarrow{S}$ by $\varphi(r)=(0,r)$. This is an
                    embedding.
                \end{bproof}
                If we let $Z'=\mathbb{Z}\times\{0_{R}\}$ and
                $R'=\{0\}\times{R}$, then $S=Z'+R'$, where $+$ denotes set
                addition with respect to the addition operation $+$ defined in
                the proof. Also, $Z'\cap{R}'=\{(0,0_{R})\}$. Both $R'$ and $Z'$
                are ideals in $S$. While this theorem shows that rngs can always
                be embedded into rings with unity, this embedding may be
                horrible. If $\ring{R}$ was a ring to begin with (contains
                identity), then the element $x=(1,\minus{1}_{R})$ has the
                property that $x\cdot{y}=y\cdot{x}=0$ for all $y\in{S}$. The
                embedding may also change the characteristic. If $\ring{R}$ was
                a rng of finite and positive characteristic, then since $S$
                contains $\mathbb{Z}$ as an ideal, $S$ has characteristic zero.
                Hence we see the integral domains may lose their fundamental
                property, and the characteristic may be lost. We wish to modify
                the theorem and construct an embedding the preserves such
                notions.
                \begin{theorem}
                    If $\ring[R]{R}$ is a rng of characteristic
                    $n\in\mathbb{N}^{+}$, then there is a ring $\ring[S]{S}$
                    such that $R$ may be embeeded into $S$.
                \end{theorem}
                \begin{proof}
                    Do the same thing as in the fundamental embedding theorem,
                    but use $\mathbb{Z}/n\mathbb{Z}$ instead of $\mathbb{Z}$.
                \end{proof}
                We now turn to fixing the problem of embedding integral domains.
                \begin{theorem}
                    If $\ring{R}$ is an integral domain, if $a,b\in{R}$ are
                    non-zero, and if there is an $m\in\mathbb{Z}$ such that
                    $ab=mb$, then for all $c\in{R}$, $ac=mc$.
                \end{theorem}
                \begin{proof}
                    For:
                    \begin{equation}
                        b(mc)=m(bc)=(mb)c=(ab)c=a(bc)
                    \end{equation}
                \end{proof}
                \begin{theorem}
                    If $\ring{R}$ is an integral domain, then there is an
                    integral domain $\ring{S}$ with identity such that $R$
                    may be embedded into $S$.
                \end{theorem}
                \begin{proof}
                    For Let $A=\mathbb{Z}\times{R}$ and let $Z$ be the set of
                    all $x\in{A}$ such that $xa=0$ for all $a\in{A}$. This is an
                    ideal. Let $S=A/Z$. This is an integral domain and $R$ can
                    be embedded into it. (Probably the canonical quotient map).
                \end{proof}
                Intersections of fields is field. Intersection of subfields is
                subfields.
                \begin{theorem}
                    If $\ring{F}$ is a field, if $R\subseteq{F}$ is a non-zero
                    subring, then the field generated by $R$ is the set of all
                    $\{ab^{\minus{1}}\;|\;a,b\in{R}\}$ with $b\ne{0}$.
                \end{theorem}
                We can add $ab^{\minus{1}}+cd^{\minus{1}}$ like with do with
                fractions and get $(ad+bc)(b^{\minus{1}}d^{\minus{1}})$.
                Since $a=abb^{\minus{1}}$, it follows that $R$ is contained in
                this set.
                \begin{ftheorem}{Integral Domain Extension Theorem}
                                {Integral_Domain_Extension_Theorem}
                    If $\ring[R]{R}$ is a commutative integral domain, then
                    there is a field $\ring[F]{F}$ such that $R$ may be
                    embedded into $F$.
                \end{ftheorem}
                \begin{bproof}
                    For let $A=R\times(R\setminus\{0\})$ and let $\sim$ be the
                    equivalence relation $(a,b)\sim(c,d)$ if and only if
                    $ad\sim{b}c$. Denote the equivalence classes of
                    $A/\sim$ be $a/b$ instead of $[(a,b)]$. Defined
                    $a/b+c/d=(ad+bc)/(bd)$ and $a/b\cdot{c}/d=(ac)/(bd)$.
                    $R$ is contained in this as all $a/1$. This is also the
                    smallest field containing $R$. If $K$ is another field,
                    then $ab^{\minus{1}}$ is contained in here, and this maps
                    to $a/b$. In particular, if $R$ was already a field, then
                    $R=F$.
                \end{bproof}
                \begin{theorem}
                    If $\monoid[S]{S}$ is a cancellative Abelian semigroup, then
                    there is a group $\monoid[G]{G}$ such that $S$ may be
                    embedded into $G$.
                \end{theorem}
                \begin{proof}
                    For Let $A=S\times{S}$ and let $R$ be the equivalence
                    relation defined by $(a,b)R(c,d)$ if and only if
                    $a*d=b*c$. Let $G=A/R$ and define:
                    \begin{equation}
                        [(a,b)]*[(c,d)]=[(a*d,b*c)]
                    \end{equation}
                    Let $1=[(a,a)]$. This is a unit in $G$ since
                    $1*g=[(a,a)]*[(g_{1},g_{2})]=[(a*g_{1},a*g_{2}])]=g$.
                    The inverse of $[(a,b)]$ is $[(b,a)]$ since:
                    \begin{equation}
                        [(a,b)]*[(b,a)]=[(a*b,b*a)]=[(a*b,a*b)]=1
                    \end{equation}
                    Hence, $\monoid{G}$ is an Abelian group. Moreover, $S$
                    embeds into $G$ as.
                \end{proof}
    \par\hfill\par
    Jacobson Lectures Vol I Page 47 exercises 4 and 5, page 48. Page 71
    exercise 1 and 2. Page 78 problem 2.
\end{document}