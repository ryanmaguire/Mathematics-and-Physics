%------------------------------------------------------------------------------%
\documentclass{article}                                                        %
%------------------------------Preamble----------------------------------------%
\makeatletter                                                                  %
    \def\input@path{{../../}}                                                  %
\makeatother                                                                   %
\input{preamble.tex}                                                           %
%----------------------------Main Document-------------------------------------%
\begin{document}
    \title{Notes from Dummit and Foote}
    \author{Ryan Maguire}
    \date{\vspace{-5ex}}
    \maketitle
    \section{Prelims}
        Def permutation, restriction of map. Partition of set.
        \begin{theorem}
            \label{thm:Equiv_Classes_Form_Partition}%
            If $A$ is a set, if $R$ is an equivalence relation on $A$, and if
            $A/R$ is the quotient set, then $A/R$ is a partition of $A$.
        \end{theorem}
        \begin{proof}
            Since for all $x\in{A}$, $[x]\in{A}/R$ and $x\in[x]$. Moreover, if
            $\mathcal{U},\mathcal{V}\in{A}/R$ and if
            $\mathcal{U}\cap\mathcal{V}$ is non-empty, then there is an
            $x\in{R}$ such that $[x]\in\mathcal{U}$ and $[x]\in\mathcal{V}$. But
            if $[y]\in\mathcal{U}$ and $[z]\in\mathcal{V}$, then
            $yRx$ and $xRz$. But then $yRz$ since $R$ is an equivalence relation
            and therefore $[y]\in\mathcal{V}$. Similarly, $[z]\in\mathcal{U}$.
            Hence, either $\mathcal{U}=\mathcal{V}$ or they are disjoint.
            Therefore, $A/R$ is a partition of $A$.
        \end{proof}
        \begin{theorem}
            If $A$ is a set, and if $\mathcal{O}\subseteq\powset{X}$ is a
            partition of $A$, then there is an equivalence relation $R$ on $A$
            such that $\mathcal{O}=A/R$.
        \end{theorem}
        \begin{proof}
            For let $R\subseteq{A}\times{A}$ be defined by:
            \begin{equation}
                R=\{\,(x,y)\in{A}\times{A}\;|\;
                    \exists_{\mathcal{U}\in\mathcal{O}}(x,y\in\mathcal{U})\,\}
            \end{equation}
            then $R$ is an equivalence relation. For since $R$ is a partition,
            for all $x\in{A}$ there is a $\mathcal{U}\in\mathcal{O}$ such that
            $x\in\mathcal{U}$. But then $x\in\mathcal{U}$ and $x\in\mathcal{U}$,
            and therefore $(x,x)\in{R}$. That is, $xRx$. Moreover, if
            $xRy$ then there is a $\mathcal{U}\in\mathcal{O}$ such that
            $x\in\mathcal{U}$ and $y\in\mathcal{U}$. But then $y\in\mathcal{U}$
            and $x\in\mathcal{U}$ and hence $yRx$. Lastly, if $xRy$ and $yRz$,
            then there is a set $\mathcal{U}\in\mathcal{O}$ and a set
            $\mathcal{V}\in\mathcal{O}$ such that $x,y\in\mathcal{U}$ and
            $y,z\in\mathcal{V}$. But $\mathcal{O}$ is a partition, and hence
            either $\mathcal{U}\cap\mathcal{V}=\emptyset$ or
            $\mathcal{U}=\mathcal{V}$. But
            $\mathcal{U}\cap\mathcal{V}\ne\emptyset$ since $y\in\mathcal{U}$ and
            $y\in\mathcal{V}$. Therefore, $\mathcal{U}=\mathcal{V}$ and thus,
            since $z\in\mathcal{V}$, it is true that $z\in\mathcal{U}$. That is,
            $xRz$. Hence, $R$ is an equivalence relation. Moreover, by
            definition, $A/R=\mathcal{O}$.
        \end{proof}
        \begin{theorem}
            \label{thm:Fibers_of_Func_Form_Equiv_Relation}%
            If $A$ and $B$ are sets, if $f:A\rightarrow{B}$ is a function, and
            if $R\subseteq{A}\times{A}$ is the relation defined by:
            \begin{equation}
                R=\{\,(x,y)\in{A}\times{A}\;|\;f(x)=f(y)\,\}
            \end{equation}
            then $R$ is an equivalence relation on $A$.
        \end{theorem}
        \begin{proof}
            For all $x\in{A}$ it is true that $f(x)=f(x)$, and hence $xRx$.
            Moreover, if $x,y\in{A}$ and $xRy$, then $f(x)=f(y)$. But equality
            is reflexive, and hence $f(y)=f(x)$. But then $yRx$. Lastly, by the
            transitivity of equality, if $xRy$ and $yRz$, then $f(x)=f(y)$ and
            $f(y)=f(z)$. But then $f(x)=f(y)$, and hence $xRz$. That is, $R$ is
            and equivalence relation.
        \end{proof}
        \begin{fdefinition}{Divisor of an Integer}{Divisor_of_Integer}
            A divisor of an integer $n\in\mathbb{Z}$ is an integer
            $m\in\mathbb{Z}$ such that there exists and integer $k\in\mathbb{Z}$
            where $k\cdot{m}=n$. We denote this by $m|n$.
        \end{fdefinition}
    \section{Stuff}
        If $\langle\cdot|\cdot\rangle$ is a symmetric bilinear form, it is
        represented by its Gram matrix relative to a bsis $\mathscr{B}$ of the
        finite dimensional vector space $\mathscr{B}$. We have:
        \begin{equation}
            \langle{v}|w\rangle
            =[V]_{\mathscr{B}}^{T}G_{\mathscr{B}}[W]\mathscr{B}
        \end{equation}
        Let $\langle\cdot|\cdot\rangle:V\times{V}\rightarrow{k}$ be bilinear,
        let $\mathscr{B}$ and $\mathscr{C}$ be bases of $V$, and let
        $G_{\mathscr{B}}$ and $G_{\mathscr{C}}$ be the corresponding Gram
        matrices. Let $x,y\in{V}$. Then:
        \begin{subequations}
            \begin{align}
                [x]_{\mathscr{B}}^{T}G_{\mathscr{B}}[y]_{\mathscr{B}}
                &=\langle{x}|y\rangle\\
                &=[x]_{\mathscr{C}}^{T}G_{\mathscr{C}}[Y]_{\mathscr{C}}\\
                &=[\textrm{Id}(x)]_{\mathscr{C}}^{T}G_{\mathscr{C}}
                    [\textrm{Id}(y)]_{\mathscr{C}}\\
                &=\Big([\textrm{Id}]_{\mathscr{C}}^{\mathscr{B}}
                    [x]_{\mathscr{B}}\Big)^{T}G_{\mathscr{C}}
                    \Big([\textrm{Id}_{\mathscr{C}}^{\mathscr{B}}]
                        [y]_{\mathscr{B}}\Big)
            \end{align}
        \end{subequations}
        From this, we obtain the formula for the Gram matrix:
        \begin{equation}
            G_{\mathscr{B}}=P^{T}G_{\mathscr{C}}P
        \end{equation}
        \begin{fdefinition}{Congruent Matrices}{Congruent_Matrices}
            Congruent matrices are matrices $A,B\in{M}_{n}(k)$ such that there
            is an invertible matrice $P\in{GL}_{n}(k)$ such that:
            \begin{equation}
                B=P^{T}AP
            \end{equation}
        \end{fdefinition}
    \section{Diagonalizing Real Quadratic Forms}
        \begin{ltheorem}{Sylvester's Theorem}{Sylvesters_Theorem}
            If $V$ is a finite dimensional vector space over $\mathbb{R}$ and if
            $\langle\cdot|\cdot\rangle$ is a symmetric bilinear form, then there
            exists a basis $\mathscr{B}$ of $V$ such that:
            \begin{equation}
                G_{\mathscr{B}}=
                \begin{bmatrix*}[r]
                    1&\dots&0&0&\dots&0&0&\dots&0\\
                    \vdots&\ddots&\vdots&\vdots&\ddots
                        &\vdots&\vdots&\ddots&\vdots\\
                    0&\dots&1&0&\dots&0&0&\dots&0\\
                    0&\dots&0&\minus{1}&\dots&0&0&\dots&0\\
                    \vdots&\ddots&\vdots&\vdots
                        &\ddots&\vdots&\vdots&\ddots&\vdots\\
                    0&\dots&0&0&\dots&\minus{1}&0&\dots&0\\
                    0&\dots&0&0&\dots&0&0&\dots&0\\
                    \vdots&\ddots&\vdots&\vdots&\ddots
                        &\vdots&\vdots&\ddots&\vdots\\
                    0&\dots&0&0&\dots&0&0&\dots&0\\
                \end{bmatrix*}
            \end{equation}
            Where there are $r$ 1's, $s$ negative 1's, and $t$ zeros, and
            $r$, $s$, and $t$ are uniquely determined.
        \end{ltheorem}
    \section{Orthogonal Transformations}
        Let $(V,\langle\cdot|\cdot\rangle)$ be a nondegenerate bilinear space.
        Then a linear map $T:V\rightarrow{V}$ is orthogonal if or a linear
        isometry if:
        \begin{equation}
            \langle{T}(v)|T(w)\rangle=\langle{v}|w\rangle
        \end{equation}
        Let $\mathscr{B}$ be a basis of $V$ and let $G_{\mathscr{B}}$ be the
        Gram matrix of $\langle\cdot|\cdot\rangle$. Let $A$ be the representing
        matrix of $T$ over the basis $\mathscr{B}$. Then:
        \begin{equation}
            \langle{v}|w\rangle
            =\langle{T}(v)|T(w)\rangle
            =[T(v)]_{\mathscr{B}}^{T}G_{\mathscr{B}}[T(w)]_{\mathscr{B}}
            =[v]_{\mathscr{B}}^{T}A^{T}G_{\mathscr{B}}A[w]_{\mathscr{B}}
        \end{equation}
        From this we can compute what the Gram matrix is:
        \begin{equation}
            G_{\mathscr{B}}=A^{T}G_{\mathscr{B}}A
        \end{equation}
        If $A$ represents an orthogonal transformation, then $A$ must satisfy
        this equation. In the special case of when $\mathscr{B}$ is orthonormal,
        then $G_{\mathscr{B}}$ is simply the identity matrix and thus we have
        that $A^{T}A=I$, or $A^{T}=A^{\minus{1}}$.
        \par\hfill\par
        A nondegenerate skew symmetric bilinear form on a $2n$ dimensional real
        vector space is called a symplectic form. The Gram matrix is:
        \begin{equation}
            J=
            \begin{bmatrix*}[r]
                0&I_{n}\\
                \minus{I}_{n}&0
            \end{bmatrix*}
        \end{equation}
        A transformation $A$ such that $A^{T}JA=J$ is called a symplectic or
        a canonical transformation.
        \par\hfill\par
        If $\langle\cdot|\cdot\rangle$ is the Lorentz form on $\mathbb{R}^{4}$,
        then an orthogonal transformation is called a Lorentz transformation.
    \section{Sesquilinear Geometry}
        Let $V$ and $W$ be $\mathbb{R}$ inner product spaces. That is, $V$ and
        $W$ are equipped with a symmetric bilinear form
        $\langle\cdot|\cdot\rangle_{V}$ and $\langle\cdot|\cdot\rangle_{W}$ that
        are positive-definite:
        \begin{equation}
            \langle{v}|v\rangle\geq{0}
        \end{equation}
        With equality if and only if $v=0$. Note that positive definite implies
        nondegenerate since $\langle{v}|v\rangle>0$ for nonzero $v$. Let
        $T:V\rightarrow{W}$ be a linear map. Then $T$ induces
        $T^{*}:W^{*}\rightarrow{V}^{*}$ by something.
        Let $V$ be a finite dimension $\mathbb{R}$ vector space. An inner
        product on $V$ is a bilinear form that is symmetric and
        positive-definite. Euclidean geometry is derived from the inner product.
        This induces a norm and the notion of angle:
        \begin{equation}
            \norm{v}=\sqrt{\langle{v}|v\rangle}
            \quad\quad
            \theta=\cos^{\minus{1}}\Big(
                \frac{\langle{v}|w\rangle}{\norm{v}\norm{w}}\Big)
        \end{equation}
        Cauchy-Schwartz comes out of this. Let $V$ be a vector space over
        $\mathbb{C}$, say $V=\mathbb{C}^{n}$. If we define:
        \begin{equation}
            \langle{v}|w\rangle=\sum_{k=1}^{n}v_{k}w_{k}
        \end{equation}
        We lose positive-definiteness since $(1,2i)\cdot(1,2i)=\minus{3}$,
        in $\mathbb{C}^{2}$, for example. We define the dot product on
        $\mathbb{C}^{n}$ by:
        \begin{equation}
            \langle{z}|w\rangle=\sum_{k=1}^{n}\overline{z}_{k}\overline{w}_{k}
        \end{equation}
        That is, $\langle{v}|w\rangle=\overline{v}\cdot{w}$. From this we have
        that $\langle{z}|\cdot\rangle$ is linear on $\mathbb{C}$. However,
        looking at $\langle\cdot|w\rangle$, we have that it is $\mathbb{R}$
        linear but only conjugate linear over $\mathbb{C}$. This gives rise to
        the notion of a sesquilinear product.
        \begin{fdefinition}{Sesquilinear Product}{Sesquilinear_Product}
            A sesquilinear product on a vector space $V$ over $\mathbb{C}$ is a
            function $\langle\cdot|\cdot\rangle:V\times{V}\rightarrow\mathbb{C}$
            such that $\langle{z}|\cdot\rangle$ is $\mathbb{C}$ linear and
            $\langle\cdot|w\rangle$ is $\mathbb{R}$ linear and $\mathbb{C}$
            conjugate linear.
        \end{fdefinition}
        With this we can define a Hermitian form on a $\mathbb{C}$ vector space
        $V$. Note that a sesquilinear form is conjugate symmetric. That is,
        $\langle{w}|z\rangle=\overline{\langle{z}|w\rangle}$.
        \begin{fdefinition}{Hermitian Form}{Hermitian_Form}
            A Hermitian form on a $\mathbb{C}$ vector space $V$ is a function
            $\langle\cdot|\cdot\rangle:V\times{V}\rightarrow\mathbb{C}$ that is
            sesquilinear and conjugate symmetric.
        \end{fdefinition}
        \begin{fdefinition}{Hermitian Inner Product Space}
                           {Hermitian_Inner_Product_Space}
            A Hermitian inner product space is a vector space $V$ over
            $\mathbb{C}$ with a Hermitian inner product.
        \end{fdefinition}
        Let $\mathcal{B}$ be a basis of a Hermitian inner product space $V$ and
        let $G_{\mathcal{B}}=[g_{ij}]_{ij}$ where
        $g_{ij}=\langle{e}_{i}|e_{j}\rangle$. Then for $v,w\in{V}$, we have:
        \begin{equation}
            v=\sum_{k=1}^{n}a_{k}e_{k}
            \quad\quad
            w=\sum_{k=1}^{n}b_{k}e_{k}
        \end{equation}
        And moreover:
        \begin{equation}
            \langle{v}|w\rangle=
            \langle\sum_{j=1}^{n}a_{j}e_{j}|\sum_{k=1}^{n}b_{k}e_{k}\rangle
            =\sum_{k=1}^{n}\sum_{j=1}^{n}\overline{a}_{j}b_{k}
                \langle{e_{j}}|e_{k}\rangle
            =\sum_{j=1}^{n}\sum_{k=1}^{n}\overline{a}_{j}g_{jk}b_{k}
        \end{equation}
        So we have:
        \begin{equation}
            \langle{v}|w\rangle
            =\overline{[v]_{\mathcal{B}}^{T}}G_{\mathcal{B}}[w]_{\mathcal{B}}
        \end{equation}
        This gives rise to the definition of a Hermitian transpose.
        \begin{fdefinition}{Hermitian Transpose}{Hermitian_Transpose}
            The Hermitian transpose of a matrix $A$ is the matrix:
            \begin{equation*}
                A^{H}=\overline{A^{T}}
            \end{equation*}
        \end{fdefinition}
    \section{Unitary Transformations}
        \begin{fdefinition}{Unitary Transformations}{Unitary_Transformations}
            A unitary transformation on a vector space $V$ over $\mathbb{C}$ is
            a linear function $T:V\rightarrow{V}$ such that:
            \begin{equation*}
                \langle{T}(v)|T(w)\rangle=\langle{v|w}\rangle
            \end{equation*}
        \end{fdefinition}
        Let $V$ and $W$ be Hermitian inner product spaces and let
        $T:V\rightarrow{W}$ be $\mathbb{C}$ linear. $T$ induces
        $T^{*}:W^{*}\rightarrow{V}^{*}$ by $T^{*}(\psi)=\psi\circ{T}$.
        \begin{fdefinition}{Self-Adjoint Hermitian Operator}
                           {Self-Adjoint_Hermitian_Operator}
            A self-adjoint operator on a Hermitian inner product space $V$ is a
            linear function $T:\mathbb{C}\rightarrow\mathbb{C}$ such that
            $T=T^{H}$.
        \end{fdefinition}
        \begin{fdefinition}{Normal Hermitian Operator}
                           {Normal_Hermitian_Operator}
            A function $T:\mathbb{C}\rightarrow\mathbb{C}$ such that
            $TT^{H}=T^{H}T$
        \end{fdefinition}
    \section{Spectral Theorem}
        \begin{theorem}
            If $V$ is a finite dimensional vector space over $\mathbb{C}$, if
            $T:V\rightarrow{V}$ is a linear map, then $T$ has an eigenvalue.
        \end{theorem}
        \begin{proof}
            For the characteristic polynomial is non-constant and thus by the
            fundamental theorem of algebra there exists a root.
        \end{proof}
        \begin{theorem}
            If $V$ is a finite dimensional Hermitian inner product space and if
            $T:V\rightarrow{V}$ is a Hermitian operator, then the eigenvalues of
            $T$ are real and if $v$ is an eigenvector of $\lambda$ and if $w$ is
            a $\mu$ eigenvector for two different eignvalues $\lambda\ne\mu$,
            then $v$ and $w$ are orthogonal.
        \end{theorem}
        \begin{proof}
            For let $\lambda$ be an eigenvalue and let $v$ be a non-zero
            eigenvector for $\lambda$. Then $T(v)=\lambda{v}$. But $T$ is
            Hermitian, and therefore:
            \begin{equation}
                \langle{T}^{H}(v)|v\rangle
                =\langle{v}|T(v)\rangle
                =\langle{v}|\lambda{v}\rangle
                =\lambda\rangle{v}|v\rangle
            \end{equation}
            But also:
            \begin{equation}
                \langle{T}^{H}(v)|v\rangle
                =\langle{T}(v)|v\rangle
                =\langle{\lambda}v|v\rangle
                =\overline{\lambda}\langle{v}|v\rangle
            \end{equation}
            And therefore, since $\langle{v}|v\rangle\ne{0}$, we have
            $\lambda=\overline{\lambda}$, and therefore $\lambda$ is real. For
            the second part, we have:
            \begin{equation}
                \langle{v}|T(w)\rangle
                =\langle{v}|\mu{w}\rangle
                =\mu\langle{v}|w\rangle
                =\langle{T}^{H}(v)|w\rangle
                =\langle{T}(v)|w\rangle
                =\langle\lambda{v}|w\rangle
                =\overline{\lambda}\langle{v}|w\rangle
            \end{equation}
            But we just proved that $\lambda$ is real, and thus
            $\overline{\lambda}=\lambda$. Moreover $\lambda\ne\mu$, and thus
            for equality to occur we must have $\langle{v}|w\rangle=0$.
        \end{proof}
        \begin{theorem}
            If $V$ is a finite dimensional Hermitian inner product space, if
            $T:V\rightarrow{V}$ is linear, and if $W\subseteq{V}$ is a $T$
            invariant subspace (that is, $T(W)\subseteq{W}$), then
            $W^{\perp}$ is $T^{H}$ invariant.
        \end{theorem}
        \begin{proof}
            For let $x\in{W}^{\perp}$ and let $w\in{W}$. Then:
            \begin{equation}
                \langle{T}^{H}(x)|w\rangle
                =\langle{x}|T(w)\rangle=0
            \end{equation}
            And thus $T(x)\in{W}^{\perp}$. Therefore,
            $T(W^{\perp})\subseteq{W}^{\perp}$.
        \end{proof}
        \begin{ltheorem}{Unitary Triangulation Theorem}
                        {Unitary_Triangulation_Theorem}
            If $V$ is a finite dimensional Hermitian inner product space over
            $V$ and if $T:V\rightarrow{V}$ is a linear operator, then there
            exists an orthonormal basis $\mathscr{B}$ of $V$ such that
            $[T]_{\mathscr{B}}^{\mathscr{B}}$ is upper triangular.
        \end{ltheorem}
        \begin{proof}
            For consider $T^{H}:V\rightarrow{V}$. It has an eigenvalue
            $\lambda$. Let $v$ be a $\lambda$ eigenvector of $T^{H}$ and let
            $W=\mathbb{C}\cdot{v}=\textrm{Span}\{zv:z\in\mathbb{C}\}$. Since
            $v$ is an eigenvector of $T^{H}$ we have that $W$ is a $T^{H}$
            invariant subspace so therefore $W^{\perp}$ is invariant under
            $(T^{H})^{H}=T$. That is, $W^{\perp}$ is a $T$ invariant subspace.
            Since $W$ is non-zero, $W^{\perp}$ has dimension less than $V$ and
            thus by induction there is an orthonormal basis of $W^{\perp}$ such
            that $[T_{W^{\perp}}]_{\mathscr{B}'}^{\mathscr{B}'}$ is upper
            triangular. Extending $\mathscr{B}$ from $\mathscr{B}'$ gives an
            orthonormal basis such that $[T]_{\mathscr{B}}^{\mathscr{B}}$ is
            upper triangular.
        \end{proof}
        \begin{theorem}
            If $A\in{M}_{n}(\mathbb{C})$ then there is a unitary matric
            $P\in{U}(n)$ such that $PAP^{\minus{1}}$ is upper triangular.
        \end{theorem}
        \begin{ftheorem}{Spectral Theorem for Hermitian Operators}
                        {Spectral_Theorem_for_Hermitian_Operators}
            If $V$ is a finite dimensional Hermitian inner product space and if
            $T:V\rightarrow{V}$ is a Hermitian operator then there exists an
            orthonormal basis $\mathscr{B}$ of $V$ such that
            $[T]_{\mathscr{B}}^{\mathscr{B}}$ is diagonal.
        \end{ftheorem}
        \begin{proof}
            For the representing matrix of $T$ is upper triangular, and thus the
            representing matrix for $T^{H}$ is lower triangular. But $T=T^{H}$
            and therefore the represeting matrix of $T$ is both upper and lower
            triangular, and therefore it is diagonal.
        \end{proof}
        The theorem holds for normal operators as well. For Hermitian we see
        that the eigenvalues are real. For skew-Hermitian ($T=\minus{T}^{H}$)
        the eigenvalues are purely imaginary, and lastly for a unitary $T$ the
        eigenvalues are unit modulus.
        \begin{ftheorem}{Real Spectral Theorem}
            If $V$ is a finite dimensional inner product space, if
            $T:V\rightarrow{V}$ is a self-adjoint operator then there is an
            orthonormal basis of $V$ such that the representing matrix is
            diagonal.
        \end{ftheorem}
        \begin{proof}
            Let $A$ be the representing matrix of $T$ relative to the standard
            basis of $\mathbb{R}^{n}$. Then $A=A^{T}$ and thus $A$ defines a
            linear map $A:\mathbb{C}^{n}\rightarrow\mathbb{C}^{n}$ by mapping
            $A(v)=Av$ as a matrix operatoion. Then $A$ is a Hermitian operator
            and therefore $A$ has real eigenvalues. Let $v\in\mathbb{C}^{n}$ be
            a complex eigenvector for $\lambda$. Let $v=x+iy$ for
            $x,y\in\mathbb{R}^{n}$. Then since $Av=\lambda{v}$ we have:
            \begin{equation}
                Ax+iAy=\lambda{x}+i\lambda{y}
            \end{equation}
            But $A$ is real, as are $x$ and $y$, and thus we have:
            \begin{equation}
                Ax=\lambda{x}
                \quad\quad
                Ay=\lambda{y}
            \end{equation}
            But since $v$ is non-zero, at least one of $x$ or $y$ is non-zero.
            Thus $A$ has a real eigenvector, $x$ or $y$. Let $u$ be a unit
            real eigenvector and let $W$ be the real span of $u$. Then $W$ is
            $T$ invariant and therefore $W^{\perp}$ is $T^{H}$ invariant, but
            $T=T^{H}$. We complete the proof by induction.
        \end{proof}
    \section{Tensor Products}
        Let $R$ be a ring nd let $M$ be a right $R$ module and let $N$ be a
        left $R$ module. We seek a universal product $M\times{N}$ into
        $M\otimes_{R}N$. Examples of products: Dot products, cross products,
        bilinear forms, matrix multiplication. All of these are Biadditive.
        Matrices also have the \textit{balanced} property:
        \begin{equation}
            A(cB)=(Ac)B
        \end{equation}
        If the underlying ring is not commutative, we may not have perfect
        bilinearity and so we replace this requirement with the balanced
        property. So we seek a function
        $\beta:M\times{N}\rightarrow{M}\otimes_{R}N$ such that:
        \begin{align}
            \beta(a+b,c)&=\beta(a,b)+\beta(b,c)\tag{Left Additivity}\\
            \beta(a,b+c)&=\beta(a,b)+\beta(a,c)\tag{Right Additivity}\\
            \beta(ar,b)&=\beta(a,rb)\tag{Balanced}
        \end{align}
        Universal means that given any Abelian group $A$ and any biadditive
        $R$ balanced map $\mu:M\times{N}\rightarrow{A}$ there is a unique
        $\mathbb{Z}$ module homomorphism
        $\tilde{\mu}:M\otimes_{R}N\rightarrow{A}$ such that the diagram
        commutes. Since if it exists it is unique up to unique isomorphism, we
        need only construct such a thing. Long complicated construction to
        follow. By construction, $M\otimes_{R}N$ is generated by $m\otimes{n}$
        for all $m\in{M}$, $n\in{N}$. As a warning, not every element of
        $M\otimes_{R}N$ need be decomposable. It's folly to try to define a map
        $M\times{N}\rightarrow{A}$ by $m\otimes{n}\mapsto{f}(m,n)$ unless
        $f(m,n)$ is biadditive and $R$ balanced.
        \begin{theorem}
            If $N$ is a left $R$ module, then $R\otimes_{R}N$ is isomorphic to
            $N$ as a $\mathbb{Z}$ module.
        \end{theorem}
        \begin{proof}
            For let $\mu:R\times{N}\rightarrow{N}$ by defined by
            $\mu(r,n)=r\cdot{n}$. Then $\mu$ is biaddiative and balanced.
        \end{proof}
        The idea is to use scalar multiplication $R\times{N}\rightarrow{N}$ to
        construct such a module. The by the universal mapping property there
        exists a unique $\mathbb{Z}$ module homomorphism
        $\tilde{\mu}:R\otimes_{R}N\rightarrow{N}$ such that the diagram
        commutes. Define $\tilde{\mu}$ by:
        \begin{equation}
            \tilde{\mu}(r\otimes{n})=r\cdot{n}
        \end{equation}
        Define $j:N\rightarrow{R}\otimes_{R}N$ by $j(n)=1\otimes{n}$.
        This is a $\mathbb{Z}$ module since the tensor product is biaddiative
        and balanced. Moreover, $\tilde{\mu}$ and $j$ are inverses of each
        other.
        \begin{theorem}
            If $R$ is a ring, if $I$ is an ideal of $R$, and if $N$ is a left
            $R$ module, then:
            \begin{equation}
                R/I\otimes_{R}N\simeq{N}/IN
            \end{equation}
        \end{theorem}
        \begin{proof}
            For define $\mu:R/I\times{N}\rightarrow{N}/IN$ by:
            \begin{equation}
                \mu(\overline{r},n)=\mu(r+I,n)=rn+IN=\overline{rn}
            \end{equation}
            For $n\in{N}$ let $f_{N}:R\rightarrow{N}/IN$ be defined by
            $f_{n}(r)=rn+IN=\overline{rn}$. Then $f_{n}$ is a map of left
            $R$ modules and therefore induces a map $\overline{f}_{n}$ from
            $R/I$ to $N/IN$ by $\overline{r}\mapsto\overline{rn}$. By the
            universal mapping property there is a $\mathbb{Z}$ linear map
            $\tilde{\mu}$ such that the diagram commutes. So we have that
            $\tilde{\mu}(\overline{r}\otimes{n})=\overline{rn}$. We now want a
            map $N/IN\rightarrow{R}/I\otimes_{R}N$. Let $j$ be defined by
            $j(n)=\overline{1}\otimes{n}$. Then $j$ vanishes on generators of
            $IN$ and therefore induces $\overline{j}$ such that
            $\overline{n}\mapsto\overline{1}\otimes{n}$. Thus we now have two
            maps that are well defined and now we need only check that they are
            inverses of each other. And it is so. So we are done.
        \end{proof}
        \begin{theorem}
            Given a sequence of modules over $R$,
            $N'\rightarrow{N}\rightarrow{N}''\rightarrow{0}$,
            and suppose that for all left $R$ modules $Y$ the
            sequence:
            \begin{equation}
                0\rightarrow\textrm{Hom}_{R}(N'',Y)
                \rightarrow\textrm{Hom}_{R}(N,Y)
                \rightarrow\textrm{Hom}_{R}(N',Y)
            \end{equation}
            is exact, then the original sequence is exact.
        \end{theorem}
        \begin{proof}
            For let $Y=\textrm{coker}(v)=N''/v(N)$. Then since:
            \begin{equation}
                0\rightarrow\textrm{Hom}_{R}(N'',Y)
                \rightarrow\textrm{Hom}_{R}(N,Y)
                \rightarrow\textrm{Hom}_{R}(N',Y)
            \end{equation}
            is exact, we have that the canonical projection $\pi$ gets mapped
            to $v^{*}(\pi)$. But $v^{*}(\pi)=\pi\circ{v}$, and this is zero
            since $n\mapsto{v}(n)$ which maps to $0$ by $\pi$. Thus $v^{*}$ is
            surjective and injective. On the other side, let $Y=N''$. Then
            $\textrm{id}_{N''}$ mapts to $v$ under $v^{*}$, and thus
            $v$ maps to $0$ under $u^{*}$ since the sequence is exact, and
            thus $u\circ{v}=0$. Thus $\textrm{im}(u)\subseteq\textrm{ker}(u)$.
        \end{proof}
    \section{Tensor Products of Algebras}
        An algebra over a commutative ring $k$ is a left $k$ module with a ring
        structure such that the ring multiplication is compatible with scalar
        multiplication:
        \begin{equation}
            (\lambda\star{a})\cdot{b}=\lambda\star(a\cdot{b})
                =a\cdot(\lambda\star{b})
        \end{equation}
        An equivalent definition is a ring $A$ with a ring homomorphism
        $\varphi:k\rightarrow{Z}(A)$. We can also define an algebra in terms of
        the tensor product. An algebra over $k$ is a $k$ module $A$ with a
        homomorphism $\mu:A\otimes{A}\rightarrow{A}$ and a module homomorphism
        $\eta:k\rightarrow{A}$ such that some diagram commutes.
\end{document}