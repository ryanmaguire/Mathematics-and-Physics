%------------------------------------------------------------------------------%
\documentclass{article}                                                        %
%------------------------------Preamble----------------------------------------%
\makeatletter                                                                  %
    \def\input@path{{../../}}                                                  %
\makeatother                                                                   %
\input{preamble.tex}                                                           %
%----------------------------Main Document-------------------------------------%
\begin{document}
    \title{Algebra}
    \author{Ryan Maguire}
    \date{\vspace{-5ex}}
    \maketitle
    \tableofcontents
    \clearpage
    \section{DF Chapter 0}
        \subsection{Arithmetic}
            Def permutation, restriction of map. Partition of set.
            \begin{theorem}
                \label{thm:Equiv_Classes_Form_Partition}%
                If $A$ is a set, if $R$ is an equivalence relation on $A$, and
                if $A/R$ is the quotient set, then $A/R$ is a partition of $A$.
            \end{theorem}
            \begin{proof}
                Since for all $x\in{A}$, $[x]\in{A}/R$ and $x\in[x]$. Moreover,
                if $\mathcal{U},\mathcal{V}\in{A}/R$ and if
                $\mathcal{U}\cap\mathcal{V}$ is non-empty, then there is an
                $x\in{R}$ such that $[x]\in\mathcal{U}$ and $[x]\in\mathcal{V}$.
                But if $[y]\in\mathcal{U}$ and $[z]\in\mathcal{V}$, then
                $yRx$ and $xRz$. But then $yRz$ since $R$ is an equivalence
                relation and therefore $[y]\in\mathcal{V}$. Similarly,
                $[z]\in\mathcal{U}$. Hence, either $\mathcal{U}=\mathcal{V}$ or
                they are disjoint. Therefore, $A/R$ is a partition of $A$.
            \end{proof}
            \begin{theorem}
                If $A$ is a set, and if $\mathcal{O}\subseteq\powset{X}$ is a
                partition of $A$, then there is an equivalence relation $R$ on
                $A$ such that $\mathcal{O}=A/R$.
            \end{theorem}
            \begin{proof}
                For let $R\subseteq{A}\times{A}$ be defined by:
                \begin{equation}
                    R=\{\,(x,y)\in{A}\times{A}\;|\;
                        \exists_{\mathcal{U}\in\mathcal{O}}
                        (x,y\in\mathcal{U})\,\}
                \end{equation}
                then $R$ is an equivalence relation. For since $R$ is a
                partition, for all $x\in{A}$ there is a
                $\mathcal{U}\in\mathcal{O}$ such that $x\in\mathcal{U}$. But
                then $x\in\mathcal{U}$ and $x\in\mathcal{U}$, and therefore
                $(x,x)\in{R}$. That is, $xRx$. Moreover, if $xRy$ then there is
                a $\mathcal{U}\in\mathcal{O}$ such that $x\in\mathcal{U}$ and
                $y\in\mathcal{U}$. But then $y\in\mathcal{U}$ and
                $x\in\mathcal{U}$ and hence $yRx$. Lastly, if $xRy$ and $yRz$,
                then there is a set $\mathcal{U}\in\mathcal{O}$ and a set
                $\mathcal{V}\in\mathcal{O}$ such that $x,y\in\mathcal{U}$ and
                $y,z\in\mathcal{V}$. But $\mathcal{O}$ is a partition, and hence
                either $\mathcal{U}\cap\mathcal{V}=\emptyset$ or
                $\mathcal{U}=\mathcal{V}$. But
                $\mathcal{U}\cap\mathcal{V}\ne\emptyset$ since $y\in\mathcal{U}$
                and $y\in\mathcal{V}$. Therefore, $\mathcal{U}=\mathcal{V}$ and
                thus, since $z\in\mathcal{V}$, it is true that
                $z\in\mathcal{U}$. That is, $xRz$. Hence, $R$ is an equivalence
                relation. Moreover, by definition, $A/R=\mathcal{O}$.
            \end{proof}
            \begin{theorem}
                \label{thm:Fibers_of_Func_Form_Equiv_Relation}%
                If $A$ and $B$ are sets, if $f:A\rightarrow{B}$ is a function,
                and if $R\subseteq{A}\times{A}$ is the relation defined by:
                \begin{equation}
                    R=\{\,(x,y)\in{A}\times{A}\;|\;f(x)=f(y)\,\}
                \end{equation}
                then $R$ is an equivalence relation on $A$.
            \end{theorem}
            \begin{proof}
                For all $x\in{A}$ it is true that $f(x)=f(x)$, and hence $xRx$.
                Moreover, if $x,y\in{A}$ and $xRy$, then $f(x)=f(y)$. But
                equality is reflexive, and hence $f(y)=f(x)$. But then $yRx$.
                Lastly, by the transitivity of equality, if $xRy$ and $yRz$,
                then $f(x)=f(y)$ and $f(y)=f(z)$. But then $f(x)=f(y)$, and
                hence $xRz$. That is, $R$ is and equivalence relation.
            \end{proof}
            \begin{theorem}
                \label{thm:Weak_Euc_Division_Alg}%
                If $n,m\in\mathbb{N}^{+}$, then there exists $q,r\in\mathbb{N}$
                such that $n=q\cdot{m}+r$ with $r<m$.
            \end{theorem}
            \begin{proof}
                For let $G\subseteq\mathbb{N}$ be defined by:
                \begin{equation}
                    G=\{\,k\in\mathbb{N}\;|\;n-k\cdot{m}\geq{0}\,\}
                \end{equation}
                if $n<m$, then $G=\{0\}$ and hence choosing $r=n$ and $q=0$
                works. If $n=m$, $q=1$ and $r=0$ does the trick. Otherwise, $G$
                is non-empty and bounded since it is bounded by $n$, and hence
                there is a greatest element. Let $k\in{G}$ be the greatest
                element, and let $r=n-k\cdot{m}$. Since $k\in{G}$, $r>0$.
                Moreover, $r<m$. For if not, then
                $r-m=n-k\cdot{m}-m=n-(k+1)\cdot{m}\geq{0}$, which contradicts
                the maximality of $k$. But then $n=km+r$ and $r<m$.
            \end{proof}
            \begin{ftheorem}{Euclid's Division Algorithm}{Euclid_Division_Algorithm}
                If $m,n\in\mathbb{Z}$, and if $b\ne{0}$, then there exists unique
                $q\in\mathbb{Z}$ and $r\in\mathbb{N}$ such that $r<|m|$ and:
                \begin{equation*}
                    n=mq+r
                \end{equation*}
            \end{ftheorem}
            \begin{bproof}
                For if $n=0$, then let $r=0$ and $q=0$. If $n>0$, and if $m>0$,
                then by Thm.~\ref{thm:Weak_Euc_Division_Alg} there exists
                $q,r$ such that $n=q\cdot{m}+r$ with $r<m$. If $n>0$ and $m<0$,
                then $\minus{m}>0$. But then by
                Thm.~\ref{thm:Weak_Euc_Division_Alg} there exists $q,r$ such that
                $n=q\cdot(\minus{m})+r$ with $r<\minus{m}$. But then
                $n=(\minus{q})\cdot{m}+r$, and $r<|m|$. Lastly, if
                $n<0$ and $m<0$, then $\minus{n}>0$ and $\minus{m}>0$. But then by
                Thm.~\ref{thm:Weak_Euc_Division_Alg} there exists $q,r$ such that
                $\minus{n}=q\cdot(\minus{m})+r$ and $r<\minus{m}$. But then
                $n=q\cdot{m}-r$, and hence $n=(q+1)\cdot{m}+(\minus{r}-m)$. But 
                $r<\minus{m}$, and therefore $0<\minus{r}-m$ and
                $\minus{r}-m<|m|$. Moreover, $q$ and $r$ are unique. For suppose
                $n=q_{0}\cdot{m}+r_{0}$ and $n=q_{1}\cdot{m}+r_{1}$. If
                $q_{0}\ne{q}_{1}$, then either $q_{0}<q_{1}$ or $q_{1}<q_{0}$.
                Suppose $q_{0}<q_{1}$ and let $k=q_{1}-q_{0}$. But then
                \begin{equation}
                    n=q_{0}\cdot{m}+r_{0}=(q_{1}-k)\cdot{m}+r_{1}
                        =q_{1}\cdot{m}+r_{0}-k\cdot{m}
                        =q_{1}\cdot{m}+r_{1}
                \end{equation}
                and therefore $r_{1}=r_{0}-k\cdot{m}$, so $r_{0}=r_{1}+k\cdot{m}$,
                a contradiction since $r_{0}<m$. Hence, $q_{0}=q_{1}$. But then
                taking the difference, we have $r_{0}=r_{1}$. Thus, they are unique.
            \end{bproof}
            \begin{fdefinition}{Divisor of an Integer}{Divisor_of_Integer}
                A divisor of an integer $n\in\mathbb{Z}$ is an integer
                $m\in\mathbb{Z}$ such that there exists and integer $k\in\mathbb{Z}$
                where $k\cdot{m}=n$. We denote this by $m|n$.
            \end{fdefinition}
            \begin{theorem}
                \label{thm:One_is_Divisor}%
                If $n\in\mathbb{Z}$, then $1|n$.
            \end{theorem}
            \begin{proof}
                For $n=1\cdot{n}$, and hence $1$ is a divisor of $n$
                (Def.~\ref{def:Divisor_of_Integer}).
            \end{proof}
            \begin{theorem}
                \label{thm:Greater_is_not_divisor_of_lesser}%
                If $n,m\in\mathbb{N}^{+}$ and $m<n$, then $n$ does not divide $m$.
            \end{theorem}
            \begin{proof}
                For suppose not. If $n|m$, then there exists $k\in\mathbb{Z}$ such
                that $m=k\cdot{n}$. But $n,m\in\mathbb{N}^{+}$ and therefore $n>0$
                and $m>0$. But $m=k\cdot{n}$, and hence $k>0$. But then
                $m=k\cdot{n}<1\cdot{n}=n$, a contradiction since $m<n$.
            \end{proof}
            Well ordering of $\mathbb{N}$. Well ordering of countable set.
            \begin{fdefinition}{Greatest Common Divisor}{GCD}
                A greatest common divisor of two integers
                $n,m\in\mathbb{Z}\setminus\{0\}$ is an integer $k\in\mathbb{N}^{+}$
                such that $k|n$, $k|m$, and for all $j\in\mathbb{N}^{+}$ where
                $j|n$ and $j|m$ it is true that $j\leq{k}$.
            \end{fdefinition}
            We say \textit{a} greatest divisor since we don't know \textit{a priori}
            if there are many, or if there is one at all.
            \begin{theorem}
                \label{thm:GCD_Existence_Theorem}%
                If $n,m\in\mathbb{Z}\setminus\{0\}$, then there exists a greatest
                common divisor.
            \end{theorem}
            \begin{proof}
                For let $G\subseteq\mathbb{N}^{+}$ be defined by:
                \begin{equation}
                    G=\{\,k\in\mathbb{N}^{+}\;|\;k|n\textrm{ and }k|m\,\}
                \end{equation}
                Then $G$ is non-empty since $1\in{G}$. That is, $1|n$ and $1|m$
                (Thm.~\ref{thm:One_is_Divisor}). If $k\in{G}$, then
                $k$ divides $|n|$ and $|m|$. But since
                $n,m\in\mathbb{Z}\setminus\{0\}$ it is true that
                $|n|,|m|\in\mathbb{N}^{+}$. But then if $k$ divides $|n|$ and $|m|$,
                then $k<|n|$ and $k<|m|$
                (Thm.~\ref{thm:Greater_is_not_divisor_of_lesser}). Hence, $G$ is
                bounded above. But then there is a greatest element $k\in{G}$.
                But then $k|m$ and $k|n$, and for all $j\in\\mathbb{N}^{+}$ such
                that $j|m$ and $j|n$, then $j\leq{k}$. Hence, $k$ is the greatest
                common denominator.
            \end{proof}
            \begin{theorem}
                \label{thm:GCD_Unique}%
                If $n,m\in\mathbb{Z}\setminus\{0\}$, if $k$ is a greatest common
                denominator of $n$ and $m$, and if $j$ is a greatest common
                denominator of $n$ and $m$, then $k=j$.
            \end{theorem}
            \begin{proof}
                For if not, then either $j<k$ or $k<j$. But if $j<k$ then $j$ is
                not the greatest common denominator since there exists an element
                $K\in\mathbb{N}^{+}$ such that $k|m$, $k|n$, and such that
                $j<k$, a contradiction (Def.~\ref{def:GCD}). Similarly $k\not<j$,
                and hence $j=k$.
            \end{proof}
            The algorithmic way to go about showing there exists a unique greatest
            common divisor is to apply Euclid's algorithm. We perform division with
            remainder repeatedly until we're left with no remaining term. Suppose
            we're given $n,m\in\mathbb{N}^{+}$ and want to compute
            $\GCD(n,m)$. We do:
            \par
            \begin{subequations}
                \begin{minipage}[b]{0.49\textwidth}
                    \centering
                    \begin{align}
                        n&=q_{0}m+r_{0}\\
                        m&=q_{1}r_{0}+r_{2}\\
                        r_{0}&=q_{2}r_{1}+r_{2}
                    \end{align}
                \end{minipage}
                \hfill
                \begin{minipage}[b]{0.49\textwidth}
                    \centering
                    \begin{align}
                        r_{n-3}&=q_{n-1}r_{n-2}+r_{n-1}\\
                        r_{n-2}&=q_{n}r_{n-1}+r_{n}\\
                        r_{n-1}&=q_{n+1}r_{n}
                    \end{align}
                \end{minipage}
            \end{subequations}
            \par\vspace{2.5ex}
            The $\GCD$ is the last non-zero remainder term.
            \begin{example}
                Let's compute the $\GCD$ of $34$ and $51$. We have:
                \twocolumneq{34=1\cdot{34}+17}{34=2\cdot{17}}
                and so $\GCD(34,51)=17$. Perhaps bigger numbers will better
                demonstrate the algorithm. Let's compute $\GCD(57970,10353)$. We
                obtain:
                \par
                \begin{subequations}
                    \begin{minipage}[b]{0.49\textwidth}
                        \centering
                        \begin{align}
                            57970&=5\cdot{10353}+6205\\
                            10353&=1\cdot{6205}+4148\\
                            6205&=1\cdot{4148}+2057
                        \end{align}
                    \end{minipage}
                    \hfill
                    \begin{minipage}[b]{0.49\textwidth}
                        \centering
                        \begin{align}
                            4148&=2\cdot{2057}+34\\
                            2057&=60\cdot{34}+17\\
                            34&=2\cdot{17}
                        \end{align}
                    \end{minipage}
                \end{subequations}
                after running the gauntlet, the last equation has zero remainder:
                $34=2\cdot{17}+0$ and hence $\GCD(57970,10353)=17$.
            \end{example}
            \begin{fdefinition}{Least Common Multiple}{LCM}
                A least common multiple of two integers $n,m\in\mathbb{N}$ is an
                integer $k\in\mathbb{N}^{+}$ such that $n|k$, $m|k$, and for all
                $j\in\mathbb{N}^{+}$ such that $n|k$ and $m|k$ it is true that
                $k|j$.
            \end{fdefinition}
            \begin{theorem}
                \label{thm:LCM_Existence_Theorem}%
                If $n,m\in\mathbb{N}^{+}$, then there is a least common multiple of
                $n$ and $m$.
            \end{theorem}
            \begin{proof}
                For let $G\subseteq\mathbb{N}^{+}$ be defined by:
                \begin{equation}
                    G=\{\,k\in\mathbb{N}^{+}\;|\;n|k\textrm{ and }n|k\,\}
                \end{equation}
                Then $G$ is non-empty since $n\cdot{m}\in{G}$, and it is bounded
                below and hence there is a least element.
            \end{proof}
            \begin{theorem}
                \label{thm:LCM_Unique}%
                If $n,m\in\mathbb{N}^{+}$, if $k$ is a least common multiple of $n$
                and $m$, and if $j$ is a least common multiple of $n$ and $m$, then
                $k=j$.
            \end{theorem}
            \begin{proof}
                For if not, then either $j<k$ or $k<j$, violating minimality.
            \end{proof}
            \begin{ftheorem}{B\'{e}zout's Identity}{Bezout_Identity}
                If $a,b\in\mathbb{Z}\setminus\{0\}$, and if $d=\GCD(a,b)$, then
                there exist $n,m\in\mathbb{N}$ such that:
                \begin{equation}
                    a\cdot{n}+b\cdot{m}=d
                \end{equation}
            \end{ftheorem}
            \begin{bproof}
                For let $G\subseteq\mathbb{N}\setminus\{0\}$ be defined by:
                \begin{equation}
                    G=\{\,k\in\mathbb{N}\setminus\{0\}\;|\;
                        \textrm{ There exists } n,m\in\mathbb{Z}\textrm{ such that }
                            k=an+bm\big)\,\}
                \end{equation}
                then $G$ is non-empty since $a^{2}+b^{2}\in{G}$. But then there is a
                least element $d\in{G}$. By Euclid's division algorithm, there
                exists $q\in\mathbb{Z}$ and $r\in\mathbb{N}$ such that $r<d$ and
                $a=q\cdot{d}+r$ (Thm.~\ref{thm:Euclid_Division_Algorithm}). But
                $d\in{G}$, and hence there are $n,m\in\mathbb{Z}$ such that
                $d=a\cdot{n}+b\cdot{m}$. But then $r=a(1-qn)+b\cdot(\minus{q}m)$.
                But if $r>0$, then $r\in{G}$ and $r<d$, a contradiction since $d$ is
                the least element of $G$. Hence, $r=0$ and $d$ divides $a$
                (Def.~\ref{def:Divisor_of_Integer}). Similarly, $d$ divides $b$.
                If $c\in\mathbb{N}^{+}$ is such that $c|a$ and $c|b$, then there
                exists $s,t\in\mathbb{Z}$ such that $a=c\cdot{s}$ and $b=c\cdot{t}$.
                But then:
                \begin{equation}
                    d=a\cdot{n}+b\cdot{m}=(c\cdot{s})n+(c\cdot{t})m=c(sn+tm)
                \end{equation}
                and therefore $d$ divides $c$. But it $d$ divides $c$, then
                $d$ is not greater than $c$
                (Thm.~\ref{thm:Greater_is_not_divisor_of_lesser}), and therefore
                $c\leq{d}$. Thus, $d$ is the greatest common divisor
                (Def.~\ref{def:GCD}).
            \end{bproof}
            \begin{theorem}
                \label{thm:Divisor_of_AB_Divides_GCD}%
                If $a,b,n\in\mathbb{Z}\setminus\{0\}$, and if $n|a$ and $n|b$, then
                $n|\GCD(a,b)$.
            \end{theorem}
            \begin{proof}
                For by B\'{e}zout's identity there exists $s,t\in\mathbb{Z}$ such
                that $as+bt=\GCD(a,b)$. But $n|a$ and $n|b$, and thus there exists
                $u,v\in\mathbb{Z}$ such that $nu=a$ and $nv=b$. But then
                $n(us+vt)=\GCD(a,b)$. Therefore, $n$ divides $\GCD(a,b)$
                (Def.~\ref{def:Divisor_of_Integer}).
            \end{proof}
            \begin{theorem}
                \label{thm:n_Div_AB_then_A_Div_AS_BT}%
                If $a,b,s,t,n\in\mathbb{N}$, if $n|a$, and if $n|b$, then $a$
                divides $as+bt$.
            \end{theorem}
            \begin{proof}
                For if $n$ divides $a$ and $b$, then there exists $u,v\in\mathbb{Z}$
                such that $nu=a$ and $nv=b$ (Def.~\ref{def:Divisor_of_Integer}). But
                then $as+bt=nus+nvt=n(us+bt)$, and thus $n$ divides $as+bt$.
            \end{proof}
            Many great theorems from antiquity then become corollaries of the
            B\'{e}zout identity.
            \begin{ftheorem}{Euclid's Prime Number Lemma}{Euclid_Prime_Number_Lemma}
                If $a,b,p\in\mathbb{Z}\setminus\{0\}$, if $\GCD(a,p)=1$, and if $p$
                divides $a\cdot{b}$, then $p$ divides $b$.
            \end{ftheorem}
            \begin{bproof}
                Since $\GCD(a,p)=1$, by B\'{e}zout's identity there exist integers
                $n,m\in\mathbb{Z}$ such that $an+pm=1$
                (Thm.~\ref{thm:Bezout_Identity}). But then $abn+apm=b$. But $p$
                divides $ab$ and hence there is a $k\in\mathbb{Z}$ such that $kp=ab$
                and therefore $kpn+apm=b$. But then $p(kn+am)=b$, and hence $p$
                divides $b$ (Def.~\ref{def:Divisor_of_Integer}).
            \end{bproof}
            The commonly quoted corrolary of this goes as follows:
            \begin{theorem}
                \label{thm:Prime_Div_AB_then_PdivA_or_PdivB}%
                If $a,b\in\mathbb{N}^{+}$, if $p\in\mathbb{N}^{+}$ is prime, and if
                $p$ divides $a\cdot{b}$, then either $p$ divides $a$ or $p$ divides
                $b$.
            \end{theorem}
            \begin{proof}
                For if $p$ does not divide $a$, then $\GCD(a,p)=1$, and thus
                $p$ divides $b$ (Thm.~\ref{thm:Euclid_Prime_Number_Lemma}).
                Similary, if $p$ does not divide $b$ then it divides $a$.
            \end{proof}
            \begin{theorem}
                If $m,n\in\mathbb{N}^{+}$, if $l$ is the least common multiple of
                $n$ and $m$, and if $d$ is the greatest common divisor of $n$ and
                $m$, then $l\cdot{d}=n\cdot{m}$.
            \end{theorem}
            \begin{ftheorem}{Fundamental Theorem of Arithmetic}
                            {Fundamental_Theorem_of_Arithmetic}
                If $n\in\mathbb{N}^{+}$, then there exists a unique subset
                $S\subseteq\mathbb{N}^{+}\times{N}^{+}$ such that for all
                $(p,n)\in{S}$ it is true that $p$ is prime, and such that:
                \begin{equation*}
                    n=\prod_{(p,n)\in{S}}p^{n}
                \end{equation*}
            \end{ftheorem}
            That is, every integer has a unique prime factorization. If $n$ is a
            prime, the factorization is simply $S=\{(n,1)\}$.
            \begin{theorem}
                \label{thm:Composite_N_Exists_AB_N_Div_AB_and_N_NDiv_A_or_B}%
                If $n\in\mathbb{N}^{+}$ is not prime, then there exists
                integers $a,b\in\mathbb{N}^{+}$ such that $n$ divides
                $ab$, but $n$ does not divide $a$ and $n$ does not divide $b$.
            \end{theorem}
            \begin{proof}
                For of $n$ is compositive, then there is a prime $p$ that divides
                $n$. Let $q=n/p$. By the fundamental theorem of arithmetic, there
                exists a subset $S\subseteq\mathbb{N}^{+}\times\mathbb{N}^{+}$ such
                that for all $(p,n)\in{S}$ it is true that $p$ is prime and
                $q=\prod_{(p,n)\in{S}}p^{n}$
                (Thm.~\ref{thm:Fundamental_Theorem_of_Arithmetic}). But since
                $p$ divides $n$ and $p<n$, $n$ does not divide $p$
                (Thm.~\ref{thm:Greater_is_not_divisor_of_lesser}). Since there are
                infinitely many primes, there is a prime $P$ not contained in the
                projection of $S$ into the first variable. Let
                $a=p$ and $b=q\cdot{P}$. Then $n$ does not divide $q$, and it is
                not divide $P$, and hence it does not divide $b$, but it does divide
                $a\cdot{b}$.
            \end{proof}
            \begin{example}
                The smallest example we have to work with is 4, and so we direct
                our attention there. 4 divides 12, and $12=4\cdot{3}$. Following the
                proof of
                Thm.~\ref{thm:Composite_N_Exists_AB_N_Div_AB_and_N_NDiv_A_or_B}, we
                remove a prime from 4 and are left with 2. We then pick a prime that
                is not in the prime factorization of 4 and multiply by this. That
                is, we have $p=2$, $q=2$, and $P=3$. We set $a=p=2$ and
                $b=qP=2\cdot{3}=6$. Then 4 does not divide 2 since $2<4$ and
                moreover 4 does not divide 6.
            \end{example}
            \begin{fdefinition}{Euler Totient Function}{Euler_Totient_Func}
                The Euler totient function is the function
                $\varphi:\mathbb{N}^{+}\rightarrow\mathbb{N}^{+}$ defined by:
                \begin{equation*}
                    \varphi(n)=\cardinality{%
                        \{\,k\in\mathbb{N}\;|\;
                        k\leq{n}\textrm{ and }\GCD(k,n)=1\,\}%
                    }
                \end{equation*}
            \end{fdefinition}
            \begin{theorem}
                \label{thm:Euler_Totient_Multiplicative}%
                If $a,b\in\mathbb{N}$, if $\varphi$ is the Euler totient function,
                and if $\GCD(a,b)=1$, then:
                \begin{equation}
                    \varphi(a\cdot{b})=\varphi(a)\cdot\varphi(b)
                \end{equation}
            \end{theorem}
            \begin{theorem}
                \label{thm:Euler_Totient_of_Prime}%
                If $p\in\mathbb{N}$ is a prime, and if $\varphi$ is the Euler
                totient function, then $\varphi(p)=p-1$.
            \end{theorem}
            \begin{proof}
                For all $k\in\mathbb{N}^{+}$ such that $k\leq{p}$ it is true that
                $\GCD(k,p)=1$ since $p$ is prime. Hence, $\varphi(p)$ is equal to
                $\cardinality{\mathbb{Z}_{p-1}\setminus\{0\}}$ which is $p-1$.
            \end{proof}
            \begin{theorem}
                \label{thm:Euler_Totient_Powers_of_Primes}%
                If $p\in\mathbb{N}$ is prime, if $\varphi$ is the Euler totient
                function, and if $n\in\mathbb{N}^{+}$, then
                $\varphi(p^{n})=p^{n-1}(p-1)$.
            \end{theorem}
            We can combine the fundamental theorem of arithmetic together with these
            theorems to quickly compute the Euler totient function of a given value.
            \begin{example}
                The prime factorization of 12 is $2^{2}\cdot{3}$. Thus, we can
                compute $\varphi$ as follows:
                \begin{equation}
                    \varphi(12)=\varphi(2^{2}\cdot{3})
                        =\varphi(2^{2})\cdot\varphi(3)
                        =2^{2-1}(2-1)\cdot(3-1)=2\cdot{1}\cdot{2}=4
                \end{equation}
                we can also just count out the relatively prime elements of
                $\mathbb{N}$ that are less than 12, and we obtain 1, 5, 7, and 11.
                Powers of primes are particularly easy:
                \begin{equation}
                    \varphi(16)=\varphi(2^{4})=2^{4-1}(2-1)=2^{3}=8
                \end{equation}
                the relatively prime elements are 1, 3, 5, 7, 9, 11, 13, and 15.
                That is, all of the odd numbers less than 16. Lastly, let's try
                $\varphi(75)$. We obtain:
                \begin{equation}
                    \varphi(75)=\varphi(5^{2}*3)=5^{2-1}(5-1)\cdot(3-1)
                        =5\cdot{4}\cdot{2}=40
                \end{equation}
            \end{example}
            \begin{theorem}
                \label{thm:Primes_are_Irrational}%
                If $p\in\mathbb{N}^{+}$ is a prime number, then $\sqrt{p}$ is
                irrational.
            \end{theorem}
            \begin{proof}
                For suppose not. If $\sqrt{p}$ is rational, then there exists
                $a,b\in\mathbb{Z}$ such that $b\ne{0}$, $\GCD(a,b)=1$, and
                $\sqrt{p}=a/b$. But then $a^{2}=pb^{2}$. But then $p$ divides $b$
                (Def.~\ref{def:Divisor_of_Integer}) and since $p$ is prime, by
                Euclid's prime number lemma $p$ divides $a$
                (Thm.~\ref{thm:Euclid_Prime_Number_Lemma}). But then there is a
                $k\in\mathbb{Z}$ such that $a=p\cdot{k}$. But then
                $a^{2}=p^{2}k^{2}=pb^{2}$, and hence $pk^{2}=b^{2}$. But then
                $p$ divides $b^{2}$ (Def.~\ref{def:Divisor_of_Integer}) and thus
                since $p$ is prime, $p$ divides $b$
                (Thm.~\ref{thm:Euclid_Prime_Number_Lemma}). But then $p$ divides
                $a$ and $b$, a contradiction since $\GCD(a,b)=1$ and $1<p$. Hence,
                $\sqrt{p}$ is irrational.
            \end{proof}
            Only finitely many $n$ have $\varphi(n)=N$ for a fixed $N$, where
            $\varphi$ is the Euler totient function.
            \begin{theorem}
                \label{thm:A_DIV_B_then_EulerTotA_Div_EulerTotB}%
                If $a,b\in\mathbb{N}^{+}$, if $a|b$, and if $\varphi$ is the Euler
                totient function, then $\varphi(a)$ divides $\varphi(b)$.
            \end{theorem}
            \begin{proof}
                For by the fundamental theorem of arithmetic there exist subsets
                $S_{a},S_{b}\subseteq\mathbb{N}^{+}\times\mathbb{N}^{+}$ such that
                for all $(p_{1},n_{1})\in{S}_{1}$ and $(p_{2},n_{2})\in{S}_{2}$ it
                is true that $p_{1}$ and $p_{2}$ are prime,
                $a=\prod_{(p_{1},n_{1})\in{S}_{1}}p_{1}^{n_{1}}$ and
                $a=\prod_{(p_{2},n_{2})\in{S}_{2}}p_{2}^{n_{2}}$. But $a$ divides
                $b$, and hence $S_{1}\subseteq{S}_{2}$. But then:
                \begin{align*}
                    \varphi(a)=&\varphi\Big(
                        \prod_{(p_{1},s_{1})\in{S}_{1}}p_{1}^{n_{1}}
                    \Big)\tag{Thm.~\ref{thm:Fundamental_Theorem_of_Arithmetic}}\\
                    &=\prod_{(p_{1},n_{1})\in{S}_{1}}p^{n_{1}-1}(p_{1}-1)
                    \tag{Thm.~\ref{thm:Euler_Totient_Multiplicative}}
                \end{align*}
                and
                \begin{align*}
                    \varphi(b)=&\varphi\Big(
                        \prod_{(p_{2},s_{2})\in{S}_{2}}p_{2}^{n_{2}}
                    \Big)\tag{Thm.~\ref{thm:Fundamental_Theorem_of_Arithmetic}}\\
                    &=\prod_{(p_{2},n_{2})\in{S}_{2}}p^{n_{2}-1}(p_{2}-1)
                    \tag{Thm.~\ref{thm:Euler_Totient_Multiplicative}}
                \end{align*}
                and since $S_{1}\subseteq{S}_{2}$, we see that
                $\varphi(a)$ divides $\varphi(b)$.
            \end{proof}
        \subsection{Modulo Arithmetic}
            \begin{theorem}
                \label{thm:Modulo_n_is_Equiv_Relation}%
                If $n\in\mathbb{N}$, and if
                $R\subseteq\mathbb{Z}\times\mathbb{Z}$ is defined by:
                \begin{equation}
                    R=\{\,(a,b)\in\mathbb{Z}^{2}\;|\;n|(b-a)\,\}
                \end{equation}
                then $R$ is an equivalence relation on $\mathbb{Z}$.
            \end{theorem}
            \begin{proof}
                For $aRa$ since $a-a=0$ and $n|a$. If $aRb$, then $n$ divides
                $b-a$ and hence there is a $k\in\mathbb{Z}$ such that
                $nk=b-a$. But then $n(\minus{k})=a-b$, and thus $n$ divides $a-b$.
                But then $bRa$. Lastly, if $aRb$ and $bRc$, then there exists
                $j,k$ such that $nj=b-a$ and $nk=c-b$. But then
                $n(k+j)=nk+nk=(c-b)+(b-a)=c-a$, and thus $aRc$.
            \end{proof}
            \begin{fdefinition}{Ring of Integers Modulo $n$}{Ring_Ints_Mod_N}
                The ring of integers modulo $n\in\mathbb{Z}$ is the quotient set
                $\mathbb{Z}/R$ where $R$ is the equivalence relation
                $R\subseteq\mathbb{Z}^{2}$ defined by:
                \begin{equation*}
                    R=\{\,(a,b)\in\mathbb{Z}^{2}\;|\;n|(b-a)\,\}
                \end{equation*}
                We denote this $\mathbb{Z}/n\mathbb{Z}$.
            \end{fdefinition}
            \begin{theorem}
                \label{thm:Z_n_is_Bij_onto_Z_mod_n}%
                If $n\in\mathbb{Z}$, if $\mathbb{Z}/n\mathbb{Z}$ is the ring of
                integers modulo $n$, and if
                $\pi:\mathbb{Z}\rightarrow\mathbb{Z}/n\mathbb{Z}$ is the canonical
                projection map: $\pi(n)=[n]$, then $\pi|_{\mathbb{Z}_{n}}$ is
                bijective.
            \end{theorem}
            \begin{proof}
                For let $x\in\mathbb{Z}/n\mathbb{Z}$. Then there exists a
                representative $k\in\mathbb{Z}$ such that $[k]=x$. But by Euclid's
                division algorithm there exists $q\in\mathbb{Z}$ and
                $r\in\mathbb{N}$ such that $r<n$ and $k=qn+r$. But then
                $qn=k-r$ and hence $n$ divides $k-r$. But then $rRk$, and hence
                $\pi(r)=\pi(k)=x$. And since $r<n$, $r\in\mathbb{Z}_{n}$. Hence,
                $\pi|_{\mathbb{Z}_{n}}$ is surjective. Moreover, if
                $a,b\in\mathbb{Z}_{n}$ and if $a\ne{b}$, then either $a<b$ or $b<a$.
                Suppose $a<b$. If $\pi(a)=\pi(b)$, then $aRb$ and hence $n$ divides
                $b-a$. But since $a,b\in\mathbb{Z}_{n}$, $a<n$ and $b<n$, and thus
                $b-a<n$. But since $a<b$, $b-a\in\mathbb{N}^{+}$. But then $n$ the
                greater divides $b-a$ the lesser, a contradiction
                (Thm.~\ref{thm:Greater_is_not_divisor_of_lesser}). Hence,
                $\pi|_{\mathbb{Z}_{n}}$ is injective, and is therefore a bijection. 
            \end{proof}
            \begin{fdefinition}{Arithmetic Modulo $n$}{Arithmetic_Mod_n}
                The additive and multiplicative binary operations on
                $\mathbb{Z}/n\mathbb{Z}$ with $n\in\mathbb{N}$ are defined by:
                \par
                \begin{minipage}[b]{0.49\textwidth}
                    \centering
                    \begin{equation*}
                        [a]+[b]=[a+b]
                    \end{equation*}
                \end{minipage}
                \hfill
                \begin{minipage}[b]{0.49\textwidth}
                    \centering
                    \begin{equation*}
                        [a]\cdot[b]=[a]\cdot[b]
                    \end{equation*}
                \end{minipage}
            \end{fdefinition}
            That this is consistent is a consequence of the previous theorem. It's
            always poor to use the same symbol for two different things that are
            frequently used in the same context, but alas it is the standard. In
            this case it is rather justifiable since by
            Thm.~\ref{thm:Z_n_is_Bij_onto_Z_mod_n} we have that $\mathbb{Z}_{n}$ is
            a representative of $\mathbb{Z}/n\mathbb{Z}$ and thus, given
            $j,k\in\mathbb{Z}_{n}$, $[j+k]$ is simply the remainder term of
            $j+k$ after division by $n$, and similarly for $[j\cdot{k}]$. Hence the
            binary operations will send elements of $\mathbb{Z}_{n}$ back to
            $\mathbb{Z}_{n}$.
            \begin{example}
                The most common example one comes across of modulo arithmetic is in
                $\mathbb{Z}_{12}$ since this represents a clock. If it is 11 A.M.
                and you wait for 3 hours, the time will then be 2 P.M. and hence
                $11+3=2$, quite paradoxical. One might claim ``Aha! I use military
                time!'' but then we simply apply the argument to $\mathbb{Z}_{24}$
                and ask what time is 23 hours + 3 hours? That answer is 2 in the
                morning, hence $23+3=2$. There's no mystery once one realizes we are
                simply using the arithmetic of $\mathbb{Z}/n\mathbb{Z}$.
            \end{example}
            While in the notation we use $\mathbb{Z}_{n}$ and $+$ and $\cdot$ it is
            worthwhile to note that the elements of $\mathbb{Z}/n\mathbb{Z}$ are
            \textit{not} the same as the elements of $\mathbb{Z}_{n}$. $\mathbb{N}$
            was constructed from the axiom of infinity and the elements look like
            $\emptyset,\{\emptyset\},\{\emptyset,\{\emptyset\}\}$, and so forth.
            Meanwhile $\mathbb{Z}/n\mathbb{Z}$ was constructed from an equivalence
            relation, and hence the elements of $\mathbb{Z}/n\mathbb{Z}$ are
            elements of the \textit{power set} of $\mathbb{N}$. Indeed, we know
            precisely what these elements are:
            \begin{equation}
                \begin{split}
                    \mathbb{Z}/n\mathbb{Z}=
                    \Big\{\,&\{\,0,\,\pm{n},\,\pm{2n},\,\pm{3n},\,\dots\,\},\\
                            &\{\,1,\,1\pm{n},\,1\pm{2n},\,1\pm{3n},\,\dots\,\},\\
                            &\{\,2,\,2\pm{n},\,2\pm{2n},\,2\pm{3n},\,\dots\,\},\\
                            &\dots,\\
                            &\{\,n-1,\,n-1\pm{n},\,n-1\pm{2n},\,n-1\pm{3n},\,
                                \dots\,\}\Big\}
                \end{split}
            \end{equation}
            set theoretically these are different.
            Thm.~\ref{thm:Z_n_is_Bij_onto_Z_mod_n} tells us they're the same size,
            and the way we've defined modulo arithmetic shows that they have the
            same structure.
            \begin{example}
                A common application of modulo arithmetic that one sees in
                elementary number theory is the computation of the last few digits
                of large powers of numbers. For example, consider $3^120$ and
                suppose we want to know the last digit of this number. That's
                equivalent to asking what is the smallest representative of the
                equivalence class of $3^{120}$ in the ring of integers modulo 10.
                First we reduce the problem and recognize that $120=2\cdot{60}$, and
                hence $3^{120}=3^{2\cdot{60}}=(3^{2})^{60}$. Well $3^{2}=9$, which
                is equivalent to $\minus{1}$ in $\mathbb{Z}/10\mathbb{Z}$, and so we
                next consider $(\minus{1})^{60}$. But this is just 1.
                But 1 is congruent to 1 in $\mathbb{Z}/10\mathbb{Z}$ and there we
                have it: The last digit of $3^{120}$ is 1. Indeed, using your
                favorite computer language, we can compute and obtain:
                \begin{equation}
                    3^{120}=
                    1797010299914431210413179829509605039731475627537851106401
                \end{equation}
                and so our calculation was correct. Let us try $2^{2000}$ and attain
                the last 2 digits (But perhaps not compute the actual value). We
                note that $2000=10*200$ and every computer recognizes instantly that
                $2^{10}=1024$, which is congruent to 24 in
                $\mathbb{Z}/100\mathbb{Z}$. So we are left with $24^{200}$. We look
                at the exponent again, note that it is equal to $2\cdot{100}$ and
                $24^{2}$ seems a far simpler computation. We get
                $24^{2}=576$, the last two digits of which are 76, and so we're down
                to $76^{100}$. We break this into $(76^{2})^{50}$ and upon computing
                we get $76^{2}=5776$ and so the last two digits are once again 76,
                meaning we can continue decomposing away all of the powers of 2,
                and we are left with $75^{25}=76^{24}\cdot{76}$. But then removing
                the powers of 2 away from 24 ($24=2^{3}\cdot{3}$), we are left with
                $76^{3}\cdot{76}=76^{4}$ which then reduces to $76$. The last two
                digits of $2^{2000}$ are 76.
            \end{example}
            \begin{theorem}
                \label{thm:Equiv_Class_of_1_is_1_Mod_n}%
                If $n\in\mathbb{N}^{+}$, if $[1]\in\mathbb{Z}/n\mathbb{Z}$ is the
                equivalence class of $1$ in the ring of integers modulo $n$, and if
                $x\in\mathbb{Z}/n\mathbb{Z}$, then $x\cdot[1]=x$.
            \end{theorem}
            \begin{proof}
                For if $x\in\mathbb{Z}/n\mathbb{Z}$, then there is a
                $k\in\mathbb{Z}$ such that $x=[k]$. But then
                $x\cdot[1]=[k]\cdot[1]=[k\cdot{1}]=[k]=x$.
            \end{proof}
            Likewise, $[0]$ is the zero element of $\mathbb{Z}/n\mathbb{Z}$.
            \begin{theorem}
                \label{thm:Invertible_Mod_n_iff_Relatively_Prime}%
                If $n\in\mathbb{N}^{+}$, if $x\in\mathbb{Z}/n\mathbb{Z}$, then there
                exists a $y\in\mathbb{Z}/n\mathbb{Z}$ such that $x\cdot{y}=1$ if and
                only if there is a representative $k\in\mathbb{Z}$ of $x$ such that
                $\GCD(k,n)=1$.
            \end{theorem}
            \begin{proof}
                For if $k$ is a representative for $x$ and $\GCD(k,n)=1$, then
                by B\'{e}zout's identity there exists $a,b\in\mathbb{Z}$ such that
                $ak+bn=1$. But then $bn=1-ak$< and hence $n$ divides $1-ak$. But
                then by the definition of the ring of integers modulo $n$,
                $1\in[ak]$ and hence $[a]\cdot[k]=[1]$. In the other direction, if
                $x$ has an inverse $y$, then there are representatives $k,m$ such
                that $x=[k]$ and $y=[m]$. But then
                $x\cdot{y}=[k]\cdot[m]=[k\cdot{m}]$. But by hypothesis
                $x\cdot{y}=[1]$ and hence $[k\cdot{m}]=[1]$. But then
                $n$ divides $1-km$ and hence there is a $j\in\mathbb{Z}$ such that
                $jn=1-km$ (Def.~\ref{def:Divisor_of_Integer}). But then
                $nj+km=1$ and therefore $\GCD(k,n)=1$.
            \end{proof}
            Hence every non-zero element of $\mathbb{Z}/p\mathbb{Z}$ for some prime
            $p$ is invertible.
            \begin{example}
                Other tricks that use modulo arithmetic appear in disguise when one
                studies divisibility tricks. If $a=\sum{a}_{n}10^{n}$ is a finite
                sum, then $a$ is congruent of $\sum{a}_{n}$. Simply use the
                additivity of modulo addition, and use the fact that
                $10^{n}\equiv{1}\mod{9}$. Another trick comes from studying if a
                number is divisible by 3. Applying the same trick, we see that if
                3 divides $n$, then it divides the sum of its digits (in base 10).
                Conversely, if 3 divides the sum of the digits of $n$ (in base 10),
                then 3 divides $n$.
            \end{example}
            \begin{example}
                Looking back at a previous example, let's compute the last digit of
                $9^{n}$ for any $n\in\mathbb{N}$. We note that
                $9\equiv\minus{1}\mod{10}$ and hence we only need to consider
                $(\minus{1})^{n}$. But $(\minus{1})^{n}$ is 1 if $n$ is even and
                $\minus{1}$ is $n$ is odd. Therefore the last digit of $9^{n}$ is
                9 if $n$ is odd and 1 if $n$ is even. And indeed, the pattern holds
                true: 1, 9, 81, 729, 6561, and so on.
            \end{example}
            Come back to excercises (DF Chapt 1).
            \begin{ftheorem}{Chinese Remainder Theorem}{Chinese_Remainder_Theorem}
                If $n\in\mathbb{N}^{+}$ is an integer, if
                $N:\mathbb{Z}_{n}\rightarrow\mathbb{N}$ is such that for all
                $i,j\in\mathbb{Z}_{n}$ with $i\ne{j}$ it is true that
                $\GCD(n_{i},n_{j})=1$, and if
                $A:\mathbb{Z}_{n}\rightarrow\mathbb{N}$ is a sequence of integers
                such that $A_{i}<n_{i}$ for all $i\in\mathbb{Z}_{n}$, then there is
                a unique $x\in\mathbb{Z}_{n}$ such that for all $i\in\mathbb{Z}_{n}$
                it is true that $x\equiv{A}_{i}\mod{N}_{i}$.
            \end{ftheorem}
            \begin{bproof}
                By induction. The base case, since $N_{1}$ and $N_{2}$ are
                relatively prime, by B\'{e}zout's identity there exist integers
                $m_{1},m_{2}\in\mathbb{Z}$ such that $m_{1}N_{1}+m_{2}N_{2}=1$.
                Let $x=m_{2}N_{2}A_{1}+m_{1}N_{1}A_{2}$. In the induction case,
                there is an $x$ such that $x\equiv{A}_{i}\mod{N}_{i}$ for
                $i\in\mathbb{Z}_{n-1}$ and $x\equiv{A}_{n}A_{n+1}\mod{N}_{n}N_{n+1}$
                since $N_{n}$ and $N_{n+1}$ are relatively prime.
            \end{bproof}
    \section{Groups}
        $\mathbb{Z}/n\mathbb{Z}$ is a group. Multiplicative elements of
        $\mathbb{Z}/n\mathbb{Z}$ form a group under multiplication (essentialy
        by definition). Def Cayley table of group.
        \begin{example}
            If $G\subseteq\mathbb{C}$ is defined by:
            \begin{equation}
                G=\{\,z\in\mathbb{C}\;|\;
                    \textrm{ There exists }n\in\mathbb{N}^{+}
                    \textrm{ such that }z^{n}=1\,\}
            \end{equation}
            then $G$ is a group under multiplication. For these are just the
            \textit{roots of unity} and using the polar representation of a
            complex number we have $z=\exp(2\pi{i}m/n)$ for some
            $m,n\in\mathbb{N}$. Multiplying $z\cdot{w}$ yields:
            \begin{equation}
                \exp\Big(\frac{2\pi{i}m}{n}\Big)
                    \exp\Big(\frac{2\pi{i}j}{k}\Big)
                =\exp\Big(\frac{2\pi{i}(mk+nj)}{nk}\Big)
            \end{equation}
            showing that $G$ is closed to multiplication. The identity element
            is contained in here since $1=1^{1}$, hence $1\in{G}$. Lastly,
            inverses: If $z=\exp(2\pi{i}m/n)$, let
            $z^{\minus{1}}=\exp(\minus{2}\pi{i}m/n)$. However, $G$ is not a
            group under multiplication since $1+1$ is not contained in $G$.
            That is, $1+1=2\exp(2\pi{i})$ in polar form and hence any power of
            this will not result in one since:
            \begin{equation}
                \norm{\big(2\exp(2\pi{i})\big)^{n}}
                =2^{n}\norm{\exp(2\pi{i}n)}=2^{n}
            \end{equation}
            and this is not equal to 1 for all $n\in\mathbb{N}^{+}$.
        \end{example}
        \begin{example}
            Define $G\subseteq\mathbb{R}$ by:
            \begin{equation}
                G=\{\,a+b\sqrt{2}\;|\;a,b\in\mathbb{Q}\,\}
            \end{equation}
            Then $G$ is a group under addition. Give $x,y\in{G}$ we have:
            \begin{equation}
                x+y=(a+b\sqrt{2})+(c+d\sqrt{2})=(a+c)+(b+d)\sqrt{2}
            \end{equation}
            and hence $G$ is closed under addition. The identity element is
            contained in $G$ since $0=0+0\sqrt{2}$, and moreover so are
            additive inverses: Let $\minus{x}=\minus{a}-b\sqrt{2}$. Since
            addition is also associative, we have that $G$ is a group under
            addition. If we consider $G\setminus\{0\}$ with multiplication, then
            this too is a group. Give $x,y\in{G}$ we have:
            \begin{equation}
                x\cdot{y}=(a+b\sqrt{2})(c+d\sqrt{2})
                    =ac+2bd+(ad+bd)\sqrt{2}
            \end{equation}
            since $ad+bd\in\mathbb{Q}$ and $ac+2bd\in\mathbb{Q}$, we have that
            $x\cdot{y}$ is again an element of $\mathbb{Q}$. The identity is in
            $G$, setting $a=1$ and $b=0$. Lastly, if $x\in{G}$ then by
            definition $x$ is not zero, and hence if $x=a+b\sqrt{2}$ then either
            $a$ is non-zero or $b$ is non-zero. But then $a^{2}-2b^{2}$ is
            non-zero since $\sqrt{2}$ is irrational. Let
            $x^{\minus{1}}=(a-b\sqrt{2})/(a^{2}-2b^{2})$. Then:
            \begin{equation}
                x\cdot{x}^{\minus{1}}=
                \big(a+b\sqrt{2}\big)\big(\frac{a-b\sqrt{2}}{a^{2}-2b^{2}}\big)
                =\frac{(a+b\sqrt{2})(a-b\sqrt{2})}{a^{2}-2b^{2}}
                =\frac{a^{2}-2b^{2}}{a^{2}-2b^{2}}=1
            \end{equation}
        \end{example}
        \begin{theorem}
            If $\monoid{G}$ is a group, if $n\in\mathbb{N}^{+}$, and if
            $g\in{G}$ has order $g$, then $g^{\minus{1}}=g^{n-1}$.
        \end{theorem}
        \begin{proof}
            For $e=g^{n}=g\cdot{g}^{n-1}$, and thus by the uniqueness of
            inverses, $g^{n-1}=g^{\minus{1}}$.
        \end{proof}
        \begin{theorem}
            If $\monoid{G}$ is a group, if $x,y\in{G}$, and if $x$ and $y$
            commute, then $y^{\minus{1}}*x*y=x$ and
            $x^{\minus{1}}*y^{\minus{1}}*x*y=e$.
        \end{theorem}
        \begin{proof}
            Since $x$ and $y$ commute, $x*y=y*x$. Apply the cancellation laws
            to this.
        \end{proof}
        \begin{theorem}[Power Laws]
            If $\monoid{G}$ is a group, $a\in{G}$, $n,m\in\mathbb{Z}$, then:
            \begin{align}
                a^{n}*a^{m}&=a^{n+m}\\
                (a^{\minus{1}})^{n}&=(a^{n})^{\minus{1}}
            \end{align}
        \end{theorem}
        \begin{theorem}
            If $x\in{G}$ has odd order, there is a $n\in\mathbb{Z}$ such that
            $x=(x^{2})^{n}$.
        \end{theorem}
        \begin{proof}
            For $e=x^{2n+1}=x^{2n}x$ and hence by cancellation,
            $x=x^{2n}=(x^{2})^{n}$.
        \end{proof}
        \begin{theorem}
            If $\monoid{G}$ is a group, $a\in{G}$ has order $n$, $g\in{G}$,
            then $g^{\minus{1}}ag$ has order $n$. Also, the order of $a*b$ is
            the order of $b*a$.
        \end{theorem}
        \begin{proof}
            We show $(g^{\minus{1}}ag)^{n}=g^{\minus{1}}a^{n}g$ by induction:
            \begin{equation}
                (g^{\minus{1}}ag)^{n+1}=(g^{\minus{1}}ag)(g^{\minus{1}}ag)^{n}
                =(g^{\minus{1}}ag)(g^{\minus{1}}a^{n}g)
                =g^{\minus{1}}a^{n+1}g
            \end{equation}
            hence if $a$ has order $n$, applying this shows that
            $g^{\minus{1}}ag$ has order $n$.
        \end{proof}
        \begin{theorem}
            If $a,b$ commute, then $(ab)^{n}=a^{n}b^{n}$.
        \end{theorem}
        \begin{theorem}[Cyclic Subgroup]
            If $\monoid{G}$ is a group, if $x\in{G}$, and if $H\subseteq{G}$ is
            defined by:
            \begin{equation}
                H=\{\,x^{n}\;|\;n\in\mathbb{Z}\,\}
            \end{equation}
            Then $\monoid{H}$ is a group.
        \end{theorem}
        \begin{proof}
            It contains the identity since $x^{0}=e$ by definition. It contains
            inverses since $(x^{n})^{\minus{1}}=x^{\minus{n}}$. Lastly, it
            is closed under $*$ since $x^{n}*x^{m}=x^{n+m}$. Hence, it is a
            group.
        \end{proof}
        \begin{theorem}
            If $\monoid[A]{A}$ and $\monoid[B]{B}$ are groups, if
            $\monoid{A\times{B}}$ is the direct product of $A$ and $B$, then
            $\monoid{A\times{B}}$ is Abelian if and only if
            $\monoid[A]{A}$ and $\monoid[B]{B}$ are Abelian.
        \end{theorem}
        \begin{proof}
            For if $\monoid[A]{A}$ and $\monoid[B]{B}$ are Abelian,
            $(a,b),(c,d)\in{A}\times{B}$, then:
            \begin{equation}
                (a,b)*(c,d)=(a*_{A}c,b*_{B}d)=
                (c*_{A}a,d*_{B}b )=(c,d)*(a,b)
            \end{equation}
            and hence $\monoid{A\times{B}}$ is Abelian. The other direction is
            similar.
        \end{proof}
        \begin{theorem}
            If $\monoid[A]{A}$ and $\monoid[B]{B}$ are groups, if $e_{A}\in{A}$
            is the unital element of $A$, if $e_{B}\in{B}$ is the unital element
            of $B$, if $a\in{A}$, if $b\in{B}$, and if $\monoid{A\times{B}}$
            is the direct product of $A$ and $B$, then $(a,e_{B})$ and
            $(e_{A},b)$ commute in $A\times{B}$.
        \end{theorem}
        \begin{proof}
            For
            \begin{equation}
                (a,e_{B})*(e_{A},b)=(a*e_{A},e_{B}*b)
                =(e_{A}*a,b*e_{B})=(e_{A},b)*(a,e_{B})
            \end{equation}
        \end{proof}
        \begin{theorem}
            If $\monoid[A]{A}$ and $\monoid[B]{B}$ are groups, if $a\in{A}$ has
            order $n\in\mathbb{N}^{+}$, if $b\in{B}$ has order
            $m\in\mathbb{N}^{+}$, and if $\monoid{A\times{B}}$ is the direct
            product of $A$ and $B$, then $(a,b)$ has order $\LCM(n,m)$ in
            $A\times{B}$.
        \end{theorem}
        \begin{proof}
            For:
            \begin{equation}
                (a,b)^{n}=\big((a,e_{B})*(e_{A},b)\big)^{N}
                =(a,e_{B})^{N}*(e_{A},b)^{N}
                =(a^{N},e_{B})*(e_{A},b^{N})
            \end{equation}
            and hence the least $N$ that makes $a^{N}=e$ and $b^{N}=e$ is the
            least $N$ such that $n|N$ and $m|N$. But this is just $\LCM(n,m)$.
        \end{proof}
        DF 1.1.32
        \begin{theorem}
            If $\monoid{G}$ is a group, $x\in{G}$ has order $n\in\mathbb{N}$,
            and if $f:\mathbb{Z}_{n}\rightarrow{G}$ is defined by
            $f(k)=x^{k}$, then $f$ is injective.
        \end{theorem}
        \begin{proof}
            For if not then there are distinct $k_{1},k_{2}\in\mathbb{Z}_{n}$
            such that $f(k_{1})=f(k_{2})$. Suppose $k_{1}<k_{2}$. Then
            $x^{k_{2}}=x^{k_{1}}$ and hence $x^{k_{2}-k_{1}}=e$, a contradiction
            since $k_{2}-k_{2}<n$ and $n$ is the least such element in
            $\mathbb{N}^{+}$ with $x^{n}=e$.
        \end{proof}
        \begin{theorem}
            If $\monoid{G}$ is a group, if $x\in{G}$ has infinite order, and if
            $f:\mathbb{Z}\rightarrow{G}$ is defined by $f(n)=x^{n}$, then
            $f$ is injective.
        \end{theorem}
        \begin{proof}
            For suppose not. Then $x^{m}=x^{n}$ which implies $x^{m-n}=e$,
            a contradictio since $x$ has infinite order. Hence, $m-n=0$ and thus
            $m=n$.
        \end{proof}
        \begin{theorem}
            If $\monoid{G}$ is a group, if $n\in\mathbb{N}^{+}$, if $x\in{G}$
            has order $n$, and if $N\in\mathbb{N}$, then there is a
            $K\in\mathbb{Z}_{n}$ such that $x^{k}=x^{N}$.
        \end{theorem}
        \begin{proof}
            For either $N=0$ or $N\ne{0}$. But if $N=0$, then since
            $0\in\mathbb{Z}_{n}$ we have that $x^{N}=x^{0}$ and we're done.
            Suppose $N\ne{0}$. But then $N,n\in\mathbb{Z}\setminus\{0\}$ and
            hence by Euclid's division algorithm there exists $q\in\mathbb{Z}$
            and $r\in\mathbb{N}$ such that $r<n$ and $N=qn+r$. But then:
            \begin{equation}
                x^{N}=x^{qn+r}=x^{qn}x^{r}=(x^{n})^{q}x^{r}
                =e^{q}x^{r}=x^{r}
            \end{equation}
            proving the claim.
        \end{proof}
        \subsection{Dihedral Groups}
            Pretty pictures, presentation, etc. If $k\in\mathbb{Z}_{k}$ then
            $fr^{k}=r^{\minus{k}}f$. Every element has unique representation
            $f^{j}r^{k}$ with $j=0$ or $j=1$ and $k\in\mathbb{Z}_{n}$.
            \begin{fdefinition}{Generator of a Group}{Generator_of_a_Group}
                A generator of a group $\monoid{G}$ is a subset $S\subseteq{G}$
                such that for all $g\in{G}$ there exists an $n\in\mathbb{N}$ and
                a sequence $a:\mathbb{Z}_{n}\rightarrow{S}$ such that:
                \begin{equation*}
                    g=\prod_{k\in\mathbb{Z}_{n}}a_{k}
                \end{equation*}
            \end{fdefinition}
            Note that sequences allow for repetition and have a notion of order,
            and so this respect the potential non-commutativity of a group.
            Another way of interpreting this definition is that every element of
            $G$ can be written as the finite product of elements in $S$.
            \begin{example}
                For any dihedral group $(D_{2n},\circ)$, the set
                $S=\{r,f\}$ containing the rotation and the reflection element
                is a generator for $D_{2n}$. To tell two such dihedral groups
                apart we introduce \textit{relations}. For example, $r^{n}=e$
                and $f^{2}=e$, and these are the least such positive integers
                with these properties. Moreover, $rf=fr^{\minus{1}}$.
            \end{example}
            \begin{example}
                Consider $G$ with the presentation:
                \begin{equation}
                    G=\langle{a},b\;|\;a^{n}=e,b^{2}=e,ab=ba^{2}\rangle
                \end{equation}
                Let's see if we can determine what this group is. We have:
                \par\vspace{-2.5ex}
                \begin{subequations}
                    \begin{minipage}[t]{0.49\textwidth}
                        \begin{align}
                            a&=ae\\
                            &=ab^{2}\\
                            &=(ab)b\\
                            &=(ba^{2})b\\
                            &=(ba)(ab)
                        \end{align}
                    \end{minipage}
                    \hfill
                    \begin{minipage}[t]{0.49\textwidth}
                        \begin{align}
                            &=(ba)(ba^{2})\\
                            &=(bab)a^{2}\\
                            &=b(ba^{2})a^{2}\\
                            &=b^{2}a^{4}\\
                            &=a^{4}
                        \end{align}
                    \end{minipage}
                \end{subequations}
                \par\vspace{2.5ex}
                and hence by the cancellation law we conclude that $a^{3}=e$.
                Hence, we may take $n$ to be either 1, 2, or 3. If $n=1$ we are
                left with the group presented by a single variable $b$ such that
                $b^{2}=e$, and this is just $\mathbb{Z}/2\mathbb{Z}$. If $n=2$
                then from $ab=ba^{2}$ we conclude $ab=b$, and hence $a=e$, again
                leading us to $\mathbb{Z}/2\mathbb{Z}$. Finally, with $n=3$ we
                have $ab=ba^{2}$ implying that $ab=ba^{\minus{1}}$, and this is
                precisely the presentation for the dihedral group $D_{6}$.
            \end{example}
            \begin{example}
                Consider $G$ defined by:
                \begin{equation}
                    G=\langle{a,b}\;|\;a^{4}=e,b^{3}=e,ab=b^{2}a^{2}\rangle
                \end{equation}
                Let's show that $G$ is just the trivial group. We have:
                \begin{equation}
                    bab=b(ab)=b(b^{2}a^{2})=b^{3}a^{2}=ea^{2}=a^{2}
                \end{equation}
                and hence:
                \begin{equation}
                    e=a^{4}=(bab)^{2}=(bab)(bab)=bab^{2}ab=
                    bab^{2}b^{2}a^{2}=bab^{3}ba^{2}=baba^{2}
                \end{equation}
                and since $b^{3}=e$, we apply the cancellation law to obtain
                $b^{2}=aba^{2}$. But then:
                \begin{equation}
                    a=ae=ab^{3}=(ab)b^{2}=b^{2}a^{2}(aba^{2})
                    =b^{2}a^{3}ba^{2}=b^{\minus{1}}a^{\minus{1}}ba^{2}
                \end{equation}
                and hence $aba=ba^{2}$, and thus $ab=ba$. So the operation
                commutes. But then:
                \begin{equation}
                    ab=b^{2}a^{2}=a^{2}b^{2}
                \end{equation}
                and so applying cancellation we have $ab=e$ and hence
                $a=b^{\minus{1}}$. But $b^{\minus{1}}=b^{2}$ since $b^{3}=e$
                and thus $a=b^{2}$. But then:
                \begin{equation}
                    a^{3}=(b^{2})^{3}=(b^{3})^{2}=e
                \end{equation}
                But $a^{4}=e$ and therefore by the cancellation law $a=e$.
                And since $b=a^{\minus{1}}$ we have that $b=e$. Hence, $G$ is
                the trivial group.
            \end{example}
            The presentation of the general dihedral group $D_{2n}$ is:
            \begin{equation}
                D_{2n}=\langle{r,f}\;|\;r^{n}=e,f^{2}=e,rf=fr^{\minus{1}}\rangle
            \end{equation}
            \begin{theorem}
                If $D_{2n}$ is the dihedral group with rotational generator $r$
                and reflectional generator $f$, and if $x\in{D}_{2n}$ is not a
                power of $r$, then $rx=xr^{\minus{1}}$.
            \end{theorem}
            \begin{proof}
                By induction. Suppose $a:\mathbb{Z}_{k}\rightarrow\{r,f\}$ is
                a least sequence such that the product is $x$. In the base case
                of $k=1$, since $x$ is not a power of $r$ we simply have that
                $x=f$. But $rf=fr^{\minus{1}}$, so we are done. Suppose it is
                true of $k\in\mathbb{N}$ and let
                $a:\mathbb{Z}_{k+1}\rightarrow\{r,f\}$ be a sequence whose
                product equals $x$. Then either $a_{k}=r$ or $a_{k}=f$. Suppose
                $a_{k}=r$. Since $x$ is not a power of $r$, there is and
                $i\in\mathbb{Z}_{k}$ such that $a_{i}\ne{r}$. Define $y$ by:
                \begin{equation}
                    y=\prod_{j\in\mathbb{Z}_{n}}a_{j}
                \end{equation}
                Then since $y$ is not a power of $r$, $ry=yr^{\minus{1}}$. But
                then:
                \begin{equation}
                    rx=r(yr)=(ry)r=(yr^{\minus{1}})r=y(r^{\minus{1}}r)=ye=y
                \end{equation}
                but $x=yr$, and hence $y=xr^{\minus{1}}$. Thus
                $rx=x^{\minus{1}}$. If $a_{k}=f$, let $y$ be defined similarly.
                If $y=r^{k}$, then:
                \begin{equation}
                    rx=ryf=rr^{k}f=r^{k}rf=r^{k}fr^{\minus{1}}=xr^{\minus{1}}
                \end{equation}
                if $y$ is not a power of $r$, then by the induction hypothesis
                we have that $ry=yr^{\minus{1}}$. But then:
                \begin{equation}
                    rx=r(yr)=(ry)r=(yr^{\minus{1}})r=y(r^{\minus{1}}r)=ye=y
                \end{equation}
                But $x=yr$ and hence $y=xr^{\minus{1}}$, so $rx=xr^{\minus{1}}$,
                as claimed.
            \end{proof}
            \begin{theorem}
                In $D_{2n}$ every element that is not a power of $r$ has order
                2.
            \end{theorem}
            \begin{proof}
                For by the previous theorem $rx=xr^{\minus{1}}$ and hence
                $rxr=x$. But then:
                \begin{equation}
                    x^{2}=(rxr)(rxr)=xr^{\minus{1}}r^{2}xr
                    =xrxr=(xr)^{2}
                \end{equation}
                And simialrly:
                \begin{equation}
                    (xr)^{2}=(xr)(xr)=
                \end{equation}
            \end{proof}
    \section{Rings}
        \begin{fdefinition}{Ring}{Ring}
            A ring is a set $R$ with two binary operations $+$ and $\cdot$,
            denoted $\ring{R}$, such that $\monoid[][+]{R}$ is a group,
            $\monoid[][\cdot]{R}$ is a monoid, and such that $\cdot$ distributes
            over $+$. That is, for all $a,b,c\in{R}$ it is true that:
            \begin{equation*}
                a\cdot(b+c)=(a\cdot{b})+(a\cdot{c})
            \end{equation*}
            The unital element of $\monoid[][+]{R}$ is denoted $0$ and the
            unital element of $\monoid[][\cdot]{R}$ is written as $1$. $+$ is
            called addition and $\cdot$ is called multiplication.
        \end{fdefinition}
        If we relax the requirement that $\monoid[][\cdot]{R}$ is a monoid and
        merely require it to be a semigroup (that is, there need not exist a
        multiplicative identity), then the resulting structure is called a
        \textit{rng}. Some authors reserve the word ring for the more general
        case of what we're calling a rng, and use the phrasing
        \textit{ring with identity} for what we're calling a ring. Commutative
        rings are rings $\ring{R}$ where $\monoid[][\cdot]{R}$ is an Abelian
        monoid. Rings (with identity) are automatically commutative in $+$. That
        is, $\monoid[][+]{R}$ is an Abelian group.
        \begin{theorem}
            \label{thm:Ring_Add_is_Commutative}%
            If $\ring{R}$ is a ring, then $\monoid[][+]{R}$ is an Abelian group.
        \end{theorem}
        \begin{proof}
            For suppose not. Then there exists $a,b\in{R}$ such that
            $a+b\ne{b}+a$. But $\ring{R}$ is a ring, and hence
            $\monoid[][\cdot]{R}$ is a monoid and thus there is an identity
            element $1\in{R}$ with respect to multiplication. But since $R$ is a
            ring, multiplication distributes over addition and therefore:
            \begin{align}
                (1+1)\cdot(a+b)
                    &=\big((1+1)\cdot{a}\big)+\big((1+1)\cdot{b}\big)
                    \tag{Left Distributive Law}\\
                    &=\big((1\cdot{a})+(1\cdot{a})\big)+
                        \big((1\cdot{b})+(1\cdot{b})\big)
                    \tag{Right Distributive Law}\\
                    &=(a+a)+(b+b)
                        \tag{Multiplicative Identity}\\
                    &=a+\big(a+(b+b)\big)
                        \tag{Associative Law}
            \end{align}
            But again by distributivity, we obtain:
            \begin{align}
                (1+1)\cdot(a+b)
                &=\big(1\cdot(a+b)\big)+\big(1\cdot(a+b)\big)
                \tag{Right Distributive Law}\\
                &=(a+b)+(a+b)
                    \tag{Multiplicative Identity}\\
                &=a+\big(b+(a+b)\big)
                    \tag{Associative Law}
            \end{align}
            By the transitivity of equality, we have:
            \begin{equation}
                a+\big(a+(b+b)\big)=a+\big(b+(a+b)\big)
            \end{equation}
            But $\monoid[][+]{R}$ is a group, and thus by the left cancellation
            law $a+(b+b)=b+(a+b)$. By associativity $a+(b+b)=(a+b)+b$ and
            $b+(a+b)=(b+a)+b$, and hence $(a+b)+b=(b+a)+b$. Thus by the right
            cancellation law, $a+b=b+a$, which is a contradiction. Therefore,
            $\monoid[][+]{R}$ is Abelian.
        \end{proof}
        \begin{example}
            If $R=\{0\}$, and if $+$ and $\cdot$ are the only binary operations
            one can define on $R$: $0\cdot{0}=0$ and $0+0=0$, then
            $\ring{R}$ is a ring. Since $\monoid[][+]{R}$ and
            $\monoid[][\cdot]{R}$ are simply the Abelian group $\mathbb{Z}_{1}$,
            we need only check the distributive law, but this holds trivially
            since $0\cdot(0+0)=0\cdot{0}=0$ and $0\cdot{0}+0\cdot{0}=0$. This is
            often called the \textit{zero ring}, or occasionally the trival
            ring.
        \end{example}
        \begin{example}
            There are other types of different trivial rng structures on any
            Abelian group $\monoid{G}$. Defined $\cdot:G\times{G}\rightarrow{G}$
            by $a\cdot{b}=e$ for all $a,b\in{G}$, where $e\in{G}$ is the unital
            element. This is a rng since $\cdot$ is associative, and hence
            $\monoid[][\cdot]{R}$ is a semigroup. Multiplication also
            distributes over $*$ trivially since:
            \vspace{-2.5ex}
            \twocolumneq{a\cdot(b*c)=e}{(a\cdot{b})*(a\cdot{c})=e*e=e}
            Thus, $\ring{R}$ is a rng. If $R$ has at least two elements then
            this cannot be a proper ring, since there can be no multiplicative
            identity.
        \end{example}
        \begin{example}
            The rational, real, and complex numbers, together with their usual
            arithmetic, form commutative rings (moreover, they form fields).
            That is, letting $+$ and $\cdot$ denote the familiar forms of
            addition and multiplication, respective, $\ring{\nspace[]}$ is a
            ring, and similarly for the other two.
        \end{example}
        \begin{example}
            A primitive example of a ring comes from studying $\mathbb{Z}$.
            Equipped with its usual arithmetic, $\ring{\mathbb{Z}}$ is a ring.
            $\monoid[][+]{\mathbb{Z}}$ is an Abelian group and and
            $\monoid[][\cdot]{\mathbb{Z}}$ forms an Abelian monoid. Moreover,
            multiplication distributes over addition, and hence
            $\ring{\mathbb{Z}}$ is a commutative ring.
        \end{example}
        \begin{example}
            Endowing $\mathbb{Z}_{n}$ with it's modulo arithmetic structure,
            $\ring{\mathbb{Z}_{n}}$ also forms a ring.
        \end{example}
        An important class of rings comes from studying function spaces.
        \begin{theorem}
            \label{thm:Ring_of_Funcs_is_Ring}%
            If $X$ is a set, if $\ring[R]{R}$ is a ring, if
            $\funcspace[R]{X}$ is the set of all functions $f:X\rightarrow{R}$,
            if $+$ is the binary operation on $\funcspace[R]{X}$ defined by:
            \begin{equation}
                (f+g)(x)=f(x)+_{R}g(x)
            \end{equation}
            and if $\cdot$ is the binary operation defined by:
            \begin{equation}
                (f\cdot{g})(x)=f(x)\cdot_{R}g(x)
            \end{equation}
            then $\ring{\funcspace[R]{X}}$ is a ring.
        \end{theorem}
        \begin{example}
            The set of even integers $\mathbb{Z}_{e}$ with addition and
            multiplication forms a rng, but not a ring. The identity element of
            $\mathbb{Z}$ is 1, and 1 is not even.
        \end{example}
        \begin{theorem}
            \label{thm:Rng_Mult_by_Zero}%
            If $\ring{R}$ is a rng, if 0 is the unital element of
            $\monoid[][+]{R}$, and if $a\in{R}$, then $a\cdot{0}=0$ and
            $0\cdot{a}=0$.
        \end{theorem}
        \begin{proof}
            For since $\ring{R}$ is a rng, $\cdot$ distributes over addition,
            and hence both left and right distributes. And since 0 is the
            additive identity, we obtain:
            \begin{equation}
                a\cdot{0}=a\cdot(0+0)=(a\cdot{0})+(a\cdot{0})
            \end{equation}
            by the left cancellation law we have that $a\cdot{0}=0$. Similary:
            \begin{equation}
                0\cdot{a}=(0+0)\cdot{a}=(0\cdot{a})+(0\cdot{a})
            \end{equation}
            Again by the cancellation law, $0\cdot{a}=0$.
        \end{proof}
        \begin{theorem}
            \label{thm:Ring_with_0_Eq_1}%
            If $\ring{R}$ is a ring, if 0 is a the multiplicative identity, if
            1 is the additive identity, and if $0=1$, then $R=\{0\}$.
        \end{theorem}
        \begin{proof}
            For suppose not. Then there is an element $x\in{R}$ such that
            $x\ne{0}$. But 1 is the multiplicative identity, and hence
            $a=a\cdot{1}$. But $1=0$, and thus $a=a\cdot{0}$. But $a\cdot{0}=0$
            (Thm.~\ref{thm:Rng_Mult_by_Zero}), and thus by the transitivity of
            equality we have $a=0$, a contradiction.
        \end{proof}
        \begin{theorem}
            \label{thm:Rng_MinusA_B_EQ_Minus_AB}%
        \end{theorem}
        \begin{theorem}
            If $\ring{R}$ is a rng, if $a,b\in{R}$, and if $\minus{a}\in{R}$ is
            the additive inverse of $a$, then:
            \begin{equation}
                (\minus{a})\cdot{b}=\minus(a\cdot{b})
            \end{equation}
        \end{theorem}
        \begin{proof}
            For:
            \begin{equation}
                a\cdot{b}+(\minus{a})\cdot{b}
                =\big(a+(\minus{a})\big)\cdot{b}
                =0\cdot{b}
                =0
            \end{equation}
            and thus $(\minus{a})\cdot{b}=\minus(a\cdot{b})$.
        \end{proof}
        \begin{theorem}
            \label{thm:Rng_MinusA_MinusB_EQ_AB}%
            If $\ring{R}$ is a rng, if $a,b\in{R}$, and if
            $\minus{a},\minus{b}\in{R}$ are their additive inverses,
            respectively, then:
            \begin{equation}
                (\minus{a})\cdot(\minus{b})=a\cdot{b}
            \end{equation}
        \end{theorem}
        \begin{proof}
            For by Thm.~\ref{thm:Rng_MinusA_B_EQ_Minus_AB},
            $(\minus{a})\cdot(\minus{b})=\minus\big(a\cdot(\minus{b})\big)$. But
            then:
            \begin{equation}
                a\cdot{b}-\big((\minus{a})\cdot(\minus{b})\big)
                =a\cdot{b}+(a\cdot(\minus{b})
                =a\cdot\big(b+(\minus{b})\big)
                =a\cdot{0}
                =0
            \end{equation}
            and hence $a\cdot{b}=(\minus{a})\cdot(\minus{b})$.
        \end{proof}
        \begin{theorem}
            \label{thm:Ring_Minus_1_Squared}%
            If $\ring{R}$ is a ring, if $1\in{R}$ is the multiplicative
            identity, and if $\minus{1}$ is the additive inverse of 1, then
            $(\minus{1})^{2}=1$.
        \end{theorem}
        \begin{proof}
            For $(\minus{1})^{2}=(\minus{1})\cdot(\minus{1})$ and by
            Thm.~\ref{thm:Rng_MinusA_MinusB_EQ_AB}
            $(\minus{1})\cdot(\minus{1})=1\cdot{1}$. But 1 is the multiplicative
            identity and thus $1\cdot{1}=1$.
        \end{proof}
        \begin{theorem}
            If $\ring{R}$ is a ring, if $r\in{R}$ has a multiplicative inverse,
            and if $\minus{r}$ is the additive inverse of $r$, then $\minus{r}$
            has a multiplicative inverse.
        \end{theorem}
        \begin{proof}
            For if $r$ has a multiplicative inverse, then there is an
            element $r^{\minus{1}}\in{R}$ such that $r\cdot{r}^{\minus{1}}=1$.
            But then:
            \begin{align}
                (\minus{r})\cdot(\minus{r}^{\minus{1}})&=r\cdot{r}^{\minus{1}}
                    \tag{Thm.~\ref{thm:Rng_MinusA_MinusB_EQ_AB}}\\
                &=1\tag{Multiplicative Inverse}
            \end{align}
            and thus $\minus{r}$ is invertible.
        \end{proof}
        \begin{fdefinition}{Zero Divisor}{Zero_Divisor}
            A zero divisor of a rng $\ring{R}$ is an element $a\in{R}$ such that
            $a\ne{0}$ and there exists a $b\in{R}$ such that $b\ne{0}$ and
            either $a\cdot{b}=0$ or $b\cdot{a}=0$.
        \end{fdefinition}
        \begin{example}
            The ring of integers contains no zero divisors. This follows from
            the work of Euclid. If $n\cdot{m}=0$, then either $n=0$ or $m=0$ and
            hence there can be no zero divisors.
        \end{example}
        \begin{example}
            Let $n\in\mathbb{N}^{+}$ be positive, and suppose
            $a\in\mathbb{Z}_{n}$ is such that $a$ divides $n$. That is, there is
            an $m\in\mathbb{N}$ such that $a\cdot{m}=n$. Then in the ring of
            integers modulo $n$, $\ring{\mathbb{Z}_{n}}$, we have that both
            $a$ and $m$ are zero divisors. That is, since $a\cdot{m}=0$, they
            are congruent to zero modulo $n$. That is,
            $a\cdot{m}\cong{0}\mod{n}$. We can also see that if $p$ is prime
            then there are no zero-divisors, for if $a\cdot{m}\cong{0}\mod{p}$,
            then $p$ divides both $a$ and $m$. But if $p$ is prime and divides
            $a$ and $m$, then either it divides $a$ or it divides $m$ or both.
            But then one of these is greater then $p$, contradicting that
            $a,m\in\mathbb{Z}_{p}$.
        \end{example}
        \begin{theorem}
            \label{thm:Group_of_Units_of_Ring_is_Group}%
            If $\ring{R}$ is a ring, and if $R^{\times}$ is the set:
            \begin{equation}
                R^{\times}=\big\{\,r\in{R}\;|\;
                    \exists_{r^{\minus{1}}\in{R}}
                    \big(r\cdot{r}^{\minus{1}}=1\big)\,\big\}
            \end{equation}
            then $\monoid[R^{\times}][\cdot]{R^{\times}}$ is a group.
        \end{theorem}
        \begin{proof}
            For if $a,b\in{R}$ are invertible, then $a\cdot{b}$ is. Moreover,
            the identity is invertible, and $\cdot$ is associative. Hence, we
            have a group.
        \end{proof}
        \begin{theorem}
            \label{thm:Ring_Zero_Divisor_Not_Invertible}%
            If $\ring{R}$ is a ring, and if $a\in{R}$ is a zero divisor, then
            $a$ is not invertible.
        \end{theorem}
        \begin{proof}
            For suppose not. Then there is a $b\in{R}$ such that $a\cdot{b}=1$.
            But since $a$ is a zero divisor, $a\ne{0}$ and there is a $c\in{R}$
            such that $c\ne{0}$ and either $a\cdot{c}=0$ or $c\cdot{a}=0$
            (Def.~\ref{def:Zero_Divisor}) $c\cdot{a}=0$. But then:
            \par
            \begin{minipage}[t]{0.49\textwidth}
                \begin{align*}
                    c&=c\cdot{1}\tag{Identity}\\
                    &=c\cdot(a\cdot{b})\tag{Inverse}\\
                    &=(c\cdot{a})\cdot{b}\tag{Associativity}
                \end{align*}
            \end{minipage}
            \hfill
            \begin{minipage}[t]{0.49\textwidth}
                \centering
                \begin{align*}
                    &=0\cdot{b}\tag{Hypothesis}\\
                    &=0\tag{Thm.~\ref{thm:Rng_Mult_by_Zero}}
                \end{align*}
            \end{minipage}
            \par\vspace{2.5ex}
            a contradiction. Similarly if $a\cdot{c}=0$.
        \end{proof}
        \begin{fdefinition}{Integral Domain}{Integral_Domain}
            An integral domain is a ring $\ring{R}$ such that for all $r\in{R}$
            it is true that $r$ is not a zero divisor.
        \end{fdefinition}
        \begin{example}
            By our previous discussion, $\ring{\mathbb{Z}}$ is an integral
            domain since it contains no zero divisors.
        \end{example}
        \begin{theorem}
            \label{thm:Left_Cancellation_Law_Int_Domain}%
            If $\ring{R}$ is an integral domain, if $a,b,c\in{R}$, and if
            $a\cdot{b}=a\cdot{c}$, then either $b=c$ or $a=0$.
        \end{theorem}
        \begin{proof}
            For suppose not. Then:
            \begin{align}
                a\cdot(b-c)=a\cdot{b}-a\cdot{c}=a\cdot{c}-a\cdot{c}=0
            \end{align}
            But $b\ne{c}$ and thus $b-c\ne{0}$. But then $a$ is a non-zero
            element of $R$ such that there exists a non-zero element $b-c$ with
            $a\cdot(b-c)=0$, and hence $a$ is a zero divisor
            (Def.~\ref{def:Zero_Divisor}). But $\ring{R}$ is an integral domain
            and hence there are no zero divisiors
            (Def.~\ref{def:Integral_Domain}), a contradiction.
        \end{proof}
        \begin{theorem}
            \label{thm:Int_Domain_AB_EQ_Zero_A_or_B_EQ_Zero}%
            If $\ring{R}$ is an integral domain, if $a,b\in{R}$, and if
            $a\cdot{b}=0$, then either $a=0$ or $b=0$.
        \end{theorem}
        \begin{proof}
            For if $\ring{R}$ is a ring, then $a\cdot{0}=0$
            (Thm.~\ref{thm:Rng_Mult_by_Zero}). But $a\cdot{b}=0$ by hypothesis,
            and hence by the transitivity of equality $a\cdot{b}=a\cdot{0}$.
            But if $\ring{R}$ is an integral domain, then by the cancellation
            law either $a=0$ or $b=0$
            (Thm.~\ref{thm:Left_Cancellation_Law_Int_Domain}).
        \end{proof}
        \begin{fdefinition}{Field}{Field}
            A field is a commutative ring $\ring{R}$ such that for all
            $r\in{R}\setminus\{0\}$ there exists a multiplicative inverse
            $r^{\minus{1}}\in{R}\setminus\{0\}$ and such that $0\ne{1}$.
        \end{fdefinition}
        \begin{theorem}
            \label{thm:Fields_are_Int_Domains}%
            If $\ring{F}$ is a field, then $\ring{F}$ is an integral domain.
        \end{theorem}
        \begin{proof}
            For suppose not. But if $\ring{F}$ is not an integral domain,
            then there is a zero divisor $r\in{F}$
            (Def.~\ref{def:Integral_Domain}). BUt if $r$ is a zero divisor, then
            $r\ne{0}$, and hence $r\in{R}\setminus\{0\}$. But $F$ is a field,
            and therefore $r$ is invertible (Def.~\ref{def:Field}). But
            zero divisors are not invertible
            (Thm.~\ref{thm:Ring_Zero_Divisor_Not_Invertible}), a contradiction.
            Therefore, $\ring{R}$ is an integral domain.
        \end{proof}
        \begin{theorem}
            \label{thm:Int_Domain_Left_Mult_is_Inj}%
            If $\ring{R}$ is an integral domain, if $a\in{R}$ is non-zero, and
            if $f:R\rightarrow{R}$ is defined by $f(x)=a\cdot{x}$, then $f$
            is injective.
        \end{theorem}
        \begin{proof}
            For suppose not. Then there are elements $x,y\in{R}$ such that
            $x\ne{y}$ and $f(x)=f(y)$. But then $a\cdot{x}=a\cdot{y}$. But since
            $R$ is an integral domain, if $a\cdot{x}=a\cdot{y}$, then either
            $a=0$ or $x=y$ (Thm.~\ref{thm:Left_Cancellation_Law_Int_Domain}).
            But $a\ne{0}$ and $x\ne{y}$, a contradiction.
        \end{proof}
        \begin{theorem}
            \label{thm:Finite_Int_Domain_if_Field}%
            If $\ring{R}$ is an integral domain, and if $R$ is finite, and if
            $0\ne{1}$, then $\ring{R}$ is a field.
        \end{theorem}
        \begin{proof}
            For suppose not. Then there is an $r\in{R}\setminus\{0\}$ such that
            $r$ has no multiplicative inverse. Let $f:R\rightarrow{R}$ be
            defined by $f(x)=r\cdot{x}$. But then $f$ is injective
            (Thm.~\ref{thm:Int_Domain_Left_Mult_is_Inj}). And since $R$ is
            finite, it is therefore surjective. But then there is an
            $r^{\minus{1}}\in{R}$ such that $f(r^{\minus{1}})=1$. But then
            $r\cdot{r}^{\minus{1}}=1$, a contradiction.
        \end{proof}
        \begin{fdefinition}{Subring}{Subring}
            A subring of a ring $\ring{R}$ is a subset $S\subseteq{R}$ such that
            $\monoid[S][+]{S}$ is a subgroup of $\monoid[][+]{R}$ and
            $\monoid[S][\cdot]{S}$ is a submonoid of $\monoid[][\cdot]{R}$.
        \end{fdefinition}
        \begin{example}
            If we take $\ring{\mathbb{C}}$ to be usual field structure on the
            complex numbers (which is therefore a ring), there are several
            familiar subrings. $\nspace[]$, $\mathbb{Q}$, and $\mathbb{Z}$ are
            all subrings.
        \end{example}
        \begin{theorem}
            \label{thm:Subring_of_Field_is_Int_Domain}%
            If $\ring{F}$ is a field, and if $R\subseteq{F}$ is a subring of
            $F$, then $\ring[R]{R}$ is an integral domain.
        \end{theorem}
        \begin{proof}
            For suppose not. Then there is a zero divisor $a\in{R}$
            (Def.~\ref{def:Integral_Domain}), and hence there is a non-zero
            element $b\in{R}$ such that either $a\cdot{b}=0$ or $b\cdot{a}=0$.
            But $R\subseteq{F}$, and hence $a,b\in{F}$. But if $\ring{F}$ is a
            field, then it is an integral domain
            (Thm.~\ref{thm:Fields_are_Int_Domains}). But if $\ring{F}$ is an
            integral domain, and if $a\cdot{b}=0$, then either $a=0$ or $b=0$,
            a contradiction.
        \end{proof}
        \begin{example}
            If $n\subseteq\mathbb{N}$ is square free (that is, there is no
            $m\in\mathbb{N}$ such that $m^{2}=n$), then we can adjoin
            $\sqrt{n}$ to $\mathbb{Z}$ by considering the set:
            \begin{equation}
                \mathbb{Z}[]\sqrt{n}]=\{\,a+b\sqrt{n}\;|\;a,b\in\mathbb{Z}\,\}
            \end{equation}
            This is a subring of $\nspace[]$. More precisely, it is a subring of
            $\mathbb{Q}(\sqrt{n})$. If $a,b,c,d\in\mathbb{Z}$, then:
            \begin{equation}
                (a+b\sqrt{n})\cdot(c+d\sqrt{n})=ac+bdn+(ad+bc)\sqrt{n}
            \end{equation}
            And hence we have that $\mathbb{Z}[\sqrt{n}]$ is closed under
            multiplication.
        \end{example}
        \begin{example}
            If $r\in\mathbb{Q}^{+}$ is not a square of another rational number,
            we can define $\mathbb{Q}(r)$ as follows:
            \begin{equation}
                \mathbb{Q}(r)=\{\,a+b\sqrt{r}\;|\;a,b\in\mathbb{Q}\,\}
            \end{equation}
            Since $\mathbb{Q}$ is a field, it may be reasonable to suspect that
            $\mathbb{Q}(\sqrt{r})$ is a field as well. First, we show that it is
            closed under addition:
            \begin{equation}
                (a+b\sqrt{r})+(c+d\sqrt{r})=(a+c)+(b+d)\sqrt{r}
            \end{equation}
            and similarly for multiplication:
            \begin{equation}
                (a+b\sqrt{r})\cdot(c+d\sqrt{r})=ac+bdr+(ad+bc)\sqrt{r}
            \end{equation}
            and hence $\mathbb{Q}(\sqrt{r})$ is a subring of $\nspace[]$. It is
            also a field. We need only show that multiplicative inverses for
            non-zero elements exist. Give $a+b\sqrt{r}$, define:
            \begin{equation}
                (a+b\sqrt{r})^{\minus{1}}=\frac{a}{a^{2}-b^{2}}-
                    \frac{b}{a^{2}-rb^{2}}\sqrt{r}
            \end{equation}
            this is well defined since $a^{2}\ne{r}b^{2}$, for other
            $r=a^{2}/b^{2}$ which contradicts the fact that $r$ is square free.
            Then $(a+b\sqrt{r})^{\minus{1}}$ is an element of
            $\mathbb{Q}(\sqrt{r})$ and moreover:
            \begin{subequations}
                \begin{align}
                    (a+b\sqrt{r})^{\minus{1}}\cdot(a+b\sqrt{r})
                    &=\Big(\frac{a}{a^{2}-rb^{2}}-
                        \frac{b}{a^{2}-rb^{2}}\sqrt{r}\Big)\cdot(a+b\sqrt{r})\\
                    &=\frac{1}{a^{2}-rb^{2}}\big(
                        (a-b\sqrt{r})\cdot(a+b\sqrt{r})\big)\\
                    &=\frac{a^{2}-rb^{2}}{a^{2}-rb^{2}}\\
                    &=1
                \end{align}
            \end{subequations}
            which shows that $a+b\sqrt{r}$ is invertible. Thus,
            $\mathbb{Q}(\sqrt{r})$ is a \textit{subfield} of $\nspace[]$.
        \end{example}
        \begin{example}
            The Gaussian integers are another common example of a subring of the
            complex numbers $\mathbb{Q}$. Consider the subring $\mathbb{Z}[i]$
            defined as follows:
            \begin{equation}
                \mathbb{Z}[i]=\{\,a+ib\;|\;a,b\in\mathbb{Z}\,\}
            \end{equation}
            this is a subring of $\mathbb{C}$ since:
            \begin{equation}
                (a+ib)+(c+id)=(a+c)+i(b+d)
            \end{equation}
            and:
            \begin{equation}
                (a+ib)\cdot(c+id)=ac-bd+i(ad+bc)
            \end{equation}
            hence, $\mathbb{Z}[i]\subseteq\mathbb{C}$ is a subring.
        \end{example}
    \section{Polynomial Rings}
        Definition of ring, commutative ring, subring, and ideal are assumed.
        \begin{fdefinition}{Prime Ideal}{Prime_Ideal}
            A prime ideal of a commutative ring $\ring{R}$ is an ideal
            $I\subsetneq{R}$ such that for all $a,b\in{R}$ such that
            $a\cdot{b}\in{I}$, it is true that either $a\in{I}$ or $b\in{I}$.
        \end{fdefinition}
        This is a generalization of a theorem due to Euclid which states that if
        $a,b\in\mathbb{Z}$ are integers, and if $p\in\mathbb{N}$ is a prime such
        that $p$ divides $a\cdot{b}$, then either $p$ divides $a$ or $p$ divides
        $b$. We can use this to prove the following theorem.
        \begin{theorem}
            \label{thm:Prime_Ideals_of_Z}%
            If $\ring{\mathbb{Z}}$ is the usual ring structure on $\mathbb{Z}$,
            and if $p\in\mathbb{Z}$, then $p\mathbb{Z}$ is a prime ideal of
            $\mathbb{Z}$ if and only if $p$ is prime.
        \end{theorem}
        \begin{proof}
            For if $p\in\mathbb{N}$ is prime, and if $a,b\in\mathbb{Z}$ are such
            that $a\cdot{b}\in{p}\mathbb{Z}$, then there is an $m\in\mathbb{Z}$
            such that $a\cdot{b}=m\cdot{p}$. But then $p$ divides $a\cdot{b}$,
            and thus either $p$ divides $a$ or $p$ divides $b$. But if $p$
            divides $a$, then $a\in{p}\mathbb{Z}$, and similarly if $p$ divides
            $b$. Hence, $p\mathbb{Z}$ is a prime ideal. In the other direction,
            if $p\mathbb{Z}$ is a prime ideal, suppose $p$ is not prime. Then
            there are integers $a,b\in\mathbb{Z}$, neither of which are units,
            such that $p=a\cdot{b}$. But by hypothesis $p\mathbb{Z}$ is a prime
            ideal, and hence if $a\cdot{b}\in{p}\mathbb{Z}$, then either
            $a\in{p}\mathbb{Z}$ or $b\in{p}\mathbb{Z}$. But if
            $a\in{p}\mathbb{Z}$, then there is an $m\in\mathbb{Z}$ such that
            $a=m\cdot{p}$. But $a\cdot{b}=p$, and hence $m\cdot{p}\cdot{b}=p$.
            But then $m\cdot{b}=1$, and thus $b$ is a unit, which is a
            contradition. Hence, $p$ is prime.
        \end{proof}
        We can use this to generate examples of prime ideals.
        \begin{example}
            The even integers $\mathbb{Z}_{e}$ form a prime ideal of
            $\mathbb{Z}$. This can be seen immediately once one notes that
            $\mathbb{Z}_{e}=2\mathbb{Z}$, and since $2$ is a prime number,
            by Thm.~\ref{thm:Prime_Ideals_of_Z} we see that $2\mathbb{Z}$ is a
            prime ideal.
        \end{example}
        A useful equivalent definition of prime ideals goes as follows:
        \begin{theorem}
            If $\ring{R}$ is a ring, and if $I\subsetneq{R}$ is an ideal, then
            it is a prime ideal if and only if for all $a,b\in{R}\setminus\{I\}$
            it is true that $a\cdot{b}\in{R}\setminus\{I\}$.
        \end{theorem}
        \begin{proof}
            For if $I$ is a prime ideal, then for all $a,b\in{R}$ such that
            $a\cdot{b}\in{I}$, it is true that either $a\in{I}$ or $b\in{I}$.
            Suppose there exists $a,b\in{R}\setminus\{I\}$ such that
            $a\cdot{b}\notin{R}\setminus\{I\}$. But since $\cdot$ is a binary
            operation it is true that $a\cdot{b}\in{R}$, and hence if
            $a\cdot{b}\notin{R}\setminus{I}$, then $a\cdot{b}\in{I}$. But $I$ is
            a prime ideal, and thus if $a\cdot{b}\in{I}$ then either $a\in{I}$
            or $b\in{I}$, a contradiction since by hypothesis $a,b\notin{I}$.
            Therefore, $a\cdot{b}\in{R}\setminus\{I\}$. In the other direction,
            if $R\setminus\{I\}$ is multiplicatively closed, suppose $a,b\in{R}$
            are such that $a\cdot{b}\in{I}$. But $R\setminus\{I\}$ is
            multiplicatively closed, and thus if $a,b\in{R}\setminus\{I\}$, then
            $a\cdot{b}\in{R}\setminus\{I\}$, a contradiction, and thus either
            $a\in{I}$ or $b\in{I}$. Thus, $I$ is a prime ideal.
        \end{proof}
        \begin{theorem}
            \label{thm:homo_pre_image_of_ideal_is_ideal}%
            If $\ring[R]{R}$ and $\ring[S]{S}$ are rings, if
            $\phi:R\rightarrow{S}$ is a ring homomoprhism, and if
            $I\subseteq{S}$ is an ideal in $S$, then $\phi^{\minus{1}}[I]$ is an
            ideal in $R$.
        \end{theorem}
        \begin{proof}
            Since $I$ is an ideal, $0_{S}\in{I}$, and since $\phi$ is a ring
            homomoprhism it is true that $\phi(0_{R})=0_{S}$. Hence,
            $\phi^{\minus{1}}[I]$ is non-empty. But if
            $a,b\in\phi^{\minus{1}}[I]$, then $\phi(a),\phi(b)\in{I}$. But
            $\phi$ is a ring homomoprhism and hence
            $\phi(a+_{R}b)=\phi(a)+_{S}\phi(b)$, and since $I$ is an ideal it is
            true that $\phi(a)+_{S}\phi(b)\in{I}$. Therefore
            $\phi(a+_{R}b)\in{I}$, and thus $a+_{R}b\in\phi^{\minus{1}}[I]$.
            Hence $\phi^{\minus{1}}[I]$ is closed to addition. But if $r\in{R}$
            and $a\in\phi^{\minus{1}}[I]$, then since $\phi$ is a ring
            homomoprhism it is true that
            $\phi(r\cdot_{R}a)=\phi(r)\cdot_{S}\phi(a)$. But since
            $a\in\phi^{\minus{1}}[I]$ it is true that $\phi(a)\in{I}$, and since
            $I$ is an ideal in $S$ it is therefore true that
            $\phi(r)\cdot_{S}\phi(a)\in{I}$. Thus, $\phi(r\cdot_{R}a)\in{I}$,
            and hence $r\cdot_{R}a\in\phi^{\minus{1}}[I]$. Therefore,
            $\phi^{\minus{1}}[I]$ is an ideal in $R$.
        \end{proof}
        \begin{theorem}
            If $\ring[R]{R}$ and $\ring[S]{S}$ are commutative rings, if
            $I\subsetneq{S}$ is a prime ideal, and if $\phi:R\rightarrow{S}$ is
            a ring homomoprhism, then $\phi^{\minus{1}}[I]$ is a prime ideal in
            $R$.
        \end{theorem}
        \begin{proof}
            For if $I$ is an ideal in $S$ and $\phi:R\rightarrow{S}$ is a ring
            homomorphism, then $\phi^{\minus{1}}[I]$ is an ideal in $R$
            (Thm.~\ref{thm:homo_pre_image_of_ideal_is_ideal}). Suppose
            $a,b\in{R}$ are such that $a\cdot_{R}b\in\phi^{\minus{1}}[I]$.
            But then $\phi(a\cdot_{R}b)\in{I}$, and since $\phi$ is a ring
            homomoprhism it is therefore true that
            $\phi(a\cdot_{R}b)=\phi(a)\cdot_{S}\phi(b)$. But $I$ is a prime
            ideal, and if $\phi(a)\cdot_{S}\phi(b)\in{I}$, then either
            $\phi(a)\in{I}$ or $\phi(b)\in{I}$. But then either
            $a\in\phi^{\minus{1}}[I]$ or $b\in\phi^{\minus{1}}[I]$, and hence
            $\phi^{\minus{1}}[I]$ is a prime ideal.
        \end{proof}
        Given a ring $\ring{R}$, the existence of prime ideals follows directly
        from the existence of \textit{maximal} ideals. We first must define such
        things, and then prove that every maximal ideal is prime.
        \begin{fdefinition}{Maximal Ideal}{Maximal_Ideal}
            A maximal ideal in a ring $\ring{R}$ is an ideal $I\subsetneq{R}$
            such that for every ideal $J\subsetneq{R}$ such that
            $I\subseteq{J}$, it is true that $I=J$.
        \end{fdefinition}
        \begin{theorem}
            \label{thm:Proper_Ideal_Does_Not_Contain_1}%
            If $\ring{R}$ is an ideal, if $I\subseteq{R}$ is a proper ideal, and
            if $1\in{R}$ is the multiplicative idenity, then $1\notin{R}$.
        \end{theorem}
        \begin{proof}
            For suppose not. But if $1\in{R}$, then since $I$ is an ideal for
            all $r\in{R}$ it is true that $r\cdot{1}\in{R}$. But $1$ is the
            multiplicative identity, and hence $r\cdot{1}\in{I}$. But then for
            all $r\in{R}$ it is true that $r\in{I}$, and hence $r=I$, a
            contradiction since $I$ is a proper ideal. Thus, $1\notin{I}$.
        \end{proof}
        \begin{ftheorem}{Krull's Theorem}{Krulls_Theorem}
            If $\ring{R}$ is a non-zero commutative ring, then there is a
            maximal ideal $I\subseteq{R}$.
        \end{ftheorem}
        \begin{bproof}
            For let $J\subseteq\powset{R}$ be defined as follows:
            \begin{equation}
                J=\{\,I\in\powset{R}\;|\;I\textit{ is a proper ideal of }R\,\}
            \end{equation}
            But $R$ is non-zero, and hence $\{0\}$ is a proper ideal of $R$,
            and therefore $J$ is non-empty. Then if $\subseteq$ is the inclusion
            relation, then $(J,\subseteq)$ is a partially ordered set. Suppose
            $\Lambda\subseteq{J}$ is a chain. But then $\bigcup\Lambda\in{J}$.
            For if $x,y\in\bigcup\Lambda$, then there is a $I_{x}\in\Lambda$ and
            an $I_{y}\in\Lambda$ such that $x\in{I}_{x}$ and $y\in{I}_{y}$. But
            since $\Lambda$ is a chain, either $I_{x}\subseteq{I}_{y}$ or
            $I_{y}\subseteq{I}_{x}$. Suppose $I_{x}\subseteq{I}_{y}$. But then
            $x,y\in{I}_{y}$, and since $I_{y}\in{J}$ it is an ideal. But then
            $x+y\in{I}_{y}$, and similarly if $I_{y}\subseteq{I}_{x}$. Hence,
            $x+y\in\bigcup\Lambda$. Moreover, if $r\in{R}$ and
            $x\in\bigcup\Lambda$, then there is an $I\in\Lambda$ such that
            $x\in{I}$. But $I$ is an ideal, and therefore $r\cdot{x}\in{I}$.
            But then $r\cdot{x}\in\bigcup\Lambda$. Therefore, $\bigcup\Lambda$
            is an ideal. Moreover, since for all $I\in\Lambda$ it is true that
            $I\in{J}$, $I$ is thus a proper ideal. But if $I$ is a proper ideal,
            then $1\not\in{I}$ (Thm.~\ref{thm:Proper_Ideal_Does_Not_Contain_1}).
            But this is true of all $I\in\Lambda$, and hence
            $1\notin\bigcup\Lambda$, and therefore $\bigcup\Lambda$ is a proper
            ideal. Therefore, every chain of $(J,\subseteq)$ is bounded, and
            therefore there exists a maximal element $I\in{J}$. But then $I$
            is a proper ideal of $R$ that is not contained in any other proper
            ideals, and is therefore a maximal ideal
            (Def.~\ref{def:Maximal_Ideal}).
        \end{bproof}
        Zorn's lemma is indeed required for this proof. Krull's original proof
        invoked transfinite induction, a consequence of the well-ordering
        theorem, and indeed Krull's theorem implies the axiom of choice. Since
        the axiom of choice and Zorn's lemma are equivalent, any axiomatic
        system capable of proving Krull's theorem must contain, or be able to
        prove, the axiom of choice. Nevertheless, we can now prove that every
        non-zero ring contains a prime ideal.
        \begin{ltheorem}{Maximal Ideals are Prime}{Maximal_Ideals_are_Prime}
            If $\ring{R}$ is a ring, if $I\subseteq{R}$ is a maximal ideal, then
            $I$ is a prime ideal.
        \end{ltheorem}
        \begin{proof}
            For suppose not. Then there exists $a,b\in{R}$ such that
            $a,b\notin{I}$, but $a\cdot{b}\in{I}$ (Def.~\ref{def:Prime_Ideal}).
            But then neither $a$ nor $b$ are units, for otherwise since $I$ is
            an ideal, since $a\cdot{b}\in{I}$, it is then true that
            $a^{\minus{1}}\cdot{a}\cdot{b}\in{I}$, and thus $b\in{I}$, a
            contradiction. But then the ideal generated by $I\cup\{a\}$ is a
            proper ideal since it does not contain 1. But then
            $I\subsetneq(I\cup\{a\})$ since $a\notin{I}$, a contradiction since
            $I$ is maximal. Thus, $I$ is prime.
        \end{proof}
        \begin{theorem}
            \label{thm:Existence_of_Prime_Ideals}%
            If $\ring{R}$ is a non-zero ring, then there exists a prime ideal
            $I\subseteq{R}$.
        \end{theorem}
        \begin{proof}
            For by Krull's theorem, there exists a maximal ideal
            $I\subseteq{R}$ (Thm.~\ref{thm:Krulls_Theorem}). But maximal ideals
            are prime ideals (Thm.~\ref{thm:Maximal_Ideals_are_Prime}). Hence,
            $I$ is a prime ideal.
        \end{proof}
        Given a ring homomoprhism $\varphi:R\rightarrow{S}$, there is a map
        $\varphi^{\#}:\textrm{Spec}(S)\rightarrow\textrm{Spec}(R)$, where
        $\textrm{Spec}(X)$ is the set of all prime ideals on $X$.
        Zariski topology. Define, for all $r\in{R}$:
        \begin{equation}
            X_{r}=\{p\in\textrm{Spec{R}}\;|\;r\in{p}\}
        \end{equation}
        The topology is $\tau=\{X_{r}\;|\;r\in{R}\}$.
        \begin{example}
            Take the polynomial ring $\mathbb{C}[x]$. The prime ideals on this
            space are $(z-a)$ for some $a\in\mathbb{C}$. The topology is then
            $\{(x-a)\;|\;a\in\mathbb{C}\}\cup\{(0)\}$. The weirdness about this
            is that $(0)$ is dense in this space.
        \end{example}
        \begin{theorem}
            If $\ring[F]{F}$ is a field, if $\ring[K]{K}$ is a field extension,
            and if $\alpha\in{K}$, then $F[\alpha]$ is a field if and only if
            $\alpha$ is algebraic. 
        \end{theorem}
        \begin{theorem}
        \end{theorem}
        \begin{fdefinition}{Irreducible Elements}{Irreducible_Element}
            An irreducible element of a ring $\ring{R}$ is an element
            $x\in{R}$ such that for all $a,b\in{R}$ such that $a\cdot{b}=x$, it
            is true that either $a$ is a unit or $b$ is a unit.
        \end{fdefinition}
        \begin{fdefinition}{Prime Element}{Prime_Element}
            A prime element of a ring $\ring{R}$ is an element $p\in{R}$
            such that for all $a,b\in{R}$ such that $p$ divides $a\cdot{b}$,
            then either $p$ divides $a$ or $p$ divides $b$.
        \end{fdefinition}
        \begin{fdefinition}{Unique Factorization Domain}
                           {Unique_Factorization_Domain}
            An integral domain is a ring $\ring{R}$ such that for all $r\in{R}$
            there exists finitely many irreducible elements $a_{i}$ such that
            $\prod{a}_{i}=r$ and such that for any other sequence of irreducible
            elements $b_{k}$ such that $\prod{b}_{k}=r$, there exists units
            $u_{i}$ such that $a_{i}=u_{i}\cdot{b}_{i}$.
        \end{fdefinition}
        \begin{example}
            The fundamental theorem of arithmetic states the $\mathbb{Z}$ is a
            UFD.
        \end{example}
        \begin{theorem}
            If $F$ is a field, then $F[x]$ is a UFD.
        \end{theorem}
        \begin{proof}
            Any principal ideal domain is a unique factorization domain.
        \end{proof}
        \begin{example}
            There are non-unique factorization domains. For example,
            $\mathbb{Z}[\sqrt{\minus{5}}]$ since
            $6=2\cdot{3}=(1+\sqrt{\minus{5}})(1-\sqrt{\minus{5}})$.
        \end{example}
        The converse of the previous example is not true.
        \begin{theorem}
            $\mathbb{Z}[x]$ is a UFD.
        \end{theorem}
        However, $\mathbb{Z}[x]$ is not a PID since $(2,x)$ is not principal.
        \subsection{Roots and Irreducibility}
            \begin{definition}
                An element $\alpha\in{F}$ is a root of a polynomial $f(x)$ if
                $f(x)=0$.
            \end{definition}
            \begin{theorem}
                $\alpha\in{F}$ is a root of $f\in{F}[x]$ if and only if
                $(x-\alpha)$ divides $f$.
            \end{theorem}
            \begin{proof}
                One was is clear, if $(x-\alpha)|f$, then $f(x)=(x-\alpha)r(x)$
                for some $r\in{F}[x]$. But then $f(\alpha)=0\cdot{r}(\alpha)=0$,
                and thus $\alpha$ is a root. In the other direction, use the
                division algorithm. Write $f(x)=(x-\alpha)q(x)+r(x)$. But $r(x)$
                must have degree less than $x-\alpha$, and is therefor a
                constant. But then $f(\alpha)=r=0$, so $r=0$. Hence,
                $f(x)=(x-\alpha)q(x)$, so $x-\alpha$ divides $f$.
            \end{proof}
            \begin{theorem}
                If $f\in{F}[x]$ is a polynomial of degree $n$, then there are at
                most $n$ roots.
            \end{theorem}
            \begin{proof}
                By induction. If there are no roots, we are done. If not, write
                $f(x)=(x-\alpha)q(x)$. Then $q$ is a polynomial of degree $n-1$,
                and by the induction hypothesis has at most $n-1$ roots. Thus,
                there are at most $n$ roots.
            \end{proof}
            \begin{example}
                $x^{2}-1\in\mathbb{Z}_{8}[x]$ has 4 roots. That is, 1, 3, 5, 7
                are all roots. This does not contradict the previous theorem
                since $\mathbb{Z}_{8}$ is not a field.
            \end{example}
            \begin{ftheorem}{Fundamental Theorem of Algebra}
                            {Fundamental_Theorem_of_Algebra}
                If $f:\mathbb{C}\rightarrow\mathbb{C}$ is a non-constant
                polynomial, then there exists an $\alpha\in\mathbb{C}$ such that
                $f(\alpha)=0$.
            \end{ftheorem}
            \begin{theorem}
                Any linear polynomial $ax+b\in{F}[x]$ is irreducible.
            \end{theorem}
            \begin{theorem}
                If $f\in{F}[x]$ is irreducible and $\textrm{deg}(g)\geq{2}$,
                then $f$ has no roots. Moreover, the converse holds if
                $\textrm{deg}(f)=2$ or 3.
            \end{theorem}
            \begin{example}
                The previous theorem is very special for degree 2 and 3. Let
                $f(x)=x^{2}+x+1$ where $f\in\mathbb{F}_{p}[x]$, $p$ a prime.
                For the case of $p=2$ have seen that this is a irreducible. For
                $p=3$ we have that 1 is a root: $1^{2}+1+1=3\cong{0}$ in
                $\mathbb{F}_{3}$. For $p=5$, this is once again irreducible,
                but not in $\mathbb{F}_{7}$.
            \end{example}
            \begin{example}
                Consider $x^{4}+x^{2}+1\in\mathbb{F}_{2}[x]$. This has no roots,
                since 0 and 1 both map to 1, but it is reducible since
                $x^{4}+x^{2}+1=(x^{2}+x+1)^{2}$ in $\mathbb{F}_{2}[x]$. That is,
                the so called \textit{freshman's dream} is true in this case, and
                we can distribute the power. Thus we have a reducible polynomial
                with no roots. Note, however, that the degree is not 2 or 3.
            \end{example}
            Next, we present Gauss's Lemma. Note that being irreducible over
            $\mathbb{Z}$ is stronger than being irreducible over $\mathbb{Z}$.
            Consider $f(x)=2x$. While this is linear in $\mathbb{Q}$, and since
            $\mathbb{Q}$ is a field, $f$ is irreducible in $\mathbb{Q}[x]$. However
            when viewed in $\mathbb{Z}[x]$, we have $2x=2\cdot{x}$, neither of
            which are units, and hence $f$ is reducible in $\mathbb{Z}[x]$.
            \begin{theorem}
                If $f\in\mathbb{Z}[x]$ is irreducible, then it is irreducible in
                $\mathbb{Q}[x]$.
            \end{theorem}
        \subsection{Reduction Modulo a Prime}
            Consider the projection map
            $\pi:\mathbb{Z}[x]\rightarrow\mathbf{F}_{p}[x]$, reduction mod $p$
            of the coefficients:
            \begin{equation}
                f(x)=\sum{a}_{k}x^{k}\longrightarrow
                    \overline{f}(x)\sum\overline{a}_{k}x^{k}
            \end{equation}
            The mapping $\pi$ is a ring homomoprhism.
            \begin{example}
                $x^{3}+21x+31\mapsto{x}^{3}+x+1$ in $\mathbb{F}_{2}[x]$.
            \end{example}
            \begin{theorem}
                If $f\in\mathbb{Z}[x]$, if $p$ is prime, and if $p\not|a_{n}$,
                and if $\pi(f)\in\mathbb{F}_{p}[x]$ is irreducible, then
                $f\in\mathbb{Z}[x]$ is irreducible.
            \end{theorem}
            \begin{proof}
                We prove by the contrapositive. If $f(x)=g(x)h(x)$, then
                $\overline{f}(x)=\overline{g}(x)\overline{h}(x)$ since $\pi$ is
                a homomoprhism. But $\overline{g}$ and $\overline{h}$ are not
                units since $p$ does not $a_{n}$, and thus the degree of the
                reduction is equal to the degree of the original polynomial.
                That is, $\textrm{deg}(f)=\textrm{deg}(\overline{f})$. But also
                $\textrm{deg}(g)=\textrm{deg}(\overline{g})$ and similarly for
                $h$ since:
                \begin{equation}
                    \textrm{deg}(\overline{g})+\textrm{deg}(\overline{h})
                    =\textrm{deg}(g)+\textrm{deg}(h)=\textrm{deg}(f)
                \end{equation}
                Since $p$ does not divide the leading coefficients of either
                $g$ or $h$, the degrees of $\overline{g}$ and $\overline{h}$
                remain the same, and hence are not units. Thus, $\overline{f}$
                is reducible.
            \end{proof}
            \begin{example}
                Consider $x^{3}+21x+31\in\mathbb{Z}[x]$. In $\mathbb{F}_{3}[x]$
                we have $x^{3}+x+1$, which is indeed irreducible, and hence
                the original polynomial $x^{3}+21x+31$ is irreducible.
            \end{example}
            \begin{example}
                The converse is not true. There are polynomials that are
                reducible for all $p$, yet in $\mathbb{Z}[x]$ it is irreducible.
                Consider $(2x+1)(x+1)=2x^{2}+3x+1$. This is reducible, since
                we have the factorization, however in $\mathbb{F}_{2}[x]$ this
                is simply $x+1$, which is irreducible. However, 2 divides 2 so
                the theorem does not apply.
            \end{example}
            \begin{example}
                In $\mathbb{Z}[x]$, consider $x^{2}+x+1$ which we know is
                irreducible since it is irreducible in $\mathbb{R}[x]$
                (the roots are complex). In $\mathbb{F}_{2}[x]$ we can check and
                see that there are no roots, and thus $x^{2}+x+1$ is irreducible
                by Gauss' lemma. However, in $\mathbb{F}_{3}[x]$ it is reducible
                since $x^{2}+x+1=(x-1)^{2}$. That is, 1 is a root of $x^{2}+x+1$
                in $\mathbb{F}_{2}[x]$ with multiplicity 2.
            \end{example}
            \begin{ftheorem}{Eisenstein's Criterion}{Eisensteins_Criterion}
                If $p$ is prime, if $f\in\mathbb{Z}[x]$, if $p$ does not divide
                $a-{n}$, if $p$ divides $a_{k}$ for all $k<n$, and if
                $p^{2}$ does not divide $a_{0}$, then $f$ is irreducible.
            \end{ftheorem}
            \begin{bproof}
                We prove by the contradiction. Suppose $f$ is reducible.
                If $p$ divides all of the $a_{i}$, then in the reduction map
                $\pi:\mathbb{Z}[x]\rightarrow\mathbb{F}_{p}[x]$, all of the
                $a_{i}$ map to zero. So we have:
                \begin{equation}
                    \overline{a}_{n}x^{n}=\overline{f}(x)
                    =\overline{g}(x)\overline{h}(x)
                \end{equation}
                But since $\mathbb{Z}[x]$ is a UFD, both $\overline{g}$ and
                $\overline{h}$ are monomials, $\overline{g}(x)=g_{0}x^{i}$ and
                $\overline{h}(x)=h_{0}x^{j}$. If $i,j<n$, then $i,j>0$, and so
                both constant terms must be divisible by $p$. Thus
                $g(x)h(x)$ has a constant term divisible by $p^{2}$, a
                contradiction.
            \end{bproof}
            \begin{example}
                Let $f(x)=x^{4}+22x^{2}+33x+44$. By Eisenstein, with $p=11$, we
                have that this is irreducible.
            \end{example}
        \subsection{Review of Previous Lecture}
            If $\mathbf{F}$ is a field, then $\zeta\in\mathbf{F}$ is called a
            root of unity of there is some $n\geq{1}$ such that $\zeta^{n}=1$.
            \begin{example}
                In the field $\mathbb{R}$, the roots of unity of 1 and
                $\minus{1}$. The number 1 is a first root of unity, whereas
                $\minus{1}$ is a second root of unity.
            \end{example}
            \begin{example}
                In the complex numbers $\mathbb{C}$, there is an $n^{th}$ root
                of unity for all $n\in\mathbb{N}^{+}$. Let
                $\zeta=\exp(2\pi{i}/n)$. The roots of unity are scattered along
                the unit circle.
            \end{example}
            A root of unity is some element $\zeta\in\mathbb{F}$ such that
            $\zeta$ is a root of the polynomial $f(x)=x^{n}-1$.
            \begin{example}
                Let $p$ be a prime integer, and consider the roots of
                $f(x)=x^{p}-1$. There is always a root since 1 satisfies this
                criterion, and hence we can factor this and obtain:
                \begin{equation}
                    f(x)=x^{p}-1=(x-1)(x^{p-1}+x^{p-2}+\dots+x+1)
                \end{equation}
                This latter polynomial $x^{p-1}+x^{p-1}+\dots+x+1$ is
                irreducible over $\mathbb{Q}$.
            \end{example}
            \begin{theorem}
                If $p$ is prime and $f(x)=x^{p-1}+x^{p-2}+\dots+x+1$,
                $f\in\mathbb{R}[x]$, then $f$ is irreducible over $\mathbb{Q}$.
            \end{theorem}
            \begin{proof}
                For:
                \begin{equation}
                    xf(x+1)=(x+1)^{p}-1
                \end{equation}
                by the binomial theorem we have:
                \begin{equation}
                    xf(x+1)=(x+1)^{p}-1=
                    \minus{1}+\sum_{k=0}^{p}\binom{p}{k}x^{k}
                    =\sum_{k=1}^{p}\binom{p}{k}x^{k}
                \end{equation}
                Thus, simplifying, we have:
                \begin{equation}
                    f(x+1)=\sum_{k=1}^{p}\binom{p}{k}x^{k-1}
                \end{equation}
                Thus by the Eisenstein criterion, $f(x+1)$ is irreducible over
                $\mathbb{Q}$. But if $f(x+1)$ is irreducible over $\mathbb{Q}$,
                then $f(x)$ is as well.
            \end{proof}
            The converse of the statement before is not true. If the translation
            of a polynomial is reducible, it need not mean the original
            polynomial was reducible. For let $f(x)\in\mathbb{F}_{2}[x]$ be
            defined by $f(x)=x^{2}+x+1$. Plugging in $x^{2}$ we get
            $f(x)^{2}=x^{4}+x^{2}+1=(x^{2}+x+1)^{2}$, which is reducible.
    \section{Gauss's Lemma}
        \begin{definition}
            A polynomial $f\in\mathbb{Z}[x]$ is called primitive if
            $\textrm{GCD}(a_{0},\dots,a_{n})=1$. Equivalently, for all primes
            $p$ there is an $i$ such that $p\not|a_{i}$. That is, $p$ does not
            divide $a_{i}$.
        \end{definition}
        \begin{example}
            Let $f(x)=10x^{3}+5x^{2}+2x+23$. This is a primitive polynomial.
            If $p$ divides all of the $a_{i}$, it must divide 2, but 2 is the
            only prime that divides 2, and hence $p=2$. But 5 and 23 are odd,
            and hence 2 does not divide them. So, $f$ is primitive.
        \end{example}
        \begin{ltheorem}{Gauss's Lemma}{Gauss_Lemma}
            FOr any $f\in\mathbb{Q}[x]$ there is a unique $c\in\mathbb{Q}$ and
            a unique primitive polynomial $g\in\mathbb{Z}[x]$ such that
            $f(x)=c\cdot{g}(x)$.
        \end{ltheorem}
        \begin{proof}
            Existence is straight forward. Clear out the denominators of $f$,
            let $c$ be the common factor, and we're done. Suppose
            $cg(x)=c'g'(x)$. If $c$ and $c'$ are not integers, multiply by their
            denominators and thus we may assume $c$ and $c'$ are integers. Then
            we have:
            \begin{equation}
                cg(x)=c'(b_{0}+b_{1}x+\dots+b_{n}x^{n})
            \end{equation}
            And hence:
            \begin{equation}
                c=\textrm{GCD}(c'b_{0},\dots,c'b_{n})
                 =c\textrm{GCD}((b_{0},\dots,b_{n}))
            \end{equation}
            But $g'$ is trivial, so the greatest common denominator is 1. Hence,
            $c=c'$. Also, $g=g'$.
        \end{proof}
        \begin{ltheorem}{Gauss' Lemma Version 2}{Gauss_Lemma_2}
            If $g\in\mathbb{Z}[x]$ is primitive and $f\in\mathbb{Z}[x]$, and if
            $g|f$ in $\mathbb{Q}[x]$, then $g|f$ in $\mathbb{Z}[x]$.
        \end{ltheorem}
        \begin{proof}
            For if $g|f$, then $f=gh$ with $h\in\mathbb{Q}[x]$. But by Gauss'
            lemma there is a unique $c\in\mathbb{Q}$ such that
            $h(x)=ch_{0}(x)$, where $h_{0}\in\mathbb{Z}[x]$ is primitive. So
            $f(x)=cg(x)h_{0}(x)$. But then $g(x)h_{0}(x)$ is primitive, and
            hence $c$ is an integer. Thus, $h\in\mathbb{Z}[x]$. That is, since
            the product of primitive is primitive, the numerator of $c$ is equal
            to the denominator of $c$ times the GCD of the coefficients of $f$.
            But this GCD is a positive integer, and hence the numerator divides
            it, and hence $c$ is an integer itself.
        \end{proof}
        \begin{ltheorem}{Gauss' Lemma V3}{Gauss_Lemma_3}
            If $f\in\mathbb{Z}[x]$ and if $g,h\in\mathbb{Q}[x]$ are such that
            $f=gh$, then $f=g_{0}h_{0}$.
        \end{ltheorem}
    \section{Field Extensions}
        \begin{ftheorem}{Tower Law}{Tower_Law}
            If $\mathbb{F}$, $\mathbb{K}$, and $\mathbb{L}$ are fields, if
            $\mathbb{K}$ is a field extension of $\mathbb{F}$, if $\mathbb{L}$
            is a field extension over $\mathbb{K}$, then $\mathbb{L}/\mathbb{F}$
            is a finite dimensional vector space if and only if
            $\mathbb{L}/\mathbb{K}$ and $\mathbb{K}/\mathbb{F}$ is finite.
            Moreover:
            \begin{equation*}
                [\mathbb{L}:\mathbb{F}]=
                [\mathbb{L}:\mathbb{K}][\mathbb{K}:\mathbb{F}]
            \end{equation*}
        \end{ftheorem}
        Suppose $K/F$ is a field extension. That is, $F\subseteq{K}$ and $F$ is
        a subfield of $K$. Let $X\subseteq{K}$ be any non-empty subset. We now
        define the adjoinment of $X$ to $F$.
        \begin{definition}
            The field extension generated by a subfield $F$ of a field $K$ by a
            subset $X\subseteq{K}$ is the subfield:
            \begin{equation}
                F(X)=
                \bigcap_{\overset{F\subseteq{L}\subseteq{K}}{X\subseteq{L}}}L
            \end{equation}
            Where $L$ is a subfield.
        \end{definition}
        $F(X)$ is non-empty since $K$ is in the intersection, and the
        intersection of subfields is again a subfield, so $F(X)$ is a field.
        If $X=\{a_{0},a_{1},\dots\}$, we often write $F(X)=F(a_{0},a_{1},\dots)$
        and if $X$ if finite we call $F(X)/F$ finitely generated. If $X$ is a
        single element, $F(\alpha)$ is called a simple extension. A field
        extension is simple if it can be written as a simple extension.
        \begin{example}
            $\mathbb{R}/\mathbb{Q}$ is not finitely generated. A simple
            cardinality argument works here, for if it were countably generated
            then since $\mathbb{Q}$ is countable, $\mathbb{R}$ would again be
            countable, which is false.
        \end{example}
        \begin{example}
            $\mathbb{Q}(\sqrt{2},\sqrt{3},\sqrt{5},\sqrt{7},\dots)$ is not
            finitely generated. We can show this by building a chain of
            subfields. First consider $\mathbb{Q}(\sqrt{2})/\mathbb{Q}$. This
            is a field extension of degree two. One can see this since
            $\sqrt{2}$ is not rational. The next field extension
            $\mathbb{Q}(\sqrt{2},\sqrt{3})/\mathbb{Q}(\sqrt{2})$ is also an
            extension of degree 2. For suppose not and suppose:
            \begin{equation}
                \sqrt{3}=a+b\sqrt{2}
            \end{equation}
            Squaring, we obtain:
            \begin{equation}
                3=a^{2}+2b^{2}+2ab\sqrt{2}
            \end{equation}
            Now since $\sqrt{2}$ is irrational we must conclude that $ab=0$.
            Thus either $a=0$ or $b=0$. f $b=0$, then $\sqrt{3}$ is an integer,
            which is false. If $a=0$ then $3=2b^{2}$, but $3$ is prime, a
            contradiction. Thus $\sqrt{3}$ is not contained in
            $\mathbb{Q}(\sqrt{3})$.
        \end{example}
        \begin{theorem}
            If $F$ is a field, if $K$ is a field extension, if $\alpha\in{K}$,
            then:
            \begin{equation}
                F(\alpha)=\big\{\frac{f(\alpha)}{g(\alpha)}\;|\;
                    f,g\in{F}[x],g(\alpha)\ne{0}\big\}
            \end{equation}
        \end{theorem}
        \begin{theorem}
            A field extension of a field $F$ is a field extension $K$ of $F$
            is an injective homomoprhism $\iota:F\rightarrow{K}$.
        \end{theorem}
        \begin{definition}
            A morphism of field extensions $K/F$ and $K'/F$ is a homomoprhism
            $\varphi:K\rightarrow{K}'$ such that the injective homomoprhisms
            $\iota$ and $\iota'$ commute with $\varphi$. That is,
            $\iota'=\varphi\circ\iota$.
        \end{definition}
        \begin{example}
            $\mathbb{C}$ and $\mathbb{R}[x]/(x^{2}+1)$ are isomorphic field
            extensions of $\mathbb{R}$.
            $\varphi\mathbb{R}[x]\rightarrow\mathbb{R}$ mapping
            $f\mapsto{f}(i)$ is surjective since $a+ib=a(i)+b(i)^{4}$. Thus this
            is a surjective $\mathbb{R}$ algebra homomoprhism. The kernel is
            the ideal generated by $x^{2}+1$. Then by the first isomorphism
            theorem,
            $\tilde{\varphi}:\mathbb{R}[x]/(x^{2}+1)\rightarrow\mathbb{C}$ is an
            isomorphism. Hence, these are isomorphic.
        \end{example}
    \section{Minimal Polynomial}
        If $F\subseteq{K}$ is a subfield, if $\alpha\in{K}$, then there exists
        an $F$ algebra homomoprhism $\varphi_{\alpha}:F[x]\rightarrow{K}$.
        Then $\textrm{Ker}(\varphi_{\alpha})\subseteq{F}[x]$. If
        $\varphi_{\alpha}$ is injective, then the kernel is 0, and this is
        true if and only if $f(\alpha)\ne{0}$ for all $f\in{F}[x]$. That is,
        $\alpha$ is \textit{transcendental} over $F$. On the other hand, if
        $\varphi_{\alpha}$ is not injective then
        $\textrm{Ker}(\varphi_{\alpha})=(m_{\alpha}/F(x))$ where
        $m_{\alpha}/F(x)$ is a monic polynomial in $F[x]$, and this is called
        the minimal polynomial of $\alpha$ over $F$. In this case we say that
        $\alpha$ is algebraic over $F$.
        \begin{theorem}
            If $K/F$ is a field extension, $\alpha\in{K}$ is algebraic over $F$,
            if $f\in{F}[x]$ with $f(\alpha)=0$, then
            $m_{\alpha/F}(x)|f$, hence $m_{\alpha}/F(x)$ is the unique monic
            polynomial over $F$ of minimal degree with $\alpha$ as a root.
            Moreover, $m_{\alpha}/F(x)$ is irreducible over $F$ and is the
            unique monic irreducible polynomial over $F$ with $\alpha$ as a
            root.
        \end{theorem}
        \begin{example}
            Let $d\in\mathbb{Z}$ be a non-square. Then the minimal polynomial of
            $d$ is $m_{\sqrt{d}}/\mathbb{Q}(x)=x^{2}-d$.
        \end{example}
        \begin{example}
            Let $\alpha=\sqrt{2}+\sqrt{3}$. What is the minimal polynomial of
            $\alpha$ over $\mathbb{Q}$? Well we first try to find a polynomial
            that has $\alpha$ as a root. Squaring, we have:
            \begin{equation}
                \alpha^{2}=2+2\sqrt{6}+3=5+2\sqrt{6}
            \end{equation}
            Bringing the 5 over and squaring:
            \begin{equation}
                (\alpha^{2}-5)^{2}=24=\alpha^{4}-10\alpha^{2}+25
            \end{equation}
            Thus letting $f(x)=x^{4}-10x+1$, we obtain a polynomial with
            $\alpha$ as a root.
        \end{example}
        \begin{theorem}
            If $\alpha\in{K}$ is algebraic over $F$, then
            $F[x]/(m_{\alpha}/F(x))$ is isomorphic to $F[\alpha]$, and
            $F[\alpha]=F(\alpha)$.
        \end{theorem}
        Since $F(\alpha)\subseteq{F}[\alpha]$ because the first part,
        $F[x]$ is a field since $m_{\alpha}/F(x)$ is irreducible, so
        $F[x]/(m_{\alpha}/F(x))$ is a field. Hence
        $F(\alpha)\subseteq{F}[\alpha]$ since $F(\alpha)$ is minimal field
        containing $\alpha$. GIven $\alpha$ algebraic over $F$,
        $\alpha^{\minus{1}}$ is a polynomial in $\alpha$. There exists an
        algorithm demonstrating this using Bezout's identity.
        \begin{definition}
            A field extension $K/F$ is algebraic if for all $\alpha\in{K}$,
            $\alpha$ is algebraic over $F$.
        \end{definition}
        \begin{theorem}
            $K/F$ finite if and only if $K/F$ is algebraic and finitely
            generated.
        \end{theorem}
    \section{Review of Previous Lectures}
        A field extension $K/F$ is algebraic if every element $\alpha\in{K}$ is
        algebraic over $F$.
        \begin{theorem}
            A field extension $K/F$ is finite if and only if $K/F$ is algebraic
            and finitely generated.
        \end{theorem}
        \begin{proof}
            For if $K/F$ is finite, then $K$ is a finite dimensional vector
            space over $F$. Thus, if $\alpha\in{K}$, then the set
            $\{1,\alpha,\alpha,\dots\}$ is linearly dependent. That is, there
            exists $a_{0},\dots,a_{n}\in{F}$ such that
            $a_{0}+\dots+a-{n}\alpha^{n}=0$. Let $\alpha_{1},\dots,\alpha_{m}$
            be an $F$ basis for $K$. Then $K=F(\alpha_{1},\dots,\alpha_{n})$,
            and so $K$ is finitely generated. The converse is trickier. Since
            $K$ is finitely generated, $K=F(\alpha_{1},\dots,\alpha_{n})$. But
            since $K$ is algebraic over $F$, $\alpha_{i}$ is algebraic in $F$.
            Thus we build a tower:
            \begin{equation}
                F\rightarrow{F}(\alpha_{1})\rightarrow{F}(\alpha_{1},\alpha_{2})
                    \rightarrow\dots\rightarrow{F}(\alpha_{1},\dots,\alpha_{n})
                    =K
            \end{equation}
            By the generalized tower law, $K$ is finite over $F$.
        \end{proof}
        $K/F$ is transcendental (that is, not algebraic) implies that $K/F$ is
        of infinite degree. Given $\alpha\in{K}$ transcendental over $F$, then
        $\varphi_{\alpha}:F[x]\rightarrow{F}[\alpha]\subseteq{F}(\alpha)$.
        \begin{theorem}
            If $K/L$ is algebraic and $L/F$ is algebraic, then $K/F$ is
            algebraic.
        \end{theorem}
        \begin{proof}
            For let $\alpha\in{K}$. We want to show that $\alpha$ is algebraic
            over $F$. This is equivalent to the claim that $F(\alpha)$ is finite
            over $F$. But $\alpha$ is algebraic over $L$, and hence there is a
            minimal polynomial
            $m_{\alpha}/L(x)=a_{0}+\dots+a_{n-1}x^{n-1}+x^{n}\in{L}[x]$. Since
            $L/F$ is algebraic, $a_{i}$ is algebraic over $F$.
        \end{proof}
        The converse is true as well.
    \section{Compass and Straight Edge}
        Consider some subset $S\subseteq\mathbb{C}$, equipped with some rules:
        \begin{itemize}
            \item Given two points $P,Q$ there is a line through $P$ and $Q$.
            \item Given $P,Q$, there is a circle $C(P,|Q-P|)$ centering at $P$
                  with radius $|P-Q|$.
        \end{itemize}
        Any point that is the intersection of any of the lines and circles is
        said to be constructible by compass and straightedge.
        \begin{example}
            Bisecting a line is possible. Bisecting an angle is possible. Can
            you trisect an angle? What about double a cube?
        \end{example}
        We'll define inductively some sets $P_{n}$, $L_{n}$, and $C_{n}$.
        $P_{0}=\{0,1\}\subseteq\mathbb{C}$, $L_{0},C_{0}=\emptyset$. If
        $L_{n}$ has been constructed, let $L_{n+1}$ be the set of all lines
        through all points in $P_{n}$. If $C_{n}$ has been constructed, let
        $C_{n+1}$ be the set of all circles about all points in $P_{n}$ with
        radii all of the distances $|z_{1}-z_{2}|$ for points
        $z_{1},z_{2}\in{P}_{n}$. Then $P_{n}$ is finite for all $n\in\mathbb{N}$
        and thus the union over all $P_{n}$ is at most countable. This
        cardinality argument shows that there are inconstructible numbers.
        Moreover, $P=\bigcup{P}_{n}$ is a subfield of $\mathbb{C}$. To see this
        we must show that $P$ is closed to addition, multiplication, and
        inverses.
        \begin{theorem}
            The set of constructible numbers is a subfield of $\mathbb{C}$.
        \end{theorem}
        \begin{proof}
            It suffices to show that $P\cap\mathbb{R}$ is a subfield. For let
            $a,b\in{P}\cap\mathbb{R}$. We need to show that $a+b$, $a\cdot{b}$,
            and $a/b$ are constructible. Given the length $a$ and the length
            $b$, we can translate the length $b$ to start at $a$, given us the
            point $a+b$, which will have length $a+b$. For $ab$ we construct
            two triangles, one with length 1 and the other with length $a$.
            We build a triangle on the first with a length $b$, and a similar
            triangle on the second length which will have length $ab$ by
            similiarity. Lastly, do the same triangle with $b$ on the inside to
            get $a/b$. Draw some pictures later.
        \end{proof}
        \begin{theorem}
            $\mathbb{Q}\subseteq{P}$.
        \end{theorem}
        \begin{proof}
            Since $1\in{P}$, every integer multiple of 1 is also contained in
            $P$ since we can add 1 to itself $n$ times. Thus $n/m$ is contained
            in $P$, so $\mathbb{Q}\subseteq{P}$.
        \end{proof}
        \begin{theorem}
            $P$ is closed to square roots.
        \end{theorem}
        \begin{proof}
            Draw a circle of diameter $a+1$. Do that fancy circle.
        \end{proof}
        Let $Q^{py}$ be the intersection of all subfields $K\subseteq\mathbb{C}$
        such that for all $z\in{K}$ it is true that $\sqrt{z}\in{K}$. This is
        the smallest subfield of $\mathbb{C}$ in which one can always take
        square roots. It's called the Pythagorean closure of $\mathbb{Q}$. From
        the previous theorem, $Q^{py}\subseteq{P}$ is a subfield.
        \begin{theorem}
            $\mathbb{Q}^{py}=P$.
        \end{theorem}
        \begin{proof}
            For let $z\in{P}$. It suffices to show that
            $P_{n}\subseteq\mathbb{Q}^{n}$ for all $n$, since the
            $\bigcup{P}_{n}=P\subseteq\mathbb{Q}^{py}$. We prove this by
            induction. The base case is true since $P_{0}=\{0,1\}$ and this is a
            subset of $\mathbb{Q}^{py}$. Suppose $P_{n}\subseteq\mathbb{Q}^{py}$
            and recall that $P_{n+1}$ is defined as all $z\in\mathbb{C}$ such
            that $z\in{L}\cap{L}'$, or $z\in{L}\cap{C}$, or $z\in{C}\cap{C}'$,
            where $L$, $L'$, $C$, and $C'$ are lines and circles through points
            in $P_{n}$. Case 1, $z\in{L}\cap{L}'$ Then there are four points
            $z_{1},z_{2},z_{3},z_{4}$ such that:
            \begin{equation}
                z=z_{1}\alpha+(1-\alpha)z_{2}=z_{3}\beta+(1-\beta)z_{4}
            \end{equation}
            We can solve this and note that $P_{n}$ is closed under complex
            conjugation. Thus, in case 1 we have that $z$ is contained in
            $\mathbb{Q}^{py}$. Here we have:
            \begin{equation}
                z=z_{1}\alpha+(1-\alpha)z_{2}=z_{3}+r\exp(i\theta)
            \end{equation}
            This be true as well. The final case is two circles:
            \begin{equation}
                z=z_{1}+r_{1}\exp(i\theta_{1})=z_{2}+r_{2}\exp(i\theta_{2})
            \end{equation}
            Which also be like it is.
        \end{proof}
        \begin{theorem}
            $\mathbb{Q}^{py}$ is algebraic over $\mathbb{Q}$.
        \end{theorem}
        We can build the Pythagorean closure from $\mathbb{Q}$ by considering
        $\sqrt{\mathbb{Q}}$, all numbers of the for $a+b\sqrt{c}$ with
        $a,b,c\in\mathbb{Q}$, and then $\sqrt{\sqrt{\mathbb{Q}}}$, and so on.
        \begin{theorem}
            $z\in\mathbb{Q}^{py}$ if and only if there is a tower of extensions
            $K_{0},\dots,K_{n}$ such that $\mathbb{Q}=K_{0}$ and
            $K_{n}=\mathbb{Q}[z]$ where $K_{j+1}$ has degree 2 over $K_{j}$.
        \end{theorem}
        \begin{theorem}
            If $\alpha\in\mathbb{Q}^{py}$, then
            $[\mathbb{Q}[\alpha]:\mathbb{Q}]=2^{n}$ for some $n\in\mathbb{N}$.
        \end{theorem}
        \begin{proof}
            Apply the tower law to the previous theorem.
        \end{proof}
        From this we can do all of the impossibility proofs of various
        constructions.
        \begin{example}
            It is impossible to double the volume of a cube with lengths 1. The
            minimal polynomial of $\sqrt[3]{2}$ is $x^{3}-2$ by Eisenstein. Thus
            the degree of the field extension
            $[\mathbb{Q}[\sqrt[3]{2}]:\mathbb{Q}]=3$ which is not a power of 2.
            It is also impossible to trisect angles. For let $\theta=2\pi/3$.
            Trisecting this is equivalent to constructed $2\pi/9$. The minimal
            polynomial of this is $x^{3}-\frac{3}{4}x+\frac{1}{8}$, and thus
            the degree of the extension is again 3, which is not a power of 2.
        \end{example}
    \section{Review}
        If $f\in{F}[x]$ is irreducible, then $f$ has no roots. The converse is
        false. If $f\in{F}[x]$ has no roots, it may still be reducible. This
        holds for any field if $f$ has degree 2 or 3.
        \begin{example}
            If $f(x)=x^{4}+1$, $f\in\mathbb{R}[x]$, then $f$ is reducible. For
            we have:
            \begin{equation}
                x^{4}+1=(x^{2}-\sqrt{x}x+1)(x^{2}+\sqrt{2}x+1)
            \end{equation}
            We can now complete this since $x^{2}-\sqrt{2}x+1$ and
            $x^{2}+\sqrt{2}x+1$ have no real roots by the quadratic formula,
            and hence these are irreducible. Thus, $x^{4}+1$ is reducible and
            factors as above.
        \end{example}
        \begin{example}
            There is only one irreducible quadratic polynomial over
            $\mathbb{F}_{2}[x]$ and that is $x^{2}+x+1$. Using this, is the
            polynomial $x^{4}+x^{2}+x+1$ irreducible in $\mathbb{F}_{2}[x]$?
            Since this has no roots, we know that it has no linear factors, and
            thus if it is reducible it must be the product of irreducible
            quadratics. But there is only one irreducible quadratic, so we can
            use this to check. We have:
            \begin{equation}
                (x^{2}+x+1)^{2}=
                x^{4}+2x^{3}+x^{2}+2x+1=x^{4}+x^{2}+1
            \end{equation}
            Since in $\mathbb{F}_{2}$ we have that $2=0$. But this is not equal
            to the original polynomial $x^{4}+x^{3}+x^{2}+x+1$, and hence
            this is indeed irreducible.
        \end{example}
    \section{More Stuff}
        If $\alpha\in\mathbb{C}$ is constructible, and if
        $K_{i}$ is a tower of field extensions each of degree 2 over the
        previous one, then $[\mathbb{Q}(\alpha):\mathbb{Q}]=2^{n}$.
        \begin{theorem}
            A regular $n$ gon is constructible if and only if the complex number
            $\exp(2\pi{i}/n)$ is constructible.
        \end{theorem}
        If $p$ is a prime number, then the minimal polynomial of
        $\exp(i2\pi{i}/n)$ is just $x^{p-1}+\dots+1$. Thus
        $[\mathbb{Q}(\zeta_{p}):\mathbb{Q}]=p-1$. Thus we need $p-1$ to be a
        power of 2.
        \begin{theorem}
            If $p-1$ is not a power of 2 then we can not construct a regular
            $p$ gone.
        \end{theorem}
        \begin{theorem}
            If $2^{k}+1$ is a prime, then $k=2^{n}$ for some $n$.
        \end{theorem}
        There are only 5 Fermat primes, $p=3,5,17,257,65537$. An unsolved
        problem (as of 2020) is whether or not there are more such primes, or
        are there infinitely many primes? 
    \section{Splitting Fields}
        If $f\in{F}[x]$, we have given a construction of $K/F$ in which $f$ has
        a root $\alpha\in{K}$. That is, $(x-\alpha)$ divides $f$ in$ K[x]$. For
        if $f$ splits completely (factors into a product of linear polynomials
        over $f$), then there is nothing to prove. If not, let $g(x)$ be an
        irreducible factor of $f$ and define $K=F[x]/(g)$, where $(g)$ is the
        ideal generated by $g$ in $F[x]$. Then $g(x)$ has a root $\overline{x}$,
        $K=F(\alpha)$, so $(x-\alpha)$ divides $g$, which divides $f$, so we're
        done.
        \begin{example}
            Let $f(x)=x^{3}-2$, $f\in\mathbb{Q}[x]$. This has three roots,
            $\sqrt[3]{2}$, $\sqrt[3]{2}\omega$, and
            $\sqrt[3]{2}\overline{\omega}$, where $\omega$ is a complex cubed
            root of unity, and $\overline{\omega}$ is its complex conjugate.
            Thus $\mathbb{Q}[x]/(x^{3}-2)$ is isomorphic to
            $\mathbb{Q}(\sqrt[3]{2})$.
        \end{example}
        \begin{definition}
            An extension $K/F$ is a splitting field of $f\in{F}[x]$ if
            $f$ splits completely in $K[x]$ and does not split in any proper
            subfield.
        \end{definition}
        \begin{theorem}
            Splitting fields exist.
        \end{theorem}
        \begin{proof}
            Proof by induction on the degree of $n$. If it is true for $n$,
            write $f=(x-\alpha)g$ for some $\alpha$ in $F[x]/(h)$. Then
            $g$ is of degree $n$ or less, and hence has a splitting field
            $K$. Adjoing $\alpha$ to $K$. Let $E$ be the intersection of all
            subfields of $E$ containing this $K$ adjoing $\alpha$.
        \end{proof}
        \subsection{Review from Previous Lecture}
            If $F$ is a field, and if $f\in{F}[x]$, then an extension field
            $K$ over $F$ is a splitting field if $f\in{K}[x]$ is a splitting
            polynomial. That is, there are linear factors
            $a_{k}x+b_{k}$, with $a_{k},b_{k}\in{K}$, such that:
            \begin{equation}
                f(x)=\prod_{k\in\mathbb{Z}_{n}}(a_{k}x+b_{k})
            \end{equation}
            Splitting fields exists since given $f$ we can find an irreducible
            factor $g$, and thus $F[x]/(g)$ will be a field extension of $F$.
            Letting $\alpha_{1}=\overline{x}$, the equivalence class of $x$,
            this will be a root of $g$ in $F[x]/(g)$, and hence a root of
            $f$ in $G$. The extension field will have degree less than or equal
            to $n$. Continuing on inductively, obtaining $\alpha_{k}$, we find
            that $f\in{F}[\alpha_{1},\dots,\alpha_{n}]$ splits completely and
            that this field extension has at most degree $n!$. If $f$ was
            irreducible to begin with then $f$ is a minimal polynomial up to
            scalar multiplication, and hence the degree of $F[\alpha_{1}]$ over
            $F$ will be $n$. From the tower law, $n$ will divide the degree of
            $F[\alpha_{1},\dots,\alpha_{n}]$.
            \begin{example}
                Let $f(x)=x^{4}+4$. This factors as:
                \begin{equation}
                    x^{4}+4(x^{2}-2x+2)(x^{2}+2x+2)
                \end{equation}
                We can invoke the quadratic formula to find the roots of these
                two over $\mathbb{C}$:
                \twocolumneq{
                    \alpha_{1}=\frac{2\pm\sqrt{\minus{4}}}{2}
                              =1\pm{i}
                }
                {
                    \alpha_{2}=\frac{\minus{2}\pm\sqrt{\minus{4}}}{2}
                              =\minus{1}\pm{i}
                }
                So the splitting field is $K=\mathbb{Q}(\pm{1}\pm{i})$, and this
                is just $\mathbb{Q}(i)$. Thus, $[K:\mathbb{Q}]=2$.
            \end{example}
            \begin{theorem}
                If $F$, $F'$ are fields, if $\varphi:F\rightarrow{F}'$ is
                a field isomorphism, and if $f\in{F}[x]$, if
                $f'(x)(\varphi\circ{f})(x)\in{F}'[x]$, if $K$ is a splitting
                field for $f$, and if $K'$ is a splitting field for
                $f'$, then there is an isomorphism $\sigma:K\rightarrow{K}'$
                such that for all $c\in{F}$, $\sigma(c)=\varphi(c)$.
            \end{theorem}
            \begin{proof}
                We prove by induction on the degree of $f$. Since $\varphi$ is
                an isomorphism, $f'$ and $f$ have the same degree. Thus if
                $f\in{F}[x]$ and $f'\in{F}'[x]$ are degree one polynomials, then
                they already split and hence if we let $\sigma=\varphi$, then
                we have shown that the splitting fields are isomorphic. Assume
                the claim is true for $n\in\mathbb{N}$. We can assume that $f$
                has an irreducible factor $g$ of degree greater than 1. Then
                $g$ has a root in $K$, label it $\alpha$, and let
                $g'=\varphi\circ{g}$. Then $g'|f'$ and $g'$ is irreducible
                over $F'$. So let $\alpha$ be a root of $g'$ in $K'$. Then
                $F[x]/(g)\simeq{F}[\alpha]\simeq{F}'[\alpha']\simeq{F}'[x]/(g')$
                and thus we can extend this isomorphism one step up. So now we
                have:
                \twocolumneq{f(x)=(x-\alpha)f_{1}(x)}
                            {f'(x)=(x-\alpha')f_{1}'(x)}
                $f_{1}$ splits in $K$ and thus we need to show that $K$ is
                minimal. Suppose $K/L/F_{1}$. Then since $f_{1}$ splits in $L$,
                $F$ splits in $L$ as well, contradicting the minimality of $K$
                over $F$, and hence $K$ is minimal over $F_{1}$. Thus, $K$ is
                a splitting field for $f_{1}$ over $F_{1}$. Thus by induction
                there is an isomorphism $\sigma:K\rightarrow{K}'$ with the
                desired property over $F_{1}$ and $F_{1}'$. But $F_{1}$ and
                $F_{1}'$ are extensions over $F$ and $F'$, so we're done.
            \end{proof}
    \section{Separability}
            Let $F$ be a field, $f\in{F}[x]$ be a polynomial of degree $n$, and
            $K$ the splitting field of $f$ over $F$. Then:
            \begin{equation}
                f(x)=a_{n}\prod_{k\in\mathbb{Z}_{r}}(x-\alpha_{k})^{m_{k}}
            \end{equation}
            where the $\alpha_{k}$ are distinct roots, and the $m_{k}$ are all
            positive integers.
            \begin{definition}
                A simply root of a polynomial $f$ over a field $F$ is an element
                $\alpha\in{F}$ such that $\alpha$ is a root of $f$ and
                $(x-\alpha)^{2}$ does not divide $f$. Otherwise $\alpha$ is
                called a multiple root.
            \end{definition}
            \begin{example}
                In $\mathbb{Q}[x]$ we can consider $x^{2}-1$. This splits as
                $(x-1)(x+1)$ and so we see that the roots are $\pm{1}$. However
                neither $(x-1)^{2}$ nor $(x+1)^{2}$ divides $f$, and hence both
                roots are simple. If we consider $x^{2}+2x+1$ then we know this
                factors as $(x+1)^{2}$ and so the only root is $\minus{1}$ and
                it is a multiple root with multiplicity 2.
            \end{example}
            \begin{definition}
                A separable polynomial is one such that every rot is simple.
                Otherwise, $f$ is inseparable.
            \end{definition}
            Over $\mathbb{C}$ we can use calculus to determine separability.
            Let $\alpha\in\mathbb{C}$ be a root of $f(x)\in\mathbb{C}[x]$ and
            suppose:
            \begin{equation}
                f(x)=(x-\alpha)^{m}g(x)
            \end{equation}
            with $m\geq{1}$. If we look at the derivative of $f$, we obtain:
            \begin{equation}
                \dot{f}(x)=m(x-\alpha)^{m-1}g(x)+(x-\alpha)^{m}\dot{g}(x)
            \end{equation}
            where $g(\alpha)\ne{0}$. Then there are two possibilities:
            \begin{equation}
                \dot{f}(\alpha)=
                \begin{cases}
                    0,&m\geq{2}\\
                    g(\alpha),&m=1
                \end{cases}
            \end{equation}
            Since $g(\alpha)$ is non-zero we see that $\alpha$ is a simply root
            if and only if the derivative of $f$ evaluated at $\alpha$ is
            non-zero. The idea is to generalize such notions to general fields
            where we may not have the structure to perform calculus. We thus
            define a derivation on $F[x]$.
            \begin{definition}
                A derivation on an algebra $\mathscr{A}$ over a field $F$ is a
                function $D:\mathscr{A}\rightarrow\mathscr{A}$ such that:
                \begin{align}
                    D(af+bg)&=aD(f)+bD(g)\\
                    D(x^{n})&=nx^{n-1}\\
                    D(f\circ{g})&=Df(g(x))\cdot{D}g(x)\\
                    D(fg)&=D(f)g+fD(g)
                \end{align}
            \end{definition}
            \begin{theorem}
                Linearity, Liebnizean, and $D(x)=1$ imply the other properties.
            \end{theorem}
            \begin{example}
                Let $p\in\mathbb{N}$ be a prime, and consider the field
                $\ring{\mathbb{Z}_{p}}$ with the usual arithmetic modulo $p$.
                Let $t\in\mathbb{Z}_{p}$ be some fixed constant, and consider
                the polynomial $f\in\mathbb{Z}_{p}[x]$ defined by
                $f(x)=x^{p}-t$. Then $f$ is irreducible, regardless of choice of
                $t$ by applying Eisenstein's criterion to the field
                $\mathbb{Z}_{p}[t]\subseteq\mathbb{Z}_{p}$. Let
                $\mathbb{Z}_{p}(\sqrt[p]{t})=\mathbb{Z}_{p}[x]/(x^{p}-t)$. Then:
                \begin{equation}
                    f(x)=x^{p}-t=x^{p}-(\sqrt[p]{t})^{p}-(x-\sqrt[p]{t})^{p}
                \end{equation}
                and thus $f$ has root $\sqrt[p]{t}$ with multiplicity $p$. Note
                that we can distribute the power only because $\mathbb{Z}_{p}$
                has characteristic $p$, and hence $(a+b)^{p}=a^{p}+b^{p}$,
                a consequence of the binomial theorem.
            \end{example}
            \begin{theorem}
                If $\ring{F}$ is a field, $f\in{F}[x]$ is a nonconstant, then
                $f$ is separable if and only if $f$ and $Df$ are relatively
                prime.
            \end{theorem}
            \begin{proof}
                First note that the greatest common denominator is independent
                of the base field for two polynomials $f$ and $g$. Thus, suppose
                $K$ is a splitting field for $f$ and $\alpha$ is a root. Then
                $f$ being separable implies that:
                \begin{align}
                    f(x)&=(x-\alpha)g(x)\\
                    \Rightarrow{Df}(x)&=g(x)+(x-\alpha)Dg(x)\\
                    \Rightarrow(Df)(\alpha)&=g(\alpha)
                \end{align}
                But $g(\alpha)\ne{0}$, and hence $Df(\alpha)\ne{0}$. In the
                other direction, let $\alpha\in{K}$ be a multiple root of $f$ so
                that $f(x)=(x-\alpha)^{2}h(x)$ with $h\in{K}[x]$. But then:
                \begin{align}
                    f(x)&=(x-\alpha)^{2}h(x)\\
                    \Rightarrow{Df}(x)&=2(x-\alpha)h(x)+(x-\alpha)^{2}Dh(x)
                    \Rightarrow{Df}(\alpha)&=0)
                \end{align}
                So $\alpha$ is also a root of $Df$ in $K$.
                If $f\in{F}[x]$ and $K/F$ is an extension field with
                $\alpha\in{K}$ such that $f(\alpha)=0$, then $m_{\alpha/F}(x)$
                divides $f(x)$. So $m_{\alpha/F}(x)$ divides the greatest
                common denominator of $f$ and $Df$, and thus $f$ and $Df$ are
                relatively prime.
            \end{proof}
            \begin{example}
                Let $F$ be a field, and let $f(x)=x^{n}-a$ for some $a\in{F}$.
                Using the previous theorem we can determine when $f$ is
                separable over $F$. Suppose $a\ne{0}$. Then we have
                $Df(x)=nx^{n-1}$, and thus we have two cases: $n=0$ and
                $n\ne{0}$. In the first case we have that the characteristic of
                $F$ then divides $n$, and hence $f$ is not separable. In the
                latter, $n\ne{0}$ and thus the characteristic does not divide
                $n$, and since the GCD of $nx^{n-1}$ and $x^{n}-a$ is 1,
                for otherwise $0$ would be a root of $f$ but it is not, and thus
                $f$ is separable.
            \end{example}
            \begin{theorem}
                If $f\in{F}[x]$ is irreducible, then $f$ is separable if and
                only if $Df$ is non-zero.
            \end{theorem}
            \begin{theorem}
                If $F$ is a field with characteristic zero, then any irreducible
                polynomial is separable. Hence $f\in{F}[x]$ is separable if and
                only if $f$ is the product of distinct irreducibles.
            \end{theorem}
            \begin{theorem}
                If $F$ is a field with characteristic $p>0$ then any irreducible
                $f(x)=g(x^{p^{r}})$ for $g\in{F}[x]$ is irreducible, separable,
                and uniquely determined for some $r\geq{1}$.
            \end{theorem}
            \begin{definition}
                The Frobenius map on a field of characteristic $p\in\mathbb{N}$
                is the function $\phi:F\rightarrow{F}$ defined by
                $\phi(x)=x^{p}$.
            \end{definition}
            \begin{definition}
                A perfect field is a field $F$ such that every irreducible
                polynomial is separable.
            \end{definition}
            \begin{example}
                Every field of characteristic zero is perfect by the previous
                theorems.
            \end{example}
            Do there exists fields of characteristic $p>0$ that are perfect?
            The answer is yes, and this can be categorized by the following
            theorem.
            \begin{theorem}
                If $\ring{F}$ is a field, then $F$ is perfect if and only if
                either $F$ has characteristic zero, or if the Frobenius
                map $\phi:F\rightarrow{F}$ is surjective.
            \end{theorem}
    \section{Subfields and Automorphisms}
        If $K/F$ is a finite field extension, $A=\autgroup[F]{K}$ the
        automorphism group, $H\subseteq{A}$ a subgroup, the fixed field of $H$
        is the set $K^{H}$ defined by all $\alpha\in{K}$ such that for all
        $\sigma\in{H}$ it is true that $\sigma(\alpha)=\alpha$.
        \begin{theorem}
            If $\ring{F}$ is a field, if $\ring{K}$ is a field extension of $F$,
            if $\autgroup[F]{K}$ is the automorphism group of $K$ over $F$,
            and if $H\subseteq\autgroup[F]{K}$ is a subgroup, then the fixed
            field $K^{H}$ is a subfield of $K$.
        \end{theorem}
        \begin{proof}
            For if $\alpha,\beta\in{K}^{H}$, then:
            \begin{equation}
                \sigma(\alpha+\beta)=\sigma(\alpha)+\sigma(\beta)
                    =\alpha+\beta
            \end{equation}
            and thus $\alpha+\beta\in{K}^{H}$. Similarly:
            \begin{equation}
                \sigma(\alpha\cdot\beta)=\sigma(\alpha)\cdot\sigma(\beta)
                    =\alpha\cdot\beta
            \end{equation}
            and therefore $\alpha\cdot\beta\in{K}^{H}$. Lastly:
            \begin{equation}
                \sigma(\alpha^{\minus{1}})=\sigma(\alpha)^{\minus{1}}
                    =\alpha^{\minus{1}}
            \end{equation}
            and hence $\alpha^{\minus{1}}\in{K}^{H}$.
        \end{proof}
        Note that since $\sigma\in\autgroup[F]{K}$, for all $x\in{F}$ it is true
        that $\sigma(x)=x$. Hence, $F\subseteq{K}^{H}$.
        \begin{theorem}
            If $\ring{F}$ is a field, if $\ring{L}$ is a field extension of
            $F$, and if $\ring{K}$ is a field extension of $L$, then
            $\autgroup[L]{K}$ is a subgroup of $\autgroup[F]{K}$.
        \end{theorem}
        \begin{theorem}
            If $\ring[F]{F}$ is a field, if $\ring[K]{K}$ is a field extension
            of $F$, if $H$ is a subgroup of the automorphism group
            $\autgroup[F]{K}$, and if $K^{H}$ is the fixed field of $H$,
            then $H$ is a subgroup of $\autgroup[K^{H}]{K}$.
        \end{theorem}
        \begin{theorem}
            If $\ring[F]{F}$ is a field, if $\ring[L]{L}$ is an extension field
            of $F$, if $\ring[K]{K}$ is an extension field of $L$, if
            $\autgroup[L]{K}$ is the automorphism group, and if
            $K^{\autgroup[L]{K}}$ is the fixed field, then
            $L\subseteq{K}^{\autgroup[L]{K}}$.
        \end{theorem}
        \begin{theorem}
            $K/L_{1}/L_{2}/F$ implies that $\autgroup[L_{2}]{K}$ is a subgroup
            of $\autgroup[L_{1}]{K}$.
        \end{theorem}
        \begin{theorem}
            If $H_{1}\subseteq{H}_{2}\subseteq\autgroup[F]{K}$, then
            $K^{H_{2}}\subseteq{K}^{H_{1}}$.
        \end{theorem}
        \begin{definition}
            A Galois extension of a field $\ring[F]{F}$ is a field $\ring[K]{K}$
            such that $\cardinality{\autgroup[F]{K})}=[K:F]$.
        \end{definition}
    \section{More on Symmetric Polynomials}
        Given the symmetric group $S_{n}$, this acts on $F[x_{1},\dots,x_{n}]$
        by permuting the variables. The symmetric polynomials are those that are
        invariant under permutations of variables. This is a ring.
        If $K/F$ is a finite extension field, then the number of automorphisms
        is bounded by the degree. That is,
        $\cardinality{\autgroup[F]{K}}\leq[K:F]$. Let $K/F$ and $L/F$ are
        extensions, where $K/F$ is finite (there is no constraint on $L/F$). We
        look at $\homomorphisms[F]{K}{L}$ which is the set of all homomoprhisms
        $\varphi:K\rightarrow{L}$ that fix $F$. Then $\homomorphisms[F]{K}{L}$
        is finite. For if $K/F$ is finite, then it is finitely generated:
        $K=F(\alpha_{1},\dots,\alpha_{n})$, and thus any $\varphi$ is determined
        by where it maps the $\alpha_{i}$, and the $\varphi(\alpha_{i})$ are
        roots of the minimal polynomial $m_{\alpha_{i}/F}$. So there are
        finitely many cuch choices for $\varphi$. If we consider the vector
        space homomoprhisms $\homomorphisms[F(VS)]{K}{L}$ of all linear
        transformations $\varphi:K\rightarrow{L}$ that fix $F$, then there are
        infinitely many elements. If $L/F$ is also finite, then the dimension
        of this space is also finite since it's the space of $m\times{n}$
        matrices where $m=[K:F]$ and $n=[L:F]$.
        \begin{theorem}
            $\homomorphisms[F]{K}{L}\subseteq\homomorphisms[F(VS)]{K}{L}$.
        \end{theorem}
        \begin{proof}
            Asuume $\homomorphisms[F]{K}{L}$ is dependent. So there exists
            elements $a_{j}\in{L}$, not all of which are zero, and function
            $\varphi_{j}\in\homomorphisms[F]{K}{L}$ such that:
            \begin{equation}
                \sum_{i=1}^{m}a_{j}\varphi_{j}=0
            \end{equation}
        \end{proof}
        \begin{theorem}
            Letting $L=K$, we have $\autgroup[F]{K}$ has less than $[K:F]$
            elements.
        \end{theorem}
        Heading back to the before time:
        \begin{theorem}
            If $\ring{K}$ is a field, if $H\subseteq\autgroup{K}$ is a finite
            subgroup of the automorphism group of $K$, and if $K^{H}$ is the
            fixed field of $H$, then $K/K^{H}$ is a finite field extension and
            $[K:K^{H}]=\cardinality{H}$.
        \end{theorem}
        Let $K/F$ be a finite Galois extension, and let $G\in\autgroup[F]{G}$.
        There is a bijection between the subgroups of $H$ and the subextensions
        of $L$ such that $K/L/F$. We map $H\mapsto{K}^{H}$, the fixed field
        of $H$, and we map $L$ to $\autgroup[L]{K}$. Moreover,
        $H=\autgroup[K^{H}]{K}$. Next we need to prove that for any extension
        field $L$ of $F$ such that $K$ is an extension of $L$, we must show
        that $L=K^{\autgroup[L]{K}}$. That is, $L$ is equal to the fixed field
        generated by the automorphism group $\autgroup[L]{K}$.
        \begin{theorem}
            If $\ring[F]{F}$ is a field, if $\ring[L]{L}$ is a field
            extension of $F$, is $\ring[K]{K}$ is a field extension of $L$,
            and if $K$ is a Galois extension of $F$, then $K$ is a Galois
            extension of $L$.
        \end{theorem}
        \begin{proof}
            For let $X=\homomorphisms[F]{L}{K}$. Then for $\iota\in{X}$,
            $\iota:L\rightarrow{K}$ is an inclusion mapping. Note that
            $G$ acts on $\autgroup[F]{K}$ by $g\cdot\varphi=g\circ\varphi$
            for all $g\in\autgroup[F]{K}$ and $\varphi\in{X}$. Then the
            stabilizer $G_{\iota}$ of $\iota$, which is the set of all
            $g\in{G}$ such that $g\cdot\iota=\iota$. But then
            $G_{\iota}=\autgroup[L]{K}$ since $\iota$ fixes $L$. But since $K$
            is a Galois extension of $F$,
            $[K:F]=\cardinality{\autgroup[F]{G}}$. But by the orbit stabilizer
            theorem:
            \begin{equation}
                \cardinality{\autgroup[F]{G}}
                =\cardinality{\autgroup[F]{K}_{\iota}}\cdot
                    \cardinality{\autgroup[F]{K}\cdot\iota}
            \end{equation}
            where $G\cdot\iota=\{g\cdot\iota|g\in\autgroup[F]{K}\}$. But
            $\autgroup[F]{K}_{\iota}=\autgroup[L]{K}$, and hence:
            \begin{equation}
                [K:F]=\cardinality{\autgroup[L]{K}}\cdot
                    \cardinality{\autgroup[F]{K}\cdot\iota}
            \end{equation}
            But $G\cdot\iota\subseteq{X}$, and hence:
            \begin{equation}
                [K:F]=\cardinality{\autgroup[L]{K}}\cdot\cardinality{X}
                \leq[K:L]\cdot[L:F]=[K:F]
            \end{equation}
            and hence these inequalities are actually equalities. Thus,
            $\autgroup[L]{K}=[K:L]$.
        \end{proof}
        \begin{theorem}
            If $\ring[F]{F}$ is a field, and if $\ring[K]{K}$ is a Galois
            extension of $F$, then $F=K^{\autgroup[F]{K}}$, where
            $K^{\autgroup[F]{G}}$ is the fixed field of $\autgroup[F]{K}$.
        \end{theorem}
        \begin{proof}
            For let $E=K^{\autgroup[F]{G}}$. We know that $F\subseteq{E}$.
            But since $K$ is a Galois extension of $F$, it is true that
            $[K:F]=\cardinality{\autgroup[F]{K}}$. But any
            $\varphi:K\rightarrow{K}$ that fixed $F$ also fixed $E$, and hence
            $\autgroup[F]{G}=\autgroup[E]{G}$, and therefore:
            \begin{equation}
                [K:F]=\cardinality{\autgroup[F]{K}}
                    =\cardinality{\autgroup[E]{K}}
                    \leq[K:E]=[K:F]/[E:F]
            \end{equation}
            and thus $[E:F]=1$. But if $E$ has degree 1 over $F$, then
            $E$ is equal to $F$. Thus, $F=K^{\autgroup[F]{K}}$.
        \end{proof}
        Some known properties of the bijection given by the fundamental theorem
        of Galois theory: If $H_{1}\subseteq{H}_{2}$, then
        $K^{H_{2}}\subseteq{K}^{H_{1}}$. That is, the bijection is inclusion
        reversing. If $H$ is a subgroup of $\autgroup[F]{K}$, then
        the index $[G:H]=[K^{H}:F]$. In other words, if $L$ is the
        subextension corresponding to $H$, then the index of $G$ over $H$ is
        equal to the degree of $L$ over $F$.
        \begin{theorem}
            If $K/L/F$ is a subextension, if $K/F$ is a finite Galois extension,
            if $G=\autgroup[F]{K}$, and if $H=\autgroup[L]{K}$, then
            $L/F$ is Galois if and only if $H$ is a normal subgroup of $G$.
        \end{theorem}
        \begin{proof}
            Let $X=\homomorphisms[F]{L}{K}$, $\iota\in{X}$. $G$ acts on $X$ and
            $G_{\iota}=\autgroup[L]{K}=H$. If $L/F$ is Galois, then
            $\cardinality{\autgroup[F]{L}}=[L:F]$ and this is equal to
            $\cardinality{X}$. The mapping $\autgroup[F]{L}\rightarrow{X}$
            defined by $\tau\mapsto\iota\circ\tau$ is injective since $\iota$
            is injective, and hence this is also surjective since
            $\cardinality{\autgroup[F]{L}}=\cardinality{X}$, and these are
            finite sets. So given any $\sigma\in\autgroup[F]{K}$,
            $\sigma|_{L}:L\rightarrow{K}$, so $\sigma_{L}\in{X}$ and hence
            there is a $\tau\in\autgroup[F]{L}$ such that
            $\sigma|_{L}=\iota\circ\tau$. Hence $\sigma(L)=\iota(\tau(L))=L$.
        \end{proof}
        The fundamental theorem of Galois theory states that if $K/F$ is a
        finite Galois extension, then there is a bijection between the subgroups
        of $\autgroup[F]{K}$ and subextensions $K/L/F$. We map
        $H\mapsto{K}^{H}$, the fixed field of $H$, and we map
        $L\mapsto\autgroup[L]{K}$.
        \begin{theorem}
            If $K/F$ is a finite Galois extension, $K/L/F$ a subextension, and
            if $H=\autgroup[L]{K}$, then the following are equivalent:
            \begin{itemize}
                \item $L/F$ is Galois.
                \item $\sigma(L)=L$ for all $\sigma\in\autgroup[F]{K}$
                \item $H$ is a normal subgroup of $\autgroup[F]{K}$.
            \end{itemize}
        \end{theorem}
        \begin{theorem}
            If $L/F$ and $K/F$ are field extensions, and if $K/F$ is finite,
            then $\cardinality{\homomorphisms[F]{K}{L}}\leq[L:F]$.
        \end{theorem}
        \begin{theorem}
            Given a Galois correspondence, if the subgroup $H$ corresponds to
            the subextension $L$, and if $\sigma\in\autgroup[F]{K}$, then
            $\sigma{H}\sigma^{\minus{1}}$ corresponds to $\sigma(L)$.
        \end{theorem}
        \begin{example}
            $\mathbb{Q}(\sqrt[3]{2},\omega)$.
        \end{example}
        \begin{example}
            $\mathbb{Q}(\exp(2\pi{i}/7))$. The minimal polynomial is
            $1+x+\dots+x^{5}+x^{6}$. Roots of minimal polynomial are 
        \end{example}
    \section{Normal Extension}
        \begin{definition}
            A normal extension of a field $\ring[F]{F}$ is a field extension
            $\ring[K]{K}$ of $F$ such that for all $f\in{F}[x]$ such that $f$
            has a root in $K$, it is true that $f$ splits over $K$.
        \end{definition}
        \begin{theorem}
            The minimal polynomial $m_{\alpha/F}(x)$ of any $\alpha\in{K}$
            splits in $K$. Equivalently, given any irreducible polynomial
            $f(x)$ with coefficients in $F$, either $f$ is irreducible over $K$
            or splits over $K$.
        \end{theorem}
        \begin{theorem}
            If $K/F$ is finite, and normal, then $K$ is the splitting field of
            a polynomial $f\in{F}[x]$. The converse is true as well.
        \end{theorem}
        Show that if $a_{1},\dots,a_{n}\in{F}$, with $F$ a field of
        characteristic not equal to 2 such that no product $a_{i}\cdot{a}_{j}$
        is a square. Prove $K=F(\sqrt{a_{1}},\dots,\sqrt{a_{n}})$ has degree
        $2^{n}$ over $F$. We prove by induction on $n$. The base case of $n=1$
        is true. We use the tower law and the hypothesis that
        $F(\sqrt{a_{1}},\dots,\sqrt{a_{n-1}})$ has degree $2^{n-1}$ and show
        that $F(\sqrt{a_{1}},\dots,\sqrt{a_{n}})$ has degree 2 over
        $F(\sqrt{a_{1}},\dots,\sqrt{a_{n-1}})$. We just need to show that
        $\sqrt{a_{n}}$ is not in $F(\sqrt{a_{1}},\dots,\sqrt{a_{n-1}})$.
        An algebraic extension $K/F$ is normal if for every $f\in{F}[x]$ with a
        root $\alpha\in{K}$, then $f$ splits over $K$.
        \begin{theorem}
            If $K/F$ is a finite, then $K$ is normal if and only if $K$ is the
            splitting field of some $f\in{F}[x]$.
        \end{theorem}
        \begin{proof}
            For if $K=F(\alpha_{1},\dots,\alpha_{n})$, and if $f$ is the
            minimum polynomial $f=\prod_{i}m_{\alpha_{i}/F}$, then $K$ is the
            splitting field of $f$. Now, suppose $K$ is the splitting field of
            $f\in{F}[x]$ where $f$ has degree $n$. Let $\alpha\in{K}$ and $g$
            the minimal polynomial of $\alpha$ over $F$. We want to prove that
            $g$ splits in $K$. Let $M$ be the splitting field of $g$ over $K$
            and let $\beta\in{M}$ be any root of $g$.
        \end{proof}
        \begin{definition}
            The normal closure of an algebraic field extension $K/F$ is an
            extension $N/K$ such that $N/F$ is normal and minimal with this
            proprty.
        \end{definition}
        \begin{theorem}
            If $K/F$ is finite, then a normal closure $N/K$ exists and is unique
            up to $F$ isomorphism.
        \end{theorem}
        \begin{proof}
            Let $K=F(\alpha_{1},\dots,\alpha_{n})$ and $f$ the product over the
            minimal polynomials. Let $N$ be the splitting field of $f$ over $F$.
        \end{proof}
        \begin{definition}
            A separable extension of a field $\ring[F]{F}$ is an algebraic field
            extension $\ring[K]{K}$ of $F$ such that for all $\alpha\in{K}$ and
            $m_{\alpha/F}(x)\in{F}[x]$ it is true that $m_{\alpha/F}$ is
            separable.
        \end{definition}
        \begin{ftheorem}{Separable Field Extensions Theorem}{}
            If $K/F$ is finite, then it is Galois if and only if it is separble
            and normal.
        \end{ftheorem}
    \section{Notes from Milne (Chapter 1)}
        Def of rings, subrings, ring homomorphisms, commutative rings.
        \begin{definition}
            An integral domain is a ring $\ring{R}$ such that for all
            $a,b\in{R}$ such that $a\cdot{b}=0$, it is true that either $a=0$ or
            $b=0$.
        \end{definition}
        \begin{definition}
            An ideal of a ring $\ring{R}$ is a subring $\ring[I]{I}$ such that
            for all $a,b\in{I}$ it is true that $a+b\in{I}$, and for all
            $r\in{R}$ and for all $a\in{I}$ it is true that $r\cdot{a}\in{I}$
            and $a\cdot{r}\in{I}$.
        \end{definition}
        \begin{definition}
            A field is a commutative division ring $\ring{F}$.
        \end{definition}
        \begin{example}
            Since by definition a division ring is a non-zero ring, fields are
            required to have at least two elements. The smallest field is thus
            $\ring{\mathbb{Z}_{2}}$ with modulo arithmetic.
        \end{example}
        \begin{theorem}
            If $\ring{F}$ is a commutative ring, then it is a field if and only
            if there only ideals are the zero ideal and the entire ring.
        \end{theorem}
        \begin{proof}
            For if $\ring{F}$ is a field then for all $a\in{F}$ such that
            $a\ne{0}$ there exists a multiplicative inverse
            $a^{\minus{1}}\in{F}$ such that $a\cdot{a}^{\minus{1}}=1$. But then
            if $\ring[I]{I}$ is a non-zero subring of $R$, there is a non-zero
            element $a\in{I}$, and thus $a\cdot{a}^{\minus{1}}\in{I}$, and
            therefore $1\in{I}$. But then for all $r\in{F}$, $r\cdot{1}\in{I}$,
            but $r\cdot{1}=r$. Hence, $I=R$. In the other direction, if
            the only ideals are the zero ideal and the entire ring, then for
            any non-zero $a\in{F}$ the ideal generated by $a$ must be the entire
            ring. Hence, there is a $b\in{I}$ such that $a\cdot{b}=1$, and so
            $1\in{I}$, and hence $I$ is the entire ideal.
        \end{proof}
        \begin{example}
            Fields: $\mathbb{Q}$, $\mathbb{R}$, $\mathbb{C}$, $\mathbb{Z}_{p}$
            with $p$ a prime.
        \end{example}
        Def field homomoprhism, same as rings.
        \begin{theorem}
            If $\ring[F]{F}$ and $\ring[K]{K}$ are fields, and if
            $\phi:F\rightarrow{K}$ is a field homomoprhism, then it is
            injective.
        \end{theorem}
        \begin{proof}
            For $\phi^{\minus{1}}[\{0_{K}\}]$ is an ideal in $F$, and since it
            doesn't contain 1 by the definition of a field homomoprhism, it
            must be a proper ideal. Hence, it is the zero ideal. But then if
            $\phi(a)=\phi(b)$, then $\phi(a-b)=0_{K}$, and thus $a-b=0_{F}$.
            That is, $a=b$.
        \end{proof}
        \begin{theorem}
            If $\ring[F]{F}$ is a field, if $\ring{\mathbb{Z}}$ is the standard
            ring of integers, if $\ring{\mathbb{Q}}$ is the field of
            rational numbers, and if $f:\mathbb{Z}\rightarrow{F}$ is an
            injective ring homomoprhism, then there exists a field homomoprhism
            $\tilde{f}:\mathbb{Q}\rightarrow{F}$.
        \end{theorem}
        \begin{proof}
            For any $n\in\mathbb{Z}$, we have:
            \begin{equation}
                f(n)=f\Big(\sum_{k\in\mathbb{Z}_{n}}1\Big)
                =\sum_{k\in\mathbb{Z}_{n}}f(1)
            \end{equation}
            But $f$ is injective, and hence $f(1)\ne{0}_{R}$. Thus, since
            $\ring[F]{F}$ is a field, for all $n\in\mathbb{Z}\setminus\{0\}$
            there is an $m\in\mathbb{Z}\setminus\{0\}$ such that
            $f(n)\cdot{f}(m)=f(1)$. Define $\tilde{f}$ as follows:
            \begin{equation}
                \tilde{f}\big((n,m)\big)=
                \begin{cases}
                    0,&n=0\\
                    f(n)\cdot_{F}f(m)^{\minus{1}},&\textrm{otherwise}
                \end{cases}
            \end{equation}
        \end{proof}
        \begin{definition}
            A field of characteristic zero is a field $\ring{F}$ such that there
            exists a field homomoprhism $f:\mathbb{Q}\rightarrow{F}$.
        \end{definition}
        Equivalently one could say that adding $1_{F}$ to itself $n$ times never
        results in zero.
        \begin{example}
            $\mathbb{Q}$, $\mathbb{R}$, and $\mathbb{C}$ are fields of
            characteristic zero.
        \end{example}
        \begin{definition}
            The characteristic of a field $\ring{F}$ is the smallest non-zero
            $n\in\mathbb{N}$ such that:
            \begin{equation}
                \sum_{k\in\mathbb{Z}_{n}}1_{F}=0_{F}
            \end{equation}
        \end{definition}
        \begin{theorem}
            If $n\in\mathbb{N}^{+}$ is a non-negetive integer, and if
            $\ring{F}$ is a field of characteristic $n$, then $n$ is a prime
            number.
        \end{theorem}
        \begin{proof}
            For suppose not. Then there are integers $p,q\in\mathbb{N}^{+}$ such
            that $p,q<n$ and $p\cdot{q}=n$. Let $f:\mathbb{Z}\rightarrow{F}$ be
            defined by:
            \begin{equation}
                f(n)=
                \begin{cases}
                    0_{F},&n=0\\
                    \sum_{k\in\mathbb{Z}_{n}}1_{F},&n>0\\
                    \minus\sum_{k\in\mathbb{Z}_{|n|}}1_{F},&n<0
                \end{cases}
            \end{equation}
            This is a ring homomoprhism, and hence
            $f(p\cdot{q})=f(p)\cdot{f}(q)$. But $f(n)=0$, and since all fields
            are integral domains, either $f(p)=0$ or $f(q)=0$. But then there
            exists a positive integer smaller than $n$ such that $f(p)=0$,
            a contradiction as $n$ is the characteristic of $F$. Hence, $n$ is
            a prime.
        \end{proof}
        \begin{ftheorem}{Binomial Theorem}{Binomial_Theorem}
            If $\ring{R}$ is a commutative ring, if $a,b\in{R}$, and if
            $n\in\mathbb{N}$, then:
            \begin{equation*}
                (a+b)^{n}=\sum_{k\in\mathbb{Z}_{n}}\binom{n}{k}a^{k}b^{n-k}
            \end{equation*}
        \end{ftheorem}
        \begin{theorem}
            If $p\in\mathbb{N}$ is a prime number, if $r\in\mathbb{N}$ is such
            that $1\leq{r}$ and $r\leq{p}^{n}-1$, then $p$ divides
            $\binom{p^{n}}{r}$.
        \end{theorem}
        \begin{theorem}
            If $\ring{F}$ is a field of characteristic $p\in\mathbb{N}^{+}$,
            if $n\in\mathbb{N}$, and if $a,b\in{F}$, then:
            \begin{equation}
                (a+b)^{p^{n}}=a^{p^{n}}+b^{p^{n}}
            \end{equation}
        \end{theorem}
        \begin{proof}
            Apply the binomial in combintation with the previous theorem.
        \end{proof}
        \begin{definition}
            The ring of polynomials over a field $\ring{F}$ is the set of all
            finitely supported sequences $a:\mathbb{N}\rightarrow{F}$ with the
            following addition and multiplication:
            \begin{align}
                (a+b)_{n}&=a_{n}+b_{n}\\
                (ab)_{n}&=\sum_{k=0}^{n}a_{k}b_{n-k}
            \end{align}
        \end{definition}
        This is very much mimicing polynomials. The sum rule says we simply add
        the coefficients of two polynomials, and the product is the Cauchy
        product of two polynomials. That is, we multiply
        $(a_{0}+a_{1}x+\dots+a_{n}x^{n})$ by $(b_{0}+b_{1}x+\dots+b_{n}x^{n})$
        and collect the coefficients of all terms with order $x^{k}$ and group
        them. The resulting coefficient is precisely this sum.
        \begin{theorem}
            If $\ring{F}$ is a field, then $\ring{F[x]}$ is a commutative ring.
        \end{theorem}
        \begin{theorem}
            If $\ring{F}$ is a field, then $\ring{F[x]}$ is a commutative
            algebra over $F$.
        \end{theorem}
        There is a natural embedding of $F$ into $F[x]$ by looking at the
        subspace of all sequence $a:\mathbb{N}\rightarrow{F}$ such that
        $a_{k}=0$ for all $k>0$. That is, the only possible non-zero term is
        $a_{0}$.
        \begin{theorem}
            If $\ring[F]{F}$ is a field, if $\ring[R]{R}$ is a ring, if
            $F\subseteq{R}$ is a subring, and if $r\in{R}$, then there is a
            unique homomoprhism $f:F[x]\rightarrow{R}$ such that for all
            $a\in{F}$, $f(a)=r$.
        \end{theorem}
        \begin{definition}
            The degree of a non-zero polynomial $a\in{F}[x]$ is the largest
            $n\in\mathbb{N}$ such that $a_{n}\ne{0}$. The degree of the zero
            polynomial is zero.
        \end{definition}
        Since $F[x]$ is the space of all finitely supported sequences, for any
        such $a\in{F}[x]$ there will eventually be an $N\in\mathbb{N}$ such that
        for all $n>N$ it is true that $a_{n}=0$. By the well-ordering all of the
        integers, there will be a least such element, and hence the above
        definition is well defined. There's a natural way of looking at
        $F[x]$ as a subset of $\funcspace[F]{F}$, the set of all functions from
        $F$ to itself. Given $a\in{F}[x]$ of degree $n\in\mathbb{N}$ we consider
        the function $f\in\funcspace[F]{F}$ defined by:
        \begin{equation}
            f(x)=\sum_{k\in\mathbb{Z}_{n+1}}a_{k}\cdot_{F}x^{k}
            =a_{0}+_{F}a_{1}\cdot_{k}x+_{F}a_{2}\cdot_{F}x^{2}+_{F}\dots
            +_{F}a_{n}\cdot_{F}x^{n}
        \end{equation}
        In more familiar language (dropping the subscripts and using
        concatenation to denote multiplication), we have:
        \begin{equation}
            f(x)=a_{0}+a_{1}x+\cdots+a_{n}x^{n}
        \end{equation}
        The sequence definition is good for rigor and solving theorems, since
        the Cauchy product allows one to easily manipulate expressions without
        worrying about the non-existent dummy variable $x$, whereas the function
        definition (as a subset of $\funcspace[F]{F}$) is good for intuition.
        \begin{definition}
            A monic polynomial of degree $n$ in a field $\ring{F}$ is a
            polynomial $a\in{F}[x]$ of degree $n\in\mathbb{N}$ such that
            $a_{n}=1$.
        \end{definition} 
        \begin{theorem}
            If $\ring{F}$ is a field, then $F[x]$ is a Euclidean domain.
        \end{theorem}
        \begin{theorem}
            If $\ring{F}$ is a field, if $\ring[I]{I}$ is an ideal in $F[x]$,
            and if $a\in{I}$ is of least degree, then $I=(a)$, where $(a)$ is
            the ideal generated by $a$.
        \end{theorem}
        \begin{proof}
            For given $b\in(a)$, there is an $r\in{F}[x]$ such that
            $b=r\cdot{a}$. But $I$ is an ideal, and $a\in{I}$, and hence
            $b\in{I}$. Thus, $(a)\subseteq{I}$. If $b\in{I}$, then by the
            division algorithm there are polynomials $p,q\in{F}[x]$ such that
            $b=aq+r$ where the degree of $r$ is strictly less than the degree of
            $a$. But then $r=b-aq$. And since $b\in{I}$ and $q\in{F}[x]$, it is
            true that $bq\in{I}$ since $I$ is an ideal. But if $a\in{I}$ and
            $bq\in{I}$, then $b-aq\in{I}$ since $I$ is an ideal. Thus,
            $r\in{I}$. But $r$ is a polynomial of degree strictly less than
            $a$, and $a$ is a non-zero polynomial of least degree in $I$.
            Therefore, $r=0$. But then $b=aq$, and hence $b\in(a)$. Thus,
            $I\subseteq(a)$. From the definition of equality, $I=(a)$.
        \end{proof}
        \begin{theorem}
            There exists a bijection between monic polynomials in $F[x]$ and
            the ideals of $F[x]$.
        \end{theorem}
        \begin{definition}
            A root of a polynomial $a\in{F}[x]$ of degree $n\in\mathbb{N}$ over
            a field $\ring{F}$ is an element $r\in{F}$ such that:
            \begin{equation}
                \sum_{k\in\mathbb{Z}_{n+1}}a_{k}\cdot_{F}x^{k}=0_{F}
            \end{equation}
        \end{definition}
        \begin{theorem}
            If $\ring{\mathbb{Q}}$ is the field of rational numbers, if
            $a\in\mathbb{Q}[x]$ is a polynomial of degree $n\in\mathbb{N}$,
            if $r\in\mathbb{Q}$ is a root of $a$, and if $p,q\in\mathbb{Q}$
            are such that $r=p/q$ and $\GCD(p,q)=1$, then $p$ divides $a_{0}$
            and $q$ divides $a_{n}$.
        \end{theorem}
        \begin{proof}
            For if $r$ is a root, then:
            \begin{equation}
                \sum_{k\in\mathbb{Z}_{n+1}}a_{k}\cdot{r}^{k}=0
            \end{equation}
            But $r=p/q$, and thus:
            \begin{equation}
                \sum_{k\in\mathbb{Z}_{n+1}}a_{k}\cdot\big(\frac{p}{q}\big)^{k}
                =0
            \end{equation}
            Simplifying, and multiplying both sides by $q^{k}$, we have:
            \begin{equation}
                \sum_{k\in\mathbb{Z}_{n+1}}a_{k}\cdot{p}^{k}q^{n-k}=0
            \end{equation}
            From this, $q$ divides $a_{n}p^{n}$. But $\GCD(p,q)=1$, and hence
            $q$ does not divide $p^{n}$. Thus, $q$ divides $a_{n}$. Similarly,
            $p$ divides $a_{0}$.
        \end{proof}
        \begin{example}
            Sticking with $\mathbb{Q}[x]$, the polynomial $f(x)=x^{3}-3x-1$ is
            irreducible over $\mathbb{Q}$. By the previous theorem, the only
            possible roots $p/q$ must be such that $p$ divides $\minus{1}$ and
            $q$ divides $1$. Hence, $p/q=\pm{1}$. But $f(1)=\minus{3}$ and
            $f(\minus{1})=1$, neither of which are zero. Hence, $f$ has no roots
            over $\mathbb{Q}$. Since it is a cubic, it must be irreducible.
        \end{example}
        \begin{ftheorem}{Gauss's Lemma for Polynomials}
                        {Gauss's Lemma for Polynomials}
            If $\ring{\mathbb{Z}}$ is the ring of integers, if
            $\ring{\mathbb{Q}}$ is the field of rational numbers, if
            $a\in{\mathbb{Z}}[x]$ is such that $a$ factors non-trivially in
            $\mathbb{Q}[x]$, then $a$ factors non-trivially in $\mathbb{Z}[x]$.
        \end{ftheorem}
        \begin{bproof}
            For if $b,c\in\mathbb{Q}[x]$ are such that $a=b\cdot{c}$, if
            $M,N\in\mathbb{N}$ are the products of the denominators of $b$
            and $c$, respectively, then $Ma,Nb\in\mathbb{Z}[x]$. But then
            $NMa\in\mathbb{Z}[x]$. By the fundamental theorem of arithmetic,
            there exists a prime $p\in\mathbb{N}$ such that $p$ divides $MN$.
            But then $Ma\cdot{N}b$ is the zero polynomial in $\mathbb{F}_{p}[x]$
            and since $\mathbb{F}_{p}$ is a field, this means that $p$ divides
            the coefficients of every element of either $Ma$ or $Nb$. Continuing
            we remove all of the prime factor of $MNa$ and obtain a
            factorization of $a$ in $\mathbb{Z}[x]$.
        \end{bproof}
        \begin{theorem}
            If $\ring{\mathbb{Z}}$ is the ring of integers, if
            $a\in\mathbb{Z}[x]$ is a monic polynomial of degree
            $n\in\mathbb{N}$, and if $b\in\mathbb{Q}[x]$ is a monic factor of
            $a$, then $b\in\mathbb{Z}[x]$.
        \end{theorem}
        \begin{proof}
            For if $b,c\in\mathbb{Q}[x]$ are such that $a=bc$, with $b$ a monic
            polynomial, then by the Cauchy product, since $a$ is monic, $c$
            must also be monic. Let $m$ and $n$ by the least integers such that
            $mb,nc\in\mathbb{Z}[x]$. If $p\in\mathbb{N}$ is a prime that divides
            $mn$, then it divides all of the coefficients of either $mb$ of $nc$
            and hence either $(m/p)b\in\mathbb{Z}[x]$ or
            $(n/p)c\in\mathbb{Z}[x]$, a contradiction since $m$ and $n$ are the
            least such integers with this property. Hence, $m=1$ and $n=1$.
        \end{proof}
        \begin{definition}
            An algebraic integer in $\mathbb{C}$ is a complex number
            $z\in\mathbb{C}$ such that $z$ is the root of a monic polynomial
            $a\in\mathbb{Z}[x]$.
        \end{definition}
        \begin{ftheorem}{Eisenstein's Criterion}{Eisenstein_Criterion}
            If $\ring{\mathbb{Z}}$ is the ring of integers, if
            $a\in\mathbb{Z}[x]$ is a polynomial in $\mathbb{Z}$ of degree
            $n\in\mathbb{N}$, if $p\in\mathbb{N}$ is a prime number such that
            $p$ does not divide $a_{n}$, $p^{2}$ does not divide $a_{0}$, and
            such that $p$ divides $a_{k}$ for all $k\in\mathbb{Z}_{n}$, then $a$
            is irreducible in $\mathbb{Q}[x]$.
        \end{ftheorem}
        \begin{bproof}
            For suppose $a$ factors in $\mathbb{Q}[x]$. But then it factors in
            $\mathbb{Z}[x]$. Suppose $b,c\in\mathbb{Z}[x]$ are non-trivial
            factors. By the Cauchy product, $a_{0}=b_{0}c_{0}$, and thus
            $p$ divides either $b_{0}$ or $c_{0}$. Suppose it divides $b_{0}$.
            But since $p^{2}$ does not divide $a_{0}$, $p$ does not divide
            $c_{0}$. But again by the Cauchy product,
            $a_{1}=b_{0}c_{1}+b_{1}c_{0}$. But $p$ divides $a_{1}$, and hence
            $p$ divides $b_{1}$. Continuing by induction on the Cauchy product,
            $p$ divides all of the $b_{k}$, contradicting that $p$ does not
            divide $a_{n}$.
        \end{bproof}
        It is important again to note that $\mathbb{Z}_{n}=\{0,1,\dots,n-1\}$,
        hence $\mathbb{Z}_{n}$ does not contain $n$. So we require $p$ to divide
        $a_{0},\dots,a_{n-1}$, but not $a_{n}$, and we require $p^{2}$ not to
        divide $a_{0}$. Eisenstein's criterion holds for any unique
        factorization domain.
        \begin{theorem}
            If $\ring{Z}$ is the ring of integers, if $a\in\mathbb{Z}[x]$ is a
            reducible polynomial in $\mathbb{Z}$ of degree $n\in\mathbb{N}$, if
            $p\in\mathbb{N}$ is a prime, and if $p$ does not divide $a_{n}$,
            then $\overline{a}\in\mathbb{F}_{p}[x]$ is reducible.
        \end{theorem}
        \begin{proof}
            For since $a\in\mathbb{Z}[x]$ is reducible, there exists
            $b,c\in\mathbb{Z}[x]$ such that $a=b\cdot{c}$. But since
            $p$ does not divide $a_{n}$, it does not divide
            $a_{0}b_{n}+a_{n}b_{0}$, and thus
            $\overline{a}=\overline{b}\cdot\overline{c}$ is a non-trivial
            factorization. Hence, $\overline{a}$ is reducible in
            $\mathbb{F}_{p}$.
        \end{proof}
        A more useful result is the contrapositive.
        \begin{theorem}
            If $\ring{\mathbb{Z}}$ is the ring of integers, if
            $p\in\mathbb{N}$ is a prime, if $\ring[p]{\mathbb{F}_{p}}$ is the
            field of integers modulo $p$, if $a\in\mathbb{Z}[x]$ is a polynomial
            of degree $n\in\mathbb{N}$ such that $p$ does not divide $a_{n}$,
            and if $\overline{a}\in\mathbb{F}_{p}[x]$ is irreducible, then
            $a$ is irreducible in $\mathbb{Z}[x]$.
        \end{theorem}
        The converse of this theorem does not hold. Indeed, there exist
        polynomials $a\in\mathbb{Z}[x]$ that are reducible in
        $\mathbb{F}_{p}[x]$ for every prime integer $p\in\mathbb{N}$, yet $a$
        is not reducible in $\mathbb{Z}[x]$.
        \begin{theorem}
            If $\ring{F}$ is a field, if $a\in{F}$, and if $f\in{F}[x]$, then
            there exists a polynomial $q\in{F}[x]$ such that, for all
            $x\in{F}$ it is true that:
            \begin{equation}
                f(x)=q(x)\cdot(x-a)+f(a)
            \end{equation}
        \end{theorem}
        \begin{proof}
            For by the division algorithm, there exists polynomials
            $q,r\in{F}[x]$ such that $f(x)=q(x)\cdot(x-a)+r(x)$ and $r$ has
            degree less than $x-a$. But the degree of $x-a$ is 1, and hence
            $r$ is a constant (has degree zero). Moreover:
            \begin{equation}
                f(a)=q(a)\cdot(a-a)+r(a)=q(a)\cdot{0}+r(a)=0+r(a)=r(a)
            \end{equation}
            Therefore $r(x)=f(a)$. Thus, for all $x\in{F}$,
            $f(x)=q(x)\cdot(x-a)+f(a)$.
        \end{proof}
        \begin{theorem}
            If $\ring{F}$ is a field, if $f\in{F}[x]$ is a polynomial, if
            $a\in{F}$, and if $x-a$ divides $f(x)$, then $f(a)=0$.
        \end{theorem}
        \begin{proof}
            For by the previous theorem, there is a polynomial $q\in{F}[x]$
            such that $f(x)=q(x)\cdot(x-a)+f(a)$. But if $f(a)=0$, then
            $f(x)=q(x)\cdot(x-a)$, and hence $x-a$ divides $f$. In the other
            direction, if $x-a$ divides $f(x)$, then there is a $q\in{F}[x]$
            such that $f(x)=q(x)\cdot(x-a)$. But then $f(a)=q(a)\cdot(a-a)$,
            and thus $f(a)=0$.
        \end{proof}
        \begin{theorem}
            If $\ring{F}$ is a field, if $f\in{F}[x]$ is a non-zero polynomial
            in $F$ of degree $n\in\mathbb{N}$, the there are at most $n$ roots.
        \end{theorem}
        \begin{proof}
            For by induction. If $f\in{F}[x]$ is a polynomial of degree 1, then
            $f(x)=ax+b$ for some $a,b\in{F}$. But if $\alpha$ is a root, then
            $f(x)=q(x)\cdot(x-\alpha)$. But since $f$ is degree 1, and since
            $x-\alpha$ is degree 1, $q$ must be of degree 0, and hence a
            constant. But $f$ is non-zero, and hence $\alpha\ne{0}$. Thus
            $f(x)=0$ if and only if $x=\alpha$. Suppose the proposition is true
            for $n\in\mathbb{N}$, and let $f\in{F}[x]$ be a non-zero polynomial
            of degree $n+1$. Suppose $f$ has more than $n+1$ roots. But if
            $\alpha$ is a root, then $x-\alpha$ divides $f$. Hence
            $f(x)=q(x)\cdot(x-\alpha)$, where $q$ is a polynomial of degree less
            than $f$. But by hypothesis $q$ then has at most $n$ roots. And any
            root of $q$ is a root of $f$, and hence $f$ has at most $n+1$ roots,
            a contradiction. Thus, $f$ has at most $n+1$ roots.
        \end{proof}
        \begin{theorem}
            If $\monoid{G}$ is a finite Abelian group, if $m\in\mathbb{N}$ is
            such that $m$ divides $\cardinality{G}$, then there are at most
            $m$ elements in $G$ whose order divides $m$ if and only if $G$ is
            cyclic.
        \end{theorem}
        \begin{proof}
            For if $G$ is cycle, and if $d$ divides $\cardinality{G}$, let
            $G_{d}=\{x\in{G}\;|\;x^{d}=1\}$. If $G_{d}$ is empty, then
            $\cardinality{G_{d}}<m$. If not then there is a $y\in{G}_{d}$.
            But then $\innerprod{y}\subseteq{G}_{d}$. For if
            $x\in\innerprod{y}$, then $x=y^{k}$ for some $k\in\mathbb{Z}_{n}$.
            But then $x^{d}=(y^{k})^{d}=y^{kd}=1$, and hence $x\in{G}_{d}$.
            Therefore, $\innerprod{y}\subseteq{G}_{d}$. Now let $x\in{G}_{d}$
            Since $\monoid{G}$ is cyclic there exists $z\in{G}$ such that
            $\innerprod{z}=G$. But then there exists
            $k_{1},k_{2}\in\mathbb{Z}_{n}$ such that $z^{k_{1}}=y$ and
            $z^{k_{2}}=x$. But then $y^{k_{2}-k_{1}}=x$, and therefore
            $x\in\innerprod{y}$. Thus, $\innerprod{y}=G_{d}$. But
            $\innerprod{y}$ has $d$ elements, and hence there are at most $d$
            elements of order $d$. Going the other way, suppose that if $d$
            divides $n$ then there are at most $d$ elements of order $d$.
            Since $\monoid{G}$ is finite and Abelian, it is the product of
            finite cyclic groups $\monoid[][+]{\mathbb{Z}_{{p_{k}}^{n_{k}}}}$.
            Suppose two of the primes $p_{k}$ are equal. But then there are at
            least $p_{k}^{2}$ elements of order $p_{k}$, and $p_{k}$ divides the
            order of the group, a contradiction. Thus, all of the primes are
            distinct. By the direct product of coprime cyclic groups is a cyclic
            group. Thus, $\monoid{G}$ is cyclic.
        \end{proof}
        \begin{theorem}
            If $\ring{F}$ is a field, if $\monoid[][\cdot]{F\setminus\{0\}}$ is
            the multiplicative group of $F$, then it is cyclic.
        \end{theorem}
        \begin{proof}
            Since $\ring{F}$ is a field, $\monoid[][\cdot]{F\setminus\{0\}}$ is
            Abelian. Suppose $d\in\mathbb{N}$ divides the cardinality of
            $F\setminus\{0\}$ and let $f\in{F}[x]$ be the polynomial
            $f(x)=x^{d}-1$. Then by the previous theorem there are at most
            $d$ roots. But then there are at most $d$ elements of
            $F\setminus\{0\}$ such that $x^{d}=1$, and thus by the previous
            theorem $\monoid[][\cdot]{F\setminus\{0\}}$ is cyclic.
        \end{proof}
        With this we now return to the claim that there are irreducible
        polynomials in $\mathbb{Q}[x]$ that are reducible if $\mathbb{Z}_{p}[x]$
        for every prime $p\in\mathbb{N}$.
        \begin{theorem}
            If $\monoid{G}$ is a cyclic group, if $a,b\in{G}$ do not have
            square roots, then $a*b$ has a square root.
        \end{theorem}
        \begin{proof}
            For since $\monoid{G}$ is cyclic there is an $x\in{G}$ such that
            $\innerprod{x}=G$. But then if $a,b\in{G}$, there exists
            $n,m\in\mathbb{N}$ such that $x^{n}=a$ and $x^{m}=b$. But $a$ and
            $b$ do not have square roots, and hence $n$ and $m$ are odd. But
            then $a*b=x^{n}*x^{m}=x^{n+m}$, and $n+m$ is even. Hence,
            $x^{n+m}$ has a square root, and thus $a*b$ has a square root.
        \end{proof}
        \begin{example}
            The polynomial $f(x)=x^{4}-10x^{2}+1$ is irreducible in
            $\mathbb{Z}[x]$, yet it is reducible in $\mathbb{F}_{p}[x]$ for
            every prime $p\in\mathbb{N}$. If $p\in\mathbb{N}$ is such that
            2 has a square root in $\mathbb{Z}_{p}$, then we can factor this
            to obtain:
            \begin{equation}
                (x^{2}-2\sqrt{2}x-1)(x^{2}+2\sqrt{2}x-1)
                =x^{4}-10x^{2}-1
            \end{equation}
            And hence $f$ is reducible over all such $\mathbb{Z}_{p}[x]$. If $p$
            is such that 3 has a square root, then:
            \begin{equation}
                (x^{2}-2\sqrt{3}x-1)(x^{2}+2\sqrt{2}x+1)
                =x^{4}-10x^{2}+1
            \end{equation}
            For all other such $p$, neither 2 nor 3 are square roots, and hence
            by the previous theorem their product $2\cdot{3}=6$ does have a
            square root. But then:
            \begin{equation}
                x^{4}-10x^{2}+1=
                    \big(x^{2}-(5+2\sqrt{6})\big)\big(x^{2}-(5-2\sqrt{6})\big)
            \end{equation}
        \end{example}
        There is a stronger result that for every non-prime
        $n\in\mathbb{N}^{+}$ there is a polynomial $f\in\mathbb{Z}[x]$ of degree
        $n$ such that $f$ is irreducible over $\mathbb{Z}[x]$ but reducible
        over $\mathbb{Z}_{p}[x]$ for every prime $p$.
        (See Brandl, R. American Mathematical Monthly, 1986).
        \begin{definition}
            An extension field of a field $\ring[F]{F}$ is a field $\ring[K]{K}$
            such that $\ring[F]{F}$ is a subfield of $\ring[K]{K}$.
        \end{definition}
        Thus, a field extension is to fields as supersets are to sets. Unlike
        set theory where one considers subsets most of the time, most of field
        theory is obsessed with field extensions.
        \begin{theorem}
            If $\ring[F]{F}$ and $\ring[K]{K}$ are fields, and if $K$ is an
            extension field of $F$, then $\ring[K]{K}$ is a vector field
            over $\ring[F]{F}$.
        \end{theorem}
        \begin{definition}
            The degree of a field extension $\ring[K]{K}$ of a given field
            $\ring[F]{F}$ is the dimension of the vector space
            $\ring[K]{K}$ over $\ring[F]{F}$. This is denoted $[K:F]$.
        \end{definition}
        \begin{definition}
            A finite extension of a field $\ring[F]{F}$ is an extension field
            $\ring[K]{K}$ of $F$ such that $[K:F]\in\mathbb{N}$.
        \end{definition}
        \begin{example}
            The field of complex numbers $\mathbb{C}$ is a field extension over
            $\mathbb{R}$. Moreover, it is a finite extension since $\mathbb{C}$
            has the bases $\{1,i\}$. Thus, $[\mathbb{C}:\mathbb{R}]=2$
        \end{example}
        \begin{example}
            The field $\mathbb{R}$ is also a field extension over the rational
            numbers $\mathbb{Q}$. However, unlike $[\mathbb{C}:\mathbb{R}]$,
            which is finite, $[\mathbb{R}:\mathbb{Q}]=\cardinality{\mathbb{R}}$.
            This can be shown since any finite extension of $\mathbb{Q}$ must
            be countable, and any countable extension must have cardinality
            $\cardinality{\mathbb{Q}\times\mathbb{Q}}$, but this is equal to
            $\cardinality{\mathbb{N}}$, and $\mathbb{R}$ is uncountable. Hence,
            at the very least, $[\mathbb{Q}:\mathbb{R}]$ is uncountable, and if
            we assume the continuum hypothesis then the cardinality must then
            be equal to the cardinality of $\mathbb{R}$ (for it is certinaly not
            greater).
        \end{example}
        \begin{example}
            The Gaussian numbers are a subfield of $\mathbb{C}$ defined as
            follows:
            \begin{equation}
                \mathbb{Q}(i)=\{\,a+ib\in\mathbb{C}\;|\;a,b\in\mathbb{Q}\,\}
            \end{equation}
            We can then see that $[\mathbb{Q}:\mathbb{Q}(i)]=2$ since this
            vector field has $\{1,i\}$ as a basis.
        \end{example}
        \begin{example}
            While $[\mathbb{Q}:\mathbb{R}]$ is uncountable, there are countably
            infinite extension fields. Given any field $\ring{F}$, the field of
            rational functions $F(x)$ has the countable basis $x^{n}$ for all
            $n\in\mathbb{N}$. The subspace of polynomials $F[x]$, while not a
            field, is still a vector space over $F$ and has the same basis.
        \end{example}
        \begin{theorem}
            If $\ring[F]{F}$ is a field, if $\ring[K]{K}$ is a finite field
            extension of $F$, and $\ring[L]{L}$ is a finite field extension of
            $K$, then $L$ is a finite field extension of $F$ and:
            \begin{equation}
                [L:F]=[L:K][K:F]
            \end{equation}
        \end{theorem}
        \begin{proof}
            For if $\ring[K]{K}$ is a finite field extension of $\ring[F]{F}$,
            then it is a finite dimensional vector space over $F$ and thus there
            is an $m\in\mathbb{N}$ and a finite sequence
            $a:\mathbb{Z}_{m}\rightarrow{K}$ such that $a[\mathbb{Z}_{m}]$ is a
            basis for $K$ over $F$. And similarly if $\ring[L]{L}$ is a finite
            field extension over $\ring[K]{K}$ then there is an $n\in\mathbb{N}$
            and a finite sequence $b:\mathbb{Z}_{n}\rightarrow{L}$ such that
            $b[\mathbb{Z}_{n}]$ is a basis for $L$ over $K$. Define
            $A:\mathbb{Z}_{m}\times\mathbb{Z}_{n}\rightarrow{L}$ by
            $A(i,j)=a_{i}\cdot{b}_{j}$ for all
            $(i,j)\in\mathbb{Z}_{m}\times\mathbb{Z}_{n}$. Let
            $f:\mathbb{Z}_{m\cdot{n}}\rightarrow%
             \mathbb{Z}_{m}\times\mathbb{Z}_{n}$ be a bijection and define
            $e:\mathbb{Z}_{n\cdot{m}}\rightarrow{L}$ by $e=A\circ{f}$. Suppose
            $e[\mathbb{Z}_{n\cdot{m}}]$ does not span all of $L$ over $F$. Then
            there exists $x\in{L}$ such that for every sequence
            $c:\mathbb{Z}_{n\cdot{m}}\rightarrow{F}$, $\alpha$ is not the
            sum over $c_{i}\cdot{e}_{i}$. But $b[\mathbb{Z}_{n}]$ forms a basis
            of $L$ over $K$, and thus there is a sequence
            $\beta:\mathbb{Z}_{n}\rightarrow{K}$ such that:
            \begin{equation}
                \alpha=\sum_{k\in\mathbb{Z}_{n}}\beta_{k}\cdot_{L}{b}_{k}
            \end{equation}
            But for each $k$ it is true that $\beta_{k}\in{K}$, and since
            $a[\mathbb{Z}_{m}]$ is a basis for $K$ over $F$, there is a sequence
            $\gamma_{k}:\mathbb{Z}_{m}\rightarrow{F}$ such that:
            \begin{equation}
                \beta_{k}=\sum_{i\in\mathbb{Z}_{m}}\gamma_{i}\cdot_{K}a_{i}
            \end{equation}
            Let $c:\mathbb{Z}_{m}\times\mathbb{Z}_{n}\rightarrow{F}$ be defined
            by $c(i,j)=\gamma_{i}$ and let
            $d:\mathbb{Z}_{m\cdot{n}}\rightarrow{F}$ be defined by
            $d=c\circ{f}$. But then:
            \begin{align}
                \alpha&=\sum_{j\in\mathbb{Z}_{n}}\beta_{j}\cdot_{L}b_{j}\\
                &=\sum_{j\in\mathbb{Z}_{n}}\Big(
                    \sum_{i\in\mathbb{Z}_{m}}\gamma_{i}\cdot_{K}a_{i}\Big)
                    \cdot_{L}b_{j}\\
                &=\sum_{j\in\mathbb{Z}_{n}}\sum_{i\in\mathbb{Z}_{m}}
                    \Big(\gamma_{k}\cdot_{k}\big(a_{i}\cdot_{L}b_{j}\big)\Big)\\
                &=\sum_{j\in\mathbb{Z}_{n}}\sum_{i\in\mathbb{Z}_{m}}
                    c(i,j)\cdot_{K}A(i,j)\\
                &=\sum_{k\in\mathbb{Z}_{n\cdot{m}}}
                    \big((c\cdot_{K}A)\circ{f}\big)(k)\\
                &=\sum_{k\in\mathbb{Z}_{n\cdot{m}}}d_{k}\cdot_{K}e_{k}
            \end{align}
            A contradiction. Hence, $e[\mathbb{Z}_{n\cdot{m}}]$ does span
            $L$. Moreover, it is linearly independent. For if
            $d:\mathbb{Z}_{m\cdot{n}}\rightarrow{F}$ is a sequence such that:
            \begin{equation}
                \sum_{k\in\mathbb{Z}_{m\cdot{n}}}d_{k}\cdot_{K}e_{k}=0
            \end{equation}
            The composing with $f^{\minus{1}}$ such that:
            \begin{equation}
                \sum_{j\in\mathbb{Z}_{n}}\sum_{i\in\mathbb{Z}_{n}}
                    d_{ij}a_{i}b_{j}=0
            \end{equation}
            From the linear independence of the $a_{i}$ and the $b_{j}$,
            all of the $d_{ij}$ are zero. Hence, by the basis theorem,
            $[L:K]=n\cdot{m}$.
        \end{proof}
        Given a field $F$ and a monic irreducible polynomial of positive degree
        $f\in{F}[x]$, we can always obtain a field extension for $F$ by
        considering the quotient $F[x]/(f)$, where $(f)$ is the ideal generated
        by $f$.
        \begin{example}
            Consider the field of real numbers $\ring{\mathbb{R}}$ and let
            $f(x)=x^{2}+1$. This is irreducible over $\mathbb{R}$ since it is
            a quadratic with no roots, as one can determine via the quadratic
            formula. If we consider $\mathbb{R}[x]/(x^{2}+1)$, we obtain the
            following arithmetic:
            \begin{equation}
                (a+bx)+(c+dx)=(a+c)+(b+d)x
            \end{equation}
            Multiplication is carried out as follows:
            \begin{equation}
                (a+bx)(c+dx)=ac+(bc+ad)x+bdx^{2}
            \end{equation}
            But in the quotient we have $x^{2}+1=0$, and hence
            $x^{2}=\minus{1}$. Thus, we can simplify this to:
            \begin{equation}
                (a+bx)(c+dx)=(ac-bd)+(ad+bc)x
            \end{equation}
            And this is precisely the multiplicative structure on $\mathbb{C}$.
            Thus the field $\mathbb{R}[x]/(x^{2}+1)$ is isomorphic to the
            complex numbers $\mathbb{C}$ under the mapping
            $\phi(1)=1$ and $\phi(x)=i$.
        \end{example}
        Such fields are called stem fields. That is, stem fields are fields of
        the form $F[x]/(f)$. The intersection of subrings is a subring. The
        subring generated by a subset $S\subseteq{R}$ in a ring $\ring{R}$ is
        the intersection of all subrings containing $S$. Given a ring
        $\ring{R}$ and a subring $\ring{S}$, and given any subset
        $A\subseteq{R}$, the subring generated by $S$ over $A$ is the
        intersection of all subrings of $R$ that contains $A\cup{S}$.
        \begin{example}
            Letting $\mathbb{C}$ and $\mathbb{R}$ have their usual field
            structures, and taking $A\subseteq\mathbb{C}$ to be $A=\{i\}$, the
            subring generated by $\mathbb{R}$ over $A$ is simply the entire
            complex plane $\mathbb{C}$. We can write this as
            $\mathbb{C}=\mathbb{R}[i]$ or
            $\mathbb{C}=\mathbb{R}[\sqrt{\minus{1}}]$
        \end{example}
        The subring generated by a subset is equal to the set of all possible
        linear combintations of elements in the subset. Rigorously, we look at
        all finite sequence $a:\mathbb{Z}_{n}\rightarrow{F}$ and
        $b:\mathbb{Z}_{n}\rightarrow\mathcal{P}(S)$ such that $b_{k}$ is finite
        for all $k\in\mathbb{Z}_{n}$, and we form the sums:
        \begin{equation}
            z=\sum_{k\in\mathbb{Z}_{n}}\Big(
                a_{k}\prod_{\alpha\in{b}_{k}}\alpha
            \Big)
        \end{equation}
        \begin{theorem}
            If $\ring{R}$ is an integral domain, if $\ring[F]{F}$ is a subring
            of $R$ such that $\ring[F]{F}$ is also a field, and if $R$ is a
            finite dimensional vector space over $F$, then $\ring{R}$ is a
            field.
        \end{theorem}
        \begin{proof}
            For if not then there is a non-zero element $a\in{R}$ with no
            inverse element. But the function $f:R\rightarrow{R}$ defined by
            $f(x)=a\cdot{x}$ is injective. For if $f(x)=f(y)$, then:
            \begin{equation}
                a\cdot{x}-a\cdot{y}=a\cdot(x-y)=0
            \end{equation}
            But $\ring{R}$ is an integral domain and thus either $a=0$ or
            $x-y=0$. But by hypothesis $a$ is non-zero, and hence $x-y=0$.
            Thus, $f$ is injective. But it is also linear since:
            \begin{equation}
                f(x+y)=a\cdot(x+y)=a\cdot{x}+a\cdot{y}=f(x)+f(y)
            \end{equation}
            And thus $f$ is a linear map from a finite dimensional vector space
            to itself and is therefore surjective. But then there exists
            $x\in{R}$ such that $f(x)=1$. But then $a\cdot{x}=1$, a
            contradiction. Hence, $\ring{R}$ is a field.
        \end{proof}
        \begin{theorem}
            If $\ring{F}$ is a field, if $\ring{K}$ is a finite extension field,
            and if $\ring{S}$ is a subring of $K$ that contains $F$, then it is
            a field.
        \end{theorem}
        \begin{theorem}
            For such a ring will be an integral domain, and will also be a
            finite dimensional vector space over $F$, and hence will be a field
            by the previous theorem.
        \end{theorem}
        The intersection of subfields is a field. We can similarly define field
        generated by subset. Given the ring generated by $S$, $F[S]$, the field
        generated by $S$ $F(S)$ is also called the field of fractions of $F[S]$.
        This is because it is obtained by considering all fractions of elements
        in $F[S]$. For example, in the polynomial ring $F[x]$, the field that
        this generates is the field of rational functions $F(x)$.
        \begin{example}
            The ring $\mathbb{Q}[\pi]$ is a subset of $\mathbb{R}$ that is all
            linear combinations of powers of $\pi$ with rational coefficients.
            The field generated by this $\mathbb{Q}(\pi)$ is the set of all
            $f(\pi)/g(\pi)$ where $f,g$ are rational polynomials evaluated at
            $\pi$, and $g$ is non-zero.
        \end{example}
        \begin{definition}
            A simple extension of a field $\ring{F}$ is an extension field
            $\ring{K}$ such that there exists $\alpha\in{K}$ such that
            $K=F(\alpha)$.
        \end{definition}
        \begin{definition}
            The composite of two subfields $\ring{F_{1}}$ and $\ring{F_{2}}$
            of a field $\ring{F}$ is the field generated by $F_{1}\cup{F}_{2}$.
        \end{definition}
        \begin{theorem}
            If $\ring{F}$ is a field, if $\ring{F_{1}}$ and $\ring{F_{2}}$ are
            subfields, and if $K$ is the composite of $F_{1}$ and $F_{2}$,
            then $K$ is the field generated by $F_{1}$ over $F_{2}$, and $K$ is
            the field generated by $F_{2}$ over $F_{1}$.
        \end{theorem}
        Given a field $\ring{F}$ and an extension field $\ring{K}$, for any
        element $\alpha\in{E}$ we can define the homomoprhism
        $\phi:F[x]\rightarrow{E}$ be $\phi(f)=f(\alpha)$. If the kernel of this
        is zero, then we say $\alpha$ is transcendental. It then turns out that
        $\phi:F[x]\rightarrow{F}[\alpha]$ is an isomorphism and this extends
        to an isomorphism $\tilde{\phi}:F(x)\rightarrow{F}(\alpha)$. In the
        latter case, where the kernel is non-zero, we call $\alpha$ algebraic.
        \begin{definition}
            An algebraic element of a field extension $\ring{K}$ over a field
            $\ring{F}$ is an element $\alpha\in{K}$ such that there exists a
            polynomial $f\in{F}[x]$ such that $f(\alpha)=0$.
        \end{definition}
        \begin{definition}
            A minimal polynomial of an algebraic element $\alpha$ of a field
            extension $\ring{K}$ over a field $\ring{F}$ is a monic irreducible
            polynomial such that $f(\alpha)=0$.
        \end{definition}
        Minimal polynomials are unique. We can define
        $\phi:F[x]/(f)\rightarrow{F}[\alpha]$ by
        $\phi(\overline{f})=f(\alpha)$. Since $F[x]/(f)$ is a field, $F[\alpha]$
        is as well. Therefore $F[\alpha]=F(\alpha)$, and $F[\alpha]$ is a stem
        field.
        \begin{definition}
            An algebraic extension field of a field $\ring{F}$ is an extension
            field $\ring{K}$ such that for all $\alpha\in{K}$ it is true that
            $\alpha$ is algebraic in $F$.
        \end{definition}
        \begin{example}
            The set of algebraic numbers in $\mathbb{R}$ are an algebraic
            extension over $\mathbb{Q}$. The complex numbers are an algebraic
            extension over $\mathbb{R}$. The real numbers are not an algebraic
            extension over $\mathbb{Q}$ however, since $\pi$ is transcendental.
            Indeed, the set of all algebraic numbers in $\mathbb{R}$ form a
            countable subset, and so most real numbers are transcendental.
        \end{example}
        \begin{theorem}
            If $\ring{F}$ is a field, and if $\ring{K}$ is an extension field,
            then $K$ is a finite extension if and only if it is algebraic and
            finitely generated.
        \end{theorem}
        \begin{ftheorem}{Liouville's Transcendental Theorem}
                        {Liouvilles_Transcendental_Theorem}
            If $a\in\mathbb{N}$, if $a\geq{2}$, and if $s\in\mathbb{R}$ is
            defined by:
            \begin{equation}
                s=\sum_{k\in\mathbb{N}}\frac{1}{a^{n!}}
            \end{equation}
            then $s$ is transcendental.
        \end{ftheorem}
        \begin{bproof}
            For suppose not. Then $s$ is algebraic and thus there is a
            $d\in\mathbb{N}$ and a minimal polyomial $f\in\mathbb{Q}[x]$ of
            degree $d$ such that $f(s)=0$. But then $\mathbb{Q}[s]$ is a
            $d$ dimensional vector space over $\mathbb{Q}$. Let $N\in\mathbb{N}$
            be such that $N\cdot{f}\in\mathbb{Z}[x]$ and let $S_{n}$ be the
            $n^{th}$ partial sums:
            \begin{equation}
                S_{n}=\sum_{k\in\mathbb{Z}_{n}}\frac{1}{a^{k!}}
            \end{equation}
            Thus, $|S_{n}-s|\rightarrow{0}$. If $s$ is rational, then the
            minimal polynomial has degree one and hence $f(x)=x-s$. Otherwise,
            since $f$ is irreducible and of degree greater than one, $f$ has no
            rational roots. Moreover, for all $n\in\mathbb{N}$, $S_{n}\ne{s}$
            and hence $f(S_{n})\ne{0}$. Moreover, by induction
            $S_{n}\in\mathbb{Q}$ for all $n\in\mathbb{N}$, and
            $(a^{n!})^{d}DS_{n}$ is an integer, and therefore:
            \begin{equation}
                |(a^{n!})^{d}DS_{n}|\geq{1}
            \end{equation}
            From the fundamental theorem of algebra, $f$ splits into it's roots
            and so:
            \begin{equation}
                f(x)=\prod_{k\in\mathbb{Z}_{d}}(x-\alpha_{k})
            \end{equation}
            Where the $\alpha_{k}$ are the complex roots of $f$. Let
            $M_{1}=\maximum{|\alpha_{k}|\;:\;k\in\mathbb{Z}_{d}\setminus\{0\}}$.
            That is, $M$ is the largest modulus of all the roots, neglecting the
            zeroth term. Let $M=\maximum{M_{1},1}$. Then:
            \begin{equation}
                |f(S_{n})|=\prod_{k\in\mathbb{Z}_{d}}|S_{n}-\alpha_{k}|
                    \leq|S_{n}-\alpha_{0}|(S_{n}+M)^{d-1}
            \end{equation}
            But we can simplify this further, since:
            \begin{equation}
                |S_{n}-\alpha_{1}|=\sum_{k=n+1}^{\infty}\frac{1}{a^{k!}}
                \leq\frac{1}{a^{(k+1)}!}\sum_{k\in\mathbb{N}}\frac{1}{a^{k}}
                =\frac{a}{a-1}\frac{1}{a^{(n+1)!}}
            \end{equation}
            Piecing this together, we obtain:
            \begin{equation}
                |f(S_{n})|\leq\frac{2}{2^{(n+1)!}}(S_{n}+M)^{d-1}
            \end{equation}
            And this convergences to zero. Therefore:
            \begin{equation}
                |(a^{n!})^{d}DS_{n}|\leq\frac{a}{a-1}
                    \frac{a^{d\cdot{n!}}}{a^{(n+1)!}}(S_{n}+M)^{d-1}
            \end{equation}
            Which thus convergences to zero, contradicting that this is always
            greater than one.
        \end{bproof}
        \begin{definition}
            A splitting polynomial in a field $\ring{F}$ is a polynomial
            $f\in{F}[x]$ of degree $n\in\mathbb{N}$ such that there exists
            finite sequences $a,b:\mathbb{Z}_{n}\rightarrow{F}$ such that:
            \begin{equation}
                f(x)=\prod_{k\in\mathbb{Z}_{n}}(a_{k}x-b_{k})
            \end{equation}
        \end{definition}
        \begin{theorem}
            If $\ring{F}$ is a field, then every non-constant polynomial
            $f\in{F}[x]$ is a splitting polynomial if and only if every
            non-constant polynomial has at least one root in $F$.
        \end{theorem}
        \begin{proof}
            If every non-constant polynomial $f\in{F}[x]$ splits, then given
            such an $f$ of degree $n\in\mathbb{N}$ there are sequences
            $a,b:\mathbb{Z}_{n}\rightarrow{F}$ such that:
            \begin{equation}
                f(x)=\prod_{k\in\mathbb{Z}_{n}}(a_{k}x-b_{k})
            \end{equation}
            But then $b^{k}/a_{k}$ is a root for all $k$, and hence there is at
            least one root. Going the other way, let $f\in{F}[x]$ be a
            non-constant polynomial. Then there is a root $\alpha$ and hence
            $f(x)=(x-\alpha)g(x)$. Then either $g$ is a constant or a
            non-constant polynomial. In the latter case it has a root $\beta$
            and so $f(x)=(x-\alpha)g(x)=(x-\alpha)(x-\beta)h(x)$. Continuing
            inductively we find that $f$ is a splitting polynomial.
        \end{proof}
        \begin{theorem}
            If $\ring{F}$ is a field, then every non-constant polynomial
            $f\in{F}[x]$ has at least one root in $F$ if and only if every
            irreducible polynomial in $F[x]$ has degree 1.
        \end{theorem}
        \begin{proof}
            For if $f\in{F}[x]$ is a non-constant polynomial, then either
            the degree of $f$ is 1 or it is greater than 1. But if the degree
            of $f$ s 1, then $f(x)=ax+b$ and thus $f$ has a root. If the degree
            of $f$ is greater than 1, then it is reducible and hence there
            are polynomials $g,h\in{F}[x]$ of positive degree such that
            $f=gh$. Continuing inductively we eventually factor $f$ down to the
            product of degree 1 polynomials, and thus $f$ has a root. Going the
            other way, if $f\in{F}[x]$ is non-constant and irreducible then it
            has a root, and thus $f(x)=ax+b$. Thus, the only irreducible
            polynomials have degree 1.
        \end{proof}
        \begin{theorem}
            If $\ring{F}$ is a field, then every irreducible polynomial
            $f\in{F}[x]$ has degree 1 if and only if for every finite field
            extension $\ring{K}$ over $F$, $K=F$.
        \end{theorem}
        \begin{proof}
            For if $\ring{K}$ is a finite field extension over $F$, then the
            minimal polynomial $f\in{F}[x]$ for any element $\alpha\in{K}$ has
            degree 1 and hence $f(x)=x-\alpha$. But then $\alpha\in{F}$, and
            thus $F=K$. In the reverse direction, if $f\in{F}[x]$ is an
            irreducible polynomial, then $F[x]/(f)$ is a finite extension field
            of $F$. But by hypothesis, $F[x]/(f)=F$, and since the degree of
            $f$ is equal to $[F[x]/(f):F]=1$, $f$ is a degree 1 polynomial.
        \end{proof}
        \begin{definition}
            An algebraically closed field is a field $\ring{F}$ such that for
            all non-constant polynomials $f\in{F}[x]$, there exists a root
            $\alpha\in{F}$ of $f$.
        \end{definition}
        \begin{definition}
            An algebraic closure of a field $\ring{F}$ is an extension field
            $\ring{K}$ of $F$ such that $K$ is algebraically closed.
        \end{definition}
        \begin{example}
            By the fundamental theorem of algebra, $\mathbb{C}$ is algebraically
            closed. It is therefore an algebraic closure of $\mathbb{R}$. To
            note that $\mathbb{R}$ is not algebraically closed one need only
            consider the polynomial $f(x)=x^{2}+1$.
        \end{example}
        \begin{theorem}
            If $\ring{F}$ is a field, if $\ring{K}$ is an algebraic extension
            field of $F$, and if for every polynomial $f\in{F}[x]$ it is true
            that $f$ is a splitting polynomial in $K[x]$, then $\ring{K}$ is
            algebraically closued.
        \end{theorem}
        \begin{proof}
            For if $f\in{K}[x]$ is a non-constant polynomial then there is a
            finite extension field $\ring{L}$ of $K$ such that $f\in{L}[x]$ has
            a root $\alpha\in{L}$. But then by the tower law, $L$ is a finite
            extension field over $F$. Thus $\alpha$ is algebraic over $F$ and
            so there is a polynomial $g\in{F}[x]$ such that $g(\alpha)=0$. But
            by hypothesis, $g$ splits in $K$ and so the roots of $g$ lie in
            $K$. Thus, $\alpha\in{K}$. Hence, $K$ is algebraically closed.
        \end{proof}
        \begin{theorem}
            If $\ring{K}$ is an extension field of a field $\ring{F}$ and if
            $\mathbb{A}_{F}$ is the set:
            \begin{equation}
                \mathbb{A}_{F}=\{\,\alpha\in\Omega\;|\;
                    \alpha\textrm{ is algebraic over }F\}
            \end{equation}
            Then $\ring{\mathbb{A}_{F}}$ is a field.
        \end{theorem}
        \begin{proof}
            For if $\alpha,\beta\in{K}$ are algebraic over $F$, then
            $F[\alpha,\beta]$ is a field of finite degree over $F$. Thus every
            element of $F[\alpha,\beta]$ is algebraic over $F$, and hence
            $\alpha+\beta$, $\alpha-\beta$, $\alpha\cdot\beta$, and
            $\alpha/\beta$ are all algebraic. Therefore, $\mathbb{A}_{F}$ is a
            field.
        \end{proof}
        \begin{definition}
            The algebraic closure of a field $\ring{F}$ with respect to an
            extension field $\ring{K}$ is the subfield $\ring{\mathbb{A}_{F}}$
            defined by:
            \begin{equation}
                \mathbb{A}_{F}=\{\,\alpha\in\Omega\;|\;
                \alpha\textrm{ is algebraic over }F\}
            \end{equation}
        \end{definition}
        By the previous theorem, the algebraic of a field $\ring{F}$ with
        respect to any field extension $\ring{K}$ is a subfield, and hence this
        is well defined.
        \begin{theorem}
            If $\ring{F}$ is a field, if $\ring{K}$ is an algebraically closed
            field extension of $F$, and if $\mathbb{A}_{F}$ is the algebraic
            closure of $F$ with respect to $K$, then $\mathbb{A}_{F}$ is an
            algebraic closure of $F$.
        \end{theorem}
        \begin{proof}
            Since $\mathbb{A}_{F}$ is algebraic over $F$, and since every
            polynomial $f\in{F}[x]$ splits in $\mathbb{A}_{F}[x]$, we thus have
            that, since $K$ is algebraically closed, the $f$ has a root in
            $\mathbb{A}_{F}$. Hence, $\mathbb{A}_{F}$ is an algebraic closure
            of $F$.
        \end{proof}
        Combining this with the fundamental theorem of algebra we see that
        every subfield of $\mathbb{C}$ has an algebraic closure.
        \begin{example}
            Let $\mathbb{Q}$ denote the standard field of rational numbers, and
            let $\mathbb{Q}[\sqrt{2},\sqrt{3}]$ be the field extension obtained
            by appending $\sqrt{2}$ and $\sqrt{3}$. The degree of
            $[\mathbb{Q}[\sqrt{2},\sqrt{3}]:\mathbb{Q}]$ is four, and to prove
            this we use the tower law. Firstly, note that
            $[\mathbb{Q}[\sqrt{2}]:\mathbb{Q}]=2$ for $\{1,\sqrt{2}\}$ form a
            linear independent basis of $\mathbb{Q}[\sqrt{2}]$ over
            $\mathbb{Q}$. Next,
            $[\mathbb{Q}[\sqrt{2},\sqrt{3}]:\mathbb{Q}[\sqrt{2}]]=2$. For
            $\{1,\sqrt{3}\}$ certainly forms a basis, but moreover it is
            linearly independent. For suppose not and suppose we have:
            \begin{equation}
                a+b\sqrt{3}=0\quad\quad
                a,b\in\mathbb{Q}[\sqrt{2}]
            \end{equation}
            Then we have that $\sqrt{3}=\minus{a}/b$ with
            $a,b\in\mathbb{Q}[\sqrt{2}]$. With this we may write:
            \begin{equation}
                \sqrt{3}=\minus\frac{a_{1}+a_{2}\sqrt{2}}{b_{1}+b_{2}\sqrt{2}}
            \end{equation}
            Squaring both sides yields:
            \begin{equation}
                a_{1}^{2}+2a_{1}a_{2}\sqrt{2}+2a_{2}^{2}=
                3b_{1}^{2}+6b_{1}b_{2}\sqrt{2}+6b_{2}^{2})
            \end{equation}
            If we collect the $\sqrt{2}$ terms, we obtain:
            \begin{equation}
                \sqrt{2}(2a_{1}a_{2}-6b_{1}b_{2})=
                3b_{1}^{2}-a_{1}^{2}+6b_{2}^{2}-2a_{2}^{2}
            \end{equation}
            Now if $2a_{1}a_{2}-6b_{1}b_{2}\ne{0}$, then we may divide through
            by this showing that $\sqrt{2}$ is a rational number, which is a
            contradiction. Thus, $2a_{1}a_{2}-6b_{1}b_{2}=0$ and hence
            $a_{1}a_{2}=3b_{1}b_{2}$. But then:
            \begin{align}
                a_{1}+a_{2}\sqrt{2}+b_{1}\sqrt{3}+b_{2}\sqrt{6}&=0\\
                \Rightarrow
                a_{1}^{2}+3b_{1}b_{2}\sqrt{2}
                    +a_{1}b_{1}\sqrt{3}+a_{1}b_{2}\sqrt{6}&=0\\
                \Rightarrow
            \end{align}
        \end{example}
    \section{Chapter 2 (Milne)}
        \begin{definition}
            An $F$ homomoprhism from an extension field $\ring{E}$ over a field
            $\ring{F}$ to an extension field $\ring{E'}$  over $F$ is a field
            homomoprhism $\phi:E\rightarrow{E}'$ such that
            $\phi|_{F}=\identity{F}$.
        \end{definition}
        \begin{theorem}
            If $\ring{F}$ is a field, if $\ring{E}$ is a simply field extension
            of $F$, if $\ring{K}$ is a field extension of $F$, and if
            $\varphi:E\rightarrow{K}$ is an $F$ homomoprhism, if $\alpha\in{E}$
            is such that $E=F(\alpha)$, if $\alpha$ is transcendental over $F$,
            then $\varphi(\alpha)$ is transcendental over $F$ and the function
            $f:\homomorphisms{E}{K}\rightarrow{E}\setminus\mathbb{A}_{F}$
            defined by $f(\varphi)=\varphi(\alpha)$ is a bijection.
        \end{theorem}
    \section{Chapter 13 (Dummit and Foote)}
    \section{Stuff}
        If $\langle\cdot|\cdot\rangle$ is a symmetric bilinear form, it is
        represented by its Gram matrix relative to a bsis $\mathscr{B}$ of the
        finite dimensional vector space $\mathscr{B}$. We have:
        \begin{equation}
            \langle{v}|w\rangle
            =[V]_{\mathscr{B}}^{T}G_{\mathscr{B}}[W]\mathscr{B}
        \end{equation}
        Let $\langle\cdot|\cdot\rangle:V\times{V}\rightarrow{k}$ be bilinear,
        let $\mathscr{B}$ and $\mathscr{C}$ be bases of $V$, and let
        $G_{\mathscr{B}}$ and $G_{\mathscr{C}}$ be the corresponding Gram
        matrices. Let $x,y\in{V}$. Then:
        \begin{subequations}
            \begin{align}
                [x]_{\mathscr{B}}^{T}G_{\mathscr{B}}[y]_{\mathscr{B}}
                &=\langle{x}|y\rangle\\
                &=[x]_{\mathscr{C}}^{T}G_{\mathscr{C}}[Y]_{\mathscr{C}}\\
                &=[\textrm{Id}(x)]_{\mathscr{C}}^{T}G_{\mathscr{C}}
                    [\textrm{Id}(y)]_{\mathscr{C}}\\
                &=\Big([\textrm{Id}]_{\mathscr{C}}^{\mathscr{B}}
                    [x]_{\mathscr{B}}\Big)^{T}G_{\mathscr{C}}
                    \Big([\textrm{Id}_{\mathscr{C}}^{\mathscr{B}}]
                        [y]_{\mathscr{B}}\Big)
            \end{align}
        \end{subequations}
        From this, we obtain the formula for the Gram matrix:
        \begin{equation}
            G_{\mathscr{B}}=P^{T}G_{\mathscr{C}}P
        \end{equation}
        \begin{fdefinition}{Congruent Matrices}{Congruent_Matrices}
            Congruent matrices are matrices $A,B\in{M}_{n}(k)$ such that there
            is an invertible matrice $P\in{GL}_{n}(k)$ such that:
            \begin{equation}
                B=P^{T}AP
            \end{equation}
        \end{fdefinition}
    \section{Diagonalizing Real Quadratic Forms}
        \begin{ltheorem}{Sylvester's Theorem}{Sylvesters_Theorem}
            If $V$ is a finite dimensional vector space over $\mathbb{R}$ and if
            $\langle\cdot|\cdot\rangle$ is a symmetric bilinear form, then there
            exists a basis $\mathscr{B}$ of $V$ such that:
            \begin{equation}
                G_{\mathscr{B}}=
                \begin{bmatrix*}[r]
                    1&\dots&0&0&\dots&0&0&\dots&0\\
                    \vdots&\ddots&\vdots&\vdots&\ddots
                        &\vdots&\vdots&\ddots&\vdots\\
                    0&\dots&1&0&\dots&0&0&\dots&0\\
                    0&\dots&0&\minus{1}&\dots&0&0&\dots&0\\
                    \vdots&\ddots&\vdots&\vdots
                        &\ddots&\vdots&\vdots&\ddots&\vdots\\
                    0&\dots&0&0&\dots&\minus{1}&0&\dots&0\\
                    0&\dots&0&0&\dots&0&0&\dots&0\\
                    \vdots&\ddots&\vdots&\vdots&\ddots
                        &\vdots&\vdots&\ddots&\vdots\\
                    0&\dots&0&0&\dots&0&0&\dots&0\\
                \end{bmatrix*}
            \end{equation}
            Where there are $r$ 1's, $s$ negative 1's, and $t$ zeros, and
            $r$, $s$, and $t$ are uniquely determined.
        \end{ltheorem}
    \section{Orthogonal Transformations}
        Let $(V,\langle\cdot|\cdot\rangle)$ be a nondegenerate bilinear space.
        Then a linear map $T:V\rightarrow{V}$ is orthogonal if or a linear
        isometry if:
        \begin{equation}
            \langle{T}(v)|T(w)\rangle=\langle{v}|w\rangle
        \end{equation}
        Let $\mathscr{B}$ be a basis of $V$ and let $G_{\mathscr{B}}$ be the
        Gram matrix of $\langle\cdot|\cdot\rangle$. Let $A$ be the representing
        matrix of $T$ over the basis $\mathscr{B}$. Then:
        \begin{equation}
            \langle{v}|w\rangle
            =\langle{T}(v)|T(w)\rangle
            =[T(v)]_{\mathscr{B}}^{T}G_{\mathscr{B}}[T(w)]_{\mathscr{B}}
            =[v]_{\mathscr{B}}^{T}A^{T}G_{\mathscr{B}}A[w]_{\mathscr{B}}
        \end{equation}
        From this we can compute what the Gram matrix is:
        \begin{equation}
            G_{\mathscr{B}}=A^{T}G_{\mathscr{B}}A
        \end{equation}
        If $A$ represents an orthogonal transformation, then $A$ must satisfy
        this equation. In the special case of when $\mathscr{B}$ is orthonormal,
        then $G_{\mathscr{B}}$ is simply the identity matrix and thus we have
        that $A^{T}A=I$, or $A^{T}=A^{\minus{1}}$.
        \par\hfill\par
        A nondegenerate skew symmetric bilinear form on a $2n$ dimensional real
        vector space is called a symplectic form. The Gram matrix is:
        \begin{equation}
            J=
            \begin{bmatrix*}[r]
                0&I_{n}\\
                \minus{I}_{n}&0
            \end{bmatrix*}
        \end{equation}
        A transformation $A$ such that $A^{T}JA=J$ is called a symplectic or
        a canonical transformation.
        \par\hfill\par
        If $\langle\cdot|\cdot\rangle$ is the Lorentz form on $\mathbb{R}^{4}$,
        then an orthogonal transformation is called a Lorentz transformation.
    \section{Sesquilinear Geometry}
        Let $V$ and $W$ be $\mathbb{R}$ inner product spaces. That is, $V$ and
        $W$ are equipped with a symmetric bilinear form
        $\langle\cdot|\cdot\rangle_{V}$ and $\langle\cdot|\cdot\rangle_{W}$ that
        are positive-definite:
        \begin{equation}
            \langle{v}|v\rangle\geq{0}
        \end{equation}
        With equality if and only if $v=0$. Note that positive definite implies
        nondegenerate since $\langle{v}|v\rangle>0$ for nonzero $v$. Let
        $T:V\rightarrow{W}$ be a linear map. Then $T$ induces
        $T^{*}:W^{*}\rightarrow{V}^{*}$ by something.
        Let $V$ be a finite dimension $\mathbb{R}$ vector space. An inner
        product on $V$ is a bilinear form that is symmetric and
        positive-definite. Euclidean geometry is derived from the inner product.
        This induces a norm and the notion of angle:
        \begin{equation}
            \norm{v}=\sqrt{\langle{v}|v\rangle}
            \quad\quad
            \theta=\cos^{\minus{1}}\Big(
                \frac{\langle{v}|w\rangle}{\norm{v}\norm{w}}\Big)
        \end{equation}
        Cauchy-Schwartz comes out of this. Let $V$ be a vector space over
        $\mathbb{C}$, say $V=\mathbb{C}^{n}$. If we define:
        \begin{equation}
            \langle{v}|w\rangle=\sum_{k=1}^{n}v_{k}w_{k}
        \end{equation}
        We lose positive-definiteness since $(1,2i)\cdot(1,2i)=\minus{3}$,
        in $\mathbb{C}^{2}$, for example. We define the dot product on
        $\mathbb{C}^{n}$ by:
        \begin{equation}
            \langle{z}|w\rangle=\sum_{k=1}^{n}\overline{z}_{k}\overline{w}_{k}
        \end{equation}
        That is, $\langle{v}|w\rangle=\overline{v}\cdot{w}$. From this we have
        that $\langle{z}|\cdot\rangle$ is linear on $\mathbb{C}$. However,
        looking at $\langle\cdot|w\rangle$, we have that it is $\mathbb{R}$
        linear but only conjugate linear over $\mathbb{C}$. This gives rise to
        the notion of a sesquilinear product.
        \begin{fdefinition}{Sesquilinear Product}{Sesquilinear_Product}
            A sesquilinear product on a vector space $V$ over $\mathbb{C}$ is a
            function $\langle\cdot|\cdot\rangle:V\times{V}\rightarrow\mathbb{C}$
            such that $\langle{z}|\cdot\rangle$ is $\mathbb{C}$ linear and
            $\langle\cdot|w\rangle$ is $\mathbb{R}$ linear and $\mathbb{C}$
            conjugate linear.
        \end{fdefinition}
        With this we can define a Hermitian form on a $\mathbb{C}$ vector space
        $V$. Note that a sesquilinear form is conjugate symmetric. That is,
        $\langle{w}|z\rangle=\overline{\langle{z}|w\rangle}$.
        \begin{fdefinition}{Hermitian Form}{Hermitian_Form}
            A Hermitian form on a $\mathbb{C}$ vector space $V$ is a function
            $\langle\cdot|\cdot\rangle:V\times{V}\rightarrow\mathbb{C}$ that is
            sesquilinear and conjugate symmetric.
        \end{fdefinition}
        \begin{fdefinition}{Hermitian Inner Product Space}
                           {Hermitian_Inner_Product_Space}
            A Hermitian inner product space is a vector space $V$ over
            $\mathbb{C}$ with a Hermitian inner product.
        \end{fdefinition}
        Let $\mathcal{B}$ be a basis of a Hermitian inner product space $V$ and
        let $G_{\mathcal{B}}=[g_{ij}]_{ij}$ where
        $g_{ij}=\langle{e}_{i}|e_{j}\rangle$. Then for $v,w\in{V}$, we have:
        \begin{equation}
            v=\sum_{k=1}^{n}a_{k}e_{k}
            \quad\quad
            w=\sum_{k=1}^{n}b_{k}e_{k}
        \end{equation}
        And moreover:
        \begin{equation}
            \langle{v}|w\rangle=
            \langle\sum_{j=1}^{n}a_{j}e_{j}|\sum_{k=1}^{n}b_{k}e_{k}\rangle
            =\sum_{k=1}^{n}\sum_{j=1}^{n}\overline{a}_{j}b_{k}
                \langle{e_{j}}|e_{k}\rangle
            =\sum_{j=1}^{n}\sum_{k=1}^{n}\overline{a}_{j}g_{jk}b_{k}
        \end{equation}
        So we have:
        \begin{equation}
            \langle{v}|w\rangle
            =\overline{[v]_{\mathcal{B}}^{T}}G_{\mathcal{B}}[w]_{\mathcal{B}}
        \end{equation}
        This gives rise to the definition of a Hermitian transpose.
        \begin{fdefinition}{Hermitian Transpose}{Hermitian_Transpose}
            The Hermitian transpose of a matrix $A$ is the matrix:
            \begin{equation*}
                A^{H}=\overline{A^{T}}
            \end{equation*}
        \end{fdefinition}
    \section{Unitary Transformations}
        \begin{fdefinition}{Unitary Transformations}{Unitary_Transformations}
            A unitary transformation on a vector space $V$ over $\mathbb{C}$ is
            a linear function $T:V\rightarrow{V}$ such that:
            \begin{equation*}
                \langle{T}(v)|T(w)\rangle=\langle{v|w}\rangle
            \end{equation*}
        \end{fdefinition}
        Let $V$ and $W$ be Hermitian inner product spaces and let
        $T:V\rightarrow{W}$ be $\mathbb{C}$ linear. $T$ induces
        $T^{*}:W^{*}\rightarrow{V}^{*}$ by $T^{*}(\psi)=\psi\circ{T}$.
        \begin{fdefinition}{Self-Adjoint Hermitian Operator}
                           {Self-Adjoint_Hermitian_Operator}
            A self-adjoint operator on a Hermitian inner product space $V$ is a
            linear function $T:\mathbb{C}\rightarrow\mathbb{C}$ such that
            $T=T^{H}$.
        \end{fdefinition}
        \begin{fdefinition}{Normal Hermitian Operator}
                           {Normal_Hermitian_Operator}
            A function $T:\mathbb{C}\rightarrow\mathbb{C}$ such that
            $TT^{H}=T^{H}T$
        \end{fdefinition}
    \section{Spectral Theorem}
        \begin{theorem}
            If $V$ is a finite dimensional vector space over $\mathbb{C}$, if
            $T:V\rightarrow{V}$ is a linear map, then $T$ has an eigenvalue.
        \end{theorem}
        \begin{proof}
            For the characteristic polynomial is non-constant and thus by the
            fundamental theorem of algebra there exists a root.
        \end{proof}
        \begin{theorem}
            If $V$ is a finite dimensional Hermitian inner product space and if
            $T:V\rightarrow{V}$ is a Hermitian operator, then the eigenvalues of
            $T$ are real and if $v$ is an eigenvector of $\lambda$ and if $w$ is
            a $\mu$ eigenvector for two different eignvalues $\lambda\ne\mu$,
            then $v$ and $w$ are orthogonal.
        \end{theorem}
        \begin{proof}
            For let $\lambda$ be an eigenvalue and let $v$ be a non-zero
            eigenvector for $\lambda$. Then $T(v)=\lambda{v}$. But $T$ is
            Hermitian, and therefore:
            \begin{equation}
                \langle{T}^{H}(v)|v\rangle
                =\langle{v}|T(v)\rangle
                =\langle{v}|\lambda{v}\rangle
                =\lambda\rangle{v}|v\rangle
            \end{equation}
            But also:
            \begin{equation}
                \langle{T}^{H}(v)|v\rangle
                =\langle{T}(v)|v\rangle
                =\langle{\lambda}v|v\rangle
                =\overline{\lambda}\langle{v}|v\rangle
            \end{equation}
            And therefore, since $\langle{v}|v\rangle\ne{0}$, we have
            $\lambda=\overline{\lambda}$, and therefore $\lambda$ is real. For
            the second part, we have:
            \begin{equation}
                \langle{v}|T(w)\rangle
                =\langle{v}|\mu{w}\rangle
                =\mu\langle{v}|w\rangle
                =\langle{T}^{H}(v)|w\rangle
                =\langle{T}(v)|w\rangle
                =\langle\lambda{v}|w\rangle
                =\overline{\lambda}\langle{v}|w\rangle
            \end{equation}
            But we just proved that $\lambda$ is real, and thus
            $\overline{\lambda}=\lambda$. Moreover $\lambda\ne\mu$, and thus
            for equality to occur we must have $\langle{v}|w\rangle=0$.
        \end{proof}
        \begin{theorem}
            If $V$ is a finite dimensional Hermitian inner product space, if
            $T:V\rightarrow{V}$ is linear, and if $W\subseteq{V}$ is a $T$
            invariant subspace (that is, $T(W)\subseteq{W}$), then
            $W^{\perp}$ is $T^{H}$ invariant.
        \end{theorem}
        \begin{proof}
            For let $x\in{W}^{\perp}$ and let $w\in{W}$. Then:
            \begin{equation}
                \langle{T}^{H}(x)|w\rangle
                =\langle{x}|T(w)\rangle=0
            \end{equation}
            And thus $T(x)\in{W}^{\perp}$. Therefore,
            $T(W^{\perp})\subseteq{W}^{\perp}$.
        \end{proof}
        \begin{ltheorem}{Unitary Triangulation Theorem}
                        {Unitary_Triangulation_Theorem}
            If $V$ is a finite dimensional Hermitian inner product space over
            $V$ and if $T:V\rightarrow{V}$ is a linear operator, then there
            exists an orthonormal basis $\mathscr{B}$ of $V$ such that
            $[T]_{\mathscr{B}}^{\mathscr{B}}$ is upper triangular.
        \end{ltheorem}
        \begin{proof}
            For consider $T^{H}:V\rightarrow{V}$. It has an eigenvalue
            $\lambda$. Let $v$ be a $\lambda$ eigenvector of $T^{H}$ and let
            $W=\mathbb{C}\cdot{v}=\textrm{Span}\{zv:z\in\mathbb{C}\}$. Since
            $v$ is an eigenvector of $T^{H}$ we have that $W$ is a $T^{H}$
            invariant subspace so therefore $W^{\perp}$ is invariant under
            $(T^{H})^{H}=T$. That is, $W^{\perp}$ is a $T$ invariant subspace.
            Since $W$ is non-zero, $W^{\perp}$ has dimension less than $V$ and
            thus by induction there is an orthonormal basis of $W^{\perp}$ such
            that $[T_{W^{\perp}}]_{\mathscr{B}'}^{\mathscr{B}'}$ is upper
            triangular. Extending $\mathscr{B}$ from $\mathscr{B}'$ gives an
            orthonormal basis such that $[T]_{\mathscr{B}}^{\mathscr{B}}$ is
            upper triangular.
        \end{proof}
        \begin{theorem}
            If $A\in{M}_{n}(\mathbb{C})$ then there is a unitary matric
            $P\in{U}(n)$ such that $PAP^{\minus{1}}$ is upper triangular.
        \end{theorem}
        \begin{ftheorem}{Spectral Theorem for Hermitian Operators}
                        {Spectral_Theorem_for_Hermitian_Operators}
            If $V$ is a finite dimensional Hermitian inner product space and if
            $T:V\rightarrow{V}$ is a Hermitian operator then there exists an
            orthonormal basis $\mathscr{B}$ of $V$ such that
            $[T]_{\mathscr{B}}^{\mathscr{B}}$ is diagonal.
        \end{ftheorem}
        \begin{proof}
            For the representing matrix of $T$ is upper triangular, and thus the
            representing matrix for $T^{H}$ is lower triangular. But $T=T^{H}$
            and therefore the represeting matrix of $T$ is both upper and lower
            triangular, and therefore it is diagonal.
        \end{proof}
        The theorem holds for normal operators as well. For Hermitian we see
        that the eigenvalues are real. For skew-Hermitian ($T=\minus{T}^{H}$)
        the eigenvalues are purely imaginary, and lastly for a unitary $T$ the
        eigenvalues are unit modulus.
        \begin{ftheorem}{Real Spectral Theorem}
            If $V$ is a finite dimensional inner product space, if
            $T:V\rightarrow{V}$ is a self-adjoint operator then there is an
            orthonormal basis of $V$ such that the representing matrix is
            diagonal.
        \end{ftheorem}
        \begin{proof}
            Let $A$ be the representing matrix of $T$ relative to the standard
            basis of $\mathbb{R}^{n}$. Then $A=A^{T}$ and thus $A$ defines a
            linear map $A:\mathbb{C}^{n}\rightarrow\mathbb{C}^{n}$ by mapping
            $A(v)=Av$ as a matrix operatoion. Then $A$ is a Hermitian operator
            and therefore $A$ has real eigenvalues. Let $v\in\mathbb{C}^{n}$ be
            a complex eigenvector for $\lambda$. Let $v=x+iy$ for
            $x,y\in\mathbb{R}^{n}$. Then since $Av=\lambda{v}$ we have:
            \begin{equation}
                Ax+iAy=\lambda{x}+i\lambda{y}
            \end{equation}
            But $A$ is real, as are $x$ and $y$, and thus we have:
            \begin{equation}
                Ax=\lambda{x}
                \quad\quad
                Ay=\lambda{y}
            \end{equation}
            But since $v$ is non-zero, at least one of $x$ or $y$ is non-zero.
            Thus $A$ has a real eigenvector, $x$ or $y$. Let $u$ be a unit
            real eigenvector and let $W$ be the real span of $u$. Then $W$ is
            $T$ invariant and therefore $W^{\perp}$ is $T^{H}$ invariant, but
            $T=T^{H}$. We complete the proof by induction.
        \end{proof}
    \section{Tensor Products}
        Let $R$ be a ring nd let $M$ be a right $R$ module and let $N$ be a
        left $R$ module. We seek a universal product $M\times{N}$ into
        $M\otimes_{R}N$. Examples of products: Dot products, cross products,
        bilinear forms, matrix multiplication. All of these are Biadditive.
        Matrices also have the \textit{balanced} property:
        \begin{equation}
            A(cB)=(Ac)B
        \end{equation}
        If the underlying ring is not commutative, we may not have perfect
        bilinearity and so we replace this requirement with the balanced
        property. So we seek a function
        $\beta:M\times{N}\rightarrow{M}\otimes_{R}N$ such that:
        \begin{align}
            \beta(a+b,c)&=\beta(a,b)+\beta(b,c)\tag{Left Additivity}\\
            \beta(a,b+c)&=\beta(a,b)+\beta(a,c)\tag{Right Additivity}\\
            \beta(ar,b)&=\beta(a,rb)\tag{Balanced}
        \end{align}
        Universal means that given any Abelian group $A$ and any biadditive
        $R$ balanced map $\mu:M\times{N}\rightarrow{A}$ there is a unique
        $\mathbb{Z}$ module homomorphism
        $\tilde{\mu}:M\otimes_{R}N\rightarrow{A}$ such that the diagram
        commutes. Since if it exists it is unique up to unique isomorphism, we
        need only construct such a thing. Long complicated construction to
        follow. By construction, $M\otimes_{R}N$ is generated by $m\otimes{n}$
        for all $m\in{M}$, $n\in{N}$. As a warning, not every element of
        $M\otimes_{R}N$ need be decomposable. It's folly to try to define a map
        $M\times{N}\rightarrow{A}$ by $m\otimes{n}\mapsto{f}(m,n)$ unless
        $f(m,n)$ is biadditive and $R$ balanced.
        \begin{theorem}
            If $N$ is a left $R$ module, then $R\otimes_{R}N$ is isomorphic to
            $N$ as a $\mathbb{Z}$ module.
        \end{theorem}
        \begin{proof}
            For let $\mu:R\times{N}\rightarrow{N}$ by defined by
            $\mu(r,n)=r\cdot{n}$. Then $\mu$ is biaddiative and balanced.
        \end{proof}
        The idea is to use scalar multiplication $R\times{N}\rightarrow{N}$ to
        construct such a module. The by the universal mapping property there
        exists a unique $\mathbb{Z}$ module homomorphism
        $\tilde{\mu}:R\otimes_{R}N\rightarrow{N}$ such that the diagram
        commutes. Define $\tilde{\mu}$ by:
        \begin{equation}
            \tilde{\mu}(r\otimes{n})=r\cdot{n}
        \end{equation}
        Define $j:N\rightarrow{R}\otimes_{R}N$ by $j(n)=1\otimes{n}$.
        This is a $\mathbb{Z}$ module since the tensor product is biaddiative
        and balanced. Moreover, $\tilde{\mu}$ and $j$ are inverses of each
        other.
        \begin{theorem}
            If $R$ is a ring, if $I$ is an ideal of $R$, and if $N$ is a left
            $R$ module, then:
            \begin{equation}
                R/I\otimes_{R}N\simeq{N}/IN
            \end{equation}
        \end{theorem}
        \begin{proof}
            For define $\mu:R/I\times{N}\rightarrow{N}/IN$ by:
            \begin{equation}
                \mu(\overline{r},n)=\mu(r+I,n)=rn+IN=\overline{rn}
            \end{equation}
            For $n\in{N}$ let $f_{N}:R\rightarrow{N}/IN$ be defined by
            $f_{n}(r)=rn+IN=\overline{rn}$. Then $f_{n}$ is a map of left
            $R$ modules and therefore induces a map $\overline{f}_{n}$ from
            $R/I$ to $N/IN$ by $\overline{r}\mapsto\overline{rn}$. By the
            universal mapping property there is a $\mathbb{Z}$ linear map
            $\tilde{\mu}$ such that the diagram commutes. So we have that
            $\tilde{\mu}(\overline{r}\otimes{n})=\overline{rn}$. We now want a
            map $N/IN\rightarrow{R}/I\otimes_{R}N$. Let $j$ be defined by
            $j(n)=\overline{1}\otimes{n}$. Then $j$ vanishes on generators of
            $IN$ and therefore induces $\overline{j}$ such that
            $\overline{n}\mapsto\overline{1}\otimes{n}$. Thus we now have two
            maps that are well defined and now we need only check that they are
            inverses of each other. And it is so. So we are done.
        \end{proof}
        \begin{theorem}
            Given a sequence of modules over $R$,
            $N'\rightarrow{N}\rightarrow{N}''\rightarrow{0}$,
            and suppose that for all left $R$ modules $Y$ the
            sequence:
            \begin{equation}
                0\rightarrow\textrm{Hom}_{R}(N'',Y)
                \rightarrow\textrm{Hom}_{R}(N,Y)
                \rightarrow\textrm{Hom}_{R}(N',Y)
            \end{equation}
            is exact, then the original sequence is exact.
        \end{theorem}
        \begin{proof}
            For let $Y=\textrm{coker}(v)=N''/v(N)$. Then since:
            \begin{equation}
                0\rightarrow\textrm{Hom}_{R}(N'',Y)
                \rightarrow\textrm{Hom}_{R}(N,Y)
                \rightarrow\textrm{Hom}_{R}(N',Y)
            \end{equation}
            is exact, we have that the canonical projection $\pi$ gets mapped
            to $v^{*}(\pi)$. But $v^{*}(\pi)=\pi\circ{v}$, and this is zero
            since $n\mapsto{v}(n)$ which maps to $0$ by $\pi$. Thus $v^{*}$ is
            surjective and injective. On the other side, let $Y=N''$. Then
            $\textrm{id}_{N''}$ mapts to $v$ under $v^{*}$, and thus
            $v$ maps to $0$ under $u^{*}$ since the sequence is exact, and
            thus $u\circ{v}=0$. Thus $\textrm{im}(u)\subseteq\textrm{ker}(u)$.
        \end{proof}
    \section{Tensor Products of Algebras}
        An algebra over a commutative ring $k$ is a left $k$ module with a ring
        structure such that the ring multiplication is compatible with scalar
        multiplication:
        \begin{equation}
            (\lambda\star{a})\cdot{b}=\lambda\star(a\cdot{b})
                =a\cdot(\lambda\star{b})
        \end{equation}
        An equivalent definition is a ring $A$ with a ring homomorphism
        $\varphi:k\rightarrow{Z}(A)$. We can also define an algebra in terms of
        the tensor product. An algebra over $k$ is a $k$ module $A$ with a
        homomorphism $\mu:A\otimes{A}\rightarrow{A}$ and a module homomorphism
        $\eta:k\rightarrow{A}$ such that some diagram commutes.
    \section{Radical Stuff}
        \begin{theorem}
            If $K/F$ is a radical Galois extension, then $\autgroup{F}{K}$
            is solvable.
        \end{theorem}
        \begin{proof}
            If $K$ is a radical field extension, then
            $K=F(\alpha_{1},\dots,\alpha_{n})$ with
            $\alpha_{i}^{n_{i}}\in{F}(\alpha_{i},\dots,\alpha_{i-1})$. We may
            assume that all of the $n_{i}$ are prime or equal to 1. For let
            $p$ be a prime that divides $n_{m}$. So then
            $n_{m}=p^{r}k$, where $p$ does not divide $k$. But then:
            \begin{equation}
                F(\alpha_{1},\dots,\alpha_{m})
                    =F(\alpha_{1},\dots,a_{m}^{k},a_{m}^{p^{r}})
            \end{equation}
            Since $\alpha_{m}^{k}$ and $\alpha_{m}^{p^{r}}$ can be generated
            by $\alpha_{m}$, the right side is a subset of the left. By Bezout's
            identity there exists $a,b$ such that $ak+bp^{r}=1$, since
            $k$ and $p^{r}$ are coprime. But then:
            \begin{equation}
                \alpha_{m}=(\alpha_{m}^{k})^{a}(\alpha_{m}^{p^{r}})^{b}
            \end{equation}
            and hence we have equality. Some stuff, and now we prove the theorem
            by induction on the length $m$ of elements $\alpha_{i}$. Since $K$
            is a Galois extension, it is normal, and hence $\alpha_{1}$ has
            another conjugate $\beta$, that is, a root of the minimal polynomial
            $m_{\alpha_{1}/F}(x)$, in $K$, call it $\beta$. But then
            $\alpha_{1}^{n_{1}}\in{F}$ implies that $\beta^{n_{1}}\in{F}$, and
            then $(\alpha_{1}/\beta)^{n_{1}}=1$.
        \end{proof}
    \section{DF Chapt 14}
        
\end{document}