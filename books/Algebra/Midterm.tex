%------------------------------------------------------------------------------%
\documentclass[crop=false,class=article]{standalone}                           %
%------------------------------Preamble----------------------------------------%
\makeatletter                                                                  %
    \def\input@path{{../../../}}                                               %
\makeatother                                                                   %
\input{preamble.tex}                                                           %
%----------------------------Main Document-------------------------------------%
\begin{document}
    \title{Midterm}
    \author{Ryan Maguire}
    \date{\vspace{-5ex}}
    \maketitle
    \section{}
        Let $(k,+,\cdot)$ be a commutative ring,
        $(M,\boldsymbol{+},\boldsymbol{\cdot})$ be a module over $k$, and let
        $k[x]$ be the ring of polynomials. That is, $k[x]$ is the space of
        finitely supported sequences of ring elements in $k$ equipped with
        sequence addition and the Cauchy product of two sequences. Let $M[x]$ be
        the same space but for sequences of elements in $M$. Give $M[x]$ the
        same pointwise addition and let
        $\star:k[x]\times{M}[x]\rightarrow{M}[x]$ be the following product:
        \begin{equation}
            r(x)\star{m}(x)=
            \sum_{i=0}^{N}\sum_{j=0}^{M}(r_{i}\boldsymbol{\cdot}m_{j})x^{i+j}
        \end{equation}
        This makes $M[x]$ a module over $k[x]$. For we have:
        \begin{equation}
            1(x)\star{m}(x)=\sum_{i=0}^{N}(1\boldsymbol{\cdot}m_{i})x^{i}
            =\sum_{i=0}^{N}m_{i}x^{i}
            =m(x)
        \end{equation}
        For the distributive law let $n$ and $m$ be polynomials in $M[x]$ and
        extend $n$ or $m$ with zeros so that we may write them as a sum over the
        same powers. Then we have:
        \begin{equation}
            r(x)\star(m(x)+n(x))=
            \sum_{i=0}^{N}\sum_{j=0}^{M}r_{i}
            \boldsymbol{\cdot}(m_{j}+n_{j})x^{i+j}
            =\sum_{i=0}^{N}\sum_{j=0}^{M}
                (r_{i}\boldsymbol{\cdot}m_{j}
                +r_{i}\boldsymbol{\cdot}n_{j})x^{i+j}
        \end{equation}
        Thinking of these as finitely supported sequences we'd be done here, but
        alas, they are polynomials. So:
        \begin{subequations}
            \begin{align}
                r(x)\star(m(x)+n(x))
                &=\sum_{i=0}^{N}\sum_{j=0}^{M}
                    r_{i}\boldsymbol{\cdot}m_{j}x^{i+j}
                    +\sum_{i=0}^{N}\sum_{j=0}^{M}
                    r_{i}\boldsymbol{\cdot}n_{j}x^{i+j}\\
                &=(r(x)\star{m}(x))+(r(x)\star{n}(x))
            \end{align}
        \end{subequations}
        Similarly for the distributive law over modules. Lastly, checking the
        compatibility of multiplication, we have:
        \begin{subequations}
            \begin{align}
                r(x)\star(s(x)\star{m}(x))
                &=r(x)\star\sum_{i=0}^{N}\sum_{j=0}^{M}s_{i}m_{j}x^{i+j}\\
                &=\sum_{\ell=0}^{L}\sum_{i=0}^{N}\sum_{j=0}^{M}
                    r_{\ell}s_{i}m_{j}x^{\ell+i+j}\\
                &=\Big(\sum_{\ell=0}^{L}\sum_{i=0}^{N}
                    r_{\ell}s_{i}x^{\ell+i}\Big)\star{m}(x)\\
                &=(r(x)\cdot{s}(x))\star{m}(x)
            \end{align}
        \end{subequations}
        Thus $M[x]$ is a $k[x]$ module. Let $\iota:M\rightarrow{M}[x]$ be the
        inclusion mapping. That is $m\mapsto{m}(x)$, where $m(x)$ is the
        constant polynomial $m(x)=m$. Suppose $N$ is a $k$ module and
        $\varphi:M\rightarrow{N}$ is a module homomorphism. If $\tilde{\varphi}$
        is a $k[x]$ module homomorphism from $M[x]$ to $N$ such that the diagram
        commutes then $(\tilde{\varphi}\circ\iota)(m)=\varphi(m)$. But:
        \begin{equation}
            (\tilde{\varphi}\circ\iota)(m)
            =\tilde{\varphi}(\iota(m))
            =\tilde{\varphi}(m)
            =\varphi(m)
        \end{equation}
        Thus let $\tilde{\varphi}(m(x))=\varphi(m_{0})$, where $m_{0}$ is the
        constant term of the polynomial $m(x)$. Then $\tilde{\varphi}$ makes the
        diagram commute. Moreover, from our chain of equalities, we have that
        such a map must be unique. Thus $\iota$ is universal.
    \section{}
        Put $A$ into rational canonical form. That is, the matrix $R_{A}$ where:
        \begin{equation}
            R_{A}=\textrm{diag}(C(f_{1}),\dots,C(f_{n}))
        \end{equation}
        where the $C(f_{1})$ are the companion matrices to the polynomials
        $f_{i}$ such that $f_{i}|f_{i+1}$ are minimal and whose product is the
        characteristic polynomial. $A$ is similar to $R_{A}$. But the rational
        canonical form for $A^{T}$ is also $R_{A}$ since $A$ and $A^{T}$ have
        the same characteristic polynomial and since:
        \begin{equation}
            \prod_{k=1}^{N}(\lambda_{k}I-A)=0
            \Leftrightarrow\prod_{k=1}^{N}(\lambda_{k}I-A^{T})=0
        \end{equation}
        and therefore $A$ and $A^{T}$ will have the same minimial polynomial,
        and thus their rational canonical forms will be made from the same
        block diagonal matrices. But if $A$ is similar to $R_{A}$ and
        $A^{T}$ is similar to $R_{A}$, then $A$ is similar to $A^{T}$ since
        similarity is an equivalence relation. Thus, $A$ is similar to its
        transpose.
    \section{}
        Since this equivalence relation is counting the number of similarity
        classes, it suffices to count the number of rational canonical matrices.
        This is because the rational canonical matrices of two similar matrices
        are identical. Looking at the first block, we have:
        \begin{equation}
            A_{1}=
            \begin{bmatrix}
                1&0&0\\
                0&0&1\\
                0&1&0
            \end{bmatrix}
            \quad\quad
            A_{2}=
            \begin{bmatrix}
                1&0&0\\
                0&0&1\\
                0&1&1
            \end{bmatrix}
        \end{equation}
        Lastly, looking at the entire block:
        \begin{align}
            A_{3}=
            \begin{bmatrix}
                0&0&1\\
                1&0&0\\
                0&1&0
            \end{bmatrix}
            \quad\quad
            A_{4}=
            \begin{bmatrix}
                0&0&1\\
                1&0&1\\
                0&1&1\\
            \end{bmatrix}\\
            A_{5}=
            \begin{bmatrix}
                0&0&1\\
                1&0&0\\
                0&1&1\\
            \end{bmatrix}
            \quad\quad
            A_{6}=
            \begin{bmatrix}
                0&0&1\\
                1&0&1\\
                0&1&0\\
            \end{bmatrix}
        \end{align}
        Thus there are 6 conjugacy classes. If two elements are in the same
        conjugacy class they have the same order, for:
        \begin{equation}
            a^{n}=(gxg^{\minus{1}})^{n}
            =gx^{n}g^{\minus{1}}
            =gg^{\minus{1}}
            =e
        \end{equation}
        If there is a smaller such integer for $a$, write $x=g^{\minus{1}}ag$,
        contradicting the claim. It is possible for two non-conjugate elements
        of a group to have the same order, for in $\mathbb{Z}_{2}^{2}$, which is
        Abelian, the conjugacy class of any element is itself, but every
        nonzero element has order 2. For $GL_{3}(\mathbb{Z}_{2})$ we can simply
        find the orders for the six matrices above.
        The orders are:
        \begin{table}
            \centering
            \begin{tabular}{c|c}
                $A_{n}$&$n$\\
                \hline
                $A_{1}$&2\\
                $A_{2}$&3\\
                $A_{3}$&3
            \end{tabular}
        \end{table}
        And thus we find that $A_{2}$ and $A_{3}$ have the same order but are of
        different conjugacy classes.
    \section{}
        Since $J$ is a real valued matrix, $\det(A^{2})=\det(A)^{2}\geq{0}$.
        But if $n$ is odd, then $\det(\minus{I})=(\minus{1})^{n}=\minus{1}$,
        a contradiction since $\det(A^{2})=\det(\minus{I})$. Therefore $n$ is
        even. From the fact that the characteristic polynomial of $J^{2}$ is
        $(\xi+1)^{n}$ we have that the characteristic polynomial of $J$ is
        $(\lambda^{2}+1)^{n}$. Over $\mathbb{R}$ the minimal polynomial is then
        $\lambda^{2}+1$. The rational canonical form is therefore:
        \begin{equation}
            R_{J}=
            \begin{bmatrix}
                0&\minus{1}&\dots&0&0\\
                1&0&\dots&0&0\\
                \vdots&\vdots&\ddots&\vdots&\vdots\\
                0&0&\dots&0&\minus{1}\\
                0&0&\dots&1&0
            \end{bmatrix}
        \end{equation}
        And this is similar to the block matrix:
        \begin{equation}
            \begin{bmatrix}
                0&\minus{I}\\
                I&0
            \end{bmatrix}
        \end{equation}
        Thus there is a basis for $V$ such that the representing matrix for $J$
        has the desired form.
    \section{}
        I don't know.
    \section{}
        Let $\textrm{Nil}(A)$ be the set of nilpotent element of $A$. This is an
        ideal. For if $r\in{A}$ and $n\in{N}$, then since $A$ is commutative we
        have that $(rn)^{k}=r^{k}n^{k}=r^{k}0=0$ for some $k\in\mathbb{N}$, and
        therefore $rn$ is nilpotent. Similarly, if $n,m\in\textrm{Nil}(A)$, then
        from the binomial theorem:
        \begin{equation}
            (n+m)^{k}=\sum_{i=1}^{k}\binom{k}{i}n^{k-i}y^{i}
        \end{equation}
        For large enough $k$ every term in this expansion is zero, and thus
        $n+m$ is nilpotent. If $\mathfrak{p}$ is a prime ideal, then
        $\textrm{Nil}(A)\subseteq\mathfrak{p}$. For if $n\in\textrm{Nil}(A)$,
        then $n^{k}=0$, which is contained in $\mathfrak{p}$, and therefore
        $n^{k-1}n=0$ so either $n\in\mathfrak{p}$ or $n^{k-1}\in\mathfrak{p}$.
        Continuing by induction we find that $n\in\mathfrak{p}$. Therefore
        $\textrm{Nil}(\mathfrak{p})=\textrm{Nil}(A)$ for any prime ideal.
        Therefore $A$ is reduced if and only if for every prime ideal, the
        only nilpotent element is zero.
        \par\hfill\par
        Let $R=\mathbb{Z}_{2}^{2}$. The prime ideals of this are the ideals
        generated by $(1,0)$ and $(0,1)$, both of which are domains
        (since they are essentially $\mathbb{Z}_{2}$). However
        $\mathbb{Z}_{2}^{2}$ is not a domain since $(1,0)\cdot(0,1)=(0,0)$,
        yet neither $(1,0)$ nor $(0,1)$ are the zero element.
        \section{}
        The characteristic polynomial is:
        \begin{equation}
            (\lambda-4)^{3}
        \end{equation}
        And from this we have eigenvalues $4$ with triple multiplicity. From
        this we compute the Jordan canonical form by:
        \begin{equation}
            J_{A}=
            \begin{bmatrix}
                4&1&0\\
                0&4&1\\
                0&0&4
            \end{bmatrix}
        \end{equation}
        For the rational canonical form we have again that the characteristic
        polynomial is $(\lambda-4)^{3}$. We have that $A-4I\ne{0}$ and
        $(A-4I)^{2}\ne{0}$, and lastly $(A-4I)^{3}=0$. So the minimal
        polynomial is simply:
        \begin{equation}
            (\lambda-4)^{3}=x^{3}-12x^{2}+48x-64
        \end{equation}
        Reading this off, we have that the rational canonical form is:
        \begin{equation}
            R_{A}=
            \begin{bmatrix*}[r]
                0&0&64\\
                1&0&\minus{48}\\
                0&1&12
            \end{bmatrix*}
        \end{equation}
        To find an invertible matrix $P$ such that $A=P^{\minus{1}}J_{A}P$ we
        set up the following augmented matrix:
        \begin{equation}
            \begin{bmatrix*}[r]
                3&1&0&\vline&4&1&0\\
                1&4&1&\vline&0&4&1\\
                3&\minus{2}&5&\vline&0&0&4
            \end{bmatrix*}
        \end{equation}
        Interchanging the two with elementary row operations gives us:
        \begin{equation}
            P^{\minus{1}}=
            \begin{bmatrix*}[r]
                0&0&1\\
                3&\minus{2}&1\\
                \minus{2}&1&\minus{1}
            \end{bmatrix*}
        \end{equation}
        This is indeed intervtible since it has non-zero determinant (and since
        it was obtain via elementary operations) and the inverse is:
        \begin{equation}
            P=
            \begin{bmatrix*}[r]
                \minus{1}&\minus{1}&\minus{2}\\
                \minus{1}&\minus{2}&\minus{3}\\
                1&0&0
            \end{bmatrix*}
        \end{equation}
\end{document}