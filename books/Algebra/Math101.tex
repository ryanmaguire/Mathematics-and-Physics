%------------------------------------------------------------------------------%
\documentclass{article}                                                        %
%------------------------------Preamble----------------------------------------%
\makeatletter                                                                  %
    \def\input@path{{../../}}                                                  %
\makeatother                                                                   %
\input{preamble.tex}                                                           %
%----------------------------Main Document-------------------------------------%
\begin{document}
    \title{Topics in Algebra}
    \author{Ryan Maguire}
    \date{\vspace{-5ex}}
    \maketitle
    \section{Stuff}
        If $\langle\cdot|\cdot\rangle$ is a symmetric bilinear form, it is
        represented by its Gram matrix relative to a bsis $\mathscr{B}$ of the
        finite dimensional vector space $\mathscr{B}$. We have:
        \begin{equation}
            \langle{v}|w\rangle
            =[V]_{\mathscr{B}}^{T}G_{\mathscr{B}}[W]\mathscr{B}
        \end{equation}
        Let $\langle\cdot|\cdot\rangle:V\times{V}\rightarrow{k}$ be bilinear,
        let $\mathscr{B}$ and $\mathscr{C}$ be bases of $V$, and let
        $G_{\mathscr{B}}$ and $G_{\mathscr{C}}$ be the corresponding Gram
        matrices. Let $x,y\in{V}$. Then:
        \begin{subequations}
            \begin{align}
                [x]_{\mathscr{B}}^{T}G_{\mathscr{B}}[y]_{\mathscr{B}}
                &=\langle{x}|y\rangle\\
                &=[x]_{\mathscr{C}}^{T}G_{\mathscr{C}}[Y]_{\mathscr{C}}\\
                &=[\textrm{Id}(x)]_{\mathscr{C}}^{T}G_{\mathscr{C}}
                    [\textrm{Id}(y)]_{\mathscr{C}}\\
                &=\Big([\textrm{Id}]_{\mathscr{C}}^{\mathscr{B}}
                    [x]_{\mathscr{B}}\Big)^{T}G_{\mathscr{C}}
                    \Big([\textrm{Id}_{\mathscr{C}}^{\mathscr{B}}]
                        [y]_{\mathscr{B}}\Big)
            \end{align}
        \end{subequations}
        From this, we obtain the formula for the Gram matrix:
        \begin{equation}
            G_{\mathscr{B}}=P^{T}G_{\mathscr{C}}P
        \end{equation}
        \begin{fdefinition}{Congruent Matrices}{Congruent_Matrices}
            Congruent matrices are matrices $A,B\in{M}_{n}(k)$ such that there
            is an invertible matrice $P\in{GL}_{n}(k)$ such that:
            \begin{equation}
                B=P^{T}AP
            \end{equation}
        \end{fdefinition}
    \section{Diagonalizing Real Quadratic Forms}
        \begin{ltheorem}{Sylvester's Theorem}{Sylvesters_Theorem}
            If $V$ is a finite dimensional vector space over $\mathbb{R}$ and if
            $\langle\cdot|\cdot\rangle$ is a symmetric bilinear form, then there
            exists a basis $\mathscr{B}$ of $V$ such that:
            \begin{equation}
                G_{\mathscr{B}}=
                \begin{bmatrix*}[r]
                    1&\dots&0&0&\dots&0&0&\dots&0\\
                    \vdots&\ddots&\vdots&\vdots&\ddots
                        &\vdots&\vdots&\ddots&\vdots\\
                    0&\dots&1&0&\dots&0&0&\dots&0\\
                    0&\dots&0&\minus{1}&\dots&0&0&\dots&0\\
                    \vdots&\ddots&\vdots&\vdots
                        &\ddots&\vdots&\vdots&\ddots&\vdots\\
                    0&\dots&0&0&\dots&\minus{1}&0&\dots&0\\
                    0&\dots&0&0&\dots&0&0&\dots&0\\
                    \vdots&\ddots&\vdots&\vdots&\ddots
                        &\vdots&\vdots&\ddots&\vdots\\
                    0&\dots&0&0&\dots&0&0&\dots&0\\
                \end{bmatrix*}
            \end{equation}
            Where there are $r$ 1's, $s$ negative 1's, and $t$ zeros, and
            $r$, $s$, and $t$ are uniquely determined.
        \end{ltheorem}
    \section{Orthogonal Transformations}
        Let $(V,\langle\cdot|\cdot\rangle)$ be a nondegenerate bilinear space.
        Then a linear map $T:V\rightarrow{V}$ is orthogonal if or a linear
        isometry if:
        \begin{equation}
            \langle{T}(v)|T(w)\rangle=\langle{v}|w\rangle
        \end{equation}
        Let $\mathscr{B}$ be a basis of $V$ and let $G_{\mathscr{B}}$ be the
        Gram matrix of $\langle\cdot|\cdot\rangle$. Let $A$ be the representing
        matrix of $T$ over the basis $\mathscr{B}$. Then:
        \begin{equation}
            \langle{v}|w\rangle
            =\langle{T}(v)|T(w)\rangle
            =[T(v)]_{\mathscr{B}}^{T}G_{\mathscr{B}}[T(w)]_{\mathscr{B}}
            =[v]_{\mathscr{B}}^{T}A^{T}G_{\mathscr{B}}A[w]_{\mathscr{B}}
        \end{equation}
        From this we can compute what the Gram matrix is:
        \begin{equation}
            G_{\mathscr{B}}=A^{T}G_{\mathscr{B}}A
        \end{equation}
        If $A$ represents an orthogonal transformation, then $A$ must satisfy
        this equation. In the special case of when $\mathscr{B}$ is orthonormal,
        then $G_{\mathscr{B}}$ is simply the identity matrix and thus we have
        that $A^{T}A=I$, or $A^{T}=A^{\minus{1}}$.
        \par\hfill\par
        A nondegenerate skew symmetric bilinear form on a $2n$ dimensional real
        vector space is called a symplectic form. The Gram matrix is:
        \begin{equation}
            J=
            \begin{bmatrix*}[r]
                0&I_{n}\\
                \minus{I}_{n}&0
            \end{bmatrix*}
        \end{equation}
        A transformation $A$ such that $A^{T}JA=J$ is called a symplectic or
        a canonical transformation.
        \par\hfill\par
        If $\langle\cdot|\cdot\rangle$ is the Lorentz form on $\mathbb{R}^{4}$,
        then an orthogonal transformation is called a Lorentz transformation.
    \section{Sesquilinear Geometry}
        Let $V$ and $W$ be $\mathbb{R}$ inner product spaces. That is, $V$ and
        $W$ are equipped with a symmetric bilinear form
        $\langle\cdot|\cdot\rangle_{V}$ and $\langle\cdot|\cdot\rangle_{W}$ that
        are positive-definite:
        \begin{equation}
            \langle{v}|v\rangle\geq{0}
        \end{equation}
        With equality if and only if $v=0$. Note that positive definite implies
        nondegenerate since $\langle{v}|v\rangle>0$ for nonzero $v$. Let
        $T:V\rightarrow{W}$ be a linear map. Then $T$ induces
        $T^{*}:W^{*}\rightarrow{V}^{*}$ by something.
        Let $V$ be a finite dimension $\mathbb{R}$ vector space. An inner
        product on $V$ is a bilinear form that is symmetric and
        positive-definite. Euclidean geometry is derived from the inner product.
        This induces a norm and the notion of angle:
        \begin{equation}
            \norm{v}=\sqrt{\langle{v}|v\rangle}
            \quad\quad
            \theta=\cos^{\minus{1}}\Big(
                \frac{\langle{v}|w\rangle}{\norm{v}\norm{w}}\Big)
        \end{equation}
        Cauchy-Schwartz comes out of this. Let $V$ be a vector space over
        $\mathbb{C}$, say $V=\mathbb{C}^{n}$. If we define:
        \begin{equation}
            \langle{v}|w\rangle=\sum_{k=1}^{n}v_{k}w_{k}
        \end{equation}
        We lose positive-definiteness since $(1,2i)\cdot(1,2i)=\minus{3}$,
        in $\mathbb{C}^{2}$, for example. We define the dot product on
        $\mathbb{C}^{n}$ by:
        \begin{equation}
            \langle{z}|w\rangle=\sum_{k=1}^{n}\overline{z}_{k}\overline{w}_{k}
        \end{equation}
        That is, $\langle{v}|w\rangle=\overline{v}\cdot{w}$. From this we have
        that $\langle{z}|\cdot\rangle$ is linear on $\mathbb{C}$. However,
        looking at $\langle\cdot|w\rangle$, we have that it is $\mathbb{R}$
        linear but only conjugate linear over $\mathbb{C}$. This gives rise to
        the notion of a sesquilinear product.
        \begin{fdefinition}{Sesquilinear Product}{Sesquilinear_Product}
            A sesquilinear product on a vector space $V$ over $\mathbb{C}$ is a
            function $\langle\cdot|\cdot\rangle:V\times{V}\rightarrow\mathbb{C}$
            such that $\langle{z}|\cdot\rangle$ is $\mathbb{C}$ linear and
            $\langle\cdot|w\rangle$ is $\mathbb{R}$ linear and $\mathbb{C}$
            conjugate linear.
        \end{fdefinition}
        With this we can define a Hermitian form on a $\mathbb{C}$ vector space
        $V$. Note that a sesquilinear form is conjugate symmetric. That is,
        $\langle{w}|z\rangle=\overline{\langle{z}|w\rangle}$.
        \begin{fdefinition}{Hermitian Form}{Hermitian_Form}
            A Hermitian form on a $\mathbb{C}$ vector space $V$ is a function
            $\langle\cdot|\cdot\rangle:V\times{V}\rightarrow\mathbb{C}$ that is
            sesquilinear and conjugate symmetric.
        \end{fdefinition}
        \begin{fdefinition}{Hermitian Inner Product Space}
                           {Hermitian_Inner_Product_Space}
            A Hermitian inner product space is a vector space $V$ over
            $\mathbb{C}$ with a Hermitian inner product.
        \end{fdefinition}
        Let $\mathcal{B}$ be a basis of a Hermitian inner product space $V$ and
        let $G_{\mathcal{B}}=[g_{ij}]_{ij}$ where
        $g_{ij}=\langle{e}_{i}|e_{j}\rangle$. Then for $v,w\in{V}$, we have:
        \begin{equation}
            v=\sum_{k=1}^{n}a_{k}e_{k}
            \quad\quad
            w=\sum_{k=1}^{n}b_{k}e_{k}
        \end{equation}
        And moreover:
        \begin{equation}
            \langle{v}|w\rangle=
            \langle\sum_{j=1}^{n}a_{j}e_{j}|\sum_{k=1}^{n}b_{k}e_{k}\rangle
            =\sum_{k=1}^{n}\sum_{j=1}^{n}\overline{a}_{j}b_{k}
                \langle{e_{j}}|e_{k}\rangle
            =\sum_{j=1}^{n}\sum_{k=1}^{n}\overline{a}_{j}g_{jk}b_{k}
        \end{equation}
        So we have:
        \begin{equation}
            \langle{v}|w\rangle
            =\overline{[v]_{\mathcal{B}}^{T}}G_{\mathcal{B}}[w]_{\mathcal{B}}
        \end{equation}
        This gives rise to the definition of a Hermitian transpose.
        \begin{fdefinition}{Hermitian Transpose}{Hermitian_Transpose}
            The Hermitian transpose of a matrix $A$ is the matrix:
            \begin{equation*}
                A^{H}=\overline{A^{T}}
            \end{equation*}
        \end{fdefinition}
    \section{Unitary Transformations}
        \begin{fdefinition}{Unitary Transformations}{Unitary_Transformations}
            A unitary transformation on a vector space $V$ over $\mathbb{C}$ is
            a linear function $T:V\rightarrow{V}$ such that:
            \begin{equation*}
                \langle{T}(v)|T(w)\rangle=\langle{v|w}\rangle
            \end{equation*}
        \end{fdefinition}
        Let $V$ and $W$ be Hermitian inner product spaces and let
        $T:V\rightarrow{W}$ be $\mathbb{C}$ linear. $T$ induces
        $T^{*}:W^{*}\rightarrow{V}^{*}$ by $T^{*}(\psi)=\psi\circ{T}$.
        \begin{fdefinition}{Self-Adjoint Hermitian Operator}
                           {Self-Adjoint_Hermitian_Operator}
            A self-adjoint operator on a Hermitian inner product space $V$ is a
            linear function $T:\mathbb{C}\rightarrow\mathbb{C}$ such that
            $T=T^{H}$.
        \end{fdefinition}
        \begin{fdefinition}{Normal Hermitian Operator}
                           {Normal_Hermitian_Operator}
            A function $T:\mathbb{C}\rightarrow\mathbb{C}$ such that
            $TT^{H}=T^{H}T$
        \end{fdefinition}
        \section{Homework I}
        \begin{problem}
            Let $k$ be a field, let $V$ and $W$ be finite dimensional vector
            spaces over $k$, and let $T:V\rightarrow{W}$ be a linear
            transformations. Show that there exists bases $\mathscr{B}$ of $V$
            and $\mathscr{C}$ of $W$ such that the representing matrix
            $[T]_{\mathscr{C}}^{\mathscr{B}}$ has the following form: All
            off-diagonal entries are zero, while all the entries lie in the set
            $\{0,\,1\}$. Give two proofs:
            \begin{enumerate}
                \item   An abstract proof (Rank-Nullity Theorem)
                \item   A computational proof [Recall row/column operations]
            \end{enumerate}
        \end{problem}
        \begin{solution}
            If $V$ and $W$ are finite dimensional vector spaces then there
            exists a basis $\mathcal{V}$ and $\mathcal{W}$ of $V$ and $W$,
            respectively, such that $\textrm{Card}(\mathcal{V})\in\mathbb{N}$
            and $\textrm{Card}(\mathcal{W})\in\mathbb{N}$. Let
            $m=\textrm{Card}(\mathcal{V})$ and $n=\textrm{Card}(\mathcal{W})$.
            We want a matrix $[T]_{\mathscr{C}}^{\mathscr{B}}$ such that:
            \begin{equation}
                T\mathbf{x}=T
                \begin{bmatrix}
                    x_{1}\\
                    \vdots\\
                    x_{m}
                \end{bmatrix}=
                \begin{bmatrix}
                    a_{11}&\cdots&a_{1m}\\
                    \vdots&\ddots&\vdots\\
                    a_{n1}&\cdots&a_{nm}
                \end{bmatrix}
                \begin{bmatrix}
                    x_{1}\\
                    \vdots\\
                    x_{m}
                \end{bmatrix}
            \end{equation}
            Where the $a_{ij}$ are zero off of the diagonal. First, removing
            this requirement, such a matrix does exists if we define
            $a_{ij}$ by:
            \begin{equation}
                \begin{bmatrix}
                    a_{1i}\\
                    \vdots\\
                    a_{ni}
                \end{bmatrix}=Tv_{i}
            \end{equation}
            To obtain bases of the form we desire, write:
            \begin{equation}
                \begin{bmatrix}
                    y_{1}\\
                    \vdots\\
                    y_{n}
                \end{bmatrix}=
                \begin{bmatrix}
                    a_{11}&\cdots&a_{1m}\\
                    \vdots&\ddots&\vdots\\
                    a_{n1}&\cdots&a_{nm}
                \end{bmatrix}
                \begin{bmatrix}
                    x_{1}\\
                    \vdots\\
                    x_{m}
                \end{bmatrix}
            \end{equation}
            We can row reduce this matrix by means of Gaussian elimination,
            keeping track of the row operations as applied to the $x_{i}$
            and $y_{j}$, so that the matrix has only entries $0$ or $1$.
            That is, place the matrix into row-echelon form. To obtain a
            matrix that has only zeroes of ones on the diagonal, further
            reduce this matrix by column operations, placing the matrix
            in \textit{column} echelon form. The resulting matrix will thus
            be both row reduced and column reduced, and is therefore only
            non-zero on the diagonal. The bases $\mathscr{B}$ and
            $\mathscr{C}$ are obtained from these operations.
            \par\hfill\par
            Going the abstract way, by the rank-nullity theorem:
            \begin{equation}
                \textrm{dim}\big(T(V)\big)+
                \textrm{dim}\big(\ker(T)\big)=
                \textrm{dim}(V)=m
            \end{equation}
            Moreover the kernel of $T$ is a subspace of $V$. Let
            $\mathcal{N}$ be a basis of this and let $\mathcal{O}$ be such
            that $\mathcal{O}\cup\mathcal{N}$ is a basis of $V$, and such
            that $\mathcal{O}\cap\mathcal{N}=\emptyset$.
            Define $\mathscr{B}=\mathcal{O}\cup\mathcal{N}$. By the
            definition of the kernel, for all $e\in\mathcal{N}$, $Te=0$.
            Moreover, $T\mathcal{O}$ is a basis for the image of $T$.
            Let $w_{i}=Tv_{i}$ for $v_{i}\in\mathcal{O}$. We can then
            extend $T\mathcal{O}$ with a set $\mathcal{U}$ such that
            $T\mathcal{O}\cup\mathcal{U}$ is a basis of $W$ and such that
            $T\mathcal{O}\cap\mathcal{U}=\emptyset$. None of the
            points in this span (other than zero) are mapped to as they lie
            outside of the image. Let
            $\mathscr{C}=T\mathcal{O}\cup\mathcal{U}$.
            The representing matrix $[T]_{\mathscr{C}}^{\mathscr{B}}$ will
            then have elements only in $\{0,1\}$, and by construction all
            off-diagonal elements will be zero.
        \end{solution}
        \begin{problem}
            Let $V$ be a finite dimensional vector space and
            $T:V\rightarrow{V}$ a linear operator. When does $V$ have a
            basis $\mathscr{B}$ such that $[T]_{\mathscr{B}}^{\mathscr{B}}$
            is a diagonal matrix with all diagonal entires in the set
            $\{0,1\}$? Find a necessary and sufficient condition on $T$.
            Why was no condition needed in the previous problem?
        \end{problem}
        \begin{solution}
            No condition was required in the previous problem sinced we
            allowed the bases of both the domain and the target space to
            vary. Now we require that both bases are identical. If
            $T$ is idempotent, then $T$ will have a basis such that
            $[T]_{\mathscr{B}}^{\mathscr{B}}$ is non-zero only on the
            diagonal, and having values in $\{0,1\}$ on the diagonal.
            Let $\mathcal{N}$ be a basis for $\ker(T)$, and let
            $\mathcal{B}$ be a basis for image of $T$. By the rank-nullity
            theorem, $\mathcal{B}\cup\mathcal{N}$ is a basis for $V$.
            Let $\mathscr{B}=\mathcal{N}\cup{T}\mathcal{B}$, where
            $T\mathcal{B}$ is the set of all $Tv$ such that
            $v\in\mathcal{B}$. Then this is a basis for $V$. Moreover,
            the representative matrix is only non-zero on the diagonal.
            For if $v\in{T}\mathcal{B}$, then:
            \begin{equation}
                Tv_{i}=\sum_{j}a_{ij}v_{j}
                \Longrightarrow
                T^{2}v_{i}=\sum_{j}a_{ij}Tv_{j}
            \end{equation}
            But $T$ is idempotent, so we have:
            \begin{equation}
                Tv_{i}=\sum_{j}a_{ij}Tv_{j}
                \Longrightarrow
                T(v_{i}-\sum_{j}a_{ij}v_{i})=0
                \Longrightarrow
                v_{i}-\sum_{j}a_{ij}v_{i}=0
            \end{equation}
            But then $v_{j}$ are independent, and therefore
            $a_{ij}=0$ for all $i\ne{j}$.
            \par\hfill\par
            Going the other way, if there is a basis $\mathscr{B}$ such
            that $[T]_{\mathscr{B}}^{\mathscr{B}}$ is non-zero only on
            the diagonal, and whose diagonal elements are either $0$ or
            $1$, then $T$ is idempotent. For If $T$ has such a
            representation, then $T^{2}=T$, since all non-diagonal
            components are zero and for all $a_{ii}$ on the diagonal,
            $a_{ii}^{2}=a_{ii}$.
        \end{solution}
        \begin{problem}
            Let $R_{1}$ and $R_{2}$ be rings. Let $M$ be a module over
            the product ring $R=R_{1}\times{R}_{2}$. Show that $M$ is
            the direct sum $M=M_{1}\oplus{M}_{2}$ of two
            $R$ submodules $M_{1}$ and $M_{2}$ where $M_{1}$ is an
            $R_{1}$ submodule (viewed as an $R$ module by pullback along
            the projection $\pi_{1}:R_{1}\times{R}_{2}\rightarrow{R}_{1}$
            defined by $(x,y)\mapsto{x}$) and $M_{2}$ is an
            $R_{2}$ module (viewed similarly as above).
        \end{problem}
        \begin{solution}
            For let $M_{1}$ and $M_{2}$ be defined as follows:
            \par
            \begin{subequations}
                \begin{minipage}[b]{0.49\textwidth}
                    \begin{equation}
                        M_{1}=\{\,(1_{1},\,0_{2})
                                \boldsymbol{\cdot}\mathbf{v}\,:\,
                                \mathbf{v}\in{M}\,\}
                    \end{equation}
                \end{minipage}
                \hfill
                \begin{minipage}[b]{0.49\textwidth}
                    \begin{equation}
                        M_{2}=\{\,(0_{1},\,1_{2})
                                \boldsymbol{\cdot}\mathbf{v}\,:\,
                                \mathbf{v}\in{M}\,\}
                    \end{equation}
                \end{minipage}
            \end{subequations}
            \par\vspace{2.5ex}
            Where $\boldsymbol{\cdot}$ is the multiplicative operation
            $\boldsymbol{\cdot}:R\times{M}\rightarrow{M}$,
            $1_{1}$ is the multiplicative identity of $R_{1}$,
            $1_{2}$ is the multiplicative identity of $R_{2}$,
            $0_{1}$ is the additive identity of $R_{1}$, and
            $0_{2}$ is the additive identity of $R_{2}$. Then
            $M_{1}$ is a submodule of $M$. For if $\mathbf{u}\in{M}_{1}$,
            then there is a $\mathbf{v}\in{M}$ such that
            $\mathbf{u}=(1_{1},0_{2})\boldsymbol{\cdot}\mathbf{v}$.
            But then:
            \begin{subequations}
                \begin{align}
                    (1_{1},\,1_{2})\boldsymbol{\cdot}\mathbf{u}
                    &=(1_{1},\,1_{2})\boldsymbol{\cdot}
                        \big[
                            (1_{1},\,0_{2})\boldsymbol{\cdot}\mathbf{v}
                        \big]\\
                    &=\big[(1_{1},\,1_{2})\cdot(1_{1},\,0_{2})\big]
                        \boldsymbol{\cdot}\mathbf{v}\\
                    &=(1_{1},\,0_{2})\boldsymbol{\cdot}\mathbf{v}\\
                    &=\mathbf{u}
                \end{align}
            \end{subequations}
            Associativity of ring elements with $M_{1}$ comes for free from
            the associativity of $M$, similarly for the distributive laws.
            Moreover, it is closed:
            \begin{equation}
                \mathbf{u}_{1}+\mathbf{u}_{2}=
                (1_{1},\,0_{2})\boldsymbol{\cdot}\mathbf{v}_{1}+
                (1_{1},\,0_{2})\boldsymbol{\cdot}\mathbf{v}_{2}
                =(1_{1},\,0_{2})\boldsymbol{\cdot}
                    \big[\mathbf{v}_{1}+\mathbf{v}_{2}\big]
            \end{equation}
            And this is an element of $M_{1}$. Similarly:
            \begin{equation}
                (a,\,b)\boldsymbol{\cdot}\mathbf{u}
                =(a,\,b)\boldsymbol{\cdot}
                    \big[
                        (1_{1},\,0_{2})\boldsymbol{\cdot}\mathbf{v}
                    \big]
                =\big[
                    (a,\,b)\cdot(1_{1},\,0_{2})
                \big]
                    \boldsymbol{\cdot}\mathbf{v}
                =(a,\,0_{2})\boldsymbol{\cdot}\mathbf{v}
            \end{equation}
            Which is again contained in $M_{1}$. Similarly, $M_{2}$ is an
            $R$ submodule. Moreover, $M=M_{1}\oplus{M}_{2}$ For if
            $\mathbf{v}\in{M}$, then:
            \begin{equation}
                \mathbf{v}=(1_{1},\,0_{2})\boldsymbol{\cdot}\mathbf{v}+
                           (0_{1},\,1_{2})\boldsymbol{\cdot}\mathbf{v}
            \end{equation}
            And the left part of the sum is an element of $M_{1}$ and the
            right part is an element of $M_{2}$. It also follows that
            $M_{1}$ is an $R_{1}$ module, viewing along pullback by the
            projection $\pi_{1}(x,y)=x$. That is, form the operation
            $*_{1}:R_{1}\times{M}_{1}\rightarrow{M}_{1}$ by:
            \begin{equation}
                r*_{1}\mathbf{m}=(r,\,0_{2})\boldsymbol{\cdot}\mathbf{m}
                \quad\quad
                r\in{R}_{1},\,\mathbf{m}\in{M}_{1}
            \end{equation}
            Similarly for $M_{2}$:
            \begin{equation}
                r*_{2}\mathbf{m}=(0_{1},\,r)\boldsymbol{\cdot}\mathbf{m}
                \quad\quad
                r\in{R}_{2},\,\mathbf{m}\in{M}_{2}
            \end{equation}
        \end{solution}
        \begin{problem}
            The equivalence class one.
        \end{problem}
        \begin{problem}
            Let $A$ and $B$ be rings and let $R=A\times{B}$ be the
            direct product ring. Show that $R$ contains elements $e_{1}$
            and $e_{2}$ such that:
            \begin{enumerate}
                \item Both $e_{i}$ are idempotent.
                \item Each $e_{i}$ is central.
                \item They are orthogonal: $e_{1}\cdot{e}_{2}=0_{R}$.
                \item They are complete: $e_{1}+e_{2}=1_{R}$.
            \end{enumerate}
            Conversely, show if $R$ is a ring with two such elements then
            there are rings $A$ and $B$ such that $R$ is isomorphic to
            $A\times{B}$.
        \end{problem}
        \begin{solution}
            For let $e_{1}$ and $e_{2}$ be defined as follows:
            \par\hfill\par
            \begin{subequations}
                \begin{minipage}[b]{0.49\textwidth}
                    \begin{equation}
                        e_{1}=(1_{A},\,0_{B})
                    \end{equation}
                \end{minipage}
                \hfill
                \begin{minipage}[b]{0.49\textwidth}
                    \begin{equation}
                        e_{2}=(0_{A},\,1_{B})
                    \end{equation}
                \end{minipage}
            \end{subequations}
            \par\vspace{2.5ex}
            Where $0_{\alpha}$ and $1_{\alpha}$ are the additive and
            multiplicative identities of their respective rings,
            respectively. Then:
            \begin{subequations}
                \begin{align}
                    e_{1}\cdot{e}_{1}
                    =(1_{A},\,0_{B})\cdot(1_{A},\,0_{B})
                    =(1_{A}*_{A}1_{A},\,0_{B}*_{B}0_{B})
                    =(1_{A},\,0_{B})
                    =e_{1}\\
                    e_{2}\cdot{e}_{2}
                    =(0_{A},\,1_{B})\cdot(0_{A},\,1_{B})
                    =(0_{A}*_{A}0_{A},\,1_{B}*_{B}1_{B})
                    =(0_{A},\,1_{B})
                    =e_{2}
                \end{align}
            \end{subequations}
            Thus satisfying idempotency. They are central, for:
            \begin{subequations}
                \begin{align}
                    e_{1}\cdot{r}
                    &=(1_{A},\,0_{B})\cdot(r_{1},\,r_{2})\\
                    &=(1_{A}*_{A}r_{1},\,0_{B}*_{B}r_{2})\\
                    &=(r_{1}*_{A}1_{A},\,r_{2}*_{B}0_{B})\\
                    &=(r_{1},\,r_{2})\cdot(1_{A},\,0_{B})\\
                    &=r\cdot{e}_{1}
                \end{align}
            \end{subequations}
            Similarly for $e_{2}$. They are indeed orthogonal:
            \begin{equation}
                e_{1}\cdot{e}_{2}
                =(1_{A},\,0_{B})\cdot(0_{A},\,1_{B})
                =(1_{A}*_{A}0_{A},\,0_{B}*_{B}1_{B})
                =(0_{A},\,0_{B})
                =0_{R}
            \end{equation}
            Completeness follows from:
            \begin{equation}
                e_{1}+e_{1}
                =(1_{A},\,0_{B})+(0_{A},\,1_{B})
                =(1_{A}+0_{A},\,0_{B}+1_{B})
                =(1_{A},\,1_{B})
                =1_{R}
            \end{equation}
            Conversely, suppose such elements $e_{1}$ and $e_{2}$ exist.
            Let $A$ and $B$ be defined by:
            \par\hfill\par
            \begin{subequations}
                \begin{minipage}[b]{0.49\textwidth}
                    \begin{equation}
                        A=\{\,e_{1}\cdot{r}\,:\,r\in{R}\}
                    \end{equation}
                \end{minipage}
                \hfill
                \begin{minipage}[b]{0.49\textwidth}
                    \begin{equation}
                        B=\{\,e_{2}\cdot{r}\,:\,r\in{R}\}
                    \end{equation}
                \end{minipage}
            \end{subequations}
            \par\vspace{2.5ex}
            Let $\varphi:R\rightarrow{A}\times{B}$ be defined by:
            \begin{equation}
                \varphi(r)=(r\cdot{e}_{1},\,r\cdot{e}_{2})
            \end{equation}
            Then $\varphi$ is an isomorphism. Firstly, it is injective.
            For if $\varphi(r_{1})=\varphi(r_{2})$, then:
            \begin{equation}
                r_{1}\cdot{e}_{1}=r_{2}\cdot{e}_{1}
                \quad\quad
                \textrm{and}
                \quad\quad
                r_{1}\cdot{e}_{2}=r_{2}\cdot{e_{2}}
            \end{equation}
            From the completeness, summing these we obtain $r_{1}=r_{2}$.
            It is also surjective, for if $(r_{1},r_{2})\in{A}\times{B}$
            let $r=r_{1}+r_{2}$. Since $r_{1}\in{A}$ there is a
            $u_{1}\in{R}$ such that $r_{1}=u_{1}\cdot{e}_{1}$, and similarly
            a $u_{2}$ for $r_{2}$. From orthogonality and idempotency,
            we have:
            \begin{subequations}
                \begin{align}
                    \varphi(r)
                    &=\big((r_{1}+r_{2})\cdot{e}_{1},\,
                           (r_{1}+r_{2})\cdot{e}_{2})\big)\\
                    &=\big(
                        (u_{1}\cdot{e}_{1}+u_{2}\cdot{e}_{2})\cdot{e}_{1},\,
                        (u_{1}\cdot{e}_{1}+u_{2}\cdot{e}_{2})\cdot{e}_{2})
                    \big)\\
                    &=(u_{1}\cdot{e}_{1}^{2}+
                       u_{2}\cdot{e}_{1}\cdot{e}_{2},\,
                       u_{1}\cdot{e}_{1}\cdot{e}_{2}+
                       u_{2}\cdot{e}_{2}^{2})\\
                     &=(u_{1}\cdot{e}_{1},\,u_{2}\cdot{e}_{2})\\
                     &=(r_{1},\,r_{2})
                \end{align}
            \end{subequations}
            And thus $\varphi$ is a bijection. Lastly, it is a homomorphism.
            For, by idempotency and centrality:
            \begin{subequations}
                \begin{align}
                    \varphi(r\cdot{s})
                    &=\big((r\cdot{s})\cdot{e}_{1},\,
                           (r\cdot{s})\cdot{e}_{2}\big)\\
                    &=\big((r\cdot{e}_{1})\cdot(s\cdot{e}_{1}),\,
                           (r\cdot{e}_{1})\cdot(s\cdot{e}_{1})\big)\\
                    &=(r\cdot{e}_{1},\,s\cdot{e}_{2})\boldsymbol{\cdot}
                      (s\cdot{e}_{1},\,s\cdot{e}_{2})\\
                    &=\varphi(r)\boldsymbol{\cdot}\varphi(s)
                \end{align}
            \end{subequations}
            Similarly for addition:
            \begin{subequations}
                \begin{align}
                    \varphi(r+s)
                    &=\big((r+s)\cdot{e}_{1},\,
                           (r+s)\cdot{e}_{2}\big)\\
                    &=(r\cdot{e}_{1}+r\cdot{e}_{1},\,
                       r\cdot{e}_{2}+s\cdot{e}_{2})\\
                    &=(r\cdot{e}_{1},\,r\cdot{e}_{2})\boldsymbol{+}
                      (s\cdot{e}_{1},\,s\cdot{e}_{2})\\
                    &=\varphi(r)\boldsymbol{+}\varphi(s)
                \end{align}
            \end{subequations}
            Thus, $\varphi$ is a bijective homomorphism and is therefore
            an isomorphism. It follows that $R$ is isomorphic to
            $A\times{B}$.
        \end{solution}
        \begin{problem}
            \par\hfill\par
            \begin{enumerate}
                \item   Let $A$ and $B$ be rings, and let
                        $\mathfrak{a}\subseteq{A}$ and
                        $\mathfrak{b}\subseteq{B}$ be left ideals.
                        Show that $\mathfrak{a}\times\mathfrak{b}$ is a
                        left ideal of $A\times{B}$.
                \item   Show that if $\mathfrak{c}$ is a left ideal of
                        $A\times{B}$ then there are left ideals
                        $\mathfrak{a}\subseteq{A}$ and
                        $\mathfrak{b}\subseteq{B}$ such that
                        $\mathfrak{c}=\mathfrak{a}\times\mathfrak{b}$.
                \item   Show that, if $R$ is a ring and $M_{1}$ and $M_{2}$
                        are left $R$ modules, then it may not be true that
                        every submodule of $M_{1}\times{M}_{2}$ is of the
                        form $N_{1}\times{N}_{2}$, where
                        $N_{1}\subseteq{M}_{2}$ and $N_{2}\subseteq{M}_{2}$.
            \end{enumerate}
        \end{problem}
        \begin{solution}
            For if $\mathfrak{a}$ and $\mathfrak{b}$ are left ideals of
            $A$ and $B$, respectively, let
            $r_{1},r_{2}\in\mathfrak{a}\times\mathfrak{b}$. Then:
            \begin{equation}
                r_{1}\boldsymbol{+}r_{2}
                =(a_{1},\,b_{1})\boldsymbol{+}(a_{2},\,b_{2})
                =(a_{1}+a_{2},\,b_{1}+b_{2})
            \end{equation}
            But $\mathfrak{a}$ and $\mathfrak{b}$ are left ideals, and
            thus $a_{1}+a_{2}\in\mathfrak{a}$ and
            $b_{1}+b_{2}\in\mathfrak{b}$, and thus
            $r_{1}+r_{2}\in\mathfrak{a}\times\mathfrak{b}$. Moreover, if
            $r\in{A}\times{B}$ and $s\in\mathfrak{a}\times\mathfrak{b}$,
            then:
            \begin{equation}
                r\boldsymbol{\cdot}{s}
                =(r_{1},\,r_{2})\boldsymbol{\cdot}(s_{1},\,s_{2})
                =(r_{1}\cdot{s}_{2},\,r_{2}\cdot{s}_{2})
            \end{equation}
            But $\mathfrak{a}$ is a left ideal, and thus
            $r_{1}\cdot{s}_{1}\in\mathfrak{a}$. Similarly, $\mathfrak{b}$
            is a left ideal and thus $r_{2}\cdot{s}_{2}\in\mathfrak{b}$.
            Thus, $r\boldsymbol{\cdot}s\in\mathfrak{a}\times\mathfrak{b}$.
            That is, $\mathfrak{a}\times\mathfrak{b}$ is a left ideal of
            $A\times{B}$.
            \par\hfill\par
            If $\mathfrak{c}\subseteq{A}\times{B}$ is a left ideal,
            let $\mathfrak{a}=\pi_{1}(\mathfrak{c})$ and
            $\mathfrak{b}=\pi_{2}(\mathfrak{c})$, where $\pi_{1}$ and
            $\pi_{2}$ are the projection mappings. Then $\mathfrak{a}$ is
            a left ideal of $A$. For if $r,s\in\mathfrak{a}$, then
            $(r,0_{B})$ and $(s,0_{B})$ are elements of $\mathfrak{c}$,
            and thus:
            \begin{equation}
                (r,\,0_{B})\boldsymbol{+}(s,\,0_{B})
                =(r+s,\,0_{B}+0_{B})
                =(r+s,\,0_{B})
            \end{equation}
            Which is an element of $\mathfrak{c}$ since $\mathfrak{c}$ is
            a left ideal, and therefore $r+s\in\mathfrak{a}$. If
            $r\in\mathfrak{a}$ and $s\in{A}$, then:
            \begin{equation}
                (r,\,0_{B})\boldsymbol{\cdot}(s,\,0_{B})
                =(r\cdot{s},\,0_{B})
            \end{equation}
            Which is an element of $\mathfrak{c}$ since $\mathfrak{c}$
            is a left ideal, and therefore $r\cdot{s}\in\mathfrak{a}$. Thus,
            $\mathfrak{a}$ is a left ideal of $A$. Similarly,
            $\mathfrak{b}$ is a left ideal of $B$. Moreover, since
            $\pi_{1}$ and $\pi_{2}$ are simply the projection mappings,
            it follows that:
            \begin{equation}
                \mathfrak{c}=\mathfrak{a}\times\mathfrak{b}
            \end{equation}
            Thus completing the proof.
            \par\hfill\par
            Lastly, let $R=\mathbb{R}$ with it's usual field structure.
            Since all fields are also rings, $\mathbb{R}$ has a ring
            structure. Consider then the vector space of $\mathbb{R}$ over
            itself. Since all vector spaces are also modules, we have that
            $\mathbb{R}$ is an $\mathbb{R}$ module. The product of
            $\mathbb{R}$ with itself will be $\mathbb{R}^{2}$ with its
            usual vector space structure. Any line through the origin will
            then be subspace, however the only subspaces of
            $\mathbb{R}$ are $0$ and the entire real line itself. Thus,
            the only possible product spaces are:
            \begin{equation*}
                \mathbf{0}
                \quad\quad
                \{\,(r,\,0)\,:\,r\in\mathbb{R}\,\}
                \quad\quad
                \{\,(0,\,r)\,:\,r\in\mathbb{R}\,\}
                \quad\quad
                \mathbb{R}^{2}
            \end{equation*}
            Thus the subspace $\{(r,r):r\in\mathbb{R}\}$ cannot be written
            as a product.
        \end{solution}
        \begin{problem}
            \par\hfill\par
            \begin{enumerate}
                \item   Let $R$ be a ring and $M$ be a left $R$ module.
                        Then the set $\textrm{Hom}_{R}(R,M)$ of $R$ module
                        homomorphisms can be given an Abelian group
                        structure by defining $f+g$ as the map
                        $(f+g)(x)=f(x)+g(x)$ for all $x\in{R}$. Show that
                        there is an isomorphism between this group and $M$.
                \item   If $R$ is a commutative ring, then
                        $\textrm{Hom}_{R}(R,M)$ has the structure of an
                        $R$ module give by: if $r\in{R}$ and if
                        $f\in\textrm{Hom}_{R}(R,M)$, define
                        $(rf)(x)=r\cdot{f(x)}$ for all $x\in{R}$. Show that
                        this makes $\textrm{Hom}_{R}(R,M)$ an $R$ module
                        and that $\varepsilon$ from above is an isomorphism
                        of $R$ modules. If $R$ is a field, what simple
                        observation is this?
                \item   If $R$ is a ring and $M=R$, so
                        $\textrm{Hom}_{R}(R,M)=\textrm{End}_{R}(R)$ is
                        now the set of endomorphisms on $R$ as a left
                        $R$ module. Then $\textrm{End}_{R}(R)$ is a ring
                        where the product is given by function
                        composition. Is $\varepsilon$ as defined before
                        a ring isomorphism? If not, how can we modify this
                        so that it is?
            \end{enumerate}
        \end{problem}
        \begin{solution}
            For let $\varepsilon:\textrm{Hom}_{R}(R,M)\rightarrow{M}$ be
            defined by:
            \begin{equation}
                \varepsilon(f)=f(1)
            \end{equation}
            This is injective, for if $\varepsilon(f)=\varepsilon(g)$,
            then $f(1)=g(1)$. But then, for all $x\in{R}$:
            \begin{equation}
                f(x)=f(x\cdot{1})=xf(1)=xg(1)=g(x\cdot{1})=g(x)
            \end{equation}
            And thus $f=g$. Moreover, it is surjective. For let
            $\mathbf{m}\in{M}$ and let $f:R\rightarrow{M}$ be defined by:
            \begin{equation}
                f(r)=r\cdot\mathbf{m}
            \end{equation}
            From the module structure of $M$, $f\in\textrm{Hom}_{R}(R,M)$,
            but also $\varepsilon(f)=f(1)=m$. Therefore, $f$ is a bijection.
            Lastly, $\varepsilon$ is indeed a group homomorphism. For if
            $f,g\in\textrm{Hom}_{R}(R,M)$, then:
            \begin{equation}
                \varepsilon(f+g)=(f+g)(1)=f(1)+g(1)
                                =\varepsilon(f)+\varepsilon(g)
            \end{equation}
            Thus, $\varepsilon$ is an isomorphism.
            \par\hfill\par
            This is indeed an $R$ module. For if
            $f\in\textrm{Hom}_{R}(R,M)$, then:
            \begin{equation}
                (1f)(x)=1\cdot{f}(x)=f(x)
            \end{equation}
            Since $M$ is an $R$ module. If $r,s\in{R}$ and
            $f\in\textrm{Hom}_{R}(R,M)$, then:
            \begin{equation}
                \big((rs)f\big)(x)
                =(rs)f(x)
                =r\big(sf(x)\big)
                =\big(r(sf)\big)(x)
            \end{equation}
            Again from the module structure of $M$. The distributive law
            is satisfied as well:
            \begin{equation}
                \big((r+s)f)(x)=(r+s)f(x)=rf(x)+sf(x)=(rf)(x)+(sf)(x)
            \end{equation}
            And lastly, if $r\in{R}$ and $u,v\in{M}$, then:
            \begin{subequations}
                \begin{align}
                    \big(r(u+v)\big)(x)
                    &=r\big((u+v)(x)\big)\\
                    &=r\big(u(x)+rv(x)\big)\\
                    &=ru(x)+rv(x)\\
                    &=(ru)(x)+(rv)(x)
                \end{align}
            \end{subequations}
            Moreover, $\varepsilon$ is an isomorphism. We have seen that
            it is bijective and preserves addition, we must now check
            that it preserve multiplication from the ring:
            \begin{equation}
                \varepsilon(rf)
                =(rf)(1)
                =r\cdot{f}(1)
                =r\cdot\varepsilon(f)
            \end{equation}
            \par\hfill\par
            For part 3, it is not, in general, an isomorphism, since:
            \begin{equation}
                \varepsilon(f\circ{g})
                =f(g(1))
                =f(g(1)\cdot{1})
                =g(1)f(1)
            \end{equation}
            And this may not, necessarily, be equal to $f(1)g(1)$ if
            $R$ is not commutative. If we define a new operation
            $*$ by:
            \begin{equation}
                r*s=s\cdot{r}
            \end{equation}
            Then $\varepsilon$ will be a ring isomorphism from
            $\textrm{End}_{R}(R)$ to $R$, since:
            \begin{equation}
                \varepsilon(f\circ{g})=
                f(g(1))=g(1)f(1)=f(1)*g(1)
                =\varepsilon(f)*\varepsilon(g)
            \end{equation}
        \end{solution}
        And thus $\varepsilon$ is a ring isomorphism.
    \begin{problem}
        A $k$ algebra is a $k$ module $A$ with a ring structure such
        that ring multiplication and scalar multiplication by elements
        of $k$ satisfy:
        \begin{equation}
            (\lambda\cdot{a})b=\lambda\cdot(ab)=a(\lambda\cdot{b})
        \end{equation}
        Show that $A$ is a $k$ algebra if and only if $A$ is a ring such
        that there exists a ring homomorphism $\varphi:k\rightarrow{Z}(A)$,
        where $Z(A)$ is the center of $A$.
    \end{problem}
    \begin{solution}
        Going one way, suppose $A$ is a ring with homomorphism
        $\varphi:k\rightarrow{Z}(A)$. Define $\cdot$ by:
        \begin{equation}
            \lambda\cdot{a}=\varphi(\lambda)a
        \end{equation}
        Then $A$ is a $k$ algebra. For:
        \begin{equation}
            (\lambda\cdot{a})b
            =(\varphi(\lambda)a)(b)
            =\varphi(\lambda)(ab)
            =\lambda\cdot(ab)
        \end{equation}
        And also:
        \begin{equation}
            (\lambda\cdot{a})b
            =(a\varphi(\lambda))b
            =a(\varphi(\lambda)b)
            =a(\lambda\cdot{b})
        \end{equation}
        From associativity and from the fact that $\varphi(\lambda)$ is
        a commutative element. Going the other way, if $A$ is a $k$ algebra,
        let $\varphi:k\rightarrow{Z}(A)$ be defined by:
        \begin{equation}
            \varphi(\lambda)=\lambda\cdot{1}
        \end{equation}
        Then:
        \begin{equation}
            \varphi(a+b)
            =(a+b)\cdot{1}
            =a\cdot{1}+b\cdot{1}
            =\varphi(a)+\varphi(b)
        \end{equation}
        And also:
        \begin{equation}
            \varphi(ab)
            =(ab)\cdot{1}
            =(a\cdot{1})b
            =(a\cdot{1})(b\cdot{1})
            =\varphi(a)\varphi(b)
        \end{equation}
        And thus $\varphi$ is a homomorphism.
    \end{solution}
    \section{Homework IV}
        \begin{problem}
            Let $\varphi:\mathbb{Z}^{n}\rightarrow\mathbb{Z}^{n}$ be a
            $\mathbb{Z}\textrm{-Module}$ homomorphism such that the cokernel is
            finite. Let $\mathscr{E}$ by the standard basis of $\mathbb{Z}^{n}$
            and let $A=[\varphi]_{\mathscr{E}}^{\mathscr{E}}$ be the
            representing matrix of $\varphi$. Show that that cardinality of
            $\varphi$ is given by:
            \begin{equation}
                \textrm{Card}\big(\textrm{coker}(\varphi)\big)=|\textrm{det}(A)|
            \end{equation}
        \end{problem}
        \begin{solution}
            We prove by induction on $n$. If $n=1$ then for all $m\in\mathbb{Z}$
            we have:
            \begin{equation}
                \varphi(m)=\varphi(n\cdot{1})=m\varphi(1)
            \end{equation}
            Since the cokernel is finite, $\varphi(1)\ne{0}$. Otherwise the
            cokernel would be $\mathbb{Z}$, which is infinite. Let
            $k=\varphi(1)$. Then we have that the cokernel is isomorphic to
            $\mathbb{Z}_{k}$, and there are thus $k$ elements. Moreover the
            transformation matrix $[\varphi]_{\mathscr{B}}^{\mathscr{B}}$ is
            simply the singleton $[k]$. Thus
            $\textrm{det}([k])=\textrm{Card}(\mathbb{Z}_{k})=k$. Suppose the
            proposition is true for $n\in\mathbb{N}$. Let
            $\varphi:\mathbb{Z}^{n+1}\rightarrow\mathbb{Z}^{n+1}$ be a
            $\mathbb{Z}\textrm{-Module}$ homomorphism such that the cokernel is
            finite. But then the restriction $\varphi|_{\mathbb{Z}^{n}}$ is a
            $\mathbb{Z}\textrm{-Module}$ homomorphism from $\mathbb{Z}^{n}$ to
            itself and therefore:
            \begin{equation}
                \textrm{Card}\big(\textrm{coker}(\varphi|_{\mathbb{Z}^{n}})\big)
                =\big|\det\big([\varphi|_{\mathbb{Z}^{n}}
                    ]_{\mathscr{E}_{n}}^{\mathscr{E}_{n}}\big)\big|
            \end{equation}
            Doing this for every entry and the summing along, using the minor
            expansion formula, we get the end result.
        \end{solution}
        \begin{problem}
            Prove the short five-lemma.
        \end{problem}
        \begin{solution}
            It suffices to show that $f$ is bijective. That is, that $f$ is both
            injective and surjective. It is surjective for let $y\in{N}$. Since
            $f''$ is an isomorphism it is bijective, and thus there is an
            $m''\in{M}''$ such that $f''(m'')=h(y)$. But since the rows are
            short exact, $v$ is surjective and thus there is an $x\in{M}$ such
            that $v(x)=m''$. But then $f''(v(x))=f''(m'')=h(y)$. From the
            commutativity of the diagram we have that $h(f(x))=h(y)$. Let
            $\alpha=f(x)-y$. If $\alpha=0$ then we are done, if not then
            $\alpha$ is in the kernel of $h$ and is therefore in the image of
            $g$ since the sequence is exact. But then there is an $n'\in{N}'$
            such that $g(n')=\alpha$. But $f'$ is an isomorphism so there is
            an $m'\in{M}'$ such that $f'(m')=n'$. But $u$ is injective and thus
            there is a unique $m\in{M}$ such that $u(m')=m$. From commutativity
            we have that $f(u(m'))=\alpha$. But then $f(x-u(m'))=y$, and thus
            $f$ is surjective. In a similar manner it is injective. For suppose
            $f(x)=0$ for some $x\in{M}$. Then $f(x)$ is in the kernel of $h$
            and thus in the image of $g$. Therefore there is a unique
            $n'\in{N}'$ such that $g(n')=f(x)$. But $0$ is such that $g(0)=0$,
            and thus $n'=0$. Going back along $f'$, since it is bijective, we
            get a unique $m'$ such that $f(m')=0$. But again $0$ does this so
            $m'=0$. Going along $u$ we conclude that $x=0$, and thus $f$ is
            injective.
        \end{solution}
        \begin{problem}
            Let $M$ and $N$ be finitely generated Abelian group. If $M\oplus{M}$
            is isomorphic to $N\oplus{N}$, does it follow that $N$ is isomorphic
            to $N$?
        \end{problem}
        \begin{solution}
            Yes, for by the primary decomposition theorem we have that:
            \par
            \begin{subequations}
                \begin{minipage}[b]{0.49\textwidth}
                    \begin{equation}
                        M\simeq\mathbb{Z}^{m}
                        \bigoplus_{k=1}^{N}\mathbb{Z}_{q_{k}}
                    \end{equation}
                \end{minipage}
                \hfill
                \begin{minipage}[b]{0.49\textwidth}
                    \begin{equation}
                        N\simeq\mathbb{Z}^{n}
                        \bigoplus_{k=1}^{N}\mathbb{Z}_{p_{k}}
                    \end{equation}
                \end{minipage}
            \end{subequations}
            Where $q_{k}|q_{k+1}$ and $p_{k}|p_{k+1}$ are unique. From this we
            have:
            \begin{equation}
                M\oplus{M}\simeq
                \mathbb{Z}^{2m}\bigoplus_{k=1}^{N}
                \Big(\mathbb{Z}_{q_{k}}\oplus\mathbb{Z}_{q_{k}}\Big)
                \simeq
                \mathbb{Z}^{2n}\bigoplus_{k=1}^{N}
                \Big(\mathbb{Z}_{p_{k}}\oplus\mathbb{Z}_{p_{k}}\Big)
                \simeq{N}\oplus{N}
            \end{equation}
            By then this is a primary decomposition of both $M\oplus{M}$ and
            $N\oplus{N}$ and thus the sequence $q_{1},q_{1},q_{2},q_{2},\dots$
            all the way up to $q_{N},q_{N}$ is unique and so we conclude that
            $p_{k}=q_{k}$ for all $k\in\mathbb{Z}_{N}$. Lastly we conclude that
            $n=m$. Therefore, since $\simeq$ is an equivalence relation, we have
            that $N\simeq{M}$.
        \end{solution}
        \begin{problem}
            Let $R$ be a ring and let $M$ be and $R\textrm{-Module}$. $M$ is
            Noetherian if it satisfies the descending chain condition on
            submodules. That is, for any sequence of submodules $M_{n}$ such
            that $M_{n}\subseteq{M}_{n+1}$, there exists an $N\in\mathbb{N}$
            such that for all $n>N$, $M_{n}=M_{N}$.
        \end{problem}
        \begin{problem}
            Let $m$ and $n$ be positive integers. Show that as a $\mathbb{Z}$
            module:
            \begin{equation}
                \textrm{Hom}_{\mathbb{Z}}(\mathbb{Z}/n\mathbb{Z},
                                          \mathbb{Z}/m\mathbb{Z})
                \simeq\mathbb{Z}/\textrm{gcd}(n,m)\mathbb{Z}
            \end{equation}
        \end{problem}
    \section{Spectral Theorem}
        \begin{theorem}
            If $V$ is a finite dimensional vector space over $\mathbb{C}$, if
            $T:V\rightarrow{V}$ is a linear map, then $T$ has an eigenvalue.
        \end{theorem}
        \begin{proof}
            For the characteristic polynomial is non-constant and thus by the
            fundamental theorem of algebra there exists a root.
        \end{proof}
        \begin{theorem}
            If $V$ is a finite dimensional Hermitian inner product space and if
            $T:V\rightarrow{V}$ is a Hermitian operator, then the eigenvalues of
            $T$ are real and if $v$ is an eigenvector of $\lambda$ and if $w$ is
            a $\mu$ eigenvector for two different eignvalues $\lambda\ne\mu$,
            then $v$ and $w$ are orthogonal.
        \end{theorem}
        \begin{proof}
            For let $\lambda$ be an eigenvalue and let $v$ be a non-zero
            eigenvector for $\lambda$. Then $T(v)=\lambda{v}$. But $T$ is
            Hermitian, and therefore:
            \begin{equation}
                \langle{T}^{H}(v)|v\rangle
                =\langle{v}|T(v)\rangle
                =\langle{v}|\lambda{v}\rangle
                =\lambda\rangle{v}|v\rangle
            \end{equation}
            But also:
            \begin{equation}
                \langle{T}^{H}(v)|v\rangle
                =\langle{T}(v)|v\rangle
                =\langle{\lambda}v|v\rangle
                =\overline{\lambda}\langle{v}|v\rangle
            \end{equation}
            And therefore, since $\langle{v}|v\rangle\ne{0}$, we have
            $\lambda=\overline{\lambda}$, and therefore $\lambda$ is real. For
            the second part, we have:
            \begin{equation}
                \langle{v}|T(w)\rangle
                =\langle{v}|\mu{w}\rangle
                =\mu\langle{v}|w\rangle
                =\langle{T}^{H}(v)|w\rangle
                =\langle{T}(v)|w\rangle
                =\langle\lambda{v}|w\rangle
                =\overline{\lambda}\langle{v}|w\rangle
            \end{equation}
            But we just proved that $\lambda$ is real, and thus
            $\overline{\lambda}=\lambda$. Moreover $\lambda\ne\mu$, and thus
            for equality to occur we must have $\langle{v}|w\rangle=0$.
        \end{proof}
        \begin{theorem}
            If $V$ is a finite dimensional Hermitian inner product space, if
            $T:V\rightarrow{V}$ is linear, and if $W\subseteq{V}$ is a $T$
            invariant subspace (that is, $T(W)\subseteq{W}$), then
            $W^{\perp}$ is $T^{H}$ invariant.
        \end{theorem}
        \begin{proof}
            For let $x\in{W}^{\perp}$ and let $w\in{W}$. Then:
            \begin{equation}
                \langle{T}^{H}(x)|w\rangle
                =\langle{x}|T(w)\rangle=0
            \end{equation}
            And thus $T(x)\in{W}^{\perp}$. Therefore,
            $T(W^{\perp})\subseteq{W}^{\perp}$.
        \end{proof}
        \begin{ltheorem}{Unitary Triangulation Theorem}
                        {Unitary_Triangulation_Theorem}
            If $V$ is a finite dimensional Hermitian inner product space over
            $V$ and if $T:V\rightarrow{V}$ is a linear operator, then there
            exists an orthonormal basis $\mathscr{B}$ of $V$ such that
            $[T]_{\mathscr{B}}^{\mathscr{B}}$ is upper triangular.
        \end{ltheorem}
        \begin{proof}
            For consider $T^{H}:V\rightarrow{V}$. It has an eigenvalue
            $\lambda$. Let $v$ be a $\lambda$ eigenvector of $T^{H}$ and let
            $W=\mathbb{C}\cdot{v}=\textrm{Span}\{zv:z\in\mathbb{C}\}$. Since
            $v$ is an eigenvector of $T^{H}$ we have that $W$ is a $T^{H}$
            invariant subspace so therefore $W^{\perp}$ is invariant under
            $(T^{H})^{H}=T$. That is, $W^{\perp}$ is a $T$ invariant subspace.
            Since $W$ is non-zero, $W^{\perp}$ has dimension less than $V$ and
            thus by induction there is an orthonormal basis of $W^{\perp}$ such
            that $[T_{W^{\perp}}]_{\mathscr{B}'}^{\mathscr{B}'}$ is upper
            triangular. Extending $\mathscr{B}$ from $\mathscr{B}'$ gives an
            orthonormal basis such that $[T]_{\mathscr{B}}^{\mathscr{B}}$ is
            upper triangular.
        \end{proof}
        \begin{theorem}
            If $A\in{M}_{n}(\mathbb{C})$ then there is a unitary matric
            $P\in{U}(n)$ such that $PAP^{\minus{1}}$ is upper triangular.
        \end{theorem}
        \begin{ftheorem}{Spectral Theorem for Hermitian Operators}
                        {Spectral_Theorem_for_Hermitian_Operators}
            If $V$ is a finite dimensional Hermitian inner product space and if
            $T:V\rightarrow{V}$ is a Hermitian operator then there exists an
            orthonormal basis $\mathscr{B}$ of $V$ such that
            $[T]_{\mathscr{B}}^{\mathscr{B}}$ is diagonal.
        \end{ftheorem}
        \begin{proof}
            For the representing matrix of $T$ is upper triangular, and thus the
            representing matrix for $T^{H}$ is lower triangular. But $T=T^{H}$
            and therefore the represeting matrix of $T$ is both upper and lower
            triangular, and therefore it is diagonal.
        \end{proof}
        The theorem holds for normal operators as well. For Hermitian we see
        that the eigenvalues are real. For skew-Hermitian ($T=\minus{T}^{H}$)
        the eigenvalues are purely imaginary, and lastly for a unitary $T$ the
        eigenvalues are unit modulus.
        \begin{ftheorem}{Real Spectral Theorem}
            If $V$ is a finite dimensional inner product space, if
            $T:V\rightarrow{V}$ is a self-adjoint operator then there is an
            orthonormal basis of $V$ such that the representing matrix is
            diagonal.
        \end{ftheorem}
        \begin{proof}
            Let $A$ be the representing matrix of $T$ relative to the standard
            basis of $\mathbb{R}^{n}$. Then $A=A^{T}$ and thus $A$ defines a
            linear map $A:\mathbb{C}^{n}\rightarrow\mathbb{C}^{n}$ by mapping
            $A(v)=Av$ as a matrix operatoion. Then $A$ is a Hermitian operator
            and therefore $A$ has real eigenvalues. Let $v\in\mathbb{C}^{n}$ be
            a complex eigenvector for $\lambda$. Let $v=x+iy$ for
            $x,y\in\mathbb{R}^{n}$. Then since $Av=\lambda{v}$ we have:
            \begin{equation}
                Ax+iAy=\lambda{x}+i\lambda{y}
            \end{equation}
            But $A$ is real, as are $x$ and $y$, and thus we have:
            \begin{equation}
                Ax=\lambda{x}
                \quad\quad
                Ay=\lambda{y}
            \end{equation}
            But since $v$ is non-zero, at least one of $x$ or $y$ is non-zero.
            Thus $A$ has a real eigenvector, $x$ or $y$. Let $u$ be a unit
            real eigenvector and let $W$ be the real span of $u$. Then $W$ is
            $T$ invariant and therefore $W^{\perp}$ is $T^{H}$ invariant, but
            $T=T^{H}$. We complete the proof by induction.
        \end{proof}
    \section{Tensor Products}
        Let $R$ be a ring nd let $M$ be a right $R$ module and let $N$ be a
        left $R$ module. We seek a universal product $M\times{N}$ into
        $M\otimes_{R}N$. Examples of products: Dot products, cross products,
        bilinear forms, matrix multiplication. All of these are Biadditive.
        Matrices also have the \textit{balanced} property:
        \begin{equation}
            A(cB)=(Ac)B
        \end{equation}
        If the underlying ring is not commutative, we may not have perfect
        bilinearity and so we replace this requirement with the balanced
        property. So we seek a function
        $\beta:M\times{N}\rightarrow{M}\otimes_{R}N$ such that:
        \begin{align}
            \beta(a+b,c)&=\beta(a,b)+\beta(b,c)\tag{Left Additivity}\\
            \beta(a,b+c)&=\beta(a,b)+\beta(a,c)\tag{Right Additivity}\\
            \beta(ar,b)&=\beta(a,rb)\tag{Balanced}
        \end{align}
        Universal means that given any Abelian group $A$ and any biadditive
        $R$ balanced map $\mu:M\times{N}\rightarrow{A}$ there is a unique
        $\mathbb{Z}$ module homomorphism
        $\tilde{\mu}:M\otimes_{R}N\rightarrow{A}$ such that the diagram
        commutes. Since if it exists it is unique up to unique isomorphism, we
        need only construct such a thing. Long complicated construction to
        follow. By construction, $M\otimes_{R}N$ is generated by $m\otimes{n}$
        for all $m\in{M}$, $n\in{N}$. As a warning, not every element of
        $M\otimes_{R}N$ need be decomposable. It's folly to try to define a map
        $M\times{N}\rightarrow{A}$ by $m\otimes{n}\mapsto{f}(m,n)$ unless
        $f(m,n)$ is biadditive and $R$ balanced.
        \begin{theorem}
            If $N$ is a left $R$ module, then $R\otimes_{R}N$ is isomorphic to
            $N$ as a $\mathbb{Z}$ module.
        \end{theorem}
        \begin{proof}
            For let $\mu:R\times{N}\rightarrow{N}$ by defined by
            $\mu(r,n)=r\cdot{n}$. Then $\mu$ is biaddiative and balanced.
        \end{proof}
        The idea is to use scalar multiplication $R\times{N}\rightarrow{N}$ to
        construct such a module. The by the universal mapping property there
        exists a unique $\mathbb{Z}$ module homomorphism
        $\tilde{\mu}:R\otimes_{R}N\rightarrow{N}$ such that the diagram
        commutes. Define $\tilde{\mu}$ by:
        \begin{equation}
            \tilde{\mu}(r\otimes{n})=r\cdot{n}
        \end{equation}
        Define $j:N\rightarrow{R}\otimes_{R}N$ by $j(n)=1\otimes{n}$.
        This is a $\mathbb{Z}$ module since the tensor product is biaddiative
        and balanced. Moreover, $\tilde{\mu}$ and $j$ are inverses of each
        other.
        \begin{theorem}
            If $R$ is a ring, if $I$ is an ideal of $R$, and if $N$ is a left
            $R$ module, then:
            \begin{equation}
                R/I\otimes_{R}N\simeq{N}/IN
            \end{equation}
        \end{theorem}
        \begin{proof}
            For define $\mu:R/I\times{N}\rightarrow{N}/IN$ by:
            \begin{equation}
                \mu(\overline{r},n)=\mu(r+I,n)=rn+IN=\overline{rn}
            \end{equation}
            For $n\in{N}$ let $f_{N}:R\rightarrow{N}/IN$ be defined by
            $f_{n}(r)=rn+IN=\overline{rn}$. Then $f_{n}$ is a map of left
            $R$ modules and therefore induces a map $\overline{f}_{n}$ from
            $R/I$ to $N/IN$ by $\overline{r}\mapsto\overline{rn}$. By the
            universal mapping property there is a $\mathbb{Z}$ linear map
            $\tilde{\mu}$ such that the diagram commutes. So we have that
            $\tilde{\mu}(\overline{r}\otimes{n})=\overline{rn}$. We now want a
            map $N/IN\rightarrow{R}/I\otimes_{R}N$. Let $j$ be defined by
            $j(n)=\overline{1}\otimes{n}$. Then $j$ vanishes on generators of
            $IN$ and therefore induces $\overline{j}$ such that
            $\overline{n}\mapsto\overline{1}\otimes{n}$. Thus we now have two
            maps that are well defined and now we need only check that they are
            inverses of each other. And it is so. So we are done.
        \end{proof}
        \begin{theorem}
            Given a sequence of modules over $R$,
            $N'\rightarrow{N}\rightarrow{N}''\rightarrow{0}$,
            and suppose that for all left $R$ modules $Y$ the
            sequence:
            \begin{equation}
                0\rightarrow\textrm{Hom}_{R}(N'',Y)
                \rightarrow\textrm{Hom}_{R}(N,Y)
                \rightarrow\textrm{Hom}_{R}(N',Y)
            \end{equation}
            is exact, then the original sequence is exact.
        \end{theorem}
        \begin{proof}
            For let $Y=\textrm{coker}(v)=N''/v(N)$. Then since:
            \begin{equation}
                0\rightarrow\textrm{Hom}_{R}(N'',Y)
                \rightarrow\textrm{Hom}_{R}(N,Y)
                \rightarrow\textrm{Hom}_{R}(N',Y)
            \end{equation}
            is exact, we have that the canonical projection $\pi$ gets mapped
            to $v^{*}(\pi)$. But $v^{*}(\pi)=\pi\circ{v}$, and this is zero
            since $n\mapsto{v}(n)$ which maps to $0$ by $\pi$. Thus $v^{*}$ is
            surjective and injective. On the other side, let $Y=N''$. Then
            $\textrm{id}_{N''}$ mapts to $v$ under $v^{*}$, and thus
            $v$ maps to $0$ under $u^{*}$ since the sequence is exact, and
            thus $u\circ{v}=0$. Thus $\textrm{im}(u)\subseteq\textrm{ker}(u)$.
        \end{proof}
    \section{Tensor Products of Algebras}
        An algebra over a commutative ring $k$ is a left $k$ module with a ring
        structure such that the ring multiplication is compatible with scalar
        multiplication:
        \begin{equation}
            (\lambda\star{a})\cdot{b}=\lambda\star(a\cdot{b})
                =a\cdot(\lambda\star{b})
        \end{equation}
        An equivalent definition is a ring $A$ with a ring homomorphism
        $\varphi:k\rightarrow{Z}(A)$. We can also define an algebra in terms of
        the tensor product. An algebra over $k$ is a $k$ module $A$ with a
        homomorphism $\mu:A\otimes{A}\rightarrow{A}$ and a module homomorphism
        $\eta:k\rightarrow{A}$ such that some diagram commutes.
        \section*{Problem 2}
        \setcounter{section}{2}
        Since $\mathbb{C}$ is algebraically closed, there is a Jordan normal
        form for $T$. Since $\lambda$ is the only eigenvalue, we have:
        \begin{equation}
            J=
            \begin{bmatrix}
                \lambda&1&0&\dots&0&0\\
                0&\lambda&1&\dots&0&0\\
                0&0&\lambda&\dots&0&0\\
                \vdots&\vdots&\vdots&\ddots&\vdots&\vdots\\
                0&0&0&\dots&\lambda&1\\
                0&0&0&\dots&0&\lambda
            \end{bmatrix}
        \end{equation}
        Let $B_{n}$ be the matrix with 1 on the super-diagonal, and 0 everywhere
        else. Then $J=\lambda{I}_{n}+B_{n}$. But then:
        \begin{equation}
            J^{2}-\lambda^{2}I_{n}=(J-\lambda{I}_{n})(J+\lambda{I}_{n})
        \end{equation}
        But $J-\lambda{I}_{n}$ is just the super-diagonal $B_{n}$. If
        $\lambda\ne{0}$, then:
        \begin{equation}
            J+\lambda{I}_{n}=
            \begin{bmatrix}
                2\lambda&1&0&\dots&0&0\\
                0&2\lambda&1&\dots&0&0\\
                0&0&2\lambda&\dots&0&0\\
                \vdots&\vdots&\vdots&\ddots&\vdots&\vdots\\
                0&0&0&\dots&2\lambda&1\\
                0&0&0&\dots&0&2\lambda
            \end{bmatrix}
        \end{equation}
        Which is invertible, denote this by $P$. But then:
        \begin{equation}
            J^{2}=P^{\minus{1}}\big(\lambda^{2}I_{n}+B_{n}\big)P
        \end{equation}
        And therefore the Jordan normal form of $J^{2}$ is
        $\lambda^{2}I_{n}+B_{n}$:
        \begin{equation}
            J'=
            \begin{bmatrix}
                \lambda^{2}&1&0&\dots&0&0\\
                0&\lambda^{2}&1&\dots&0&0\\
                0&0&\lambda^{2}&\dots&0&0\\
                \vdots&\vdots&\vdots&\ddots&\vdots&\vdots\\
                0&0&0&\dots&\lambda^{2}&1\\
                0&0&0&\dots&0&\lambda^{2}
            \end{bmatrix}
        \end{equation}
    \section*{Problem 3}
        For if $G$ is a finite group and $H$ is a subgroup of $G$, then by
        Lagrange's theorem we have that $|G|=n\cdot|H|$ for some
        $n\in\mathbb{N}^{+}$. But if $H$ is a proper subset of $G$, then
        $n\ne{1}$, and so we have $n\geq{2}$. Looking at the conjugacy classes
        $gHg^{\minus{1}}$, there are at most $n$ such elements for otherwise
        $|G|/|H|>n$. But $H$ is a subgroup, and thus $e\in{H}$, and so
        $g*e*g^{\minus{1}}=e$ is in each conjugacy class. If we sum over the
        $|H|$ possible conjugates, we get, at most, $|H|-1$ distinct elements,
        and the identity element, in each and thus we obtain:
        \begin{equation}
            n\cdot(|H|-1)+1=n|H|-n+1=n|H|-(n-1)
        \end{equation}
        But by hypothesis, $n\geq{2}$ and thus $n-1>0$, so we get:
        \begin{equation}
            n|H|-(n-1)<n|H|
        \end{equation}
        But $n|H|=|G|$, and thus not every element of $G$ will be in one of the
        conjugacy classes. For the infinite case, consider the free group on
        two elements $a,b$ and consider the subgroup of all elements such that
        the first element and the last element of a given word are not inverses
        of each other. Given a word that is not in this form, say
        $x=ab\cdots{a}^{\minus{1}}$, we may reduce this by conjugation by
        multiplying $a^{\minus{1}}*x*a$. Thus every word can be reduced to this
        form, however this subgroup is not the entirety of $F(a,b)$.
    \section*{Problem 5}
        For a matrix $B$ is invertible if and only if $Bx=0$ has only the zero
        vector as a solution. Suppose $A+I$ is not invertible. Then there is a
        non-zero vector $x\in\mathbb{R}^{n}$ such that $(A+I)x=0$. But then:
        \begin{equation}
            Ax=\minus{x}
        \end{equation}
        But if $Ax=\minus{x}$, then:
        \begin{equation}
            (Ax)^{T}=x^{T}A^{T}=\minus{x}^{T}
        \end{equation}
        But $A$ is skew symmetric, and thus $A^{T}=\minus{A}$. Therefore:
        \begin{equation}
            \minus{x}^{T}A=\minus{x}^{T}
            \Longrightarrow
            x^{T}A=x^{T}
        \end{equation}
        But then:
        \begin{equation}
            x^{T}x=(x^{T}A)x=x^{T}(Ax)=\minus{x}^{T}x
        \end{equation}
        And thus $\norm{x}=\minus\norm{x}$ which implies $\norm{x}=0$. But this
        is true if and only if $x=0$, a contradiction as $x$ is a non-zero
        vector. Thus, $A+I$ is invertible.
\end{document} 