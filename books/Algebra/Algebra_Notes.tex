%------------------------------------------------------------------------------%
\documentclass{book}                                                           %
%------------------------------Preamble----------------------------------------%
\makeatletter                                                                  %
    \def\input@path{{../../}}                                                  %
\makeatother                                                                   %
\input{preamble.tex}                                                           %
\makeindex[intoc]                                                              %
%----------------------------Main Document-------------------------------------%
\begin{document}
\pagenumbering{gobble}
\title{Algebra}
\author{Notes by: Ryan Maguire}
\date{\vspace{-5ex}}
\maketitle
\tableofcontents
\chapter{Jacobson Volume I}
\pagenumbering{arabic}
    \section{Sets}
        That is, the equivalence class $[x]$ in $X/R$ is mapped to the image of
        the representative. This is well defined by our hypothesis about the
        fibers of $f$. It's useful when $f^{\minus{1}}[\{y\}]$ correspnds to
        precisely one element of $X/R$.
        \begin{example}
            Projection of $\nspace[2]$ onto the $x$ axis. The fibers are
            vertical lines.
        \end{example}
        Peano's axioms for $\mathbb{N}$. Principle of induction. Addition,
        associativity of addition, commutativity. Same with multiplication.
        Distributive law. Order: $a\leq{b}$ if there exists $x$ such that
        $a+x=b$. This is a total order by the Peano axioms. Trichotomy.
        Well ordering principle.
        \begin{theorem}
            If $a+c\leq{b}+c$, then $a\leq{b}$.
        \end{theorem}
        \begin{theorem}
            If $a\leq{b}$, then $a+c\leq{b}+c$.
        \end{theorem}
        \begin{theorem}
            If $c\in\mathbb{N}^{+}$, and if $ac\leq{b}c$, then $a\leq{c}$.
        \end{theorem}
        \begin{theorem}
            If $a\leq{b}$, then $ac\leq{b}c$.
        \end{theorem}
        Define $\mathbb{Z}$ to be $\mathbb{N}\times\mathbb{N}/R$ where $R$
        is the relation $(a,b)R(c,d)$ if and only if $a+d=b+c$. That is,
        equivalence classes are straight lines through the lattice
        $\mathbb{N}\times\mathbb{N}$ with slope 1. We take zero to be the
        equivalence class of $(x,x)$. This acts as zero, since
        $(a,b)+(x,x)=(a+x,b+x)$ and this is in the same equivalence class as
        $(a,b)$. The additive inverse of $(a,b)$ is $(b,a)$ since
        $(a,b)+(b,a)=(a+b,b+a)=(a+b,a+b)$ which is in the equivalence class
        $(x,x)$. We have an order $(a,b)>(c,d)$ if $a+d>b+c$. This is well
        defined on the equivalence classes of $\mathbb{N}\times\mathbb{N}$.
        \begin{theorem}
            If $z>0$, then $x>y$ if and only if $xz>yz$.
        \end{theorem}
        \begin{theorem}
            If $x<y$, then $\minus{y}<\minus{x}$.
        \end{theorem}
        \begin{proof}
            For if $x\in\mathbb{Z}$, then there are $a,b\in\mathbb{N}$ such
            that $x=[(a,b)]$ and similarly $y=[(c,d)]$. But if
            $x<y$, then $a+d<b+c$. But $\minus{x}=[(b,a)]$ and
            $\minus{y}=[(d,c)]$, and if $a+d<b+c$, then $d+a<c+b$, and hence
            $[(d,c)]<[(b,a)]$ and hence $\minus{y}<\minus{x}$.
        \end{proof}
        We can match $\mathbb{N}$ directly to the non-negative elements of
        $\mathbb{Z}$. That is, the equivalence classes of
        $\mathbb{N}\times\mathbb{N}/R$. Given this function $f$, we have
        $f(n+m)=f(n)+f(m)$, $f(nm)=f(n)f(m)$, and $f(n)>f(m)$ if and only if
        $n>m$. $\mathbb{Z}$ has least and greatest upper bound properties.
        Absolute value function: $\mathbb{Z}\rightarrow\mathbb{N}$ defined
        by $|x|=x$ if $x\geq{0}$ and $|x|=\minus{x}$.
        \begin{theorem}
            $|xy|=|x||y|$, $|x+y|\leq|x|+|y|$.
        \end{theorem}
        \begin{theorem}
            $0\leq{n}^{2}$
        \end{theorem}
        Euclid's division algorithm.
    \section{Groups and Semigroups}
        Cayley table of semigroup. Cayley table of $S_{2}$, $S_{3}$.
        Generalized associative law, proof by induction. Power law
        $a^{m}a^{n}=a^{m+n}$. If Abelian, any permutation of the product is
        equal. If Abelian, $(ab)^{n}=a^{n}b^{n}$. Uniqueness of identity,
        inverses. Inverse of inverse is self. Inverse of product. Groups.
        \begin{example}
            The roots of unity $z^{n}-1$ form a subgroup of $\mathbb{C}$
            with multiplication.
        \end{example}
        \begin{example}
            Rotations about a point $(x,y)\in\nspace[2]$ defined by:
            \begin{equation}
                \begin{pmatrix}
                    x'\\
                    y'
                \end{pmatrix}=
                \begin{pmatrix*}[r]
                    \cos(\theta)&\minus\sin(\theta)\\
                    \sin(\theta)&\cos(\theta)
                \end{pmatrix*}
                \begin{pmatrix}
                    x\\
                    y
                \end{pmatrix}
            \end{equation}
            form a group under rotation by an angle $\theta$. $\theta=0$ is
            the identity, and given a rotation $\theta$, the rotation
            $\minus\theta$ in the reverse direction serves as inverse.
        \end{example}
        \begin{example}
            Define $*$ on $\nspace[2]$ (excluding $(0,\cdot)$) by
            $(a,b)*(c,d)=(ac,bc+d)$. This is a group. The element $(1,0)$
            serves as identity since:
            \begin{align}
                (1,0)*(c,d)&=(1\cdot{c},0\cdot{c}+d)=(c,d)\\
                (a,b)*(1,0)&=(a\cdot{1},b\cdot{1}+0)=(a,b)
            \end{align}
            Lastly, given $(a,b)$, the inverse is $(1/a,\minus{b}/a)$ since:
            \begin{equation}
                \big(a,b\big)*\big(\frac{1}{a},\minus\frac{b}{a}\big)
                    =\big(a\cdot\frac{1}{a},\frac{b}{a}-\frac{b}{a}\big)
                    =(1,0)
            \end{equation}
            and similarly for left multiplication. Hence, this is a group.
        \end{example}
        Only idempotent element of a group is the identity $x^{2}=x$ by
        cancellation law. Right identity with right inverses in a semigroup
        is a group.
        \begin{theorem}
            If $\monoid{G}$ is a semigroup with the Latin square property,
            then $\monoid{G}$ is a group.
        \end{theorem}
        \begin{proof}
            For by the Latin square property there is a right identity since
            we may solve $ax=a$. Let $e$ be such a right identity. But then
            there are right inverses since we may solve $ax=e$. Hence,
            $\monoid{G}$ is a semigroup with right identities and right
            inverses, and is therefore a group.
        \end{proof}
        \begin{theorem}
            If $G$ is a set, if $*$ is a cancellative operation on $G$,
            if $a\in{G}$, and if $f:G\rightarrow{G}$ is defined by
            $f(x)=a*x$, then $f$ is injective.
        \end{theorem}
        \begin{proof}
            For suppose not. Then there are $x_{1},x_{2}\in{G}$ such that
            $x_{1}\ne{x}_{2}$ and $f(x_{1})=f(x_{2})$. And thus
            $a*x_{1}=a*x_{2}$. But $*$ is cancellative and hence
            $x_{1}=x_{2}$, a contradiction.
        \end{proof}
        \begin{theorem}
            If $\monoid{G}$ is a finite cancellative semigroup, then it is
            a group.
        \end{theorem}
        \begin{proof}
            For let $a\in{G}$ and define $f:G\rightarrow{G}$ by
            $f(x)=a*x$. Then $f$ is injective, and since $G$ is finite it is
            surjective. Hence there is an $e\in{G}$ such that $a*e=a$. But
            then for all $b\in{G}$ we have $a*b=(a*e)*b=a*(e*b)$ and since
            $*$ is cancellative we have $e*b=b$, and therefore $e$ is a
            left identity. Given $b\in{G}$ the function $g:G\rightarrow{G}$
            defined by $g(x)=x*b$ is surjective, and hence there is a
            $c\in{G}$ such that $g(c)=e$ and thus $c*b=e$. Thus there are
            left inverses, and so $\monoid{G}$ is a group.
        \end{proof}
        Subsemigroup, subgroup.
        \begin{theorem}
            If $\monoid{G}$ is a monoid, and if $H\subseteq{G}$ is the set
            of all invertible elements of $G$, then $\monoid{H}$ is a
            subgroup.
        \end{theorem}
        \begin{proof}
            It suffices to show that $a*b$ is contained in $H$ for all
            $a,b\in{H}$. But if $a$ and $b$ are invertible, then $a*b$ is
            invertible since
            $(a*b)^{\minus{1}}=b^{\minus{1}}*a^{\minus{1}}$. Similarly, the
            identity is contained in $H$ since $e*e=e$ so $e$ is invertible.
        \end{proof}
        \begin{theorem}
            $H$ is a subgroup of $G$ if and only if for all $a,b\in{H}$,
            $a*b^{\minus{1}}\in{H}$.
        \end{theorem}
        \begin{proof}
            If $H$ is a subgroup, then it is closed to inverses and it is
            closed to multiplication, so $a*b^{\minus{1}}\in{H}$. If this
            equation holds, then the identity is in $H$ since
            $a*a^{\minus{1}}\in{H}$. Hence it is closed to inverses since
            $b^{\minus{1}}=e*b^{\minus{1}}\in{H}$. Thus it is closed to
            multiplication since $a*b=a*(b^{\minus{1}})^{\minus{1}}\in{H}$.
        \end{proof}
        \begin{theorem}
            If $\monoid{G}$ is a finite group, and if $H\subseteq{G}$ is a
            subsemigroup, then $H$ is a subgroup of $G$.
        \end{theorem}
        \begin{theorem}
            The intersection of subgroups is a subgroup.
        \end{theorem}
        \begin{theorem}
            If $\monoid{G}$ is a group, if $a\in{G}$, and if $H$ is the set
            of all elements of $G$ that commute with $a$, then $H$ is a
            subgroup of $G$.
        \end{theorem}
        Def isomorphism, example of $\exp$ and $\ln$. Isomorphism is an
        equivalence relation.
        \begin{theorem}
            If $\monoid[G]{G}$ and $\monoid[H]{H}$ are groups, and if
            $\varphi:G\rightarrow{G}$ is an isomorphism, then
            $\varphi(e_{G})=e_{H}$.
        \end{theorem}
        \begin{proof}
            For we have:
            \begin{equation}
                \varphi(a)=\varphi(a*_{G}e_{G})
                          =\varphi(a)*_{H}\varphi(e_{G})
            \end{equation}
            Hence by the cancellation law, $\varphi(e_{G})=e_{H}$.
        \end{proof}
        \begin{theorem}
            If $\monoid[G]{G}$ and $\monoid[H]{H}$ are groups, and if
            $\varphi:G\rightarrow{G}$ is an isomorphism, then
            $\varphi(a^{\minus{1}})=\varphi(a)^{\minus{1}}$.
        \end{theorem}
        \begin{proof}
            For:
            \begin{equation}
                \varphi(a)*_{H}\varphi(a^{\minus{1}})
                    =\varphi(a*_{G}a^{\minus{1}})
                    =\varphi(e_{G})
                    =e_{H}
            \end{equation}
            and since inverses are unique, we have
            $\varphi(a^{\minus{1}})=\varphi(a)^{\minus{1}}$.
        \end{proof}
        \begin{example}
            $f:\nspace[]\rightarrow\nsphere[1]$ defined by
            $f(\theta)=\exp(i\theta)$ is a homomorphism but not an
            isomorphism since it is not bijective. $2\pi{n}$ gets mapped to
            $(1,0)$ for all $n\in\mathbb{Z}$.
        \end{example}
        Symmetric groups.
        \begin{example}
            Let $\alpha,\beta:\mathbb{Z}_{5}\rightarrow\mathbb{Z}_{5}$ be
            defined as follows:
            \twocolumneq{%
                \alpha=
                \begin{pmatrix}
                    0&1&2&3&4\\
                    1&2&0&4&3
                \end{pmatrix}
            }{%
                \beta=
                \begin{pmatrix}
                    0&1&2&3&4\\
                    0&2&3&4&1
                \end{pmatrix}
            }
            Let's compute $\alpha\circ\beta$ and $\beta\circ\alpha$.
            Performing $\alpha\circ\beta$, we compute $\beta$ first, and
            hence $0\mapsto{0}$, and then $0\mapsto{1}$ under $\alpha$.
            We do the same for the other columns:
            \begin{equation}
                \alpha\circ\beta=
                \begin{pmatrix}
                    0&1&2&3&4\\
                    0&2&3&4&1\\
                \end{pmatrix}
                \begin{pmatrix}
                    0&1&2&3&4\\
                    1&2&0&4&3
                \end{pmatrix}
                =
                \begin{pmatrix}
                    0&1&2&3&4\\
                    1&0&4&3&2
                \end{pmatrix}
            \end{equation}
            and similarly we have $\beta\circ\alpha$:
            \begin{equation}
                \beta\circ\alpha=
                \begin{pmatrix}
                    0&1&2&3&4\\
                    1&2&0&4&3
                \end{pmatrix}
                \begin{pmatrix}
                    0&1&2&3&4\\
                    0&2&3&4&1
                \end{pmatrix}
                =
                \begin{pmatrix}
                    0&1&2&3&4\\
                    2&3&0&1&4
                \end{pmatrix}
            \end{equation}
        \end{example}
        \begin{example}
            Suppose we have $\identity{\mathbb{Z}_{3}}$ and the two
            functions $\alpha,\beta:\mathbb{Z}_{3}\rightarrow\mathbb{Z}_{3}$
            by:
            \twocolumneq{%
                \alpha=
                \begin{pmatrix}
                    0&1&2\\
                    1&2&0
                \end{pmatrix}
            }{%
                \beta=
                \begin{pmatrix}
                    0&1&2\\
                    2&0&1
                \end{pmatrix}
            }
            Then $(\identity{\mathbb{Z}_{3}},\alpha,\beta)$ form a group
            under function composition. Since composition is associative and
            we're including the identity, we need only show that $\circ$ is
            a valid binary operation on this set and that we have inverses.
            But $\alpha$ and $\beta$ are inverses of each other. This group
            is actually just $\mathbb{Z}/3\mathbb{Z}$.
        \end{example}
        \begin{example}
            The functions $f:\nspace[]\rightarrow\nspace[]$ defined by
            $f(x)=ax+b$ with $a\ne{0}$ is a group under composition. The
            identity is contained in this, setting $a=1$ and $b=0$. Moreover
            if $f=ax+b$, then $g=x/a-b/a$ serves as inverse. This group is
            isomorphic to the group on $\nspace[2]$ minus $(0,\cdot)$ with
            the operation $(a,b)*(c,d)=(ac,bc+d)$.
        \end{example}
        \begin{ftheorem}{Cayley's Theorem}{Cayleys_Theorem}
            If $\monoid{G}$ is a group, then there is a subgroup
            $H\subset{S}_{G}$ of the symmetric group
            $\monoid[][\circ]{S_{G}}$ such that $\monoid{G}$ is isomorphic
            to $(H,\circ|_{H})$.
        \end{ftheorem}
        \begin{bproof}
            For let $H\subseteq{S}_{G}$ be defined by:
            \begin{equation}
                H=\{\,f\in{S}_{G}\;|\;
                    \exists_{a\in{G}}\big(f(x)=a*x\big)\,\}
            \end{equation}
            Let $\varphi:G\rightarrow{H}$ be defined by
            $\varphi(a)=f$ where $f:G\rightarrow{G}$ is the function defined
            by $f(x)=a*x$. This is an isomorphism.
        \end{bproof}
        \begin{theorem}
            If $\monoid{G}$ is a finite group, then there is an
            $n\in\mathbb{N}$ such that $G$ is isomorphic to a subgroup of
            $S_{n}$.
        \end{theorem}
        We can form a different subgroup of $S_{G}$ that is isomorphic to
        $G$ by considering functions of the form $f(x)=x*a$. That is, by
        multiplying $x$ on the left rather than the right. If we label this
        group $H_{L}$, and the other group $H_{R}$, it turns out all
        elements of $H_{L}$ commute with all elements of $H_{R}$, and vice
        versa. The converse holds, if $f$ commutes with all left
        multiplications, then $f$ is a right multiplying, and if $f$
        commutes with all right multiplications, then $f$ is a left
        multiplication.
        \subsection{Cyclic Groups}
            Group generated by set (Intersection of all subgroups
            containing set). This is also the set of all finite
            multiplications of elements in $M$, combined with inverses.
            Cyclic group is group generated by a single element.
            \begin{example}
                $\mathbb{Z}$ is cycle with generator 1, and the set of roots
                of unity in $\mathbb{C}$ are cyclic with generator
                $\exp(2\pi{i}/n)$.
            \end{example}
            \begin{theorem}
                If $\monoid{G}$ is cyclic, if $a\in{G}$, and if
                $f:\mathbb{Z}\rightarrow{G}$ is defined by $f(n)=a^{n}$ is
                a homomorphism.
            \end{theorem}
            If $\monoid{G}$ is cyclic generated by $a$, then there either
            there is a least $n\in\mathbb{N}^{+}$ such that $a^{n}$ is a
            unital element, or $a^{n}$ is distinct for all
            $n\in\mathbb{N}^{+}$. Using the Euclidean division algorithm we
            have $a^{m}=a^{qn+r}=a^{qn}a^{r}=a^{r}$. Thus, every cyclic
            group is either $\mathbb{Z}$ or $\mathbb{Z}/n\mathbb{Z}$ for
            some $n\in\mathbb{N}^{+}$. The subgroups of a cyclic group
            generated by $a$ are the subgroups generated by $a^{k}$ for some
            $k\in\mathbb{N}$. If the group is finite, the $k$ the divide
            the order of $a$ give all of the subgroups. That is, if
            $G$ is cyclic of order $r$, and if $d$ is the number of divisors
            of $r$, then there are $d$ subgroups of $G$.
            \begin{example}
                Consider $\monoid[][+]{\mathbb{Z}/12\mathbb{Z}}$. There are
                12 elements in this group and it is cyclic and generated by
                the equivalence class $[1]$. For every divisor of 12 we get
                a different subgroup. The divisors are 1, 2, 3, 4, 6, and
                12. These subgroups correspond to $\mathbb{Z}_{12}$,
                $\mathbb{Z}_{6}$, $\mathbb{Z}_{4}$, $\mathbb{Z}_{3}$,
                $\mathbb{Z}_{2}$, and $\mathbb{Z}_{1}$, respectively.
            \end{example}
            \begin{theorem}
                If $\monoid{G}$ is cyclid or order $n\in\mathbb{N}^{+}$, and
                if $\varphi$ is the Euler totient function, then there are
                $\varphi(n)$ elements of $G$ that generated $G$.
            \end{theorem}
            A cycle in $\mathbb{Z}_{n}$ is a function
            $f:\mathbb{Z}_{n}\rightarrow\mathbb{Z}_{n}$ such that
            $f(k)=k+1$ for all $k<m$, $f(m)=0$, and $f(k)=k$ for all $k>m$.
            A transposition is a function $f:X\rightarrow{X}$ such that
            there exists $x,y\in{X}$ such that $f(x)=y$ and $f(y)=x$, and
            for all other $z\in{X}$ we have $f(z)=z$. Every cycle can be
            written as product of transpositions. The number of products is
            either always even or always odd, no cycle can be written as
            both an even product and an odd product. The inverse of even is
            even, as is the product of even, and hence we have the subgroup
            of even transpositions. This is the alternating group.
            \begin{example}
                There are $4!=12$ elements in $S_{4}$. We can write these
                out as cycles. There are $4!/2=6$ even cycles.
            \end{example}
            \begin{theorem}
                If $n\geq{3}$, then any element of $A_{n}$ is the product of
                three cycles $(abc)$.
            \end{theorem}
        \subsection{Coset Decomposition}
            If $X$ is a set, and if $G\subseteq{S}_{X}$ is a subgroup of the
            permutation group on $X$, and if $R$ is the relation
            $xRy$ if and only if there is an $f\in{G}$ such that
            $y=f(x)$. Then $R$ is an equivalence relation. It is reflexive
            since the identity mapping is contained in $G$ since $G$ is a
            group. Also by this fact the inverse mapping $f^{\minus{1}}$ is
            contained in $G$ and hence $R$ is symmetric. Moreover, since the
            composition of permutations is a permutation, if $xRy$ and
            $yRz$, then $y=f(x)$ and $z=g(y)$, and hence $z=g(f(x))$ so
            $xRz$. $G$ is transitive in $X$ if for all $x,y\in{X}$ it is
            true that $xRy$. The equivalence classes of $X/R$ partition $X$
            and these are called transitivity sets of $X$ with respect to
            $G$. If $H\subseteq{G}$ is a subgroup, and if $H_{R}$ is the
            elements of $G$ that are right multiplications of elements in
            $H$. Then $H_{R}$ is a subgroup of $G_{R}$, the group of all
            right multiplications. Thus $H_{R}$ acts on $G$. We can write
            $x\equiv{y}$ if there is an $h\in{H}_{R}$ such that $y=xh$. In
            other words, $x^{\minus{1}}y\in{H}_{R}$. This gives the right
            coset of $x$ relative to $H$. We define the product of subsets
            of a group as follows:
            \begin{equation}
                AB=\{\,a*b\in{G}\;|\;a\in{A}\textrm{ and }b\in{B}\,\}
            \end{equation}
            Coset of $x\in{G}$ with respect to $H$ is then just $xH$.
            \begin{theorem}
                If $\monoid{G}$ is a group, if $H\subseteq{G}$ is a
                subgroup, if $x,y\in{G}$, then either $xH=yH$ or
                $xH\cap{y}H$.
            \end{theorem}
            \begin{proof}
                For suppose $xH\ne{y}H$ and let $z\in{x}H\cap{y}H$. Then
                there is an $h_{1}\in{H}$ such that $z=xh_{1}$ and
                $h_{2}\in{H}$ such that $z=yh_{2}$. But then
                $xh_{1}=yh_{2}$ and thus $x=yh_{2}h_{1}^{\minus{1}}$, and
                so $x\in{y}H$, and hence $xH\subseteq{y}H$. And similarly,
                $yH\subseteq{x}H$ and thus $xH=yH$, a contradiction. Thus,
                $xH\cap{y}H=\emptyset$.
            \end{proof}
            \begin{example}
                Give $n\in\mathbb{N}^{+}$, and the subgroup of $\mathbb{Z}$
                generated by $n$, the cosets of this subgroup are the
                sets:
                \begin{subequations}
                    \begin{align}
                        0+\langle{n}\rangle
                            &=\{\,0,\,\pm{n},\,\pm{2}n,\,\dots\,\}\\
                        1+\langle{n}\rangle
                            &=\{\,1,\,1\pm{n},\,1\pm{2}n,\,\dots\,\}\\
                        2+\langle{n}\rangle
                            &=\{\,2,\,2\pm{n},\,2\pm{3}n,\,\dots\,\}\\
                        &\hdots\\
                        n-1+\langle{n}\rangle
                        &=\{\,n-1,\,n-1\pm{n},\,n-1\pm{3}n,\,\dots\,\}
                    \end{align}
                \end{subequations}
            \end{example}
            \begin{theorem}
                If $\monoid{G}$ is a group, if $x,y\in{G}$, and if
                $H\subseteq{G}$ is a subgroup, then
                $\cardinality{xH}=\cardinality{yH}$. That is, there is a
                bijection $f:xH\rightarrow{y}H$.
            \end{theorem}
            \begin{proof}
                Define $f$ by $f(a)=x^{\minus{1}}ya$.
            \end{proof}
            Since $eH$ is a right coset of $H$, every coset of $H$ has the
            same cardinality of $H$. Left cosets exist. The left coset
            $Hx$ is uniquely determined by $x^{\minus{1}}H$. Hence, the set
            of all left and right cosets have the same cardinal number. This
            is the index of $H$ in $G$. This gives rise to Lagrange's
            theorem.
            \begin{ftheorem}{Lagrange's Theorem}{Lagranges_Theorem}
                If $\monoid{G}$ is a finite group, and if $H\subseteq{G}$
                is a subgroup, then $\cardinality{H}$ divides
                $\cardinality{G}$.
            \end{ftheorem}
            \begin{bproof}
                For $\{\,xH\;|\;x\in{G}\,\}$ partitions $G$, and each
                element has cardinality $\cardinality{H}$. If there are $r$
                such distinct elements of this set, then $G$ thus has
                $r\cdot\cardinality{H}$ elements.
            \end{bproof}
            \begin{theorem}
                If $A_{n}$ is the alternating group on $n$ elements, then
                $\cardinality{A_{n}}=n!/2$.
            \end{theorem}
            \begin{theorem}
                If $\monoid{G}$ is a finite group, if $n=\cardinality{G}$,
                and if $x\in{G}$, then $x^{n}=e$.
            \end{theorem}
            \begin{proof}
                For let $m$ be the order of $x$. Then by Lagrange's theorem
                we have $mr=n$, and hence
                $x^{n}=x^{mr}=(x^{m})^{r}=e^{r}=e$.
            \end{proof}
            \begin{example}
                Consider the subgroup $H\subseteq{S}_{3}$ defined by
                $H=\{e,\,(1\;2)\}$. That is, the subgroup defined by the
                identity and the transposition $(1\;2)$. This is a subgroup
                since $(1\;2)(1\;2)=e$ and hence $H$ is closed under
                multiplication and inverses. Coset decomposition of $S_{3}$
                by $H$ is obtained by multiplying $H$ by all elements of
                $S_{3}$ and obtaining equivalence classed. Since
                $e$ and $(1\;2)$ are contained in $H$, we can skip these.
                The next one is $(1\;3)$. Multiplying by $e$ gives $(1\;3)$
                back, and multiplying by $(1\;2)$ gives us
                $(1\;3)(1\;2)=(1\;2\;3)$, so have:
                \begin{equation}
                    (1\;3)H=\{\,(1\;3),\,(1\;2\;3)\,\}
                \end{equation}
                Next, $(2\;3)$ and we have $(2\;3)(1\;3)=(2\;3\;1)$, hence:
                \begin{equation}
                    (2\;3)H=\{\,(2\;3),\,(2\;3\;1)\,\}
                \end{equation}
                Two left, $(1\;2\;3)=(2\;3\;1)=(3\;1\;2)$ and
                $(1\;3\;2)=(3\;2\;1)=(2\;1\;3)$.
                \begin{subequations}
                    \begin{align}
                        (1\;2\;3)H&=\{\,(1\;2\;3),\,(1\;3)\}\\
                        (1\;3\;2)H&=\{\,(1\;3\;2),\,(2\;3)\,\}
                    \end{align}
                \end{subequations}
                Since $(1\;3\;2)$ and $(2\;3\;1)$ are the same cycle, we see
                that these two new computations are the same as previous
                ones.
            \end{example}
            \begin{theorem}
                If $\monoid{G}$ is a group, if $H_{1},H_{2}\subseteq{G}$ are
                subgroups, then $x(H_{1}\cap{H}_{2})=(xH_{1})\cap(xH_{2})$.
            \end{theorem}
            \begin{ftheorem}{Poincare's Index Theorem}{Poincare_Index}
                If $\monoid{G}$ is a group, if $H_{1},H_{2}\subseteq{G}$ are
                subgroups, and if $H_{1}$ and $H_{2}$ have finite index
                in $G$, then $H_{1}\cap{H}_{2}$ has finite index in $G$.
            \end{ftheorem}
        \subsection{Normal Subgroup}
            Given a group $\monoid{G}$ with a subgroup $H\subseteq{G}$, it
            is useful to know whether or not $H$ is \textit{closed} under
            multiplication modulo $H$. That is, given $x,y\in{G}$, and
            $[x],[y]\in{G}/H$, we wish to determine with $[x*y]=[x]*[y]$.
            This says that if $a\in{x}H$ and $b\in{y}H$, then $a*b\in{x}yH$.
            We can further write this by saying that
            $(xH)(yH)\subseteq{x}*H$, which is further equivalent to
            $HxH\subseteq{x}H$, and here is the property we wish to
            capitalize on.
            \begin{theorem}
                If $\monoid{G}$ is a group, and if $H\subseteq{G}$ is a
                subgroup, then $H^{2}=H$.
            \end{theorem}
            \begin{fdefinition}{Normal Subgroup}{Normal_Subgroup}
                A normal subgroup of a group $\monoid{G}$ is a subgroup
                $H\subseteq{G}$ such that for all $g\in{G}$ and for all
                $h\in{H}$ it is true that $g^{\minus{1}}hg\in{H}$.
            \end{fdefinition}
            If $H$ is normal, then left and right cosets are identical.
            The subsets of a normal subgroup are closed under set
            multiplication, and hence $G/H$ forms a group under set
            multiplication. This is the quotient group.
            \begin{theorem}
                If $\monoid{G}$ is a group, and if $H\subseteq{G}$ is a
                subgroup of index 2, then $H$ is a normal subgroup.
            \end{theorem}
            \begin{proof}
                Since $H$ has index 2 there are two distinct cosets. Since
                $eH=H$ is one of the cosets, suppose $xH$ is the other.
                Let $g\in{G}$ and $h\in{H}$. Since $xH$ is a distinct coset,
                and since cosets are disjoint, we have that $x\notin{H}$.
                But $H\cup{x}H=G$, and hence either $g^{\minus{1}}hg\in{H}$
                or $g^{\minus{1}}hg\in{x}H$. Suppose the latter. But then
                there is a $y\in{H}$ such that $xy=g^{\minus{1}}hg$.
                But then $x=(g^{\minus{1}}hg)y^{\minus{1}}$. But $H$ is a
                normal subgroup and therefore $y^{\minus{1}}\in{H}$. IDK.
            \end{proof}
        \subsection{Homomorphisms}
            Def homomorphism.
            \begin{example}
                If we have a normal subgroup $N$ of a group $\monoid{G}$ and
                map $x\in{G}$ to the coset $xN$ in the quotient group
                $G/N$, then this gives rise to a homomorphism.
            \end{example}
            Homomorphisms need not be injective nor surjective.
            \begin{theorem}
                If $\monoid[G]{G}$ and $\monoid[H]{H}$ are groups, and if
                $\varphi:G\rightarrow{H}$ is a homomorphism, then
                the image $\varphi[G]$ is a subgroup of $H$.
            \end{theorem}
            \begin{theorem}
                If $\monoid[G]{G}$ and $\monoid[H]{H}$ are groups, and if
                $\varphi:G\rightarrow{H}$ is a homomorphism, then $\varphi$
                is an isomorphism if and only if it is surjective and
                $\varphi^{\minus{1}}[e_{H}]=\{e_{G}\}$.
            \end{theorem}
            \begin{proof}
                One direction is clear since is $\varphi$ is an isomorphism,
                then it is a bijection. In the other direction, suppose
                $\varphi^{\minus{1}}[\{e_{H}\}]=\{e_{G}\}$. For if it is not
                bijective, then it is not injective, and hence there are
                $a,b\in{G}$ such that $\varphi(a)=\varphi(b)$ and $a\ne{b}$.
                But then:
                \begin{equation}
                    e_{H}=\varphi(a)*_{H}\varphi(b)^{\minus{1}}
                         =\varphi(a*_{G}b^{\minus{1}})
                \end{equation}
                thus $a*_{G}b^{\minus{1}}\in\varphi^{\minus{1}}[\{e_{H}\}]$,
                and so $a*_{G}b^{\minus{1}}=e_{G}$. But then $a=b$, a
                contradiction. Thus, $\varphi$ is bijective.
            \end{proof}
            \begin{theorem}
                If $\monoid[G]{G}$ and $\monoid[H]{H}$ are groups, if
                $\varphi:G\rightarrow{H}$ is a homomorphism, then
                $\varphi^{\minus{1}}[\{e_{H}\}]$ is a normal subgroup of
                $G$.
            \end{theorem}
            \begin{proof}
                For let $N=\varphi^{\minus{1}}[\{e_{H}\}]$. Then
                $e_{G}\in{N}$ since $\varphi(e_{G})=e_{H}$. $N$ is closed
                under products since
                $\varphi(a*_{G}b)=\varphi(a)*_{H}\varphi(b)=e_{H}$. Next,
                since $e_{H}^{\minus{1}}=e_{H}$ we have that $N$ is closed
                to inverses. So $N$ is a subgroup. It is normal for if
                $g\in{G}$ and $x\in{N}$, then:
                \begin{subequations}
                    \begin{align}
                        \varphi(g^{\minus{1}}*_{G}x*_{G}g)
                        &=\varphi(g^{\minus{1}})*_{H}
                            \varphi(x)*_{H}\varphi(g)\\
                        &=\varphi(g^{\minus{1}})*e_{H}*\varphi(g)\\
                        &=\varphi(g^{\minus{1}})*\varphi(g)\\
                        &=e_{H}
                    \end{align}
                \end{subequations}
                hence $g^{\minus{1}}*x*g\in{N}$.
            \end{proof}
            \begin{example}
                The function $f:\nsphere[1]\rightarrow\nsphere[1]$ defined
                by $f(\theta)=\exp(ki\theta)$ is a homomorphism. The
                kernel is all $\theta$ such that $k\theta=2\pi{n}$ for some
                $n\in\mathbb{Z}$.
            \end{example}
            \begin{theorem}
                The composition of homomorphisms is a homomorphism.
            \end{theorem}
            If $G$ is a group, $N$ a normal subgroup, $\varphi$ a
            homomorphism from $G/N$ to a group $H$, and if $\pi$ is the
            canonical quotient mapping, the $\varphi\circ\pi$ is a
            homomorphism and the kernel contains $N$. The converse is true.
            If $G$ is a group, and if $\varphi:G\rightarrow{H}$ is a
            homomorphism, then given a normal subgroup $N\subseteq{G}$ that
            is contained in the kernel of $\varphi$ we have that for any two
            $a,b\in{G}$ that are contained in the same coset of $N$, since
            $N$ is normal there is an $h\in{N}$ such that $a=bh$. But then
            $\varphi(a)=\varphi(bh)=\varphi(b)\varphi(h)$, and
            $\varphi(h)=e_{H}$, so $\varphi(a)=\varphi(b)$. That is, there
            is now a well defined map $\tilde{\varphi}:G/N\rightarrow{H}$
            that maps the coset $aN$ to $\varphi(a)$. Moreover,
            $\tilde{\varphi}$ is a homomorphism. If
            $\tilde{\varphi}(aN)=e_{H}$, then $\varphi(a)=e_{H}$ by
            definition. Moreover, if $\varphi(a)=e_{H}$ then
            $\tilde{\varphi}(aH)=e_{H}$, again by definition. Thus the
            kernel of $\tilde{\varphi}$ is cosets $aN$ where $a$ is in the
            kernel of $\varphi$. $\tilde{\varphi}$ is thus injective if and
            only if the kernel of $\varphi$ is equal to $N$.
            \begin{theorem}
                If $\monoid[G]{G}$ and $\monoid[H]{H}$ are groups, if
                $\varphi:G\rightarrow{H}$ is a homomorphism, if
                $N\subseteq{G}$ is a normal subgroup, if
                $N\subseteq\varphi^{\minus{1}}[\{e_{H}\}]$, then there is a
                homomorphism $\tilde{\varphi}:G/N\rightarrow{H}$ such that
                $\varphi=\tilde{\varphi}\circ\pi$, where
                $\pi:G\rightarrow{G}/N$ is the canonical quotient mapping.
                $\tilde{\varphi}$ is an isomorphism if and only if
                $N=\varphi^{\minus{1}}[\{e_{H}\}]$.
            \end{theorem}
            \begin{ftheorem}{Fundamental Theorem of Homomorphisms}
                            {Fundamental_Theorem_of_Homomorphisms}
                If $\monoid{G}$ is a group, if $N\subseteq{G}$ is a normal
                subgroup, then there is a homomorphism
                $\varphi:G\rightarrow{G}$ such that
                $N=\varphi^{\minus{1}}[\{e\}]$.
            \end{ftheorem}
            \begin{example}
                $\mathbb{R}/\mathbb{Z}$ is isomorphic to $\nsphere[1]$.
            \end{example}
            \begin{example}
                Given a cyclic group generated by $a$, then function
                $f:\mathbb{Z}\rightarrow{G}$ defined by $f(n)=a^{n}$ is a
                surjective homomorphism. Hence $G$ is isomorphic to
                $\mathbb{Z}/N$, where $N$ is the kernel of $f$. This also
                shows that any two cyclic groups of the same cardinality
                are isomorphic.
            \end{example}
            Endomorphism is homomorphism $\varphi:G\rightarrow{G}$. That is,
            a homomorphism from a group to itself. Automorphism is a
            bijective endomorphism. That is, an isomorphism
            $\varphi:G\rightarrow{G}$. Comp of endo is endo, comp of auto is
            auto. Endomorphisms form subsemigroup of group of functions
            $f:G\rightarrow{G}$. Moreover, this has identity so it is a
            monoid. Automorphisms, since bijective, have inverses and hence
            form a group. Moreover, this is the group of all invertible
            elements of the group of functions $f:G\rightarrow{G}$.
            \begin{theorem}
                If $\monoid{G}$ is a group, if $g\in{G}$, and if
                $\varphi:G\rightarrow{G}$ is defined by
                $\varphi(x)=g^{\minus{1}}*x*g$, then $\varphi$ is an
                automorphism.
            \end{theorem}
            This is called the inner automorphism defined by $a$. The set
            of such automorphisms forms a normal subgroup of the group of
            automorphisms. The quotient group is thus called the group of
            outer automorphisms. The kernel of the function
            $\varphi:G\rightarrow\autgroup{G}$ defined by
            $a\mapsto{f}$, where $f(x)=a^{\minus{1}}*x*a$, is called the
            center. This is the set of all elements $x\in{G}$ such that
            $x*y=y*x$ for all $y\in{G}$. The group of inner automorphisms is
            isomorphic to the quotient group of $G$ by its center.
            $a\mapsto{a}^{\minus{1}}$ is an automorphism if and only if
            $G$ is Abelian. $a\mapsto{a}^{n}$ is an endomorphism.
            \begin{example}
                If $G$ is a finite cyclic group and $\cardinality{G}=n$,
                then there are $\varphi(n)$ automorphisms of $G$, where
                $\varphi$ is the Euler totient function. To see this note
                that a generator of $G$ must map to a generator if the
                function is to be a generator. The generators are precisely
                the elements that have order $n$ in $G$, and correspond to
                coprime elements of $\mathbb{Z}_{n}$ with respect to $n$.
            \end{example}
        \subsection{Conjugate Classes}
            The inner automorphisms determine conjugacy classes of $G$.
            $x$ and $y$ are conjugate if there is a $g\in{G}$ such that
            $x=g^{\minus{1}}yg$. The conjugacy class of $x$ is equal to
            $\{x\}$ if and only if $x$ is in the center of $G$ since then
            $g^{\minus{1}}xg=xg^{\minus{1}}g=xe=x$.
            \begin{example}
                Conjugacy classes of $S_{n}$ correspond to the partitions of
                the integer $n$. This shows for $n>2$ that $S_{n}$ is not
                Abelian since the center is simply the idenity.
            \end{example}
    \section{Rings, Integral Domains, and Fields}
        Def rng, ring. Examples: $\mathbb{Z}$, $\mathbb{Q}$,
        $\nspace[]$, $\mathbb{C}$, $\mathbb{Q}[\sqrt{2}]$,
        $\mathbb{Z}[\sqrt{2}]$, $\mathbb{Z}[i]$,
        $\mathbb{Z}/n\mathbb{Z}$, $\funcspace{\nspace[]}$ with function
        addition and composition. $n(a+b)=na+nb$ for
        $n\in\mathbb{Z}$ and $a,b\in{R}$, also $(nm)a=n(ma)$.
        Generalized distributive law:
        \begin{equation}
            \Big(\sum_{j\in\mathbb{Z}_{m}}a_{j}\Big)
            \Big(\sum_{k\in\mathbb{Z}_{n}}b_{k}\Big)
            =\sum_{j\in\mathbb{Z}_{m}}\sum_{k\in\mathbb{Z}_{n}}a_{j}b_{k}
        \end{equation}
        Mult by zero, commutativity of $\minus{1}$, $n(ab)=(na)b=a(nb)$.
        Binomial theorem. If $\ring{R}$ is a rng, and if $\cdot$ contains a
        left cancellative element: $ae=be$ implies $a=b$, then $+$ is
        commutative. Def int domain. Ring of continuous functions is not an
        integral domain. Integral domain has cancellation. Converse is true.
        Division ring is ring $\ring{R}$ where $(R\setminus\{0\},\cdot)$ is
        a group. That is, non-zero elements have inverse elements. Field is
        commutative division ring. Division ring is integral domain. The
        converse need not hold, $\mathbb{Z}$ is an example. Division rings
        satisfy the Latin square property for non-zero elements:
        $a\cdot{x}=b$ has a unique solution for $a\ne{0}$. Mult group of
        ring. If $a$ is invertible, $\minus{a}$ is too. Finite integral
        domain is a division ring. If $\ring{R}$ is an integral domain,
        if $x\ne{0}$, and if $x^{2}=x$, then $x=1$. If $\ring{R}$ is int
        dom, $z^{n}=0$, then $z=0$. Rng with left identity has a right
        identity (It's a ring). If $\ring{R}$ is a ring, if $x\in{R}$ has
        multiple right inverses, then it has a left zero divisor and is not
        invertible.
        \begin{ftheorem}{Kaplansky's Theorem}{Kaplanskys_Theorem}
            If $\ring{R}$ is a ring, if and $x\in{R}$ has more then one
            right inverse, then it has infinitely many.
        \end{ftheorem}
        \subsection{Quasi Regularity}
            If $\ring{R}$ is a ring, and if $a$ has a right inverse $b$,
            then suppose $a=1-x$ and $b=1-y$. Then we have:
            \begin{equation}
                1=a\cdot{b}=(1-x)\cdot(1-y)=1-x-y+xy
            \end{equation}
            and hence by cancellation we have $y+x=xy$, or in other words
            $x+y-xy=0$. Here's a condition of multiplicative inverse that
            does not rely on the notion of a unital element 1.
            \begin{fdefinition}{Right Quasi Regular}{Right_Quasi_Regular}
                A right quasi regular element of a rng $\ring{R}$ is an
                element $x\in{R}$ such that there exists a $y\in{R}$ with
                $x+y-xy=0$.
            \end{fdefinition}
            If $\ring{R}$ is an actual ring (rng with identity), then given
            a quasi regular element $r\in{R}$, $1-r$ is invertible.
            \begin{theorem}
                If $\ring{R}$ is a ring, and if $r\in{R}$ is quasi regular,
                then $1-r$ is invertible with respect to $\cdot$.
            \end{theorem}
            \begin{proof}
                For since $r$ is quasi regular, there is an $s\in{R}$ such
                that $r+s=rs$. But then:
                \begin{equation}
                    (1-r)(1-s)=1-r-s+rs=1-rs+rs=1
                \end{equation}
                and hence $1-r$ is invertible.
            \end{proof}
            This gives rise to the definition of the circle composition of a
            rng.
            \begin{fdefinition}{Circle Composition of Rng}
                               {Circle_Comp_of_Rng}
                The circle composition of a rng $\ring{R}$ is the binary
                operation $\circ$ on $R$ defined by:
                \begin{equation*}
                    a\circ{b}=a+b-a\cdot{b}
                \end{equation*}
            \end{fdefinition}
            \begin{theorem}
                If $\ring{R}$ is a rng, if $\circ$ is the circle composition
                of $R$, then $\monoid[][\circ]{R}$ is a monoid.
            \end{theorem}
            \begin{proof}
                For $\circ$ is associative since:
                \begin{subequations}
                    \begin{align}
                        a\circ(b\circ{c})&=a\circ\big(b+c-bc)\\
                        &=a+(b+c-bc)-a(b+c-bc)\\
                        &=a+b+c-bc-ab-ac+abc\\
                        &=(a+b-ab)+c-(a+b-ab)c\\
                        &=(a+b-ab)\circ{c}\\
                        &=(a\circ{b})\circ{c}
                    \end{align}
                \end{subequations}
                Lastly, 0 is a unital element since:
                \begin{equation}
                    0\circ{a}=0+a-0\cdot{a}=0+a-0=a
                \end{equation}
                and:
                \begin{equation}
                    a\circ{0}=a+0-a\cdot{0}=a-0=a
                \end{equation}
                and therefore $\monoid[][\circ]{R}$ is a monoid.
            \end{proof}
            \begin{theorem}
                If $\ring{R}$ is a rng, if $\circ$ is the circle
                composition, and if $r,s\in{R}$ are right quasi regular,
                then $r\circ{s}$ is right quasi regular.
            \end{theorem}
            \begin{proof}
                For if $r$ and $s$ are right quasi regular, then there
                exists $a,b\in{R}$ such that $r+a-ra=0$ and $s+b-sb=0$
                (Def.~\ref{def:Right_Quasi_Regular}). That is,
                $r\circ{a}=0$ and $s\circ{b}=0$
                (Def.~\ref{def:Circle_Comp_of_Rng}). But then:
                \begin{equation}
                    (r\circ{s})\circ(b\circ{a})=r\circ(s\circ{b})\circ{a}
                        =r\circ{0}\circ{a}=r\circ{a}=0
                \end{equation}
                hence, $r\circ{s}$ is quasi regular.
            \end{proof}
            This pieces all together to give the following:
            \begin{theorem}
                If $\ring{R}$ is a rng, if $H\subseteq{R}$ is the set of all
                quasi regular elements of $R$, and if $\circ$ is the
                circle composition of $R$, then $\monoid[][\circ]{H}$ is a
                group.
            \end{theorem}
            \begin{proof}
                For $\circ$ is indeed a binary operation on $H$, and it is
                associative and there exists an identity. Lastly, since $H$
                is the set of all quasi regular elements, for all $x\in{H}$
                there is a $y\in{H}$ such that $x\circ{y}=0$. Hence,
                $\monoid[][\circ]{H}$ is a group.
            \end{proof}
            \begin{theorem}
                If $\ring{R}$ is a ring, if $\circ$ is the circle
                composition of $R$, and if $\varphi:R\rightarrow{R}$ is
                defined by:
                \begin{equation}
                    \varphi(r)=1-r
                \end{equation}
                then $\varphi$ is a monoid isomorphism between
                $\monoid[][\circ]{R}$ and $\monoid[][\cdot]{R}$.
            \end{theorem}
            \begin{proof}
                It is bijective, for if $1-r_{1}=1-r_{2}$, then by the
                cancellation laws we obtain $r_{1}=r_{2}$. It is surjective,
                for let $x=1-y$. The $\varphi(x)=1-(1-y)=y$. Lastly, it is
                a homomorphism since:
                \begin{subequations}
                    \begin{align}
                        \varphi(a\circ{b})
                            &=\varphi(a+b-a\cdot{b})\\
                            &=1-a-b+a\cdot{b}\\
                            &=(1-a)\cdot(1-b)\\
                            &=\varphi(a)\cdot\varphi(b)
                    \end{align}
                \end{subequations}
            \end{proof}
            \begin{theorem}
                If $\ring{R}$ is a ring, if $H$ is the set of all quasi
                regular elements of $R$, if $\circ$ is the circle
                composition of $R$, and if $R^{\times}$ is the set of all
                invertible elements of $R$ with respect to $\cdot$, then
                $\monoid[][\circ]{H}$ is group isomorphic to
                $\monoid[][\cdot]{R^{\times}}$.
            \end{theorem}
            \begin{proof}
                Use $\varphi(r)=1-r$.
            \end{proof}
            \begin{theorem}
                If $\ring{R}$ is a rng, if $x\in{R}$ is idempotent, and if
                $e$ is right quasi regular, then $x=0$.
            \end{theorem}
            \begin{proof}
                For $x\circ{x}=x+x-x^{2}=x+x-x=x$, and hence
                $x\circ{x}=x$. But $x$ is right quasi regular, and hence
                there is a $y\in{R}$ such that $x\circ{y}=0$
                (Def.~\ref{def:Right_Quasi_Regular}). But then
                \begin{equation}
                    0=x+y-xy=x(x+y-xy)=x^{2}+xy-x^{2}y=x+xy-xy=x
                \end{equation}
                and thus $x=0$.
            \end{proof}
            Nilpotent elements of a rng are quasi regular. If we had a unit
            we could write:
            \begin{equation}
                (1-x)\sum_{k=0}^{N}x^{k}=1-x^{N+1}=1
            \end{equation}
            since $x$ is nilpotent. Hence, removing the 1 from the equation,
            we can now guess what the quasi regular inverse will be.
            \begin{theorem}
                If $\ring{R}$ is a ring, and if $r\in{R}$ is nilpotent, then
                $r$ is right quasi regular.
            \end{theorem}
            \begin{proof}
                For if $r$ is nilpotent, there is an $n\in\mathbb{N}$ such
                that $r^{n}=0$. But then:
                \begin{equation}
                    x\circ\sum_{k=0}^{N}\minus{x}^{k}=0
                \end{equation}
                and hence $x$ is quasi regular.
            \end{proof}
            \begin{ftheorem}{Kaplansky's Division Ring Theorem}
                If $\ring{R}$ is a rng, then it is a division ring (with
                idenity) if and only if for all but one element of $R$ is
                right quasi regular.
            \end{ftheorem}
            \begin{bproof}
                For $e\circ{a}=e$. For if not, then it is quasi regular
                and hence there is a $b$ such that $(e\circ{a})\circ{b}=0$,
                but then $e\circ(a\circ{b})=0$, a contradiction since $e$ is
                not quasi regular. Hence $a+e-ae=e$ for all $a\in{R}$,
                and therefore $a=ae$. Thus, $e$ is a right identity. Since
                every element other than zero can be written as $e-a$ for
                some quasi regular element $a$, it follows that every
                non-zero element is invertible.
            \end{bproof}
        \subsection{Rings of Matrices}
            $n\times{n}$ matrices over a ring $\ring{R}$ form a ring. Even
            if $R$ is commutative, the ring of matrices need not be. If
            $n>0$ then there will be zero divisors in the ring of matrices
            (even if $R$ is an integral domain). For example,
            $\mathbb{Z}$ is a commutative ring (with identity), but:
            \begin{equation}
                \begin{bmatrix}
                    1&1\\
                    0&1
                \end{bmatrix}
                \begin{bmatrix}
                    1&2\\
                    0&3
                \end{bmatrix}
                =
                \begin{bmatrix}
                    1&5\\
                    0&3
                \end{bmatrix}
                \ne
                \begin{bmatrix}
                    1&3\\
                    0&3
                \end{bmatrix}
                =
                \begin{bmatrix}
                    1&2\\
                    0&3
                \end{bmatrix}
                \begin{bmatrix}
                    1&1\\
                    0&1
                \end{bmatrix}
            \end{equation}
            If we have a ring with $0\ne{1}$, and $n>1$, consider the
            following:
            \begin{equation}
                \begin{bmatrix}
                    1&0\\
                    0&0
                \end{bmatrix}
                \begin{bmatrix}
                    0&1\\
                    0&1
                \end{bmatrix}
                =
                \begin{bmatrix}
                    0&1\\
                    0&0
                \end{bmatrix}
                \ne
                \begin{bmatrix}
                    0&0\\
                    0&0
                \end{bmatrix}
                =
                \begin{bmatrix}
                    0&1\\
                    0&1
                \end{bmatrix}
                \begin{bmatrix}
                    1&0\\
                    0&0
                \end{bmatrix}
            \end{equation}
            Hence, regardless if $\ring{R}$ is a commutative ring, there
            will be non-commuting matrices for $n>1$. In general, consider:
            \begin{equation}
                \begin{bmatrix}
                    1&1&\hdots&1&1\\
                    0&0&\hdots&0&0\\
                    \vdots&\vdots&\ddots&\vdots&\vdots\\
                    0&0&\hdots&0&0
                \end{bmatrix}
                \begin{bmatrix}
                    1&0&\hdots&0&0\\
                    0&0&\hdots&0&0\\
                    \vdots&\vdots&\ddots&\vdots&\vdots\\
                    0&0&\hdots&0&0
                \end{bmatrix}
                =
                \begin{bmatrix}
                    1&0&\hdots&0&0\\
                    0&0&\hdots&0&0\\
                    \vdots&\vdots&\ddots&\vdots&\vdots\\
                    0&0&\hdots&0&0
                \end{bmatrix}
            \end{equation}
            whereas in the other direction we obtain:
            \begin{equation}
                \begin{bmatrix}
                    1&0&\hdots&0&0\\
                    0&0&\hdots&0&0\\
                    \vdots&\vdots&\ddots&\vdots&\vdots\\
                    0&0&\hdots&0&0
                \end{bmatrix}
                \begin{bmatrix}
                    1&1&\hdots&1&1\\
                    0&0&\hdots&0&0\\
                    \vdots&\vdots&\ddots&\vdots&\vdots\\
                    0&0&\hdots&0&0
                \end{bmatrix}
                =
                \begin{bmatrix}
                    1&1&\hdots&1&1\\
                    0&0&\hdots&0&0\\
                    \vdots&\vdots&\ddots&\vdots&\vdots\\
                    0&0&\hdots&0&0
                \end{bmatrix}
            \end{equation}
            and these are not equal. We also see from this how to obtain
            zero divisors. If $n>1$ and if we remove the 1 from the first
            slow we obtain:
            \begin{equation}
                \begin{bmatrix}
                    0&1&\hdots&1&1\\
                    0&0&\hdots&0&0\\
                    \vdots&\vdots&\ddots&\vdots&\vdots\\
                    0&0&\hdots&0&0
                \end{bmatrix}
                \begin{bmatrix}
                    1&0&\hdots&0&0\\
                    0&0&\hdots&0&0\\
                    \vdots&\vdots&\ddots&\vdots&\vdots\\
                    0&0&\hdots&0&0
                \end{bmatrix}
                =
                \begin{bmatrix}
                    0&0&\hdots&0&0\\
                    0&0&\hdots&0&0\\
                    \vdots&\vdots&\ddots&\vdots&\vdots\\
                    0&0&\hdots&0&0
                \end{bmatrix}
            \end{equation}
            \begin{theorem}
                If $A$ is a matrix, if $A_{ij}$ is the cofactor, then:
                \begin{equation}
                    \sum_{k=1}^{n}a_{ik}A_{jk}=
                    \begin{cases}
                        \det(A),&i=j\\
                        0,&i\ne{j}
                    \end{cases}
                \end{equation}
            \end{theorem}
            Hence, by this we have:
            \begin{equation}
                A\adj(A)=\det(A)I=\adj(A)A
            \end{equation}
            Hence if $\det(A)$ is an invertible element of the ring $R$,
            then $A$ is an invertible matrix with inverse
            $\adj(A)/\det(A)$.
            \begin{theorem}
                If $\ring{R}$ is a ring, and if $A$ is a matrix in $R$,
                then $A$ is invertible if and only if $\det(A)$ is
                invertible in $R$.
            \end{theorem}
            \begin{theorem}
                If $\ring{F}$ is a field, and if $A$ is a matrix in $F$,
                then $A$ is invertible if and only if $\det(A)\ne{0}$.
            \end{theorem}
            \begin{example}
                Consider the following matrix:
                \begin{equation}
                    A=
                    \begin{bmatrix*}[r]
                        1&4&1\\
                        0&1&\minus{1}\\
                        \minus{3}&\minus{6}&\minus{8}
                    \end{bmatrix*}
                \end{equation}
                the determinant is:
                \begin{subequations}
                    \begin{align}
                        \det(A)&=\det\Big(
                            \begin{bmatrix*}[r]
                                1&\minus{1}\\
                                \minus{6}&\minus{8}
                            \end{bmatrix*}
                        \Big)-
                        4\det\Big(
                            \begin{bmatrix*}[r]
                                0&\minus{1}\\
                                \minus{3}&\minus{8}
                            \end{bmatrix*}
                        \Big)
                        +\det\Big(
                            \begin{bmatrix*}[r]
                                0&1\\
                                \minus{3}&\minus{6}
                            \end{bmatrix*}
                        \Big)\\
                        &=(\minus{8}-6)-4(0-3)+(0+3)\\
                        &=\minus{14}+12+3\\
                        &=1
                    \end{align}
                \end{subequations}
                since 1 is a unit in $\mathbb{Z}$, there is an inverse
                matrix with elements in $\mathbb{Z}$.
            \end{example}
            \begin{theorem}
                If $\ring{R}$ is a commutative ring, if $A$ is a right
                invertible matrix, then $A$ is left invertible.
            \end{theorem}
        \subsection{Quaternions}
            The quaternions give a division ring structure on $\nspace[4]$.
            We can define it as a subset of the $2\times{2}$ matrices over
            the field $\mathbb{C}$. We consider all matrices of the form:
            \begin{equation}
                \begin{bmatrix*}[r]
                    z&w\\
                    \minus\overline{w}&\overline{z}
                \end{bmatrix*}
                =
                \begin{bmatrix*}[r]
                    a+bi&c+di\\
                    \minus{c}+di&a-bi
                \end{bmatrix*}
            \end{equation}
            This is closed under addition since:
            \begin{subequations}
                \begin{align}
                    \begin{bmatrix}
                        z_{1}&w_{1}\\[1.2ex]
                        \minus\overline{w}_{1}&\overline{z}_{1}
                    \end{bmatrix}
                    +
                    \begin{bmatrix}
                        z_{2}&w_{2}\\[1.2ex]
                        \minus\overline{w}_{2}&\overline{z}_{2}
                    \end{bmatrix}
                    &=
                    \begin{bmatrix}
                        z_{1}+z_{2}&w_{1}+w_{2}\\[1.2ex]
                        \minus\overline{w}_{1}-\overline{w}_{2}
                        &\overline{z}_{1}+\overline{z}_{2}
                    \end{bmatrix}\\
                    &=
                    \begin{bmatrix}
                        (z_{1}+z_{2})&(w_{1}+w_{2})\\[1.2ex]
                        \minus\overline{(w_{1}+w_{2})}
                            &\overline{(z_{1}+z_{2})}
                    \end{bmatrix}
                \end{align}
            \end{subequations}
            It is closed to products as well:
            \begin{subequations}
                \begin{align}
                    \begin{bmatrix}
                        z_{1}&w_{1}\\[1.2ex]
                        \minus\overline{w}_{1}&\overline{z}_{1}
                    \end{bmatrix}
                    \begin{bmatrix}
                        z_{2}&w_{2}\\[1.2ex]
                        \minus\overline{w}_{2}&\overline{z}_{2}
                    \end{bmatrix}
                    &=
                    \begin{bmatrix}
                        z_{1}z_{2}-w_{1}\overline{w}_{2}&
                        z_{1}w_{2}+w_{1}\overline{z}_{2}\\[1.2ex]
                        \minus\overline{w}_{1}z_{2}-
                            \overline{z}_{1}\overline{w}_{2}&
                        \minus\overline{w}_{1}w_{2}+
                            \overline{z}_{1}\overline{z}_{2}
                    \end{bmatrix}\\
                    &=
                    \begin{bmatrix}
                        (z_{1}z_{2}-w_{1}\overline{w}_{2})&
                        (z_{1}w_{2}+w_{1}\overline{z}_{2})\\[1.2ex]
                        \minus\overline{(z_{1}w_{1}+w_{1}\overline{z}_{2})}&
                        \overline{(z_{1}z_{2}-w_{1}\overline{w}_{2})}
                    \end{bmatrix}
                \end{align}
            \end{subequations}
            which is of the form of quaternions with
            $u=z_{1}z_{2}-w_{1}\overline{w}_{2}$ and
            $v=z_{1}w_{2}+w_{1}\overline{z}_{2}$. This subring of
            $\mathbb{C}$ is called the Quaternions. Elements have the
            following determinant:
            \begin{equation}
                \det\Big(
                    \begin{bmatrix}
                        z&w\\
                        \minus\overline{w}&\overline{z}
                    \end{bmatrix}
                \Big)
                =z\overline{z}+w\overline{w}
                =a^{2}+b^{2}+c^{2}+d^{2}
            \end{equation}
            hence if $A$ is a non-zero quaternion, then it is invertible.
            Thus the quaternions form a division ring over $\nspace[]$. The
            inverse element is:
            \begin{equation}
                \begin{bmatrix}
                    z&w\\
                    \minus\overline{w}&\overline{z}
                \end{bmatrix}^{\minus{1}}
                =\frac{1}{a^{2}+b^{2}+c^{2}+d^{2}}
                \begin{bmatrix}
                    \overline{z}&\minus{w}\\
                    \overline{w}&z
                \end{bmatrix}
            \end{equation}
            The quaternions contain an isomorphic copy of $\nspace[]$ by
            considering $w=0$ and $z=\alpha$ where $\alpha$ is a real
            number. That is, we have the following matrices:
            \begin{equation}
                A=
                \begin{bmatrix}
                    \alpha&0\\
                    0&\alpha
                \end{bmatrix}
                =\alpha
                \begin{bmatrix}
                    1&0\\
                    0&1
                \end{bmatrix}
                =\alpha{I}
            \end{equation}
            These form a 4 dimensional vector space over $\nspace[]$ as well
            with the following basis elements:
            \par
            \begin{subequations}
                \begin{minipage}[b]{0.49\textwidth}
                    \centering
                    \begin{align}
                        1&=
                        \begin{bmatrix*}[r]
                            1&0\\
                            0&\phantom{\minus}1
                        \end{bmatrix*}\\
                        i&=
                        \begin{bmatrix*}[r]
                            i&\phantom{\minus}0\\
                            0&\minus{i}
                        \end{bmatrix*}
                    \end{align}
                \end{minipage}
                \hfill
                \begin{minipage}[b]{0.49\textwidth}
                    \centering
                    \begin{align}
                        j&=
                        \begin{bmatrix*}[r]
                            0&1\\
                            \minus{1}&0
                        \end{bmatrix*}\\
                        k&=
                        \begin{bmatrix*}[r]
                            \phantom{\minus}0&i\\
                            i&0
                        \end{bmatrix*}
                    \end{align}
                \end{minipage}
            \end{subequations}
            \par\vspace{2.5ex}
            These follow the famous equations written down by William
            Rowan Hamilton:
            \begin{equation}
                i^{2}+j^{2}+k^{2}=ijk=\minus{1}
            \end{equation}
            We can also conclude the following:
            \par
            \begin{subequations}
                \begin{minipage}[b]{0.32\textwidth}
                    \centering
                    \begin{equation}
                        ij=\minus{ji}=k
                    \end{equation}
                \end{minipage}
                \hfill
                \begin{minipage}[b]{0.32\textwidth}
                    \centering
                    \begin{equation}
                        jk=\minus{kj}=i
                    \end{equation}
                \end{minipage}
                \hfill
                \begin{minipage}[b]{0.32\textwidth}
                    \centering
                    \begin{equation}
                        ki=\minus{ik}=j
                    \end{equation}
                \end{minipage}
            \end{subequations}
            Thus the quaternions are not a commutative ring. That is, unlike
            the complex numbers which form a field over $\nspace[]$, the
            quaternions do not. They form a non-commutative division ring.
            The norm of a quaternion is the determinant of its matrix
            representation. The trace is double the real part. Trace and
            norm satisfy $a^{2}-\Tr(A)a+\det(A)=0$. Rational quaternions
            form division subring of the quaternions. Norm is multiplicative
            since determinant is multiplicative.
        \subsection{Subrings and Ideals}
            Intersection of subrings is subring. Subring generated by
            subset (Intersection of all subrings containing).
            If $\ring{R}$ is a ring, if $S\subseteq{R}$, if $\overline{S}$
            is the subring generated by $S$, and if $C(S)$ is the set of
            all elements in $R$ that commute with all elements in $S$,
            then $C(S)$ is a subring, $C(S)=C(\overline{S})$, and
            $S\subseteq{C}(C(S))$. If $S_{1}\subseteq{S}_{2}$, then
            $C(S_{2})\subseteq{C}(S_{1})$ and
            $C(C(C(S)))=C(S)$. $C(R)$ is called the center of $R$.
            \begin{example}
                The center of the $n\times{n}$ matrix ring over a ring
                $\ring{R}$ is the set of matrices of the form $\alpha{I}$,
                where $\alpha\in{R}$ and $I$ is the identity matrix.
            \end{example}
            Given any additive subgroup $H\subseteq{R}$ of the Abelian
            group $\monoid[][+]{R}$, $H$ is necessarily a normal subgroup.
            If we were to look at the quotient group $R/H$ we can ask the
            question if $a$ and $a'$ are in the same equivalence class, and
            if $b$ and $b'$ are in the same equivalence class, what do we
            need to guarentee that $a\cdot{b}$ and $a'\cdot{b}'$ are in the
            same equivalence class? Since $[a]=[a']$ there is an $x\in{H}$
            such that $a'=a+x$, and similarly a $y\in{H}$ with $b'=b+y$.
            Multiplying, we obtain:
            \begin{equation}
                a'\cdot{b}'=(a+x)\cdot(b+y)
                    =a\cdot{b}+a\cdot{y}+b\cdot{x}+x\cdot{y}
            \end{equation}
            For $a\cdot{b}$ and $a'\cdot{b}'$ to be in the same equivalence
            class we require that $a\cdot{y}+b\cdot{x}+x\cdot{y}$ be
            contained in $H$. Since we want this to be true for all elements
            of $H$, we can set $b=b'$, and hence $y=0$. This implies that
            $b\cdot{x}\in{H}$. Similarly, $a\cdot{y}\in{H}$. Hence, we have
            the requirement of an ideal: $I$ is closed under addition, and
            for all $a\in{I}$ and $r\in{R}$ we have $a\cdot{r}\in{I}$ and
            $r\cdot{a}\in{I}$. If we take a ring $\ring{R}$ and mod out by
            an ideal, then we can define coset multiplication consistently
            by write $(a+I)\cdot(b+I)=a\cdot{b}+I$. This forms a ring,
            called the quotient ring. If $\ring{R}$ is commutative, then
            so is $R/I$. The coset of the identity is the identity. Not
            everything is preserved, the quotient of an integral domain
            need not be an integral domain.
            \begin{example}
                The set $nR$ for some $n\in\mathbb{N}$ is an ideal. For
                $na+nb=n(a+b)\in{n}R$, and $r\cdot(na)=n(r\cdot{a})$, as
                well as $(na)\cdot{r}=n(a\cdot{r})$. Hence, this is an
                ideal. The set of all elements in $R$ where $na=0$ is also
                an ideal. For $na+nb=0+0$, so $na+nb$ is contained in this.
                $r\cdot(na)=r\cdot{0}=0$, and also $(na)\cdot{r}=0$.
            \end{example}
            \begin{example}
                If $n\in\mathbb{N}$, then $n\mathbb{Z}$ is an ideal in
                $\mathbb{Z}$. For $na+nb=n(a+b)$, and also $a(nb)=n(ab)$.
                Suppose $m\in\mathbb{N}^{+}$ is composite, $m=a\cdot{b}$.
                Recall that $\mathbb{Z}$ is a commutative ring that is also
                an integral domain, by Euclid. That is, if
                $a,b\in\mathbb{Z}$ and if $a\cdot{b}=0$, then either $a=0$
                or $b=0$. Now we look at $\mathbb{Z}/m\mathbb{Z}$. Since $m$
                is composite, we see that $[a]\cdot[b]=[a\cdot{b}]=[m]=[0]$,
                and thus $\mathbb{Z}/m\mathbb{Z}$ is not an integral domain.
                In the case when $m$ is prime, we have proven alread that
                $\mathbb{Z}/m\mathbb{Z}$ is a field, and is hence an
                integral domain. We have also proven that the number of
                invertible elements in $\mathbb{Z}/m\mathbb{Z}$ is
                $\varphi(m)$, where $\varphi$ is the Euler totient function.
                If $m$ is prime we have $\varphi(m)=m-1$, which just states
                that everything except for 0 is invertible.
            \end{example}
            Using Lagrange's theorem, if $\monoid{G}$ is a finite group with
            $n$ elements, then $a^{n}$ is the unital element for all
            $a\in{G}$. For there is a least $m\in\mathbb{N}^{+}$ such that
            $a^{m}=e$, and $m\leq{n}$. The set of powers of $a$ forms a
            subgroup, and hence the cardinality of this subgroup divides
            $n$. That is, $m$ divides $n$: $n=mk$. But the
            $a^{n}=a^{mk}=(a^{m})^{k}=e^{k}=e$. Looking at
            $\mathbb{Z}/m\mathbb{Z}$, if $k\in\mathbb{Z}$ and
            $\GCD(k,m)=1$, $k^{\varphi(m)}\equiv{1}$ in
            $\mathbb{Z}/m\mathbb{Z}$. For $\varphi(m)$ is the order of the
            group of units in $\mathbb{Z}/m\mathbb{Z}$, and since
            $\GCD(m,k)=1$, the equivalence class of $k$ is a unit. The
            corrollary if this is Fermat's Little Theorem:
            \begin{theorem}
                If $p\in\mathbb{N}^{+}$ is prime, and if $a\ne{0}$, then
                $a^{p-1}\equiv{1}\mod{p}$.
            \end{theorem}
            \begin{theorem}
                If $\ring{R}$ is a finite division ring with $n$ elements,
                then $a^{n}=a$ for all $a\in{R}$.
            \end{theorem}
            \begin{proof}
                If $\ring{R}$ is a division ring, then $R\setminus\{0\}$ is
                a group under multiplication. Hence this follows from the
                fact that this result holds for groups.
            \end{proof}
        \subsection{Ring Homomorphisms}
            \begin{example}
                $\mathbb{C}$ is isomorphic to the set of $2\times{2}$
                matrices over $\nspace[]$ defined by:
                \begin{equation}
                    A=
                    \begin{bmatrix}
                        \alpha&\beta\\
                        \minus\beta&\alpha
                    \end{bmatrix}
                \end{equation}
                with $\alpha,\beta\in\nspace[]$. We map
                $\alpha+i\beta$ to this matrix. Then 1 maps to the identity,
                0 maps to 0, and addition is preserved. Lastly:
                \begin{subequations}
                    \begin{align}
                        \varphi\big((a+ib)(c+id)\big)
                        &=\varphi\big((ac-bd)+i(ad+bc)\big)\\
                        &=
                        \begin{bmatrix}
                            ac-bd&ad+bc\\
                            \minus(ad+bc)&ac-bd
                        \end{bmatrix}\\
                        &=
                        \begin{bmatrix*}[r]
                            a&b\\
                            \minus{b}&a
                        \end{bmatrix*}
                        \begin{bmatrix*}[r]
                            c&d\\
                            \minus{d}&c
                        \end{bmatrix*}\\
                        &=\varphi(a+ib)\varphi(c+id)
                    \end{align}
                \end{subequations}
            \end{example}
            \begin{example}
                The function $\varphi:\mathbb{C}\rightarrow\mathbb{C}$
                defined by $\varphi(a+ib)=a-ib$ is an automorphism. This is
                just conjugation, and
                $\overline{z+w}=\overline{z}+\overline{w}$ and
                $\overline{zw}=\overline{z}\overline{w}$. Morever, this is
                bijective and $\overline{\overline{z}}=z$.
            \end{example}
            \begin{example}
                There's an isomorphism from the quaternions into a subset of
                $4\times{4}$ matrices over $\nspace[]$ defined by:
                \begin{equation}
                    \varphi(a+ib+jc+kd)=
                    \begin{bmatrix*}[r]
                        a&b&c&d\\
                        \minus{b}&a&\minus{d}&c\\
                        \minus{c}&d&a&\minus{b}\\
                        \minus{d}&\minus{c}&b&a
                    \end{bmatrix*}
                \end{equation}
            \end{example}
            \begin{theorem}
                If $\ring[R]{R}$ and $\ring[S]{S}$ are rings, and if
                $\varphi:R\rightarrow{S}$ is a ring homomorphism, then
                $\varphi[R]$ is a subring of $S$.
            \end{theorem}
            \begin{theorem}
                If $\ring[R]{R}$ and $\ring[S]{S}$ are rings, and if
                $\varphi:R\rightarrow{S}$ is a ring homomorphism, then
                $\varphi^{\minus{1}}[\{0_{S}\}]$ is an ideal in $R$. That
                is, the kernel of $\varphi$ is an ideal.
            \end{theorem}
            The canonical projection map from $R$ to $R/I$ is a ring
            homomorphism. If $\varphi:R\rightarrow{S}$ is a ring
            homomorphism with kernel $K$ and if $I\subseteq{K}$ is an ideal,
            then there is an induced homomorphism
            $\tilde{\varphi}:R/I\rightarrow{S}$ defined by
            $\tilde{\varphi}(a+I)=\varphi(a)$. This is an additive
            homomorphism, and moreover since $I$ is contained in the kernel
            we have:
            \begin{equation}
                \tilde{\varphi}\big((a+I)(b+I)\big)=\tilde{\varphi}(ab+I)
                =\varphi(ab)=\varphi(a)\varphi(b)
                =\tilde{\varphi}(a+I)\tilde{\varphi}(b+I)
            \end{equation}
            \begin{theorem}
                If $\ring[R]{R}$ and $\ring[S]{S}$ are rings, if
                $\varphi:R\rightarrow{S}$ is a ring homomorphism, and if
                $I\subseteq\varphi^{\minus{1}}[\{0_{S}\}]$, and if $I$ is an
                ideal in $R$, then there is a homomorphism
                $\tilde{\varphi}:R/I\rightarrow{S}$ such that
                $\varphi=\tilde{\varphi}\circ\pi$, where
                $\pi:R\rightarrow{R}/I$ is the canonical projection map.
                $\tilde{\varphi}$ is an isomorphism if and only if
                $I=\varphi^{\minus{1}}[\{0_{S}\}]$.
            \end{theorem}
            \begin{ftheorem}{Fundamental Theorem of Ring Homomorphisms}
                            {Fundamental_Theorem_of_Ring_Homomorphism}
                If $\ring{R}$ is a ring, if $I\subseteq{R}$ is an ideal,
                then there is a homomorphism $\varphi:R\rightarrow{R}$ such
                that $\varphi[R]=I$.
            \end{ftheorem}
            \begin{fdefinition}{Simple Ring}{Simple_Ring}
                A simple ring is a ring $\ring{R}$ such that the only two
                sided ideals are the 0 ideal and all of $R$.
            \end{fdefinition}
            \begin{theorem}
                If $\ring{R}$ is a commutative ring, then $R$ is a field if
                and only if $\ring{R}$ is a simple ring.
            \end{theorem}
            \begin{proof}
                If $\ring{R}$ is a field, and $I\subseteq{R}$ is a field,
                then either $I=\{0\}$ or not. If not, then there is an
                $a\in{I}$ such that $a\ne{0}$. But $R$ is a field and hence
                there is an $a^{\minus{1}}\in{R}$ such that
                $a\cdot{a}^{\minus{1}}=1$. But $I$ is an ideal and therefore
                $a\cdot{a}^{\minus{1}}\in{I}$, and thus $1\in{I}$. But then
                for all $r\in{R}$, $r=r\cdot{1}$ and this is contained in
                $I$ since $I$ is an ideal. Hence, the only two sided ideals
                are the zero ideal and all of $R$. In the other direction,
                if $\ring{R}$ is a commutative ring and if $r\in{R}$ is
                non-zero, then the ideal generated by $r$ must be all of
                $I$. But then there is a $b\in{I}$ such that $r\cdot{b}=1$
                and thus $r$ is invertible. Hence, $\ring{R}$ is a field.
            \end{proof}
            \begin{theorem}
                If $\ring{R}$ is a simple ring, and if
                $\varphi:R\rightarrow{R}$ is a simple ring, then either
                $\varphi[R]=\{0\}$ or $\varphi[R]=R$.
            \end{theorem}
            \begin{proof}
                For by the fundamental theorem of ring homomorphisms,
                $\varphi[R]$ is an ideal in $R$. But since $R$ is simple
                the only ideals are $\{0\}$ and $R$. Hence, either
                $\varphi[R]=\{0\}$ or $\varphi[R]=R$.
            \end{proof}
            One application of this is that if $\ring{R}$ is a ring
            generated by the identity $e\in{R}$, then either $R$ is
            isomorphic to $\mathbb{Z}$, or there is an $n\in\mathbb{N}^{+}$
            such that $R$ is isomorphic to $\mathbb{Z}/n\mathbb{Z}$. For we
            can place a homomorphic image of $\mathbb{Z}$ into $R$ by
            $\varphi(n)=ne$.
            \begin{theorem}
                If $m,n\in\mathbb{N}^{+}$, if $m$ divides $n$, then
                $n\mathbb{Z}$ is a subring of $m\mathbb{Z}$.
            \end{theorem}
            \begin{theorem}
                If $m,n\in\mathbb{N}^{+}$, and if $m$ divides $n$, then
                $m\mathbb{Z}/n\mathbb{Z}$ is an ideal in
                $\mathbb{Z}/n\mathbb{Z}$ and
                $(\mathbb{Z}/n\mathbb{Z})/(m\mathbb{Z}/n\mathbb{Z})$ is
                isomorphic to $\mathbb{Z}/n\mathbb{Z}$.
            \end{theorem}
            \begin{theorem}
                If $\ring[R]{R}$ and $\ring[S]{S}$ are rings, if
                $n\in\mathbb{N}^{+}$, if $\varphi:R\rightarrow{S}$ is a
                ring homomorphism, and if
                $\tilde{\varphi}:\matspace{R}\rightarrow\matspace{S}$ is
                defined by $(a_{ij})\mapsto\big(\varphi(a_{ij})\big)$,
                then $\tilde{\varphi}$ is a ring homomorphism.
            \end{theorem}
            \begin{proof}
                For:
                \begin{subequations}
                    \begin{align}
                        \tilde{\varphi}\big((a_{ij})+(b_{ij})\big)
                        &=\tilde{\varphi}\big((a_{ij}+b_{ij})\big)\\
                        &=\big(\varphi(a_{ij}+b_{ij})\big)\\
                        &=\big(\varphi(a_{ij})+\varphi(b_{ij})\big)\\
                        &=\big(\varphi(a_{ij})\big)
                            +\big(\varphi(b_{ij})\big)\\
                        &=\tilde{\varphi}\big((a_{ij})\big)+
                            \tilde{\varphi}\big((b_{ij})\big)
                    \end{align}
                \end{subequations}
                and hence $\tilde{\varphi}$ is additive. Moreover it is
                multiplicative since:
                \par
                \begin{subequations}
                    \begin{minipage}[t]{0.55\textwidth}
                        \centering
                        \begin{align}
                            \tilde{\varphi}\big((a_{ij})(b_{ij})\big)
                            &=\tilde{\varphi}\Big(
                                \big(\sum_{k\in\mathbb{Z}_{n}}
                                    a_{ik}b_{kj}\big)
                            \Big)\\
                            &=\Big(\varphi\big(
                                \sum_{k\in\mathbb{Z}_{n}}a_{ij}b_{kj}\big)
                                \Big)\\
                            &=\Big(\sum_{k\in\mathbb{Z}_{n}}
                                \varphi\big(a_{ij}b_{kj}\big)\Big)
                        \end{align}
                    \end{minipage}
                    \hfill
                    \begin{minipage}[t]{0.44\textwidth}
                        \centering
                        \begin{align}
                            &=\Big(\sum_{k\in\mathbb{Z}_{n}}
                                \varphi(a_{ik})\varphi(b_{kj})\Big)\\
                            &=\big(\varphi\big(a_{ij}\big)\big)
                                \big(\varphi\big(b_{ij}\big)\big)
                                \vphantom{\Big(\sum_{k\in\mathbb{Z}_{n}}}\\
                            &=\tilde{\varphi}\big((a_{ij})\big)
                                \tilde{\varphi}\big((b_{ij})\big)
                                \vphantom{\Big(\sum_{k\in\mathbb{Z}_{n}}}
                        \end{align}
                    \end{minipage}
                \end{subequations}
                Lastly, since $\varphi$ is a ring homomorphism,
                $\varphi(1_{R})=1_{S}$, and hence $I_{R}$ maps to $I_{S}$.
                Thus, $\tilde{\varphi}$ is a ring homomorphism.
            \end{proof}
            \begin{theorem}
                If $\ring{R}$ is a ring, if $\varphi:R\rightarrow{R}$ is a
                ring homomorphism, and if $A\subseteq{R}$ is the set of all
                fixed elements of $\varphi$, then $A$ is a subring of $R$.
            \end{theorem}
            \begin{theorem}
                The only ring homomorphisms
                $\varphi:\mathbb{Z}\rightarrow\mathbb{Z}$ is the identity.
            \end{theorem}
            \begin{proof}
                For by induction on $n\in\mathbb{N}^{+}$ we have
                $\varphi(1)=1$, and $\varphi(n+1)=\varphi(n)+\varphi(1)=n+1$
                and similarly for the negatives.
            \end{proof}
            \begin{theorem}
                If $A$ is a set, if $\ring{R}$ is a ring, and if
                $\varphi:A\rightarrow{R}$ is a bijection, then the binary
                operations $\oplus$ and $\otimes$ on $A$ defined by
                $a\oplus{b}=f^{\minus{1}}\big(f(a)+f(b)\big)$ and
                $a\otimes{b}=f^{\minus{1}}\big(f(a)\cdot{f}(b)\big)$ makes
                $(A,\oplus,\otimes)$ a ring such that $f$ is an isomorphism.
            \end{theorem}
            \begin{theorem}
                If $\ring{R}$ is a ring, and if $\oplus$ and $\otimes$ are
                the binary operations defined by:
                \twocolumneq{a\oplus{b}=a+b-1}
                            {a\otimes{b}=a+b-a\cdot{b}}
                Then $(R,\oplus,\otimes)$ is a ring.
            \end{theorem}
        \subsection{Anti-Isomorphism}
            The conjugate of the quaternion $a+ib+jc+kd$ is $a-ib-jc-kd$.
            The inverse is given by $q^{\minus{1}}=\overline{q}/\norm{q}$.
            Conjugation gives rise to the notion of a ring anti-isomorphism.
            While conjugation preserves addition:
            $\overline{a+b}=\overline{a}+\overline{b}$, it reverses
            multiplication: $\overline{ab}=\overline{b}\overline{a}$. This
            issue does not arise in the complex numbers since multiplication
            is commutative in this ring. The quaternions are not a
            commutative ring, and therein lies the problem.
            \begin{fdefinition}{Ring Anti-Isomorphism}
                               {Ring Anti-Isomorphism}
                A ring anti-isomorphism from a ring $\ring[R]{R}$ to a ring
                $\ring[S]{S}$ is a bijection $\varphi:R\rightarrow{S}$ such
                that for all $a,b\in{R}$ it is true that:
                \par
                \begin{minipage}[b]{0.49\textwidth}
                    \centering
                    \begin{equation*}
                        \varphi(a+_{R}b)=\varphi(a)+_{S}\varphi(b)
                    \end{equation*}
                \end{minipage}
                \hfill
                \begin{minipage}[b]{0.49\textwidth}
                    \centering
                    \begin{equation*}
                        \varphi(a\cdot_{R}b)=\varphi(b)\cdot_{R}\varphi(b)
                    \end{equation*}
                \end{minipage}
            \end{fdefinition}
            \begin{theorem}
                If $\ring{R}$ is a commutative ring, if $n\in\mathbb{N}^{+}$
                is a positive integer, and if
                $T:\matspace{R}\rightarrow\matspace{R}$ is the transpose
                operator, then $T$ is an anti-isomorphism.
            \end{theorem}
            \begin{theorem}
                If $\ring{R}$ is a ring, and if $\times$ is the binary
                operation on $R$ defined by $a\times{b}=b\cdot{a}$, then
                $(R,+,\times)$ is a ring that is anti-isomorphic to
                $\ring{R}$.
            \end{theorem}
            \begin{theorem}
                If $\ring[R]{R}$ and $\ring[S]{S}$ are rngs, if
                $\varphi:R\rightarrow{S}$ is an anti-isomorphism, and if
                $R$ has a left identity, then $S$ has a right identiy.
            \end{theorem}
            If we relax the definition to groups, we see that every group is
            anti-isomorphic to itself: define $\varphi(g)=g^{\minus{1}}$.
            \begin{theorem}
                The set of anti-automorphisms form a group. So does the set
                of automorphisms.
            \end{theorem}
            \begin{theorem}
                If $\ring[R]{R}$ and $\ring[S]{S}$ are rings, if
                $n\in\mathbb{N}^{+}$ is a positive integer, if
                $T:\matspace{R}\rightarrow\matspace{S}$ is the transpose
                operator, and if $\varphi:R\rightarrow{S}$ is an
                anti-isomorphism, then
                $\tilde{\varphi}:\matspace{R}\rightarrow\matspace{S}$
                defined by:
                \begin{equation}
                    \tilde{\varphi}\big((a_{ij})\big)
                        =T\big((\varphi(a_{ij})\big))
                \end{equation}
                is an anti isomorphism.
            \end{theorem}
            \begin{ftheorem}{Hua's Theorem}{Huas_Theorem}
                If $\ring[R]{R}$ and $\ring[S]{S}$ are rings, if
                $\varphi:R\rightarrow{S}$ is a function such that for all
                $a,b\in{R}$ it is true that
                $\varphi(a+_{R}b)=\varphi(a)+_{S}\varphi(b)$ and either
                $\varphi(a\cdot_{R}b)=\varphi(a)\cdot_{S}\varphi(b)$ or
                $\varphi(a\cdot_{R}b)=\varphi(b)\cdot_{S}\varphi(b)$, then
                either $\varphi$ is a homomorphism and an anti-homomorphism.
            \end{ftheorem}
            \begin{bproof}
                For suppose not. Then there exists $a,b\in{R}$ and
                $c,d\in{R}$ such that
                $\varphi(a\cdot_{R}b)\ne\varphi(a)\cdot_{S}\varphi(b)$ and
                $\varphi(c\cdot_{R}d)\ne\varphi(b)\cdot_{S}\varphi(b)$. But
                then by hypothesis,
                $\varphi(a\cdot_{R}b)=\varphi(b)\cdot_{S}\varphi(a)$ and
                $\varphi(c\cdot_{R}d)=\varphi(c)\cdot_{S}\varphi(d)$.
            \end{bproof}
            If $1$ has order $m$, then so does every other element since
            $ma=m(1a)=(m1)a=0a=0$. If a maximum such $m$ exists for all
            $a\in{R}$ then $m$ is called the characteristic of the ring.
            If no such max exists, we say the ring has characteristic zero.
            \begin{theorem}
                If $\ring{R}$ is an integral domain, then either $R$ has
                characteristic zero, or there is an integer $m\in{R}$ such
                that for all $r\in{R}$, $mr=0$ and $m$ is the least such
                value for all $r$.
            \end{theorem}
            \begin{theorem}
                The characteristic of an integral domain is prime.
            \end{theorem}
            \begin{proof}
                For suppose not and suppose $ma=0$ with $m=pq$ and
                $a\ne{0}$. Since $R$ is an integral domain $a^{2}\ne{0}$,
                and $ma^{2}={0}$. But then $(pa)(qa)=0$, contradicting that
                $pa\ne{0}$ and $qa\ne{0}$.
            \end{proof}
            \begin{theorem}
                If $\ring{R}$ is an integral domain of characteristic zero,
                then all non-zero elements have infinite order.
            \end{theorem}
            \begin{proof}
                For suppose $a$ has finite order $m$. Then
                $ma=m(1a)=(m1)a=0$. But $a\ne{0}$, and hence $m1=0$, but
                1 has infinite order.
            \end{proof}
            \begin{theorem}
                If $\ring{R}$ is a simple ring of characteristic zero, then
                every non-zero element has infinite order.
            \end{theorem}
            \begin{proof}
                For suppose not, and let $a\in{R}$ have finite order $m$.
                Then the subring generated by $a$ is an ideal that is not
                the entirety of $R$ nor is it the zero ideal, contradicting
                that $R$ is simple.
            \end{proof}
            \begin{theorem}
                If $\ring{R}$ is a simple ring of characteristic $m$, then
                $m$ is prime.
            \end{theorem}
            \begin{proof}
                For suppose $a$ has characteristic $m$ that is not prime,
                $m=pq$. Then $a^{p}$ forms a proper ideal that is non-zero.
            \end{proof}
            \begin{theorem}
                If $\monoid{G}$ is an Abelian group, and if
                $A,B\subseteq{G}$ are subgroups, then the group generated
                by $A\cup{B}$ is equal to $AB$.
            \end{theorem}
            \begin{proof}
                Since $A$ and $B$ are subgroups of an Abelian group, they
                are normal, and hence $AB$ is a subgroup.
            \end{proof}
            The subgroup generated by the union of an arbitrary collection
            of subgroups of an Abelian group is the set of all finite sums
            of elements of the collection. The subgroup generated by the
            product of two subgroups (In a ring now) is the collection of
            all finite sums $a_{k}b_{k}$ with $a_{k}\in{A}$ and
            $b_{k}\in{B}$. The associative and distributive laws hold for
            this form of set multiplication: $A(BC)=(AB)C$ and
            $A(B+C)=AB+AC$. From this, $A^{k}$ is the set of finite sums of
            the form $a_{1}a_{2}\dots{a}_{k-1}a_{k}$.
            \begin{theorem}
                If $\ring{R}$ is a ring, if $A\subseteq{R}$ is a subgroup of
                $\monoid[][+]{R}$, then $A$ is a subring if and only if
                $A^{2}\subseteq{A}$.
            \end{theorem}
            \begin{theorem}
                If $\ring{R}$ is a ring, if $A\subseteq{R}$ is a subgroup of
                $\monoid[][+]{R}$, then $A$ is a two sided ideal if and
                only if $AR\subseteq{A}$ and $RA\subseteq{A}$.
            \end{theorem}
            Def left ideal, right ideal. $Rb$ is a left ideal,
            $aR$ is a right ideal, for all $a,b\in{R}$.
            \begin{theorem}
                $\ring{R}$ is a division ring if and only if it contains no
                proper left or right ideals.
            \end{theorem}
            A principal ideal is an ideal generated by a single element. If
            $\ring{R}$ is a rng (no identity), we need more than just
            $rb$ to characterize the ideal generated by $b$, but rather
            need $nb+rb$ for all $r\in{R}$ and $n\in\mathbb{Z}$. This
            ensures that the ideal generated by $b$ actually contains $b$ in
            it (Set $n=1$ and $x=0$). For a ring (with identity)
            $(b)=bR$, and for a rng, $bR\subseteq(b)$.
            \begin{theorem}
                If $\ring{S}$ is a ring, if $L$ is a right sided ideal,
                and if $R$ is a right sided ideal, then $LR$ is a two sided
                ideal in $S$.
            \end{theorem}
            \begin{theorem}
                If $\ring{R}$ is a ring, if $A\subseteq{R}$ is a subgroup of
                $\monoid[][+]{R}$, and if $L\subseteq{R}$ is a left ideal,
                then $LA$ is a left ideal.
            \end{theorem}
            \begin{theorem}
                If $\ring{R}$ is a ring, then $R^{n}$ is an ideal in $R$.
            \end{theorem}
        \subsection{Ring of Endomorphisms of Commutative Group}
            If $\monoid{G}$ is an Abelian group, the set of all group
            endomorphisms $\varphi:G\rightarrow{G}$ can be given a ring
            structure. We define function addition pointwise. Given
            $\alpha,\beta\in\Endomorphisms{R}$, we define $\alpha+\beta$
            to be the function:
            \begin{equation}
                (\alpha+\beta)(r)=\alpha(r)*\beta(r)
            \end{equation}
            \begin{theorem}
                If $\monoid{G}$ is an Abelian group, if
                $\alpha,\beta\in\Endomorphisms{R}$ are group endomorphisms,
                and if $+$ denotes function addition, then
                $\alpha+\beta\in\Endomorphisms{R}$.
            \end{theorem}
            \begin{proof}
                For:
                \begin{subequations}
                    \begin{align}
                        (\alpha+\beta)(a*b)
                            &=\alpha(a*b)*\beta(a*b)\\
                            &=\big(\alpha(a)*\alpha(b)\big)
                                *\big(\beta(a)*\beta(b)\big)\\
                            &=\alpha(a)*\big(\alpha(a)*\beta(b)\big)
                                *\beta(b)\\
                            &=\alpha*\big(\beta(b)*\alpha(a)\big)*\beta(b)\\
                            &=\big(\alpha(a)*\beta(a)\big)*
                                \big(\alpha(b)*\beta(b)\big)\\
                            &=(\alpha+\beta)(a)*(\alpha+\beta)(b)
                    \end{align}
                    Hence, $(\alpha+\beta)(a*b)=(\alpha+\beta)(a)*(\alpha+\beta)(b)$
                    and so $\alpha+\beta$ is a homomorphism from $G$ to
                    itself, and therefore an endomorphism.
                \end{subequations}
            \end{proof}
            \begin{theorem}
                If $\monoid{G}$ is an Abelian group, and if $+$ denotes
                function addition in $G$, then
                $\monoid[][+]{\Endomorphisms{R}}$ is an Abelian group.
            \end{theorem}
            \begin{proof}
                For let $E:G\rightarrow{G}$ be the zero
                endomorphism: $\tilde{0}(r)=e$ for all $g\in{G}$. Then if
                $\alpha\in\Endomorphisms{G}$, for all $g\in{G}$ we have:
                \begin{equation}
                    (\alpha+E)(r)=\alpha(r)*E(r)=\alpha(r)*e=\alpha(r)
                \end{equation}
                and hence $\alpha+E=\alpha$. If
                $\alpha\in\Endomorphisms{R}$, let $\minus{\alpha}$ be the
                endomorphism $(\minus\alpha)(g)=\alpha(g)^{\minus{1}}$
                where $\alpha(g)^{\minus{1}}$ is the unique inverse element
                of $\alpha(r)$ in the group $\monoid{G}$. Then for all
                $g\in{G}$ we have:
                \begin{equation}
                    \big(\alpha+(\minus{\alpha})\big)(g)
                    =\alpha(r)*(\minus\alpha)(r)
                    =\alpha(r)*\alpha(r)^{\minus{1}}
                    =e
                \end{equation}
                and therefore $\alpha+(\minus{\alpha})=\tilde{0}$. Addition
                of functions is associative, since:
                \begin{subequations}
                    \begin{align}
                        \big(\alpha+(\beta+\gamma)\big)(r)
                        &=\alpha(r)*(\beta+\gamma)(r)\\
                        &=\alpha*(\beta(r)*\gamma(r))\\
                        &=(\alpha*\beta(r))*\gamma(r)\\
                        &=(\alpha+\beta)(r)*\gamma(r)\\
                        &=\big((\alpha+\beta)+\gamma\big)(r)
                    \end{align}
                \end{subequations}
                and thus $\monoid[][+]{\Endomorphisms{G}}$ is a group. It is
                Abelian since $\monoid{G}$ is an Abelian group, and hence
                for all $\alpha,\beta\in\Endomorphisms{G}$ we have:
                \begin{equation}
                    (\alpha+\beta)(g)=\alpha(g)*\beta(g)
                        =\beta(g)*\alpha(g)
                        =(\beta+\alpha)(g)
                \end{equation}
                and hence function addition is commutative.
            \end{proof}
            To show that $\Endomorphisms{G}$ has a ring structure simply
            requires us to show that the distributive laws hold.
            \begin{theorem}
                If $\monoid{G}$ is an Abelian group, and if $+$ is function
                addition, then $(\Endomorphisms{R},+,\circ)$ is a ring.
            \end{theorem}
            \begin{proof}
                We know that $\identity{G}$ is the identity element of
                $\Endomorphisms{G}$, and that
                $\monoid[][+]{\Endomorphisms{G}}$ is an Abelian group.
                Moreover, $\circ$ is associative. All that's needed is to
                show that $\circ$ distributives over $+$. But:
                \begin{subequations}
                    \begin{align}
                        (\alpha\circ(\beta+\gamma))(g)
                        &=\alpha\big((\beta+\gamma)(g)\big)\\
                        &=\alpha\big(\beta(g)*\gamma(g)\big)\\
                        &=\alpha\big(\beta(g)\big)*
                            \alpha\big(\gamma(g)\big)\\
                        &=(\alpha\circ\beta)(g)*(\alpha\circ\gamma)(g)\\
                        &=(\alpha\circ\beta+\alpha\circ\gamma)(g)
                    \end{align}
                \end{subequations}
                similarly composition left distributes.
            \end{proof}
            We call this the ring of endomorphisms of $G$. These rings play
            a similar role as the groups of permutations do in group theory.
            \begin{example}
                Consider the Abelian group $\monoid[][+]{\mathbb{Z}}$. From
                the previous theorem, we know that
                $(\Endomorphisms{\mathbb{Z}},+,\circ)$ is a ring, where we
                have doubly used the symbol $+$ out of laziness. Any
                endomorphism is uniquely determined by what it does to
                $1$, since $\varphi(n)=\varphi(n\cdot{1})=n\cdot\varphi(1)$.
                Moreover, to every $n\in\mathbb{Z}$ there is an endomorphism
                $\varphi(1)=n$. This puts $\Endomorphisms{\mathbb{Z}}$ into
                a bijection with $\mathbb{Z}$, and moreover the ring
                structure induced on the endomorphims is precisely the
                familiar ring structure $\ring{\mathbb{Z}}$.
            \end{example}
            \begin{example}
                Given $n\in\mathbb{N}^{+}$, the hyper lattice
                $\mathbb{Z}^{n}$ can be given an Abelian group structure by
                considering pointwise addition. That is, if
                $a,b\in\mathbb{Z}^{n}$, $a+b$ is the element such that
                $(a+b)_{k}=a_{k}+b_{k}$ for all $k\in\mathbb{Z}_{n}$. By
                letting $e_{k}$ be the element that is 1 in the $k^{th}$
                entry and zero everywhere else, we see that any
                endomorphism $\alpha\in\Endomorphisms{\mathbb{Z}^{n}}$ is
                uniquely determined by how it acts on the $e_{k}$. Moreover,
                every element of $\mathbb{Z}^{n}$ gives rise to an
                endomorphism by mapping
                $e=(e_{0},\dots,e_{n-1})$ to $m=(m_{0},\dots,m_{n-1})$.
                If $\alpha\in\Endomorphisms{\mathbb{Z}^{n}}$, let
                $a_{k}\in\mathbb{Z}^{n}$ be the image of $\alpha(e_{k})$. We
                can then represent $\alpha$ uniquely by these $a_{k}$,
                forming an $n\times{n}$ matrix over $\mathbb{Z}$:
                \begin{subequations}
                    \begin{align}
                        A&=
                        \begin{bmatrix}
                            \alpha(e_{0})_{0}&\alpha(e_{1})_{0}
                                &\hdots&\alpha(e_{n-1})_{0}\\
                            \alpha(e_{0})_{1}&\alpha(e_{1})_{1}
                                &\hdots&\alpha(e_{n-1})_{1}\\
                            \vdots&\vdots&\ddots&\vdots\\
                            \alpha(e_{0})_{n-1}&\alpha(e_{1})_{n-1}
                                &\hdots&\alpha(e_{n-1})_{n-1}
                        \end{bmatrix}\\[1ex]
                        &=
                        \begin{bmatrix}
                            a_{(0,0)}&a_{(0,1)}&\hdots&a_{(0,n-1)}\\
                            a_{(1,0)}&a_{(1,1)}&\hdots&a_{(1,n-1)}\\
                            \vdots&\vdots&\ddots&\vdots\\
                            a_{(n-1,0)}&a_{(n-1,1)}&\hdots&a_{(n-1,n-1)}
                        \end{bmatrix}
                    \end{align}
                \end{subequations}
                This shows the the matrix ring of $n\times{n}$ matrices over
                $\mathbb{Z}$ is isomorphic to the endomorphism ring of
                the Abelian group $\monoid[][+]{\mathbb{Z}^{n}}$.
            \end{example}
            The above example shows that we can determine the group of
            automorphisms of $\monoid[][+]{\mathbb{Z}^{n}}$ by finding the
            invertible elements of $\matspace{\mathbb{Z}}$. The subgroup of
            invertible elements of a ring $\ring[R]{R}$ map to the
            invertible elements of a ring $\ring[S]{S}$ under isomorphism.
            \begin{example}
                If $n\in\mathbb{N}^{+}$, we can compute the ring of
                endomorphims of the Abelian group
                $\monoid[][+]{\mathbb{Z}/n\mathbb{Z}}$ in a similar manner
                as with $\mathbb{Z}$. If
                $\alpha\in\Endomorphisms{\mathbb{Z}/n\mathbb{Z}}$, then
                $\varphi(k)=\varphi(k\cdot{1})=k\cdot\varphi(1)$, and hence
                $\varphi$ is uniquely determined by what it does to 1. Also,
                if $k\in\mathbb{Z}/n\mathbb{Z}$, then $\varphi(1)=k$ defines
                an endomorphism since:
                \begin{equation}
                    \varphi(a+b)=(a+b)\varphi(1)
                        =(a+b)k
                        =ak+bk
                        =\varphi(a)+\varphi(b)
                \end{equation}
                we see that function composition corresponds to
                multiplication, and thus the ring of endomorphisms on
                $\mathbb{Z}/n\mathbb{Z}$ is isomorphic to the usual ring
                structure on $\mathbb{Z}/n\mathbb{Z}$.
            \end{example}
            If $\ring{R}$ is a rng, we can define $f:R\rightarrow{R}$ by
            $f(x)=x\cdot{r}$ for some fixed $r\in{R}$. This is an
            endomorphism of the Abelian group $\monoid[][+]{R}$ by the
            distributive law. We can also get a homomorphism
            $\varphi:R\rightarrow\Endomorphisms{R}$ by
            $r\mapsto{f}$ with $f(x)=x\cdot{r}$. The image of this is thus
            a subring of the ring of endomorphims. The kernel of $\varphi$
            is all elements $r\in{R}$ such that $x\cdot{r}=0$ for all
            $x\in{R}$. This is called the right annihilator of $R$. If $R$
            has identity, then $r=1\cdot{r}=0$, and hence $r=0$ so the
            kernel is $\{0\}$ and $\varphi$ is an isomorphism.
            \begin{theorem}
                If $\ring[R]{R}$ is a ring, then there is an Abelian group
                $\monoid{G}$ such that $\ring[R]{R}$ is isomorphic to the
                endomorphism group $(\Endomorphisms{G},+,\circ)$.
            \end{theorem}
            If we do the same thing with left multiplication we get
            anti-homomorphisms of $R$ to $\Endomorphisms{R}$. The image of
            this anti-isomorphism is thus a subring, and if $R$ has identity
            then the kernel is just $0$ and hence this is an
            anti-isomorphism.
            \begin{theorem}
                If $\ring{R}$ is a ring (with identity) and if
                $f:R\rightarrow{R}$ commutes with all left multiplications,
                then $f$ is a right multiplication.
            \end{theorem}
    \section{Extensions of Rings and Fields}
        Some rings lack certain features that are rather desirable. Indeed,
        rngs lack identity and hence it would be nice if we could convert
        these into rings. The integers $\mathbb{Z}$ lack the latin square
        property: $ax=b$ may have no solutions. It would be convenient to
        extend rngs to rings, and furthermore to extend rings to objects
        with inverses. In the case of $\mathbb{Z}$, this is done by
        constructing $\mathbb{Q}$. The method of construction extends to
        arbitrary commutative integral domains.
        \subsection{Embedding Rngs into Rings}
            \begin{fdefinition}{Embedding of a Rng}{Embedding_of_Rng}
                An embedding of a rng $\ring[R]{R}$ into a ring
                $\ring[S]{S}$ is an injective rng homomorphism
                $\varphi:R\rightarrow{S}$ such that $R$ is isomorphic to its
                image $\varphi[R]$.
            \end{fdefinition}
            \begin{ftheorem}{Embedding of Rngs into Rings}
                            {Embedding of Rngs into Rings}
                If $\ring[R]{R}$ is a rng, then there exists a ring
                $\ring[S]{S}$ such that $R$ may be embedded into $S$.
            \end{ftheorem}
            \begin{bproof}
                For let $S=\mathbb{Z}\times{R}$ and define $+_{S}$ and
                $\cdot_{S}$ as follows:
                \begin{align}
                    (m,a)+_{S}(n,b)&=(m+n,a+b)\\
                    (m,a)\cdot_{S}(n,b)&=(mn,na+mb+ab)
                \end{align}
                This is a ring with identity $I=(1,0_{R})$. Define
                $\varphi:R\rightarrow{S}$ by $\varphi(r)=(0,r)$. This is an
                embedding.
            \end{bproof}
            If we let $Z'=\mathbb{Z}\times\{0_{R}\}$ and
            $R'=\{0\}\times{R}$, then $S=Z'+R'$, where $+$ denotes set
            addition with respect to the addition operation $+$ defined in
            the proof. Also, $Z'\cap{R}'=\{(0,0_{R})\}$. Both $R'$ and $Z'$
            are ideals in $S$. While this theorem shows that rngs can always
            be embedded into rings with unity, this embedding may be
            horrible. If $\ring{R}$ was a ring to begin with (contains
            identity), then the element $x=(1,\minus{1}_{R})$ has the
            property that $x\cdot{y}=y\cdot{x}=0$ for all $y\in{S}$. The
            embedding may also change the characteristic. If $\ring{R}$ was
            a rng of finite and positive characteristic, then since $S$
            contains $\mathbb{Z}$ as an ideal, $S$ has characteristic zero.
            Hence we see the integral domains may lose their fundamental
            property, and the characteristic may be lost. We wish to modify
            the theorem and construct an embedding the preserves such
            notions.
            \begin{theorem}
                If $\ring[R]{R}$ is a rng of characteristic
                $n\in\mathbb{N}^{+}$, then there is a ring $\ring[S]{S}$
                such that $R$ may be embeeded into $S$.
            \end{theorem}
            \begin{proof}
                Do the same thing as in the fundamental embedding theorem,
                but use $\mathbb{Z}/n\mathbb{Z}$ instead of $\mathbb{Z}$.
            \end{proof}
            We now turn to fixing the problem of embedding integral domains.
            \begin{theorem}
                If $\ring{R}$ is an integral domain, if $a,b\in{R}$ are
                non-zero, and if there is an $m\in\mathbb{Z}$ such that
                $ab=mb$, then for all $c\in{R}$, $ac=mc$.
            \end{theorem}
            \begin{proof}
                For:
                \begin{equation}
                    b(mc)=m(bc)=(mb)c=(ab)c=a(bc)
                \end{equation}
            \end{proof}
            \begin{theorem}
                If $\ring{R}$ is an integral domain, then there is an
                integral domain $\ring{S}$ with identity such that $R$
                may be embedded into $S$.
            \end{theorem}
            \begin{proof}
                For Let $A=\mathbb{Z}\times{R}$ and let $Z$ be the set of
                all $x\in{A}$ such that $xa=0$ for all $a\in{A}$. This is an
                ideal. Let $S=A/Z$. This is an integral domain and $R$ can
                be embedded into it. (Probably the canonical quotient map).
            \end{proof}
            Intersections of fields is field. Intersection of subfields is
            subfields.
            \begin{theorem}
                If $\ring{F}$ is a field, if $R\subseteq{F}$ is a non-zero
                subring, then the field generated by $R$ is the set of all
                $\{ab^{\minus{1}}\;|\;a,b\in{R}\}$ with $b\ne{0}$.
            \end{theorem}
            We can add $ab^{\minus{1}}+cd^{\minus{1}}$ like with do with
            fractions and get $(ad+bc)(b^{\minus{1}}d^{\minus{1}})$.
            Since $a=abb^{\minus{1}}$, it follows that $R$ is contained in
            this set.
            \begin{ftheorem}{Integral Domain Extension Theorem}
                            {Integral_Domain_Extension_Theorem}
                If $\ring[R]{R}$ is a commutative integral domain, then
                there is a field $\ring[F]{F}$ such that $R$ may be
                embedded into $F$.
            \end{ftheorem}
            \begin{bproof}
                For let $A=R\times(R\setminus\{0\})$ and let $\sim$ be the
                equivalence relation $(a,b)\sim(c,d)$ if and only if
                $ad\sim{b}c$. Denote the equivalence classes of
                $A/\sim$ be $a/b$ instead of $[(a,b)]$. Defined
                $a/b+c/d=(ad+bc)/(bd)$ and $a/b\cdot{c}/d=(ac)/(bd)$.
                $R$ is contained in this as all $a/1$. This is also the
                smallest field containing $R$. If $K$ is another field,
                then $ab^{\minus{1}}$ is contained in here, and this maps
                to $a/b$. In particular, if $R$ was already a field, then
                $R=F$.
            \end{bproof}
            \begin{theorem}
                If $\monoid[S]{S}$ is a cancellative Abelian semigroup, then
                there is a group $\monoid[G]{G}$ such that $S$ may be
                embedded into $G$.
            \end{theorem}
            \begin{proof}
                For Let $A=S\times{S}$ and let $R$ be the equivalence
                relation defined by $(a,b)R(c,d)$ if and only if
                $a*d=b*c$. Let $G=A/R$ and define:
                \begin{equation}
                    [(a,b)]*[(c,d)]=[(a*d,b*c)]
                \end{equation}
                Let $1=[(a,a)]$. This is a unit in $G$ since
                $1*g=[(a,a)]*[(g_{1},g_{2})]=[(a*g_{1},a*g_{2}])]=g$.
                The inverse of $[(a,b)]$ is $[(b,a)]$ since:
                \begin{equation}
                    [(a,b)]*[(b,a)]=[(a*b,b*a)]=[(a*b,a*b)]=1
                \end{equation}
                Hence, $\monoid{G}$ is an Abelian group. Moreover, $S$
                embeds into $G$ as.
            \end{proof}
\end{document}