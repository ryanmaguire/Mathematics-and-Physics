\section{Random Variables}
    Let $(\Omega,\mathcal{A},\mu)$ be a probability space.
    A probability space is a measure space such that
    $\mu(\Omega)=1$. Let $f:\Omega\rightarrow\mathbb{R}$ be
    $\mathcal{A}-\mathcal{B}$ measurable, where $\mathcal{B}$ is
    the Borel $\sigma\textrm{-Algebra}$. Such functions are called
    random-variables on $\Omega$. While there's nothing random
    about this, we use such functions to model problems in
    probability theory. The probability of an event
    $A\in\mathcal{A}$ is simply $\mu(A)$. The associated
    $\sigma\textrm{-Algebra}$ is defined as:
    \begin{equation}
        \mathcal{A}_{f}=\{f^{-1}(B):B\in\mathcal{B}\}
    \end{equation}
    This is also called the $\sigma\textrm{-Algebra}$ of events
    bearing on $f$. This is a $\sigma\textrm{-Algebra}$ on
    $\Omega$.
    \begin{ldefinition}{Distribution of a Random Variable}
        The distribution of a random variable
        $f:\Omega\rightarrow\mathbb{R}$ on a probability space
        $(\Omega,\mathcal{A},\mu)$ is the image measure
        $\mu_{f}$ of $f$.
    \end{ldefinition}
    The image measure is the measure:
    \begin{equation}
        \mu_{f}(B)=\mu(f^{-1}(B))
        =\mu(\{\omega\in\Omega:f(\omega)\in{B}\})
    \end{equation}
    This is a Lebesgue-Stieljes Measure on the Borel
    $\sigma\textrm{-Algebra}$ on $\mathbb{R}$.
    \begin{equation}
        \mu_{f}(\mathbb{R})=\mu(f^{-1}(\mathbb{R}))
        =\mu(\Omega)=1
    \end{equation}
    \begin{ldefinition}{Cumulative Distribution Function}
        The Cumulative Distribution Function of a random variable
        $f:\Omega\rightarrow\mathbb{R}$ on a probability space
        $(\Omega,\mathcal{A},\mu)$ is the function
        $F:\mathbb{R}\rightarrow\mathbb{R}$ defined by:
        \begin{equation}
            F(x)=\mu_{f}\big((-\infty,a)\big)
        \end{equation}
        Where $\mu_{f}$ is the distribution of $f$.
    \end{ldefinition}
    Some facts about the cumulative distribution function:
    It is non-decreasing on $\mathbb{R}$, left continuous, and
    $F(\minus\infty)-F(\infty)=1$. By the Caratheodory extension
    theorem, and function $F$ that satisfies these three
    conditions is the cumulative distribution function of some
    Lebesgue-Stieljes probability measure on $\mathbb{R}$. From this
    we also have that every Lebesgue-Stieljes probability measure
    on $\mathbb{R}$ is a distribution for a random variable.
    \begin{example}
        Let $\Omega=\mathbb{R}$, let $\mathcal{A}=\mathcal{B}$,
        where $\mathcal{B}$ is the Borel $\sigma\textrm{-Algebra}$,
        and let $\mu$ be a Lebesgue-Stieljes probability measure
        on $\mathbb{R}$. Define the random variable
        $f:\Omega\rightarrow\mathbb{R}$ by
        $f(\omega)=\omega$. The inverse of any Borel set is itself,
        and thus we see that the distribution and the random
        variable coincide.
    \end{example}
    \begin{example}
        Let $\Omega=[0,1]$, $\mathcal{B}$ be the Borel
        $\sigma\textrm{-Algebra}$, and define
        $f_{1},f_{2}:\Omega\rightarrow\mathbb{R}$ by:
        \begin{equation}
            f_{1}(\omega)=\omega
            \quad\quad
            f_{2}(\omega)=1-\omega
        \end{equation}
        These two functions, while different, will have the same
        cumulative distribution function. For we have:
        \begin{equation}
            F_{1}(u)=\mu_{f_{1}}\big((\minus\infty,u)\big)=
            \mu\big(f^{-1}(\minus\infty,u)\big)
        \end{equation}
        We can evaluate this case by case to get:
        \begin{equation}
            F_{1}(u)=
            \begin{cases}
                \mu(\emptyset)=0,&u\leq{0}\\
                \mu\big([0,u)\big)]u,0<u<1\\
                \mu([0,1])=1,1\leq{u}
            \end{cases}
        \end{equation}
        Looking at $F_{2}$, we have:
        \begin{equation}
            F_{2}(u)=\mu_{f_{2}}\big((\minus\infty,u)\big)
            =\mu\big(f_{2}^{\minus{1}}(\minus\infty,u)\big)
        \end{equation}
        Again, evaluating case by case, we get:
        \begin{equation}
            F_{2}(u)=
            \begin{cases}
                \mu(\emptyset)=0,&u\leq{0}\\
                \mu\big((1-u,1]\big)]u,0<u<1\\
                \mu([0,1])=1,1\leq{u}
            \end{cases}
        \end{equation}
        Thus, $F_{1}=F_{2}$.
    \end{example}
    \begin{ldefinition}{Random Vector}
        A random vector on a probability space
        $(\Omega,\mathcal{A},\mu)$ is an
        $\mathcal{A}-\mathcal{B}_{n}$ measurable function
        $\mathbf{f}:\Omega\rightarrow\mathbb{R}^{n}$, where
        $\mathcal{B}_{n}$ is the Borel $\sigma\textrm{-Algebra}$
        on $\mathbb{R}^{n}$.
    \end{ldefinition}
    As a comment, if $f:\Omega\rightarrow\mathbb{R}$ is
    $\mathcal{A}-\mathcal{B}$ measurable, then
    $\mathcal{A}_{f}\subseteq\mathcal{A}$. The associated
    $\sigma\textrm{-Algebra}$ of a random vector
    $\mathbf{f}:\Omega\rightarrow\mathbb{R}^{n}$ is:
    \begin{equation}
        A_{\mathbf{f}}
        =\{\mathbf{f}^{\minus{1}}(B):B\in\mathcal{B}_{n}\}
    \end{equation}
    \begin{theorem}
        If $(\Omega,\mathcal{A},\mu)$ is a probability space,
        $\mathcal{B}_{n}$ is the Borel $\sigma\textrm{-Algebra}$
        on $\mathbb{R}^{n}$, and if
        $\mathbf{f}:\Omega\rightarrow\mathbb{R}^{n}$ is a random
        vector such that:
        \begin{equation}
            \mathbf{f}(\omega)=(f_{1}(\omega),\dots,f_{n}(\omega))
        \end{equation}
        Then:
        \begin{equation}
            \mathcal{A}_{\mathbf{f}}=
            \sigma\big(
                \mathcal{A}_{f_{1}},\dots,\mathcal{A}_{f_{n}}\big)
        \end{equation}
        Where this is the $\sigma\textrm{-Algebra}$ generated by
        these sets.
    \end{theorem}
    \begin{proof}
        For any $f_{j}$,
        $\mathcal{A}_{f_{j}}\subseteq\mathcal{A}_{\mathbf{f}}$,
        and thus the generated $\sigma\textrm{-Algebra}$ is
        contained in $\mathcal{A}_{\mathbf{f}}$. Going the other
        ways, let $\tilde{\mathcal{B}}$ be the set of subsets
        $B\subseteq\mathbb{R}^{n}$ such that:
        \begin{equation}
            \mathbf{f}^{\minus{1}}(B)\in
            \sigma\big(
                \mathcal{A}_{f_{1}},\dots,\mathcal{A}_{f_{n}}\big)
        \end{equation}
        But then for any sequence $B_{1},\dots,B_{n}\in\mathcal{B}$,
        $B_{1}\times\cdots\times{B}_{n}$ is contained in
        $\tilde{\mathcal{B}}$. But $\mathcal{B}_{n}$ is the
        smallest such $\sigma\textrm{-Algebra}$ to contain such
        sets, and thus
        $\mathcal{B}_{n}\subseteq\tilde{\mathcal{B}}$.
    \end{proof}
    \begin{ldefinition}{Distribution of a Random Vector}
        The distribution of a random vector
        $\mathbf{f}:\Omega\rightarrow\mathbb{R}^{n}$ on a
        measure space $(\Omega,\mathcal{A},\mu)$ is the measure:
        \begin{equation}
            \mu_{\mathbf{f}}(B)=
            \mu(\mathbf{f}^{\minus{1}}(B))
        \end{equation}
        Which is the joint distribution of
        $f_{1},\dots,f_{n}$, where:
        \begin{equation}
            \mathbf{f}(\omega)=(f_{1}(\omega),\dots,f_{n}(\omega))
        \end{equation}
    \end{ldefinition}
    The individual distributions can be computed in terms of the
    joint distribution. This is because:
    \begin{equation}
        \mu_{f_{1}}(B)=
        \mu(f_{1}^{\minus{1}}(B))=
        \mu\big(\mathbf{f}^{\minus{1}}(
            B\times\mathbb{R}^{n-1})\big)=
        \mu_{\mathbf{f}}\big(B\times\mathbb{R}^{n-1}\big)
    \end{equation}
    The joint distribution can not, in general, be computed
    in terms of the individual distributions. There is a special
    exception to this rule, and that is when the random variables
    are independent. That is, if the associated
    $\sigma\textrm{-Algebras}$ are independent. So events that
    bear on $f_{1},\dots,f_{n}$ are independent. If
    $E_{j}\in\mathcal{A}_{f_{j}}$, then:
    \begin{equation}
        \mu\Big(\bigcap_{k=1}^{n}E_{k}\Big)=
        \prod_{k=1}^{n}\mu(E_{k})
    \end{equation}
    \begin{theorem}
        A sequence of random variables $f_{1},\dots,f_{n}$ are
        independent if and only if the joint distribution is
        the product measure of the individual distributions.
    \end{theorem}
    \begin{proof}
        For let $B_{k}\in\mathcal{B}$ and let:
        \begin{equation}
            E_{k}=f_{k}^{\minus{1}}(B_{k})
        \end{equation}
        But then:
        \begin{subequations}
            \begin{align}
                \mu\Big(\bigcap_{k=1}^{n}E_{k}\Big)&=
                \mu\Big(\bigcap_{k=1}^{n}
                    f_{k}^{\minus{1}}(B_{k})\Big)\\
                &=\mu\big(\mathbf{f}^{\minus{1}}
                    (B_{1}\times\dots\times{B}_{n})\big)\\
                &=\mu_{\mathbf{f}}(B_{1}\times\dots\times{B}_{n})\\
                &=\prod_{k=1}^{n}\mu(E_{n})\\
                &=\prod_{k=1}^{n}\mu\big(f^{\minus{1}}(B_{k})\big)\\
                &=\prod_{k=1}^{n}\mu_{f_{k}}(B_{k})
            \end{align}
        \end{subequations}
    \end{proof}
    Let $\mu_{1},\dots,\mu_{n}$ be probability Lebesgue-stieljes
    measures on $\mathbb{R}$, and let $\mu$ be the product
    measure. Consider the probability space
    $(\mathbb{R}^{n},\mathcal{B}_{n},\mu)$ and the projection
    mappings $\pi_{k}:\mathbb{R}^{n}\rightarrow\mathbb{R}$:
    \begin{equation}
        \pi_{k}(\omega_{1},\dots,\omega_{n})=\omega_{k}
    \end{equation}
    \begin{theorem}
        Let $f_{n}$ be an infinite sequence of random variables
        on a probability space $(\Omega,\mathcal{A},\mu)$. Let
        $\mathcal{A}_{f_{n}}$ be the associated
        $\sigma\textrm{-Algebras}$. For every $\omega\in\Omega$,
        let:
        \begin{equation}
            F_{\inf}(\Omega)=
            \underset{n\rightarrow\infty}{\underline{\lim}}
            f_{n}(\omega)
            \quad\quad
            F_{\sup}(\Omega)=
            \underset{n\rightarrow\infty}{\overline{\lim}}
            f_{n}(\omega)
        \end{equation}
        Then $F_{\inf}$ and $F_{\sup}$ are measurable with
        respect to the terminal $\sigma\textrm{-Algebra}$.
    \end{theorem}
    \begin{proof}
        For $F_{\inf}$ is measurable if and only if for all
        $u\in\mathbb{R}$, we have
        $F^{\minus{1}}\big((\minus\infty,u)\big)\in\mathcal{F}$.
        But:
        \begin{subequations}
            \begin{align}
                F^{\minus{1}}\big((\minus\infty,u)\big)
                &=\{\omega:F(\omega)\leq{u}\}\\
                &=\{\omega:\underline{\lim}f_{n}(\omega)\leq{u}\}\\
                &=\{\omega:\underset{n}{\sup}
                    \underset{k\geq{n}}{\lim}f_{k}(\omega)\}\\
                &=\bigcap_{n=1}^{\infty}\Big\{\omega:
                    \underset{n\geq{k}}{\inf}f_{k}(\omega)\leq{u}
                \Big\}\\
                &=\bigcap_{n=N}^{\infty}\Big\{\omega:
                    \underset{n\geq{k}}{\inf}f_{k}(\omega)\leq{u}
                \Big\}
            \end{align}
        \end{subequations}
    \end{proof}
    \begin{theorem}
        If $\mathcal{F}$ is a self-independent
        $\sigma\textrm{-Algebra}$, if $F$ is measurable with
        respect to $\mathcal{F}$, then $F$ is constant almost
        everywhere.
    \end{theorem}
    \begin{proof}
        For since $\mathcal{F}$ is self independent:
        \begin{equation}
            \mu(\{\omega:F(\omega)<u\})=0
            \quad\textrm{or}\quad
            \mu(\{\omega:F(\omega)<u\})=1
        \end{equation}
        Define $A$ and $B$ as follows:
        \begin{align}
            A&=\{u\in\mathbb{F}:\mu(\{\omega:F(\omega)<u\})=0\}\\
            B&=\{u\in\mathbb{F}:\mu(\{\omega:F(\omega)<u\})=1\}\\
        \end{align}
        This separates the real line into two parts. By
        Dedekind's Axiom there is a $c\in\mathbb{R}$ such that,
        for all $a\in{A}$, and for all $b\in{B}$,
        $a\leq{c}\leq{b}$. But then:
        \begin{equation}
            \mu(\{u:F(u)<c+\frac{1}{n}\})=1
        \end{equation}
        From continuity from above, we're done.
    \end{proof}
    \begin{theorem}
        If $(\Omega,\mathcal{A},\mu)$ is a probability space,
        $f_{n}$ is a sequence of independent random variables,
        then the limit inferior and the limit superior are
        constants $\mu$ almost everywhere.
    \end{theorem}
    \begin{proof}
        For the limit inferior and limit superior are measurable
        with respect to the terminal $\sigma\textrm{-Algebra}$.
        By the Kolmogorov zero-one law, $\mathcal{F}$ is
        self-independent if $\mathcal{A}_{f_{n}}$ are independent.
        Thus, by the previous theorem, these functions are constants
        almost everywhere.
    \end{proof}
    Thus the limit of random-variables is entirely not random, but
    constant functions.
    \begin{theorem}
        If $f_{n}$ is a sequence of random variables, then the
        limit of $f_{n}$ almost surely exists, or almost never
        exists.
    \end{theorem}
    \begin{proof}
        For since the limit superior and limit inferior are
        constants almost everywhere, then eithere they agree,
        in which there's convergence almost surely, or they do
        not agree, in which there's convergence almost never.
    \end{proof}
    \begin{ldefinition}{Expectation Value}
        The expectation value of a summable random variable
        $f:\Omega\rightarrow\mathbb{R}$ on a measure space
        $(\Omega,\mathcal{A},\mu)$ is the real number
        $E(f)$ defined by:
        \begin{equation}
            E(f)=\int_{\Omega}f\diff{\mu}
        \end{equation}
    \end{ldefinition}
    The expectation can be expressed in terms of the distribution
    by using the measure transformation theorem. If
    $g:\mathbb{R}\rightarrow\mathbb{R}$ is a real valued function,
    then:
    \begin{equation}
        \int_{\Omega}g\diff{\mu}=
        \int_{\mathbb{R}}g\circ{f}\diff{\mu_{f}}
    \end{equation}
    Now we apply this in the simple case when $g(u)=u$. Then:
    \begin{equation}
        E(f)=\int_{\Omega}f\diff{\mu}
        =\int_\mathbb{R}u\diff{\mu_{f}}
    \end{equation}
    Where we assume that $f$ is summable against $\mu$. Thus,
    $u$ is summable against $\mu_{f}$. So, we have that:
    \begin{equation}
        \int_{\mathbb{R}}|u|\diff{\mu_{f}}<\infty
    \end{equation}
    \begin{ldefinition}{Variance}
        The variance of a random variable
        $f:\Omega\rightarrow\mathbb{R}$ on a measure space
        $(\Omega,\mathcal{A},\mu)$, is the real number
        $Var(f)$ defined by:
        \begin{equation}
            Var(f)=E\big(f-E(f)\big)^{2}
            =\int_{\Omega}\big(f-E(f)\big)^{2}\diff{\mu}
        \end{equation}
    \end{ldefinition}
    \begin{theorem}
        \begin{equation}
            Var(f)=E(f^{2})-E(f)^{2}
        \end{equation}
    \end{theorem}
    If $(\Omega,\mathcal{A},\mu)$ is a measure space,
    $f:\Omega\rightarrow\mathbb{R}$ is a Borel measurable
    function, then the expectation is:
    \begin{equation}
        E(f)=\int_{\Omega}f\diff{\mu}
    \end{equation}
    The functions $f_{1},\dots,f_{n}$ are independent if
    the associated $\sigma\textrm{-Algebras}$ are independent,
    $\mathcal{A}_{f_{1}}.\dots,\mathcal{A}_{f_{n}}$, where
    the associated $\sigma\textrm{-Algebra}$ is defined
    as:
    \begin{equation}
        \mathcal{A}_{f}=\{f^{\minus{1}}(B):B\in\mathcal{B}\}
    \end{equation}
    Where $\mathcal{B}$ is the Borel
    $\sigma\textrm{-Algebra}$. A random vector is a function
    $\mathbf{f}:\Omega\rightarrow\mathbb{R}^{n}$. The
    distribution of $\mathbf{f}$ is defined as:
    \begin{equation}
        \mu_{\mathbf{f}}(B)=
            \mu\big(\mathbf{f}^{\minus{1}}(B)\big)
    \end{equation}
    This is also called the joint distribution. We then proved
    that $f_{1},\dots,f_{n}$ are independent if and only
    if the joint distribution is the product of the
    individual distributions.
    \begin{theorem}
        If $(\Omega,\mathcal{A},\mu)$ is a probabilty space,
        and if $f_{1},\dots,f_{n}$ are independent functions,
        then:
        \begin{equation}
            E\Big(\prod_{k}f_{k}\Big)
            =\prod_{k}E(f_{k})
        \end{equation}
    \end{theorem}
    \begin{proof}
        For define $g:\Omega\rightarrow\mathbb{R}$ by:
        \begin{equation}
            g(\omega)=\prod_{k=1}^{n}f_{k}(\omega)
        \end{equation}
        Let $\mathbf{f}:\Omega\rightarrow\mathbb{R}^{n}$ be
        defined by:
        \begin{equation}
            \mathbf{f}(\omega)=
            \big(f_{1}(\omega),\dots,f_{n}(\omega)\big)
        \end{equation}
        Then using the measure transformation, we have:
        \begin{align}
            \int_{\Omega}\prod_{k=1}^{n}f_{k}\diff{\mu}
            &=\int_{\Omega}g\big(\mathbf{f}(\omega)\big)
                \diff{\mu}\\
            &\int_{\mathbb{R}^{n}}g(u_{1},\dots,u_{n})
                \mu_{\mathbf{f}}\\
            &=\int_{\mathbb{R}^{n}}\prod_{k=1}^{n}u_{k}
                \mu_{\mathbf{f}}
        \end{align}
    \end{proof}
    Suppose $n=2$. Then, since $f_{1}$ and $f_{2}$ are
    independent, $\mu_{(f_{1},f_{2})}$ is the product of
    the measures $\mu_{f_{1}}$ and $\mu_{f_{2}}$. Thus
    by Fubini's theorem:
    \begin{equation}
        \int_{\mathbb{R}^{2}}u_{1}u_{2}\mu_{(f_{1},f_{2})}
        =\int_{\mathbb{R}}\Big(
            \int_{\mathbb{R}}u_{1}u_{2}\mu_{f_{2}}\Big)
                \mu_{f_{1}}
        =\int_{\mathbb{R}}u_{1}\Big(
            \int_{\mathbb{R}}u_{1}\mu_{f_{1}}\Big)
        =\int_{\mathbb{R}}u\mu_{f_{1}}
            \int_{\mathbb{R}}u_{2}\mu_{f_{2}}
        =\int_{\Omega}f_{1}\mu\int_{\Omega}f_{2}\mu
    \end{equation}
    From a course in integral calculus, one should be very
    surprised by this result, for it says that if
    $f_{1},\dots,f_{n}$ are independent, then:
    \begin{equation}
        \int_{\Omega}\prod_{k=1}^{n}f_{k}\diff{\mu}=
        \prod_{k=1}^{n}\int_{\Omega}f_{k}\diff{\mu}
    \end{equation}
    This is almost never true for a given set of functions,
    but if they are independent then the result holds.
    \subsection{Covariance}
        The covariance of $f_{1}$ and $f_{2}$ is:
        \begin{equation}
            E\Big(\big(f_{1}-E(f_{1})\big)
                \big(f_{2}-E(f_{2})\big)\Big)
            =\int_{\Omega}\big(f_{1}-E(f_{1})\big)
                \big(f_{2}-E(f_{2})\big)\diff{\mu}
        \end{equation}
        We can simplify this down to:
        \begin{equation}
            E(f_{1}f_{2})-E(f_{1})E(f_{2})
        \end{equation}
        If $f_{1}$ and $f_{2}$ are independent, then:
        \begin{equation}
            Cov(f_{1},f_{2})=0
        \end{equation}
        The converse is not true. It does not imply that
        $f_{1}$ and $f_{2}$ are independent.
        For let:
        \begin{equation}
            \Omega=\{1,2,3\}
        \end{equation}
        Let $\mathcal{A}=\mathcal{P}(\Omega)$ and let
        $\mu$ be the counting measure on $\Omega$. That is:
        \begin{equation}
            \mu(A)=\frac{\mathrm{Card}(A)}{3}
        \end{equation}
        Then $(\Omega,\mathcal{A},\mu)$ is a probability
        measure. Define $f_{1}$ and $f_{2}$ as follows:
        \begin{align}
            f_{1}(\omega)&=
            \begin{cases}
                1,&\omega=1\\
                0,&\omega=0\\
                1&\omega=2
            \end{cases}\\
            f_{1}(\omega)&=
            \begin{cases}
                1,&\omega=1\\
                0,&\omega=0\\
                \minus{1}&\omega=2
            \end{cases}
        \end{align}
        Then we compute and get:
        \begin{equation}
            E(f_{1})=\int_{\Omega}f_{1}\diff{\mu}=0
        \end{equation}
        And also:
        \begin{equation}
            E(f_{2})=\frac{2}{3}
        \end{equation}
        But if we multiply, we see that
        $f_{1}f_{2}=f_{1}$, and therefore:
        \begin{equation}
            E(f_{1}f_{2})=E(f_{1})=0
        \end{equation}
        But then:
        \begin{equation}
            E(f_{1}f_{2})-E(f_{1})E(f_{2})=0
        \end{equation}
        And thus $f_{1}$ and $f_{2}$ are uncorrelated.
        But they are dependent. We may expect this since
        $f_{2}=f_{1}^{2}$. Let's compute the associated
        $\sigma\textrm{-Algebras}$. We have:
        \begin{align}
            f_{1}^{\minus{1}}(\{1\})
            &=\{1\}\\
            f_{2}^{\minus{1}}&=\{1,3\}
        \end{align}
        But then:
        \begin{align}
            \mu(f_{1}^{\minus{1}}(\{1\})
            &=\frac{1}{3}\\
            \mu(f_{2}^{\minus{1}}(\{1\})&=\frac{2}{3}
        \end{align}
        But the product measure is:
        \begin{equation}
            \mu_{(f_{1},f_{2})}(\{1\})=\frac{1}{3}
        \end{equation}
        And this is not the product of the two measure, and
        therefore it they are not independent.
        If $Cov(f_{1},f_{2})=0$, we say that $f_{1}$ and
        $f_{2}$ are uncorrelated.
        \begin{theorem}
            If $f_{1},\dots,f_{n}$ are random variables
            that are pairwise uncorrelated, then:
            \begin{equation}
                Var(\sum_{k=1}^{n}f_{k})=
                \sum_{k=1}^{n}Var(f_{k})
            \end{equation}
        \end{theorem}
        \begin{proof}
            For:
            \begin{equation}
                \int_{\Omega}
                    \Big(\sum_{k=1}^{n}f_{k}-
                        E(\sum_{k=1}^{n}f_{k}\Big)\diff{\mu}
                =\sum_{i,j}\int_{\Omega}
                    (f_{i}-E(f_{i}))(f_{j}-E(f_{j}))\diff{\mu}
            \end{equation}
            But the $f_{i}$ are pairwise uncorrelated, and
            thus this product is zero if $i\ne{j}$. Thus, we
            get:
            \begin{equation}
                \int_{\Omega}
                    \Big(\sum_{k=1}^{n}f_{k}-
                        E(\sum_{k=1}^{n}f_{k}\Big)\diff{\mu}
                =\sum_{k=1}^{n}Var(f_{k})
            \end{equation}
        \end{proof}