\documentclass[crop=false,class=book,oneside]{standalone}
%----------------------------Preamble-------------------------------%
\input{../../preamble.tex}
%----------------------------GLOSSARY-------------------------------%
\makeglossaries
\loadglsentries{../../glossary}
\loadglsentries{../../acronym}
%--------------------------Main Document----------------------------%
\begin{document}
    \ifx\ifmathcourses\undefined
        \pagenumbering{roman}
        \title{Real Analysis}
        \author{Ryan Maguire}
        \date{\vspace{-5ex}}
        \maketitle
        \tableofcontents
        \clearpage
        \chapter*{Real Analysis}
        \addcontentsline{toc}{chapter}{Real Analysis}
        \markboth{}{REAL ANALYSIS}
        \vspace{10ex}
        \setcounter{chapter}{1}
        \pagenumbering{arabic}
    \else
        \chapter{Real Analysis}
    \fi
    \section{Notes from Rosenlicht}
        \subsection{Sets}
            Give a function $f:X\rightarrow{Y}$, the dinstinction
            between the image of a subset $S\subseteq{X}$ and a
            point $x\in{X}$ is:
            \begin{equation}
                f(\{x\})=\{f(x)\}
            \end{equation}
            Similarly for the pre-image:
            \begin{equation}
                \{f^{\minus{1}}(y)\}=f^{\minus{1}}(\{y\})
            \end{equation}
            One definition of an infinite set is that it contains
            a bijection between itself and a proper subset. Such
            sets are called Dedekind infinite, and countable choice is
            needed here. The following are true:
            \begin{align}
                (A^{C})^{C}&=A\\
                A\cup{A}&=A\cap{A}=A\cup\emptyset=A\\
                A\cap\emptyset&=\emptyset\\
                A\times\emptyset&=\emptyset
            \end{align}
            In addition, there are De Morgan's laws and the distributive
            laws. Some more identities:
            \begin{align}
                (A\setminus{B})\cap{C}&=(A\cap{C})\setminus{B}\\
                (A\cup{B})\setminus(A\cap{B})
                    &=(A\setminus{B})\cup(B\setminus{A})\\
                (A\setminus(B\setminus{C}))
                    &=(A\setminus{B})\cup(A\cap{B}\cap{C})\\
                (A\setminus{B})\times{C}
                    &=(A\times{C})\setminus(B\times{C})
            \end{align}
            Given any collection of sets $X_{i}$, $i\in{I}$, and a
            set $B$, we have:
            \begin{align}
                B\cap\Big(\bigcup_{i\in{I}}A_{i}\Big)
                    &=\bigcup_{i\in{I}}\Big(B\cap{A_{i}}\Big)
            \end{align}
            Composition is a commutative operation. That is, given
            $f:X\rightarrow{Y}$, $g:Y\rightarrow{Z}$, and
            $h:Z\rightarrow{W}$, we have:
            \begin{equation}
                h\circ(g\circ{f})=(h\circ{h})\circ{f}
            \end{equation}
            The following is also true of functions:
            \begin{subequations}
                \begin{align}
                    f(A\cup{B})&=f(A)\cup{f}(B)\\
                    f(A\cap{B})&\subseteq{f}(A)\cap{f}(B)\\
                    f^{\minus{1}}(A\cup{B})
                        &=f^{\minus{1}}(A)\cup{f}^{\minus{1}}(B)\\
                    f^{\minus{1}}(A\cap{B})
                        &=f^{\minus{1}}(A)\cap{f}^{\minus{1}}(B)\\
                    A\subseteq{f}^{\minus{1}}(f(A))\\
                    f(f^{\minus{1}}(A)\subseteq{A}
                \end{align}
            \end{subequations}
            \begin{theorem}
                If $f:X\rightarrow{Y}$ is injective, then:
                \begin{subequations}
                    \begin{align}
                        f^{\minus{1}}(f(A))&=A\\
                        f(A\cap{B})&=f(A)\cap{f}(B)
                    \end{align}
                \end{subequations}
            \end{theorem}
            \begin{theorem}
                If $f:X\rightarrow{Y}$ is surjective, then:
                \begin{equation}
                    f(f^{\minus{1}}(A))=A
                \end{equation}
            \end{theorem}
        \subsection{The Real Number System}
            The real numbers are a set $\mathbb{R}$ with several
            properties. These properties make $\mathbb{R}$ a
            complete ordered field, and indeed the only complete
            ordered field. That is, the real numbers are unique
            up to \textit{isomorphism}. There are two functions
            $+,\cdot:\mathbb{R}^{2}\rightarrow\mathbb{R}$, called
            addition and multiplication, respectively, that satisfy
            the following \textit{field axioms}:
            \begin{align}
                a+b&=b+a&
                a\cdot{b}&=b\cdot{a}
                \tag{Commutativity}\\
                a+(b+c)&=(a+b)+c&
                a\cdot(b\cdot{c})&=(a\cdot{b})\cdot{c}
                \tag{Associativity}\\
                a\cdot(b+c)&=a\cdot{b}+a\cdot{c}
                \tag{Distributive Law}\\
                \exists_{0\in\mathbb{R}}:0+a&=a&
                \exists_{1\in\mathbb{R}}:a\cdot{1}&=a
                \tag{Neutral Elements}\\
                \forall_{a\in\mathbb{R}}\exists_{b\in\mathbb{R}}:
                a+b&=0&
                \forall_{a\in\mathbb{R},a\ne{0}}
                \exists_{a^{\minus{1}}}:
                a\cdot{a}^{\minus{1}}&=1
                \tag{Inverse Elements}
            \end{align}
            By inductively using the associative laws and the
            commutative laws, we see that adding $n$ elements
            does not depend on the order in which they are
            added. Similarly for multiplication. For a general
            field, we write $(F,+,\cdot)$.
            \begin{theorem}
                If $(F,+,\cdot)$ is a field, and if $a\in{F}$, then
                the additive inverse of $a$ is unique.
            \end{theorem}
            \begin{proof}
                For suppose $b$ and $b'$ are additive inverses. Then:
                \begin{equation}
                    b=b+0=b+(a+b')=(b+a)+b'=0+b'=b'
                \end{equation}
                And therefore $b$ is unique.
            \end{proof}
            We denote the additive inverse of an element $a$ by
            writing $\minus{a}$.
            \begin{theorem}
                If $(F,+,\cdot)$ is a field, if $a,b\in{F}$, then
                there is a unique $x\in{F}$ such that
                $x+a=b$.
            \end{theorem}
            \begin{proof}
                For let $x=a-b$. Then:
                \begin{equation}
                    x+a=(b-a)+a
                    =b+(-a+a)
                    =b+0
                    =b
                \end{equation}
                Moreover, of $x'$ is a solution, then:
                \begin{equation}
                    x'=x'+0=x'+(a+(\minus{a}))=
                    (x'+a)+(\minus{a})=b+(\minus{a})=x
                \end{equation}
                Thus, $x'=x$.
            \end{proof}
            Instead of writing $b+(\minus{a})$, we
            denote this by $b-a$. This new operation is called
            subtraction. Note that it is not commutative, nor
            is it associative. Indeed, for any $a,b\in\mathbb{R}$,
            suppose $a-b=b-a$, and let $y=a-b$. Then we have that
            $y=\minus{y}$, and thus $y+y=2y=0$. This is only possible
            in $\mathbb{R}$ if $y=0$, and thus we'd require that
            $a=b$. So subtraction is not commutative in $\mathbb{R}$.
            There are fields such that $y+y=0$ and such that
            $y\ne{0}$, but such fields can't have a notion of
            \textit{order} on them. We'll discuss these later.
            Note that the notion is not associative either. Again,
            let $a=2$ and $b=c=1$. Then $a-(b-c)=2$, but
            $(a-b)-c=0$. Again we come to the conclusion that either
            $2=0$, or subtraction is not associative. In an ordered
            field, which is what $\mathbb{R}$ is, we cannot have
            $2=0$. In finite fields, this is possible.
            \begin{theorem}
                If $(F,+,\cdot)$ is a field and if $a\in{F}$
                is non-zero, then the multiplicative inverse
                of $a$ is unique.
            \end{theorem}
            \begin{proof}
                For suppose $b$ and $b'$ are multiplicative inverses
                of $a$. Then:
                \begin{equation}
                    b=b\cdot{1}=b\cdot(a\cdot{b}')=
                    (b\cdot{a})\cdot{b}'=1\cdot{b}'=b'
                \end{equation}
                And therefore $b$ is unique.
            \end{proof}
            We write the multiplicative inverse of a non-zero element
            by $a^{\minus{1}}$.
            \begin{theorem}
                If $(F,+,\cdot)$ is a field, if $a,b\in{F}$, and if
                $a\ne{0}$, then there is a unique $x\in{F}$ such that
                $x\cdot{a}=b$.
            \end{theorem}
            \begin{proof}
                For let $x=b\cdot{a}^{\minus{1}}$. Then:
                \begin{equation}
                    x\cdot{a}=(b\cdot{a}^{\minus{1}})=
                    b\cdot(a^{\minus{1}}\cdot{a})=
                    b\cdot{1}=b
                \end{equation}
                Moreoever, if $x'$ is a solution, then:
                \begin{equation}
                    x'=x'\cdot{1}=x'\cdot(a\cdot{a^{\minus{1}}})
                    =(x'\cdot{a})\cdot{a^{\minus{1}}}=
                    b\cdot{a}^{\minus{1}}=x
                \end{equation}
                Thus, $x'=x$.
            \end{proof}
            We define division by non-zero numbers by writing
            $\frac{a}{b}=a\cdot{b}^{\minus{1}}$. Other symbols are
            used for this, like $a\div{b}$, or simply $a/b$. Similar
            to subtraction, division is neither commutative nor
            associative.
            \begin{theorem}
                If $(F,+,\cdot)$ is a field, if $a,b,c\in{F}$, and if
                $a+c=b+c$, then $a=b$.
            \end{theorem}
            \begin{proof}
                For:
                \begin{equation}
                    a=a+0=a+(c-c)=(a+c)-c=(b+c)-c=b+(c-c)=0
                \end{equation}
                Therefore, etc.
            \end{proof}
            \begin{theorem}
                If $(F,+,\cdot)$ is a field, $a,b,c\in{F}$, if
                $c\ne{0}$, and if $a\cdot{c}=b\cdot{c}$, then
                $a=b$.
            \end{theorem}
            \begin{proof}
                For:
                \begin{equation}
                    a=a\cdot{1}=a\cdot(c\cdot{c}^{\minus{1}})=
                    (a\cdot{c})\cdot{c}^{\minus{1}}=
                    (b\cdot{c})\cdot{c}^{\minus{1}}=
                    b\cdot(c\cdot{c}^{\minus{1}})=
                    b\cdot{1}=b
                \end{equation}
                Therefore, etc.
            \end{proof}
            \begin{theorem}
                If $(F,\cdot,+)$ is a field, and if $a\in{F}$, then
                $a\cdot{0}=0$.
            \end{theorem}
            \begin{proof}
                For:
                \begin{equation}
                    a\cdot{0}+a\cdot{0}=a\cdot(0+0)=
                    a\cdot{0}=a\cdot{0}+0
                \end{equation}
                And therefore from the cancellation laws,
                $a\cdot{0}=0$.
            \end{proof}
            \begin{theorem}
                If $(F,+,\cdot)$ is a field, and $a\in{F}$, then
                $\minus{a}=(\minus{1})\cdot{a}$
            \end{theorem}
            \begin{proof}
                For:
                \begin{equation}
                    (\minus{1})\cdot{a}+a=
                    (\minus{1}+1)\cdot{a}=
                    0\cdot{a}=0
                \end{equation}
                From the uniqueness of inverses,
                $\minus{a}=(\minus{1})\cdot{a}$.
            \end{proof}
            \begin{theorem}
                If $(F,+,\cdot)$ is a field and $a\in{F}$, then
                $\minus(\minus{a})=a$.
            \end{theorem}
            \begin{proof}
                For:
                \begin{equation}
                    \minus(\minus{a})+(\minus{a})=
                    (\minus{1})\cdot(\minus{a})+(\minus{a})
                    =(\minus{1}+1)\cdot(\minus{a})
                    =0\cdot(\minus{a})=0
                \end{equation}
                From the uniqueness of inverses, etc.
            \end{proof}
            \begin{theorem}
                If $(F,+,\cdot)$ is a field, and if $a,b\in{F}$ are
                non-zero, then $(a\cdot{b})^{\minus{1}}=%
                                b^{\minus{1}}\cdot{a}^{\minus{1}}$.
            \end{theorem}
            \begin{proof}
                For:
                \begin{equation}
                    (a\cdot{b})
                    \cdot(b^{\minus{1}}\cdot{a}^{\minus{1}})
                    =a\cdot
                    (b\cdot{b}^{\minus{1}})\cdot{a}^{\minus{1}}
                    =a\cdot{1}\cdot{a}^{\minus{1}}=
                    a\cdot{a}^{\minus{1}}=1
                \end{equation}
                From the uniqueness of inverses, etc.
            \end{proof}
            \begin{theorem}
                If $(F,+,\cdot)$ is a field, $a\in{F}$ is non-zero,
                then $(a^{\minus{1}})^{\minus{1}}=a$.
            \end{theorem}
            \begin{proof}
                For:
                \begin{equation}
                    (a^{\minus{1}})^{\minus{1}}\cdot{a}^{\minus{1}}=
                    (a\cdot{a}^{\minus{1}})^{\minus{1}}=
                    1^{\minus{1}}=1
                \end{equation}
                From uniquness, etc.
            \end{proof}
            \begin{theorem}
                If $(F,+,\cdot)$ is a field, and $a,b,c,d\in{F}$, and
                if $b,d\ne{0}$, then:
                \begin{equation}
                    (a\cdot{b}^{\minus{1}})+(c\cdot{d}^{\minus{1}})=
                    (a\cdot{d}+b\cdot{c})\cdot(b\cdot{d})^{\minus{1}}
                    =\frac{ad+bc}{bd}
                \end{equation}
            \end{theorem}
            As stated before, the axioms of a field are not enough
            to uniquely define the real numbers. Indeed, the rational
            numbers $\mathbb{Q}$ define a field, as do the complex
            numbers $\mathbb{C}$. To see a finite field, consider the
            set $\mathbb{F}_{2}=\{0,1\}$, and consider the following
            arithmetic:
            \par
            \begin{minipage}[b]{0.49\textwidth}
                \centering
                \begin{table}[H]
                    \centering
                    \captionsetup{type=table}
                    \begin{tabular}{c|cc}
                        $+$&0&1\\
                        \hline
                        0&0&1\\
                        1&1&0
                    \end{tabular}
                    \caption{Addition in $\mathbb{F}_{2}$}
                    \label{tab:Real_Analysis_Add_in_F_2_Field}
                \end{table}
            \end{minipage}
            \hfill
            \begin{minipage}[b]{0.49\textwidth}
                \begin{table}[H]
                    \centering
                    \captionsetup{type=table}
                    \begin{tabular}{c|cc}
                        $\cdot$&0&1\\
                        \hline
                        0&0&0\\
                        1&0&1
                    \end{tabular}
                    \caption{Multiplication in $\mathbb{F}_{2}$}
                    \label{tab:Real_Analysis_Mult_in_F_2_Field}
                \end{table}
            \end{minipage}
            Then $(F,+,\cdot)$ is a field. It's a very strange field,
            since we have $1+1=0$, but alas it satisfies all of the
            properties of a field, and all of the theorem's we have
            proved still apply. Interesting, it is the only field
            with two elements. We have no choice in deciding what
            $a\cdot{b}$ means in the field, since multiplication by
            zero must give zero, and multiplication by one must give
            back the original number. Similarly for addition.
            Adding zero must not change anything, and so all
            we are left with is deciding what $1+1$ equals. But
            to be a field, there must be an additive inverse
            element. Thus we are forced to set $1+1=0$. There is
            also a field with three elements. For let
            $\mathbb{F}_{3}=\{0,1,2\}$ and define:
            \par\hfill\par
            \begin{minipage}[b]{0.49\textwidth}
                \centering
                \begin{table}[H]
                    \centering
                    \captionsetup{type=table}
                    \begin{tabular}{c|ccc}
                        $+$&0&1&2\\
                        \hline
                        0&0&1&2\\
                        1&1&2&0\\
                        2&2&0&1
                    \end{tabular}
                    \caption{Addition in $\mathbb{F}_{3}$}
                    \label{tab:Real_Analysis_Add_in_F_3_Field}
                \end{table}
            \end{minipage}
            \hfill
            \begin{minipage}[b]{0.49\textwidth}
                \begin{table}[H]
                    \centering
                    \captionsetup{type=table}
                    \begin{tabular}{c|ccc}
                        $\cdot$&0&1&2\\
                        \hline
                        0&0&0&0\\
                        1&0&1&2\\
                        2&0&2&1
                    \end{tabular}
                    \caption{Multiplication in $\mathbb{F}_{3}$}
                    \label{tab:Real_Analysis_Mult_in_F_3_Field}
                \end{table}
            \end{minipage}
            Intuition tells us that $1+1>1>0$, and thus $1+1$ cannot
            be equal to zero. Thus, to exclude finite fields we
            need to introduce the notion of order.
            \begin{enumerate}
                \item There is a subset $\mathbb{R}^{+}$
                      of $\mathbb{R}$ such that, for all
                      $a,b\in\mathbb{R}^{+}$, we
                      have $a\cdot{b}\in\mathbb{R}^{+}$ and
                      $a+b\in\mathbb{R}^{+}$.
                \item For all $a\in\mathbb{R}$, one and only one of
                      the following statements is true:
                      \begin{itemize}
                          \item $a\in\mathbb{R}^{+}$
                          \item $a=0$
                          \item $\minus{a}\in\mathbb{R}^{+}$
                      \end{itemize}
            \end{enumerate}
            $\mathbb{R}^{+}$ is called the set of positive numbers,
            and the elements such that $\minus{a}\in\mathbb{R}^{+}$
            are called negative. We define less than by writing
            $a<b$ if $b-a\in\mathbb{R}^{+}$. Similarly, we define
            greater than by writing $a>b$ is $a-b\in\mathbb{R}^{+}$.
            The less than or equal to and greater than or equal to
            symbols, denoted $\leq$ and $\geq$, respectively,
            are such that $a\leq{b}$ if $a<b$ or $a=b$, and
            similarly $a\geq{b}$ if $a>b$ or $a=b$. This defines
            $\mathbb{R}$ to be on ordered field.
            \begin{theorem}
                If $a,b\in\mathbb{R}$, then either $a=b$, $a<b$, or
                $a>b$.
            \end{theorem}
            \begin{proof}
                For either $a-b\in\mathbb{R}^{+}$, $a-b=0$, or
                $\minus(a-b)\in\mathbb{R}^{+}$. If
                $a-b\in\mathbb{R}^{+}$, then $a>b$. If $a-b=0$, then
                $a=b$. Finally, if $\minus(a-b)\in\mathbb{R}^{+}$,
                then $b-a\in\mathbb{R}^{+}$, and thus $b>a$.
            \end{proof}
            \begin{theorem}
                If $a,b,c\in\mathbb{R}$, if $a<b$, and if $b<c$, then
                $a<c$.
            \end{theorem}
            \begin{proof}
                For if $a<b$, then $b-a\in\mathbb{R}^{+}$. But if
                $b<c$, then $c-b\in\mathbb{R}^{+}$. But then:
                \begin{equation}
                    c-a=(c-b)+(b-a)\in\mathbb{R}^{+}
                \end{equation}
                Therefore, etc.
            \end{proof}
            \begin{theorem}
                If $a,b,c,d\in\mathbb{R}$, if $a<b$, and if
                $c\leq{d}$, then $a+c<b+d$.
            \end{theorem}
            \begin{proof}
                If $a<b$, then $b-a\in\mathbb{R}^{+}$. If $c\leq{d}$,
                then either $d-c\in\mathbb{R}^{+}$, or $d-c=0$. Thus:
                \begin{equation}
                    (b+d)-(a+c)=(b-a)+(d-c)\in\mathbb{R}^{+}
                \end{equation}
                Therefore, etc.
            \end{proof}
            \begin{theorem}
                If $a,b,c,d\in\mathbb{R}^{+}$, if $a<b$, and if
                $c\leq{d}$, then $a\cdot{c}<b\cdot{d}$.
            \end{theorem}
            \begin{proof}
                For if $a<b$, then $b-a\in\mathbb{R}^{+}$. But if
                $c\leq{d}$, then $d-c\in\mathbb{R}^{+}$, or
                $d-c=0$. But then
                $b\cdot{c}-a\cdot{c}=c\cdot(b-a)\in\mathbb{R}^{+}$.
                Similarly,
                $a\cdot{d}-a\cdot{c}=a\cdot(d-c)$, and thus this is
                either positive of zero. Therefore:
                \begin{equation}
                    bd-ac=(bd-ad)+(ad-ac)=
                    d(b-d)+a(d-c)\in\mathbb{R}^{+}
                \end{equation}
            \end{proof}
            \begin{theorem}
                If $a,b\in\mathbb{R}$ are negative, then $a+b$ is
                negative.
            \end{theorem}
            \begin{proof}
                For if $a$ and $b$ are negative, then
                $\minus{a}$ and $\minus{b}$ are positive. But then
                $(\minus{a})+(\minus{b})\in\mathbb{R}^{+}$. But:
                \begin{equation}
                    (\minus{a})+(\minus{b})=
                    (\minus{1})\cdot{a}+(\minus{1})\cdot{b}
                    =(\minus{1})\cdot(a+b)
                    =\minus(a+b)\in\mathbb{R}^{+}
                \end{equation}
                Thus, $a+b$ is negative.
            \end{proof}
            \begin{theorem}
                If $a,b\in\mathbb{R}$, if $a$ is positive, and if
                $b$ is negative, than $a\cdot{b}$ is negative.
            \end{theorem}
            \begin{proof}
                For if $b$ is negative, then $\minus{b}$ is
                positive, and thus:
                \begin{equation}
                    \minus(a\cdot{b})
                    =a\cdot(\minus{b})\in\mathbb{R}^{+}
                \end{equation}
                Thus, $a\cdot{b}$ is negative.
            \end{proof}
            \begin{theorem}
                If $a,b\in\mathbb{R}$ are negative, then $a\cdot{b}$
                is positive.
            \end{theorem}
            \begin{proof}
                For if $a$ and $b$ are negative, then
                $\minus{a},\minus{b}\in\mathbb{R}^{+}$. But then:
                \begin{equation}
                    a\cdot{b}=1\cdot(a\cdot{b})
                    =\big((\minus{1})\cdot(\minus{1})\big)
                    \cdot(a\cdot{b})
                    =(\minus{a})\cdot(\minus{b})\in\mathbb{R}^{+}
                \end{equation}
                And thus $a\cdot{b}$ is positive.
            \end{proof}
            \begin{theorem}
                If $a\in\mathbb{R}$, then $a^{2}\geq{0}$.
            \end{theorem}
            \begin{proof}
                For if $a$ is positive, then $a\cdot{a}$ is positive.
                If $a$ is zero, then $a\cdot{a}=0$. Finally, from the
                previous theorem, the product of two negative numbers
                is positive, and therefore if $a$ is negative, then
                $a\cdot{a}$ is positive.
            \end{proof}
            From this we have that $1=1^{1}>0$. This generalized to
            the sum of any number of squares.
            \begin{theorem}
                If $a>0$, then $a^{\minus{1}}>0$.
            \end{theorem}
            \begin{proof}
                Suppose not. Then either $a^{\minus{1}}$ is negative
                or it is zero. But it is not zero, for zero has no
                multiplicative inverse, and $a$ is an inverse of
                $a^{\minus{1}}$. Thus $a^{\minus{1}}$ is negative.
                But $a\cdot{a}^{\minus{1}}=1>0$, a contradiction.
                Therefore, $a^{\minus{1}}$ is positive.
            \end{proof}
            \begin{theorem}
                If $0<a<b$, then $0<b^{\minus{1}}<a^{\minus{1}}$.
            \end{theorem}
            \begin{proof}
                For:
                \begin{equation}
                    0<a<b\Longrightarrow
                    0<a\cdot(a^{\minus{1}}b^{\minus{1}})<
                    b\cdot(a^{\minus{1}}b^{\minus{1}})\Longrightarrow
                    0<b^{\minus{1}}<a^{\minus{1}}
                \end{equation}
            \end{proof}
            We thus have a way to distinguish $\mathbb{R}$
            from finite fields. We define the natural numbers to by
            $2=1+1$, $3=2+1$, $4=3+1$, and so on. Order also excludes
            the complex numbers, $\mathbb{C}$, since the complex
            numbers are not ordered. However, the rational numbers,
            $\mathbb{Q}$, still satisfy all of these properties and
            are too an ordered field. We need another property to
            distinguish $\mathbb{Q}$ from $\mathbb{R}$. First, a
            discussion of exponentiation and the absolute value
            function. Given a positive integer $n$, we define the
            exponentiation of a real number $r$ by
            $r^{n}=r\cdots{r}$, where multiplication is
            carried out $n$ times. From this, we get:
            \begin{subequations}
                \begin{align}
                    a^{n}\cdot{a}^{m}&=a^{n+m}\\
                    (a^{m})^{n}&=a^{mn}\\
                    (ab)^{n}&=a^{n}b^{n}
                \end{align}
            \end{subequations}
            The abolute value of a real number is defined as:
            \begin{equation}
                |a|=
                \begin{cases}
                    a,&a\geq{0}\\
                    \minus{a},&a<-
                \end{cases}
            \end{equation}
            \begin{theorem}
                If $a\in\mathbb{R}$, then $|a|\geq{0}$.
            \end{theorem}
            \begin{theorem}
                If $a,b\in\mathbb{R}$, then
                $|a\cdot{b}|=|a|\cdot|b|$.
            \end{theorem}
            \begin{theorem}
                If $a\in\mathbb{R}$, then $a^{2}=|a|^{2}$.
            \end{theorem}
            \begin{ltheorem}{Triangle Inequality}
                If $a,b\in\mathbb{R}$, then
                $|a+b|\leq|a|+|b|$.
            \end{ltheorem}
            \begin{ltheorem}{Reverse Triangle Inequality}
                If $a,b\in\mathbb{R}$, then
                $|a-b|\geq\big||a|-|b|\big|$
            \end{ltheorem}
            Note that $|x-a|<\varepsilon$ implies that
            $\varepsilon-a<x<\varepsilon+a$. Thus, the solution set
            to this inequality is all of the points that lie in the
            interval $(a-\varepsilon,a+\varepsilon)$. Now, to
            separate $\mathbb{R}$ from $\mathbb{Q}$ we need
            to introduce the idea of \textit{completeness}.
            We will do this in the form of the Least Upper
            Bound axiom.
            \begin{definition}
                An upper bound for a subset $S\subseteq\mathbb{R}$
                is a real number $r$ such that, for all $x\in{S}$,
                we have $x\leq{r}$.
            \end{definition}
            A bounded above subset is a subset with an upper bound.
            \begin{definition}
                A least upper bound for a subset
                $S\subseteq\mathbb{R}$ is a real number $r$ such
                that $r$ is an upper bound
                for $S$, and for all upper bounds $s$, we have
                $r\leq{s}$.
            \end{definition}
            From this definition we have that least upper bounds are
            unique for a given bounded above set.
            \begin{theorem}
                If $S$ is a subset of $\mathbb{R}$, if $s$ is a
                least upper bound of $S$, and if $x\in\mathbb{R}$
                is such that $x<s$, then there is a $y\in{S}$
                such that $x<y$.
            \end{theorem}
            \begin{proof}
                For suppose not. Then $x$ is an upper bound of $S$,
                a contradiction as $s$ is the least upper bound.
            \end{proof}
            Any non-empty finite subset will have a least
            upper bound. Infinite subsets need not have a least
            upper bound, and indeed $\mathbb{R}$ does not have
            one. If the least upper bound of $S$ exists, it may
            not belong to $S$. For example, the set of all
            negative numbers has zero as its least upper bound,
            but zero is not a negative number. The real
            numbers satisfy the following property:
            \begin{enumerate}
                \item For any non-empty set of real numbers that
                      is bounded from above, there is a least
                      upper bound.
            \end{enumerate}
            This axiom distringuishes the rational numbers from the
            real numbers. That is, there are bounded above subsets
            of $\mathbb{Q}$ with no least upper bound.
    \section{Old Notes}
        The real line, or real number system, is a complete ordered
        field. That is, it is complete in the sense that all
        Cauchy sequences converge, has a total order structure
        on it, and has a field structure (That of addition,
        multiplication, subtraction, and division).
        An open subset of the real line is a set $S$ such that
        for all $x\in{S}$ there is an $\varepsilon>0$ such that
        $(x-\varepsilon,x+\varepsilon)\subset{S}$. The entire
        space $\mathbb{R}$ is open, as is the empty set
        $\emptyset$. The union of
        an arbitrary collection of open sets is open, and the
        intersection of finitely many open sets is open. The
        intersection of infinitely many open sets may not be
        open, however. A set is closed if its complement is
        open. The Euclidean plane is the set of all ordered
        pairs $(a,b)$. That is,
        $\mathbb{R}^{2}=\mathbb{R}\times\mathbb{R}$. Euclidean
        space, or 3-space, is
        $\mathbb{R}^{3}=\mathbb{R}\times\mathbb{R}\times\mathbb{R}$.
        This is the set of all ordered triplets $(x,y,z)$. Similarly,
        $n$ dimensional Euclidean space is the set of all
        $n$ tuples. This is denoted $\mathbb{R}^{n}$. The distance
        between two points $\mathbf{x}$ and $\mathbf{y}$ is defined
        by the generalized Pythagorean Theorem:
        \begin{equation*}
            d(\mathbf{x},\mathbf{y})=
            \sqrt{\sum_{k=1}^{n}(x_{k}-y_{k})^{2}}
        \end{equation*}
        \begin{definition}
            A metric on a set $X$ is a function
            $d:X\times{X}\rightarrow\mathbb{R}$ such that:
            \begin{enumerate}
                \item $d(x,y)\geq{0}$ for all $x,y\in{X}$.
                \item $d(x,y)=0$ if and only if $x=y$.
                \item $d(x,y)=d(y,x)$ for all $x,y\in{X}$.
                \item $d(x,z)\leq{d(x,y)+d(y,z)}$
                      for all $x,y,z\in{X}$.
            \end{enumerate}
        \end{definition}
        There are two types of integrals defined for functions
        of a real variable: Riemann Integration and Lebesgue Integration.
        Lebesgue integration requires the notion of \textit{measure}.
    \subsection{Definitions}
        \begin{definition*}
                The tangent line of a differentiable function
                $y:\mathbb{R}\rightarrow\mathbb{R}$ at a point
                $x_{0}\in\mathbb{R}$ is the function
                $y_{T}:\mathbb{R}\rightarrow\mathbb{R}$ defined by
                $y_{T}(x)=y'(x_0)(x-x_0)+y(x_0)$ 
            \end{definition*}
        \begin{definition*}
            If $\Gamma(t)=\big(x(t),y(t)\big)$, for $a\leq t\leq b$,
            and $\Gamma'(t)=\big(x'(t),y'(t)\big)$ exists for
            $a<t<b$, then the length of $\Gamma$ from $a$ to $b$ is:
            \begin{equation}
                L=\int_{a}^{b}\sqrt{
                    \bigg(\frac{dx}{dt}\bigg)^{2}+
                    \bigg(\frac{dy}{dt}\bigg)^{2}
                }dt
            \end{equation}
        \end{definition*}
        \begin{definition*}
            The dimension of a vector space is the cardinality of
            any basis of the space. 
        \end{definition*}
        \begin{remark*}
            By the Dimension Theorem, all bases of a vector space
            have the same cardinality.
        \end{remark*}
        \begin{definition*}
            The absolute value of $x$ is
            $|x|=\begin{cases}%
                x,&x\geq 0\\ 
                -x,&x<0
            \end{cases}$
        \end{definition*}
    \subsection{Theorems}
    \begin{theorem*}[Mean Value Theorem]
        If $f:(a,b)\rightarrow\mathbb{R}$ is continuous and
        bounded, and if $x\in(a,b)$, then there is a $c\in(a,x)$
        such that $\int_{a}^{x}f=(x-a)f(c)$.
    \end{theorem*}
    \begin{theorem*}
        [Generalized Fundamental Theorem of Calculus]
        If $\mathcal{U}$ is an open non-empty subset of
        $\mathbb{R}$, $a\in\mathcal{U}$, and if
        $f:\mathcal{U}\rightarrow\mathbb{R}$
        is bounded and continuous, then
        $F:\mathcal{U}\rightarrow\mathbb{R}$
        defined by $F(x)=\int_{\mathcal{U}\cap (a,x)}f$ is
        differentiable and $F'(x)=f(x)$
    \end{theorem*}
    \begin{proof}
        For let $x\in\mathcal{U}$. Let
        $\{x_n\}_{n=1}^{\infty}\subset\mathcal{U}$
        be a sequence such that $x_{n}\rightarrow x$,
        $x\notin\{x_{n}\}_{n=1}^{\infty}$.
        As $\mathcal{U}$ is open and $x\in\mathcal{U}$,
        there is an $\varepsilon>0$ such that
        $B_{\varepsilon}(x)\subset\mathcal{U}$. But, as
        $x_{n}\rightarrow x$, there is an $N\in \mathbb{N}$ such
        that for all $n>N$, $x_{n}\in B_{\varepsilon}(x)$.
        But then for all $n>N$:
        \begin{equation*}
            \int_{\mathcal{U}\cap(a,x)}f-%
            \int_{\mathcal{U}\cap(a,x_{n})}f=%
            \int_{x_{n}}^{x}f
        \end{equation*}
        But, as $f$ is continuous, by the mean value theorem for
        all $n>N$ there is a $c_{n}\in(x_n,x)$ such that
        $\int_{x_{n}}^{x}f=(x-x_{n})f(c_{n})$. But then 
        \begin{equation*}
            \Big|\frac{\int_{x_{n}}^{x}f}{x-x_{n}}-f(x)\Big|
            =|f(c_{n})-f(x)|
        \end{equation*}
        But $c_{n}\in(x_{n},x)$, and $x_{n}\rightarrow x$, and
        therefore $c_{n} \rightarrow x$. But $f$ is continuous,
        and therefore $f(c_{n})\rightarrow f(x)$. Therefore, by
        the definition of the derivative of $F$ at $x$,
        $F'(x)=f(x)$. 
    \end{proof}
    \begin{theorem*}
        If $V$ is a vector space and $A,B\subset V$ are
        subspaces, then $A\cap B$ is a subspace and
        $\dim(A\cap B)\leq\min\{\dim(A),\dim(B)\}$
    \end{theorem*}
    \begin{theorem*}
        If $f:\mathbb{R}\rightarrow \mathbb{R}$
        is differentiable
        and $f'(x)>0$ for all $x$,
        then $f$ is strictly increasing.
    \end{theorem*}
    \begin{theorem*}
        If $f:(a,b)\rightarrow\mathbb{R}$ is continuous and
        $f(a)<0<f(b)$, then there is a $c\in (a,b)$ such that
        $f(c)=0$.
    \end{theorem*}
    \begin{theorem*}
        If $f$ is integrable on $(a,b)$, and if $c\in(a,b)$, then
        $\int_{a}^{b}f=\int_{a}^{c}f+\int_{c}^{b}f$
    \end{theorem*}
\end{document}