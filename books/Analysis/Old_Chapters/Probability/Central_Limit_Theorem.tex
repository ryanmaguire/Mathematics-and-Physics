\section{Central Limit Theorem}
    We now wish to discuss convergence of measures and distributions.
    We restrict ourself to Lebesgue-Stieljes measures on the Borel
    $\sigma\textrm{-Algebra}$ of $\mathbb{R}$. We does it mean for a
    sequence of measures $\mu_{n}$ to converge to a measure $\mu$? For
    all $B\in\mathcal{B}$:
    \begin{equation}
        \mu_{n}(B)\rightarrow\mu(B)
    \end{equation}
    This is reminiscent of point-wise convergence of functions of a real
    variable, but turns out to be too much. What if we restrict ourselves
    to sets of the form $[a,b)$? Let:
    \begin{equation}
        f_{n}(x)=
            \begin{cases}
                0,&|x|\geq\frac{1}{n}\\
                n(1-|x|),&|x|<\frac{1}{n}
            \end{cases}
    \end{equation}
    And define:
    \begin{equation}
        \mu_{n}([a,b)]=\int_{a}^{b}\rho_{n}(x)\diff{x}
    \end{equation}
    Then by the Caratheodory extension theorem, there is a
    measure $\nu_{n}$ that agrees with $\mu_{n}$ on all
    such intervals. Then $\nu_{n}$ converges to the
    Dirac measure, which is an example of an atomic measure:
    \begin{equation}
        \delta(B)=
            \begin{cases}
                1,&0\in{B}\\
                0,&0\notin{B}
            \end{cases}
    \end{equation}
    However:
    \begin{equation}
        \mu_{n}([0,b)]\rightarrow\frac{1}{2}
    \end{equation}
    And:
    \begin{equation}
        \mu_{n}([a,0)]\rightarrow\frac{1}{2}
    \end{equation}
    However:
    \begin{align}
        \delta([0,b))&=1\\
        \delta([a,0))&=0
    \end{align}
    This leads us to the correct definition of measure:
    \begin{ldefinition}{Convergence in Measure}{Convergence_in_Measure}
        A sequence of measure $\nu_{n}$ converges to a measure $\nu$ if,
        for all measure sets $B$ such that $\nu(\partial{B})=0$, it is
        true that $\nu_{n}(B)\rightarrow\nu(B)$.
    \end{ldefinition}
    Given:
    \begin{equation}
        \int_{\mathbb{R}}\chi_{[a,b)}\diff{\nu_{n}}
        \rightarrow\int_{\mathbb{R}}\chi_{[a,b)}\diff{\nu}
    \end{equation}
    We have that $\nu(\{a\})=\nu(\{b\})=0$, and thus $\chi_{[a,b)}$ is
    continuous $\nu$ almost everywhere. Suppose $\nu_{n}$ and $\nu$ are
    probability Lebesgue-Stieltjes measure on $\mathbb{R}$. Then we
    get the equivalent form:
    \begin{theorem}
        If for every continuous bounded function $g(\omega)$, we have:
        \begin{equation}
            \int_{\mathbb{R}}g\diff{\mu}_{n}\rightarrow
            \int_{\mathbb{R}}g\diff{\mu}
        \end{equation}
        Then $\mu_{n}\rightarrow\mu$.
    \end{theorem}
    \begin{theorem}
        If for every continuous function with bounded support,
        if:
        \begin{equation}
            \int_{\mathbb{R}}g\diff{\mu}_{n}\rightarrow
            \int_{\mathbb{R}}g\diff{\mu}
        \end{equation}
        Then $\nu_{n}\rightarrow\nu$.
    \end{theorem}
    \begin{theorem}
        If:
        \begin{equation}
            \int_{\mathbb{R}}\exp(itu)\diff{\nu_{n}}\rightarrow
            \int_{\mathbb{R}}\exp(itu)\diff{\nu}
        \end{equation}
        Then $\nu_{n}\rightarrow\nu$.
    \end{theorem}
    Suppose $(\Omega,\mathcal{A},\mu)$ is a probability space,
    and suppose $f_{n}:\Omega\rightarrow\mathbb{R}$ is a
    sequence of random variables that are
    $\mathcal{A}-\mathcal{B}$ measure. Consider the
    distributions $\mu_{g_{n}}$. If $g_{n}\rightarrow{g}$ in
    measure, then the distribuctions converge to $\mu_{g}$.
    \begin{theorem}
        If $(\Omega,\mathcal{A},\mu)$ is a probability space,
        if $h_{n}:\Omega\rightarrow\mathbb{R}$ is a
        sequence of random variables, if $\mu_{h_{n}}$ are the
        distributions of $g_{n}$, and if $h_{n}\rightarrow{h}$
        in measure, then $\mu_{h_{n}}\rightarrow\mu_{h}$.
    \end{theorem}
    \begin{proof}
        For let $g$ be a continuous function with compact
        support. Then, applying the measure transformation
        theorem, we have:
        \begin{equation}
            \Big|\int_{\mathbb{R}}g\diff{\mu_{n}}-
                \int_{\mathbb{R}}\diff{\mu_{h}}\Big|
            =\Big|\int_{\Omega}g(h_{n})\diff{\mu}-
                \int_{\Omega}g(h)\diff{\mu}\Big|
            \leq\int_{\Omega}|g_{n}(h)-g(h)|\diff{\mu}
        \end{equation}
        But $g$ is continuous on a compact set, and is
        therefore uniformly continuous. Thus, for all
        $\varepsilon>0$ there is a $\delta>0$ such that, for
        all $|u'-u''|<\delta$, we have that
        $|g(u')-g(u'')|<\varepsilon$. Define the following:
        \begin{align}
            E_{1,n,\varepsilon}
            &=\{\omega:|h_{n}(\omega)-h(\omega)|\geq\delta\}\\
            E_{2,n,\varepsilon}
            &=\{\omega:|h_{n}(\omega)-h(\omega)|<\delta\}
        \end{align}
        Then:
        \begin{align}
            \int_{\Omega}|g_{n}(h)-g(h)|\diff{\mu}
            &=\int_{E_{1,n,\varepsilon}}|g_{n}(h)-g(h)|\diff{\mu}
            +\int_{E_{2,n,\varepsilon}}
                |g_{n}(h)-g(h)|\diff{\mu}\\
            &\leq{2}M\mu(E_{1,n,\varepsilon})+
                \varepsilon\mu(E_{2,n,\varepsilon})
        \end{align}
        And this converges to $\varepsilon$.
    \end{proof}
    The converse of this theorem is not true in general,
    since vastly different functions can have the same
    distributions. There is a special case, however, where the
    converse holds. Consider a function $h$ such that it's
    distribution is the Dirac distribution. That is:
    \begin{equation}
        \mu(\{\omega:h(\omega)=a\})=
        \mu_{h}(\{a\})=\delta_{a}(\{a\})=1
    \end{equation}
    Then $h(\omega)=a$ $\mu$ almost everywhere, or if we are
    in a probabilty space, almost surely.
    \begin{theorem}
        If $h_{n}$ is a sequence of random variables such that
        $\mu_{h_{n}}\rightarrow\delta_{a}$, where $\delta_{a}$
        is the Diract measure centered at $a$, then
        $h_{n}\rightarrow{a}$ almost surely.
    \end{theorem}
    \begin{proof}
        For:
        \begin{align}
            \mu(\{\omega:|h_{n}(\omega)-a|\geq\delta\})
            &=\mu_{h_{n}}
                (\mathbb{R}\setminus(a-\delta,a+\delta)\})
            &=1-\mu_{h_{n}}((a-\delta,a+\delta))\\
            &\rightarrow{1}-\delta_{a}((a-\delta,a+\delta))\\
            &=0
        \end{align}
    \end{proof}
    Thus, the weak law of large numbers can be restated by
    saying that, if:
    \begin{equation}
        \mu_{\frac{1}{n}\sum_{j=1}^{n}f_{j}}\rightarrow
        \delta_{0}
    \end{equation}
    Then $f_{j}$ obeys the weak law of large numbers.
    \subsection{Convergence of Distributions}
        A distribution is an arbitrary probability
        Lebesgue-Stieljes measure. That is, a Lebesgue-Stieljes
        measure such that the measure of the entire space is
        one. We say that a sequence of distribuctions
        $\nu_{n}$ converges to a measure $\nu$ if any of
        the following equivalent statements holds:
        \begin{enumerate}
            \item $\nu_{n}([a,b))\rightarrow\nu([a,b))$
                  for all $a<b$.
            \item $\nu_{n}((\minus\infty,c))\rightarrow%
                   \nu(\minus\infty,c))$ for all $c$ such
                   that $\nu(\{c\})=0$. This requirement
                   implies that $\nu$ is continuous at $c$.
                   That is, if $F_{\nu}$ is the cumulative
                   distribution function, then $F_{\nu}$ is
                   continuous at $c$.
            \item For every bounded continuous function $h$,
                  $\int_{\mathbb{R}}h\diff\nu_{n}\rightarrow%
                   \int_{\mathbb{R}}h\diff{\nu}$.
            \item For every continuous function with compact
                  support:
                  $\int_{\mathbb{R}}h\diff\nu_{n}\rightarrow%
                   \int_{\mathbb{R}}h\diff{\nu}$.
            \item $\int_{\mathbb{R}}\exp(itu)\diff{\nu_{n}}%
                   =\int_{\mathbb{R}}\exp(itu)\diff{\nu}$
        \end{enumerate}
        \begin{theorem}
            A sequence of random variables $f_{j}$ obeys
            the weak law of large numbers if and only if:
            \begin{equation}
                \mu_{\frac{1}{n}\sum_{j=1}^{n}f_{j}}
                \rightarrow\delta_{0}
            \end{equation}
        \end{theorem}
        \begin{proof}
            For:
            \begin{equation}
                \mu\Big(\big\{\omega:\Big|\frac{1}{n}
                    \sum_{j=1}^{n}\overset{\circ}{f}_{k}(\omega)
                    \Big|\geq\delta\big\}\Big)=
                \mu_{\frac{1}{n}\sum_{k=1}^{n}
                     \overset{\circ}{f}_{j}}
                     \big((\minus\delta,\delta)^{C}\big)
            \end{equation}
        \end{proof}
        \begin{theorem}
            If $f_{j}$ is a sequence of random variables such
            that the second moments are finite, then the
            first moments are finite.
        \end{theorem}
        \begin{proof}
            For:
            \begin{equation}
                \int_{\Omega}|f_{j}|\diff{\mu}\leq
                \int_{\Omega}(1+|f_{j}|^{2})\diff{\mu}
                =\int_{\Omega}\diff{\mu}+
                \int_{\Omega}|f_{j}|^{2}\diff{\mu}=
                1+\int_{\Omega}|f_{j}|^{2}\diff{\mu}
            \end{equation}
            Therefore, etc.
        \end{proof}
        \begin{ftheorem}{Central Limit Theorem}
              {Measure_Theory_Central_Limit_Theorem}
            If $f_{j}$ are independent and identically
            distributed, with standard deviation $\sigma$,
            then:
            \begin{equation}
                \mu_{\frac{1}{\sigma\sqrt{n}}
                    \sum_{j=1}^{n}\overset{\circ}{f}_{j}}
                \rightarrow\nu_{0,1}
            \end{equation}
            Where $\nu_{0,1}$ is the Gaussian distribution:
            \begin{equation}
                \nu_{0,1}(B)=\frac{1}{\sqrt{2\pi}}
                \int_{B}\exp(\minus{u}^{2}/2)\diff{u}
            \end{equation}
        \end{ftheorem}
        \begin{proof}
            We will use the Fourier transform to prove this.
            We have:
            \begin{equation}
                \int_{\mathbb{R}}\exp(iut)
                    \diff{\nu_{0,1}}=
                \int_{\mathbb{R}}\exp(itu)
                    \exp(\minus\frac{u^{2}}{2})\diff{u}
                    =\exp(\minus{t}^{2}/2)
            \end{equation}
            That is, the Fourier transform of a Gaussian
            is itself. We will use this to make the
            computation easier. Using the measure
            transformation theorem, we have:
            \begin{equation}
                \int_{\mathbb{R}}\exp(iut)
                \mu_{\frac{1}{\sigma\sqrt{n}}
                    \sum_{j=1}^{n}\overset{\circ}{f}_{j}}
                    \diff{\mu}
                =\int_{\Omega}\exp\Big(
                    \frac{i}{\sigma\sqrt{n}}\sum_{j=1}^{n}
                    \overset{\circ}{f}_{j}(\omega)t\Big)
                    \diff{\mu}
            \end{equation}
            We invoke independence to get:
            \begin{subequations}
                \begin{align}
                    \int_{\Omega}\exp\Big(
                        \frac{i}{\sigma\sqrt{n}}\sum_{j=1}^{n}
                        \overset{\circ}{f}_{j}(\omega)t\Big)
                        \diff{\mu}
                    &=\int_{\Omega}\prod_{j=1}^{n}
                        \exp\Big(\frac{i}{\sigma\sqrt{n}}
                        \overset{\circ}{f}_{j}\Big)\diff{\mu}\\
                    &=\prod_{j=1}^{n}\int_{\Omega}
                        \exp\Big(\frac{i}{\sigma\sqrt{n}}
                        \overset{\circ}{f}_{j}\Big)\diff{\mu}
                \end{align}
            \end{subequations}
            But the distributions are identically distributed,
            and thus we have:
            \begin{equation}
                \int_{\Omega}\exp\Big(
                    \frac{i}{\sigma\sqrt{n}}\sum_{j=1}^{n}
                    \overset{\circ}{f}_{j}(\omega)t\Big)
                    \diff{\mu}=
                \Big[\int_{\Omega}\exp\Big(
                    iu\frac{t}{\sqrt{n}}\Big)
                    \diff{\mu}\Big]^{n}
            \end{equation}
            We now need to prove that for an arbitrary
            Lebesgue-Stieltjes Measure on the Borel
            $\sigma\textrm{-Algebra}$ of $\mathbb{R}$,
            such that:
            \begin{equation}
                \int_{\mathbb{R}}\diff{\mu}=0\quad\quad
                \int_{\mathbb{R}}u\diff{\mu}=0\quad\quad
                \int_{\mathbb{R}}u^{2}\diff{\mu}=1
            \end{equation}
            Then:
            \begin{equation}
                \Big[\int_{\mathbb{R}}\exp\Big(
                    iu\frac{t}{\sqrt{n}}\Big)\diff{\mu}\Big]^{n}
                \rightarrow\exp\big(\minus{t}^{2}/2\big)
            \end{equation}
            Consider the function:
            \begin{equation}
                \varphi_{\nu}(t)=
                \int_{\mathbb{R}}\exp(iut)\diff{\nu}
            \end{equation}
            In analysis this is the Fourier transform,
            whereas in probability this is called the
            characteristic function of $\nu$. We are
            tasked with showing that:
            \begin{equation}
                \Big[\varphi_{\mu}
                    \big(\frac{t}{\sqrt{n}}\big)\Big]^{n}
                \rightarrow\exp\big(\minus{t}^{2}/2\big)
            \end{equation}
            If $\mu$ is a Lebesgue-Stieltjes measure, and
            if the second moment if finite, and if:
            \begin{equation}
                \varphi_{\nu}(t)=
                \int_{\mathbb{R}}\exp(itu)\diff{\nu}
            \end{equation}
            then the first two derivatives of $\varphi_{\nu}$
            exist and are continuous. Moreover:
            \begin{equation}
                \varphi_{\nu}(t)=
                \varphi_{\nu}(0)+
                \varphi_{\nu}'(0)t+
                \varphi_{\nu}''(0)t^{2}+h(t)
            \end{equation}
            Where $h$ is such that:
            \begin{equation}
                \underset{t\rightarrow{0}}{\lim}
                \frac{h(t)}{t^{2}}=0
            \end{equation}
            First, it is continuous. For let $t_{k}$ be
            sequence such that $t_{k}\rightarrow{t}$ and let
            $g_{k}=\exp(it_{k}u)$. Then $|g_{k}|=1$, and is
            therefore summable. Moreover, $g_{k}$ tends to
            $\exp(itu)$. Thus, by the dominated convergence
            theorem:
            \begin{equation}
                \underset{n\rightarrow\infty}{\lim}
                \varphi_{\nu}(t_{k})
                =\underset{n\rightarrow\infty}{\lim}
                \int_{\mathbb{R}}\exp(it_{k}u)\diff{\mu}
                =\int_{\mathbb{R}}
                    \underset{n\rightarrow\infty}{\lim}
                    \exp(it_{k}u)\diff{\mu}
                =\varphi_{\nu}(t)
            \end{equation}
            And thus we have continuity. For differentiability,
            suppose $\Delta{t}_{k}$ is a sequence that
            tends to zero, and consider:
            \begin{equation}
                \frac{\varphi_{\nu}(t+\Delta{t}_{k})-
                      \varphi_{\nu}(t)}{\Delta{t}_{k}}=
                \int_{\mathbb{R}}
                \frac{\exp(iu\Delta{t}_{k})-1}{\Delta{t}_{k}}
                \exp(iut)\diff{\mu}
            \end{equation}
            Again, we want to apply the dominated convergence
            theorem. Thus we need to find a summable
            majorant. Consider $f(s)=(\exp(s)-1)/s$. On the
            real axis, this function has finite limit at
            zero and has zero limit at infinity, and therefore
            $f(s)$ is bounded on the real axis by some $K$.
            Thus, $K\exp(iut)$ serves as a summable majorant.
            Applying the dominated convergence theorem shows
            that the limit exists, and thus
            $\varphi_{\nu}$ is differentiable. We obtain:
            \begin{equation}
                \varphi_{\nu}'(t)=
                \int_{\mathbb{R}}iu\exp(iut)\diff{\nu}
            \end{equation}
            Moreover, this is differentiable and:
            \begin{equation}
                \varphi_{\nu}''(t)=
                \minus\int_{\mathbb{R}}u^{2}\exp(iut)
                    \diff{\mu}
            \end{equation}
            From Taylor, we have:
            \begin{equation}
                h(t)=\varphi_{\nu}(t)-\varphi_{\nu}(0)
                    -\varphi_{\nu}'(0)t-\varphi_{\nu}''(0)
                        \frac{t^{2}}{2}
            \end{equation}
            Thus $h''(t)$ exists and is continuous,
            $h(0)=0$, $h'(0)=0$, and $h''(0)=0$. By the
            mean value theorem, we have:
            \begin{equation}
                h(t)=h'(t_{1})t
            \end{equation}
            For some $t_{1}\in(0,t)$. Moreover:
            \begin{equation}
                h(t)=h''(t_{2})t^{2}
            \end{equation}
            Where $0<t_{1}<t_{2}<t$. Thus:
            \begin{equation}
                \frac{h(t)}{t^{2}}=h''(t_{2})
            \end{equation}
            And from the continuity of $h''(t)$, this
            converges to zero as $t$ tends to zero. Thus
            we have that $\varphi_{\nu}(0)=1$,
            $\varphi_{\nu}'(0)=0$, and
            $\varphi_{\nu}''(0)=\minus{1}$. Now we need to 
            finally justify the following limit:
            \begin{equation}
                \Big[\varphi_{\mu}\big(\frac{t}{\sqrt{n}}\big)
                    \Big]^{n}\rightarrow\exp(\minus{t}^{2}/2)
            \end{equation}
            We have:
            \begin{equation}
                \varphi_{\nu}(t)=1-\frac{t^{2}}{2}+h(t)
            \end{equation}
            Where $h(t)/t^{2}\rightarrow{0}$ as
            $t\rightarrow{0}$. Thus:
            \begin{equation}
                \Big[\varphi_{\nu}\big(\frac{t}{\sqrt{n}}
                    \big)\Big]^{n}=
                \Big[1-\frac{t^{2}}{2n}+h(\frac{t}{\sqrt{n}})
                    \Big]^{n}
            \end{equation}
            Define:
            \begin{equation}
                w_{n}(t)=h(t/\sqrt{n})-\frac{t^{2}}{2n}
            \end{equation}
            Then we have:
            \begin{equation}
                \Big[\varphi_{\nu}\big(\frac{t}{\sqrt{n}}
                    \big)\Big]^{n}
                =\Big(\Big[1+w_{n}(t)\Big]^{w_{n}(t)}
                    \Big)^{\frac{n}{w_{n}(t)}}
            \end{equation}
            The inner part is the definition of $e$, so we
            now need to show that $n/w_{n}(t)$ converges to
            $\minus{t}^{2}/2$.
        \end{proof}