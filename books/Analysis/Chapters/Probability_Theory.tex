\documentclass[crop=false,class=book,oneside]{standalone}
%----------------------------Preamble-------------------------------%
\input{../../preamble.tex}
%----------------------------GLOSSARY-------------------------------%
\makeglossaries
\loadglsentries{../../glossary}
\loadglsentries{../../acronym}
%--------------------------Main Document----------------------------%
\begin{document}
    \ifx\ifmain\undefined
        \pagenumbering{roman}
        \title{Probability Theory}
        \author{Ryan Maguire}
        \date{\vspace{-5ex}}
        \maketitle
        \tableofcontents
        \clearpage
        \chapter*{Probability Theory}
        \addcontentsline{toc}{chapter}{Probability Theory}
        \markboth{}{PROBABILITY THEORY}
        \vspace{10ex}
        \setcounter{chapter}{1}
        \pagenumbering{arabic}
    \else
        \chapter{Measure Theory}
    \fi
    \section{Product Measures}
        Let $(\Omega_{1},\mathcal{A}_{1},\mu_{1})$ and
        $(\Omega_{2},\mathcal{A}_{2},\mu_{2})$ be measure spaces. We
        wish to define a \textit{natural} measure space
        on the Cartesian product $\Omega_{1}\times\Omega_{2}$.
        Let $\mathcal{P}$ be defined by:
        \begin{equation}
            \mathcal{P}=
            \{A_{1}\times{A}_{2}:
                A_{1}\in\mathcal{A}_{1},A_{2}\in\mathcal{A}_{2}\}
        \end{equation}
        Then $\mathcal{P}$ is a semi-ring, but is not a
        $\sigma\textrm{-Algebra}$ on $\Omega_{1}\times\Omega_{2}$
        This is because the union of two rectangles may not be a
        rectangle. Similarly, the difference of two rectangles may not
        be a rectangle. However, the intersection of two rectangles is
        a rectangle, and hence this is a semi-ring.
        We defined the product $\sigma\textrm{-Algebra}$ to be the
        $\sigma\textrm{-Algebra}$ that is generated by $\mathcal{P}$. 
        \begin{ltheorem}{Carath\'{e}odory Extension Theorem}
            If $(\Omega_{1},\mathcal{A},\mu_{1})$ and
            $(\Omega_{2},\mathcal{A}_{2},\mu_{2})$ are measure spaces,
            if $\mathcal{A}$ is the product $\sigma\textrm{-Algebra}$
            on $\Omega_{1}\times\Omega_{2}$, then there is a unique
            measure $\mu$ on $\mathcal{A}$ such that, for all
            $A_{1}\in\mathcal{A}_{1}$ and all
            $A_{2}\in\mathcal{A}_{2}$:
            \begin{equation}
                \mu(A_{1}\times{A}_{2})
                =\mu_{1}(A_{1})\cdot\mu_{2}(A_{2})
            \end{equation}
        \end{ltheorem}
        \begin{ltheorem}{Funini's Theorem}
            If $f:\Omega_{1}\times\Omega_{2}\rightarrow\mathbb{R}$
            is a non-negative function that is
            $\mathcal{A}-\mathcal{B}$ measurable, where
            $\mathcal{A}$ is the product $\sigma\textrm{-Algebra}$,
            then:
            \begin{equation}
                \int_{\Omega_{1}\times\Omega_{2}}f\diff{\mu}=
                \int_{\Omega_{1}}\Big(
                    \int_{\Omega_{2}}f\diff{\mu_{2}}
                \Big)\diff{\mu}_{1}
                =\int_{\Omega_{2}}
                    \Big(\int_{\Omega_{1}}f\diff{\mu_{1}}\Big)
                    \diff{\mu}_{2}
            \end{equation}
        \end{ltheorem}
        As a summary, when is the following true?
        \begin{equation}
            \underset{n\rightarrow\infty}{\lim}
            \int_{\Omega}f_{n}\diff{\mu}
            \overset{?}{=}\int_{\Omega}
            \underset{n\rightarrow\infty}{\lim}f_{n}\diff{\mu}
        \end{equation}
        There are two special cases when equality can be guarenteed.
        The first is monotone convergence. If
        $f_{n}\rightarrow{f}$, where $f_{n+1}(x)\leq{f}_{n}(x)$ for
        all $n$, and if $f_{n}(x)\geq{F}$, where $F$ is a
        summable minorant, or if $f_{n}\rightarrow{f}$,
        $f_{n+1}(x)\leq{f}_{n}(x)$, and if
        $f_{n}(x)\leq{F}$, where $F$ is a summable majorant, then
        equality holds. The next case is by dominated convergence.
        If the limit of $f_{n}$ exists almost everywhere, and if
        $|f_{n}|\leq{F}$, where $F$ is summable, then by Fatou's
        Lemma:
        \begin{equation}
            \underset{n\rightarrow\infty}{\underline{\lim}}
            \int_{\Omega}f_{n}\diff{\mu}
            \geq\int_{\Omega}
            \underset{n\rightarrow\infty}{\underline{\lim}}
            f_{n}\diff{\mu}
        \end{equation}
        And also:
        \begin{equation}
            \underset{n\rightarrow\infty}{\overline{\lim}}
            \int_{\Omega}f_{n}\diff{\mu}
            \leq\int_{\Omega}
            \underset{n\rightarrow\infty}{\overline{\lim}}
            f_{n}\diff{\mu}
        \end{equation}
    \section{Probablity Spaces}
        To add later:
        \begin{enumerate}
            \item Probability space
            \item Independent $\sigma\textrm{-Algebras}$.
            \item Independent sets.
            \item Infinite sequence of $\sigma\textrm{-Algebras}$.
            \item Tail $\sigma\textrm{-Algebra}$.
            \item Terminal $\sigma\textrm{-Algebra}$.
            \item Kologorov zero-one law.
            \item If $\mathcal{A}_{j}$ independent, $F$ is self-independent.
            \item $E_{1},E_{2}\in{F}$,
                  $\mu(E_{1}\cap{E}_{2})=\mu(E_{1})\mu(E_{2})$, then
                  $\mu(E_{1})=0$ or $\mu(E_{1})=1$.
            \item Uniting $\sigma\textrm{-Algebras}$ lemma.
            \item $A_{1},\dots,A_{n}$ independent, then $A_{k},F$ independent,
                  where $F$ is the tail $\sigma\textrm{-Algebra}$.
        \end{enumerate}
    \section{Random Variables}
        Let $(\Omega,\mathcal{A},\mu)$ be a probability space.
        A probability space is a measure space such that
        $\mu(\Omega)=1$. Let $f:\Omega\rightarrow\mathbb{R}$ be
        $\mathcal{A}-\mathcal{B}$ measurable, where $\mathcal{B}$ is
        the Borel $\sigma\textrm{-Algebra}$. Such functions are called
        random-variables on $\Omega$. While there's nothing random
        about this, we use such functions to model problems in
        probability theory. The probability of an event
        $A\in\mathcal{A}$ is simply $\mu(A)$. The associated
        $\sigma\textrm{-Algebra}$ is defined as:
        \begin{equation}
            \mathcal{A}_{f}=\{f^{-1}(B):B\in\mathcal{B}\}
        \end{equation}
        This is also called the $\sigma\textrm{-Algebra}$ of events
        bearing on $f$. This is a $\sigma\textrm{-Algebra}$ on
        $\Omega$.
        \begin{ldefinition}{Distribution of a Random Variable}
            The distribution of a random variable
            $f:\Omega\rightarrow\mathbb{R}$ on a probability space
            $(\Omega,\mathcal{A},\mu)$ is the image measure
            $\mu_{f}$ of $f$.
        \end{ldefinition}
        The image measure is the measure:
        \begin{equation}
            \mu_{f}(B)=\mu(f^{-1}(B))
            =\mu(\{\omega\in\Omega:f(\omega)\in{B}\})
        \end{equation}
        This is a Lebesgue-Stieljes Measure on the Borel
        $\sigma\textrm{-Algebra}$ on $\mathbb{R}$.
        \begin{equation}
            \mu_{f}(\mathbb{R})=\mu(f^{-1}(\mathbb{R}))
            =\mu(\Omega)=1
        \end{equation}
        \begin{ldefinition}{Cumulative Distribution Function}
            The Cumulative Distribution Function of a random variable
            $f:\Omega\rightarrow\mathbb{R}$ on a probability space
            $(\Omega,\mathcal{A},\mu)$ is the function
            $F:\mathbb{R}\rightarrow\mathbb{R}$ defined by:
            \begin{equation}
                F(x)=\mu_{f}\big((-\infty,a)\big)
            \end{equation}
            Where $\mu_{f}$ is the distribution of $f$.
        \end{ldefinition}
        Some facts about the cumulative distribution function:
        It is non-decreasing on $\mathbb{R}$, left continuous, and
        $F(\minus\infty)-F(\infty)=1$. By the Caratheodory extension
        theorem, and function $F$ that satisfies these three
        conditions is the cumulative distribution function of some
        Lebesgue-Stieljes probability measure on $\mathbb{R}$. From this
        we also have that every Lebesgue-Stieljes probability measure
        on $\mathbb{R}$ is a distribution for a random variable.
        \begin{example}
            Let $\Omega=\mathbb{R}$, let $\mathcal{A}=\mathcal{B}$,
            where $\mathcal{B}$ is the Borel $\sigma\textrm{-Algebra}$,
            and let $\mu$ be a Lebesgue-Stieljes probability measure
            on $\mathbb{R}$. Define the random variable
            $f:\Omega\rightarrow\mathbb{R}$ by
            $f(\omega)=\omega$. The inverse of any Borel set is itself,
            and thus we see that the distribution and the random
            variable coincide.
        \end{example}
        \begin{example}
            Let $\Omega=[0,1]$, $\mathcal{B}$ be the Borel
            $\sigma\textrm{-Algebra}$, and define
            $f_{1},f_{2}:\Omega\rightarrow\mathbb{R}$ by:
            \begin{equation}
                f_{1}(\omega)=\omega
                \quad\quad
                f_{2}(\omega)=1-\omega
            \end{equation}
            These two functions, while different, will have the same
            cumulative distribution function. For we have:
            \begin{equation}
                F_{1}(u)=\mu_{f_{1}}\big((\minus\infty,u)\big)=
                \mu\big(f^{-1}(\minus\infty,u)\big)
            \end{equation}
            We can evaluate this case by case to get:
            \begin{equation}
                F_{1}(u)=
                \begin{cases}
                    \mu(\emptyset)=0,&u\leq{0}\\
                    \mu\big([0,u)\big)]u,0<u<1\\
                    \mu([0,1])=1,1\leq{u}
                \end{cases}
            \end{equation}
            Looking at $F_{2}$, we have:
            \begin{equation}
                F_{2}(u)=\mu_{f_{2}}\big((\minus\infty,u)\big)
                =\mu\big(f_{2}^{\minus{1}}(\minus\infty,u)\big)
            \end{equation}
            Again, evaluating case by case, we get:
            \begin{equation}
                F_{2}(u)=
                \begin{cases}
                    \mu(\emptyset)=0,&u\leq{0}\\
                    \mu\big((1-u,1]\big)]u,0<u<1\\
                    \mu([0,1])=1,1\leq{u}
                \end{cases}
            \end{equation}
            Thus, $F_{1}=F_{2}$.
        \end{example}
        \begin{ldefinition}{Random Vector}
            A random vector on a probability space
            $(\Omega,\mathcal{A},\mu)$ is an
            $\mathcal{A}-\mathcal{B}_{n}$ measurable function
            $\mathbf{f}:\Omega\rightarrow\mathbb{R}^{n}$, where
            $\mathcal{B}_{n}$ is the Borel $\sigma\textrm{-Algebra}$
            on $\mathbb{R}^{n}$.
        \end{ldefinition}
        As a comment, if $f:\Omega\rightarrow\mathbb{R}$ is
        $\mathcal{A}-\mathcal{B}$ measurable, then
        $\mathcal{A}_{f}\subseteq\mathcal{A}$. The associated
        $\sigma\textrm{-Algebra}$ of a random vector
        $\mathbf{f}:\Omega\rightarrow\mathbb{R}^{n}$ is:
        \begin{equation}
            A_{\mathbf{f}}
            =\{\mathbf{f}^{\minus{1}}(B):B\in\mathcal{B}_{n}\}
        \end{equation}
        \begin{theorem}
            If $(\Omega,\mathcal{A},\mu)$ is a probability space,
            $\mathcal{B}_{n}$ is the Borel $\sigma\textrm{-Algebra}$
            on $\mathbb{R}^{n}$, and if
            $\mathbf{f}:\Omega\rightarrow\mathbb{R}^{n}$ is a random
            vector such that:
            \begin{equation}
                \mathbf{f}(\omega)=(f_{1}(\omega),\dots,f_{n}(\omega))
            \end{equation}
            Then:
            \begin{equation}
                \mathcal{A}_{\mathbf{f}}=
                \sigma\big(
                    \mathcal{A}_{f_{1}},\dots,\mathcal{A}_{f_{n}}\big)
            \end{equation}
            Where this is the $\sigma\textrm{-Algebra}$ generated by
            these sets.
        \end{theorem}
        \begin{proof}
            For any $f_{j}$,
            $\mathcal{A}_{f_{j}}\subseteq\mathcal{A}_{\mathbf{f}}$,
            and thus the generated $\sigma\textrm{-Algebra}$ is
            contained in $\mathcal{A}_{\mathbf{f}}$. Going the other
            ways, let $\tilde{\mathcal{B}}$ be the set of subsets
            $B\subseteq\mathbb{R}^{n}$ such that:
            \begin{equation}
                \mathbf{f}^{\minus{1}}(B)\in
                \sigma\big(
                    \mathcal{A}_{f_{1}},\dots,\mathcal{A}_{f_{n}}\big)
            \end{equation}
            But then for any sequence $B_{1},\dots,B_{n}\in\mathcal{B}$,
            $B_{1}\times\cdots\times{B}_{n}$ is contained in
            $\tilde{\mathcal{B}}$. But $\mathcal{B}_{n}$ is the
            smallest such $\sigma\textrm{-Algebra}$ to contain such
            sets, and thus
            $\mathcal{B}_{n}\subseteq\tilde{\mathcal{B}}$.
        \end{proof}
        \begin{ldefinition}{Distribution of a Random Vector}
            The distribution of a random vector
            $\mathbf{f}:\Omega\rightarrow\mathbb{R}^{n}$ on a
            measure space $(\Omega,\mathcal{A},\mu)$ is the measure:
            \begin{equation}
                \mu_{\mathbf{f}}(B)=
                \mu(\mathbf{f}^{\minus{1}}(B))
            \end{equation}
            Which is the joint distribution of
            $f_{1},\dots,f_{n}$, where:
            \begin{equation}
                \mathbf{f}(\omega)=(f_{1}(\omega),\dots,f_{n}(\omega))
            \end{equation}
        \end{ldefinition}
        The individual distributions can be computed in terms of the
        joint distribution. This is because:
        \begin{equation}
            \mu_{f_{1}}(B)=
            \mu(f_{1}^{\minus{1}}(B))=
            \mu\big(\mathbf{f}^{\minus{1}}(
                B\times\mathbb{R}^{n-1})\big)=
            \mu_{\mathbf{f}}\big(B\times\mathbb{R}^{n-1}\big)
        \end{equation}
        The joint distribution can not, in general, be computed
        in terms of the individual distributions. There is a special
        exception to this rule, and that is when the random variables
        are independent. That is, if the associated
        $\sigma\textrm{-Algebras}$ are independent. So events that
        bear on $f_{1},\dots,f_{n}$ are independent. If
        $E_{j}\in\mathcal{A}_{f_{j}}$, then:
        \begin{equation}
            \mu\Big(\bigcap_{k=1}^{n}E_{k}\Big)=
            \prod_{k=1}^{n}\mu(E_{k})
        \end{equation}
        \begin{theorem}
            A sequence of random variables $f_{1},\dots,f_{n}$ are
            independent if and only if the joint distribution is
            the product measure of the individual distributions.
        \end{theorem}
        \begin{proof}
            For let $B_{k}\in\mathcal{B}$ and let:
            \begin{equation}
                E_{k}=f_{k}^{\minus{1}}(B_{k})
            \end{equation}
            But then:
            \begin{subequations}
                \begin{align}
                    \mu\Big(\bigcap_{k=1}^{n}E_{k}\Big)&=
                    \mu\Big(\bigcap_{k=1}^{n}
                        f_{k}^{\minus{1}}(B_{k})\Big)\\
                    &=\mu\big(\mathbf{f}^{\minus{1}}
                        (B_{1}\times\dots\times{B}_{n})\big)\\
                    &=\mu_{\mathbf{f}}(B_{1}\times\dots\times{B}_{n})\\
                    &=\prod_{k=1}^{n}\mu(E_{n})\\
                    &=\prod_{k=1}^{n}\mu\big(f^{\minus{1}}(B_{k})\big)\\
                    &=\prod_{k=1}^{n}\mu_{f_{k}}(B_{k})
                \end{align}
            \end{subequations}
        \end{proof}
        Let $\mu_{1},\dots,\mu_{n}$ be probability Lebesgue-stieljes
        measures on $\mathbb{R}$, and let $\mu$ be the product
        measure. Consider the probability space
        $(\mathbb{R}^{n},\mathcal{B}_{n},\mu)$ and the projection
        mappings $\pi_{k}:\mathbb{R}^{n}\rightarrow\mathbb{R}$:
        \begin{equation}
            \pi_{k}(\omega_{1},\dots,\omega_{n})=\omega_{k}
        \end{equation}
        \begin{theorem}
            Let $f_{n}$ be an infinite sequence of random variables
            on a probability space $(\Omega,\mathcal{A},\mu)$. Let
            $\mathcal{A}_{f_{n}}$ be the associated
            $\sigma\textrm{-Algebras}$. For every $\omega\in\Omega$,
            let:
            \begin{equation}
                F_{\inf}(\Omega)=
                \underset{n\rightarrow\infty}{\underline{\lim}}
                f_{n}(\omega)
                \quad\quad
                F_{\sup}(\Omega)=
                \underset{n\rightarrow\infty}{\overline{\lim}}
                f_{n}(\omega)
            \end{equation}
            Then $F_{\inf}$ and $F_{\sup}$ are measurable with
            respect to the terminal $\sigma\textrm{-Algebra}$.
        \end{theorem}
        \begin{proof}
            For $F_{\inf}$ is measurable if and only if for all
            $u\in\mathbb{R}$, we have
            $F^{\minus{1}}\big((\minus\infty,u)\big)\in\mathcal{F}$.
            But:
            \begin{subequations}
                \begin{align}
                    F^{\minus{1}}\big((\minus\infty,u)\big)
                    &=\{\omega:F(\omega)\leq{u}\}\\
                    &=\{\omega:\underline{\lim}f_{n}(\omega)\leq{u}\}\\
                    &=\{\omega:\underset{n}{\sup}
                        \underset{k\geq{n}}{\lim}f_{k}(\omega)\}\\
                    &=\bigcap_{n=1}^{\infty}\Big\{\omega:
                        \underset{n\geq{k}}{\inf}f_{k}(\omega)\leq{u}
                    \Big\}\\
                    &=\bigcap_{n=N}^{\infty}\Big\{\omega:
                        \underset{n\geq{k}}{\inf}f_{k}(\omega)\leq{u}
                    \Big\}
                \end{align}
            \end{subequations}
        \end{proof}
        \begin{theorem}
            If $\mathcal{F}$ is a self-independent
            $\sigma\textrm{-Algebra}$, if $F$ is measurable with
            respect to $\mathcal{F}$, then $F$ is constant almost
            everywhere.
        \end{theorem}
        \begin{proof}
            For since $\mathcal{F}$ is self independent:
            \begin{equation}
                \mu(\{\omega:F(\omega)<u\})=0
                \quad\textrm{or}\quad
                \mu(\{\omega:F(\omega)<u\})=1
            \end{equation}
            Define $A$ and $B$ as follows:
            \begin{align}
                A&=\{u\in\mathbb{F}:\mu(\{\omega:F(\omega)<u\})=0\}\\
                B&=\{u\in\mathbb{F}:\mu(\{\omega:F(\omega)<u\})=1\}\\
            \end{align}
            This separates the real line into two parts. By
            Dedekind's Axiom there is a $c\in\mathbb{R}$ such that,
            for all $a\in{A}$, and for all $b\in{B}$,
            $a\leq{c}\leq{b}$. But then:
            \begin{equation}
                \mu(\{u:F(u)<c+\frac{1}{n}\})=1
            \end{equation}
            From continuity from above, we're done.
        \end{proof}
        \begin{theorem}
            If $(\Omega,\mathcal{A},\mu)$ is a probability space,
            $f_{n}$ is a sequence of independent random variables,
            then the limit inferior and the limit superior are
            constants $\mu$ almost everywhere.
        \end{theorem}
        \begin{proof}
            For the limit inferior and limit superior are measurable
            with respect to the terminal $\sigma\textrm{-Algebra}$.
            By the Kolmogorov zero-one law, $\mathcal{F}$ is
            self-independent if $\mathcal{A}_{f_{n}}$ are independent.
            Thus, by the previous theorem, these functions are constants
            almost everywhere.
        \end{proof}
        Thus the limit of random-variables is entirely not random, but
        constant functions.
        \begin{theorem}
            If $f_{n}$ is a sequence of random variables, then the
            limit of $f_{n}$ almost surely exists, or almost never
            exists.
        \end{theorem}
        \begin{proof}
            For since the limit superior and limit inferior are
            constants almost everywhere, then eithere they agree,
            in which there's convergence almost surely, or they do
            not agree, in which there's convergence almost never.
        \end{proof}
        \begin{ldefinition}{Expectation Value}
            The expectation value of a summable random variable
            $f:\Omega\rightarrow\mathbb{R}$ on a measure space
            $(\Omega,\mathcal{A},\mu)$ is the real number
            $E(f)$ defined by:
            \begin{equation}
                E(f)=\int_{\Omega}f\diff{\mu}
            \end{equation}
        \end{ldefinition}
        The expectation can be expressed in terms of the distribution
        by using the measure transformation theorem. If
        $g:\mathbb{R}\rightarrow\mathbb{R}$ is a real valued function,
        then:
        \begin{equation}
            \int_{\Omega}g\diff{\mu}=
            \int_{\mathbb{R}}g\circ{f}\diff{\mu_{f}}
        \end{equation}
        Now we apply this in the simple case when $g(u)=u$. Then:
        \begin{equation}
            E(f)=\int_{\Omega}f\diff{\mu}
            =\int_\mathbb{R}u\diff{\mu_{f}}
        \end{equation}
        Where we assume that $f$ is summable against $\mu$. Thus,
        $u$ is summable against $\mu_{f}$. So, we have that:
        \begin{equation}
            \int_{\mathbb{R}}|u|\diff{\mu_{f}}<\infty
        \end{equation}
        \begin{ldefinition}{Variance}
            The variance of a random variable
            $f:\Omega\rightarrow\mathbb{R}$ on a measure space
            $(\Omega,\mathcal{A},\mu)$, is the real number
            $Var(f)$ defined by:
            \begin{equation}
                Var(f)=E\big(f-E(f)\big)^{2}
                =\int_{\Omega}\big(f-E(f)\big)^{2}\diff{\mu}
            \end{equation}
        \end{ldefinition}
        \begin{theorem}
            \begin{equation}
                Var(f)=E(f^{2})-E(f)^{2}
            \end{equation}
        \end{theorem}
    \section{Lecture 8-ish Maybe}
        If $(\Omega,\mathcal{A},\mu)$ is a measure space,
        $f:\Omega\rightarrow\mathbb{R}$ is a Borel measurable
        function, then the expectation is:
        \begin{equation}
            E(f)=\int_{\Omega}f\diff{\mu}
        \end{equation}
        The functions $f_{1},\dots,f_{n}$ are independent if
        the associated $\sigma\textrm{-Algebras}$ are independent,
        $\mathcal{A}_{f_{1}}.\dots,\mathcal{A}_{f_{n}}$, where
        the associated $\sigma\textrm{-Algebra}$ is defined
        as:
        \begin{equation}
            \mathcal{A}_{f}=\{f^{\minus{1}}(B):B\in\mathcal{B}\}
        \end{equation}
        Where $\mathcal{B}$ is the Borel
        $\sigma\textrm{-Algebra}$. A random vector is a function
        $\mathbf{f}:\Omega\rightarrow\mathbb{R}^{n}$. The
        distribution of $\mathbf{f}$ is defined as:
        \begin{equation}
            \mu_{\mathbf{f}}(B)=
                \mu\big(\mathbf{f}^{\minus{1}}(B)\big)
        \end{equation}
        This is also called the joint distribution. We then proved
        that $f_{1},\dots,f_{n}$ are independent if and only
        if the joint distribution is the product of the
        individual distributions.
        \begin{theorem}
            If $(\Omega,\mathcal{A},\mu)$ is a probabilty space,
            and if $f_{1},\dots,f_{n}$ are independent functions,
            then:
            \begin{equation}
                E\Big(\prod_{k}f_{k}\Big)
                =\prod_{k}E(f_{k})
            \end{equation}
        \end{theorem}
        \begin{proof}
            For define $g:\Omega\rightarrow\mathbb{R}$ by:
            \begin{equation}
                g(\omega)=\prod_{k=1}^{n}f_{k}(\omega)
            \end{equation}
            Let $\mathbf{f}:\Omega\rightarrow\mathbb{R}^{n}$ be
            defined by:
            \begin{equation}
                \mathbf{f}(\omega)=
                \big(f_{1}(\omega),\dots,f_{n}(\omega)\big)
            \end{equation}
            Then using the measure transformation, we have:
            \begin{align}
                \int_{\Omega}\prod_{k=1}^{n}f_{k}\diff{\mu}
                &=\int_{\Omega}g\big(\mathbf{f}(\omega)\big)
                    \diff{\mu}\\
                &\int_{\mathbb{R}^{n}}g(u_{1},\dots,u_{n})
                    \mu_{\mathbf{f}}\\
                &=\int_{\mathbb{R}^{n}}\prod_{k=1}^{n}u_{k}
                    \mu_{\mathbf{f}}
            \end{align}
        \end{proof}
        Suppose $n=2$. Then, since $f_{1}$ and $f_{2}$ are
        independent, $\mu_{(f_{1},f_{2})}$ is the product of
        the measures $\mu_{f_{1}}$ and $\mu_{f_{2}}$. Thus
        by Fubini's theorem:
        \begin{equation}
            \int_{\mathbb{R}^{2}}u_{1}u_{2}\mu_{(f_{1},f_{2})}
            =\int_{\mathbb{R}}\Big(
                \int_{\mathbb{R}}u_{1}u_{2}\mu_{f_{2}}\Big)
                    \mu_{f_{1}}
            =\int_{\mathbb{R}}u_{1}\Big(
                \int_{\mathbb{R}}u_{1}\mu_{f_{1}}\Big)
            =\int_{\mathbb{R}}u\mu_{f_{1}}
                \int_{\mathbb{R}}u_{2}\mu_{f_{2}}
            =\int_{\Omega}f_{1}\mu\int_{\Omega}f_{2}\mu
        \end{equation}
        From a course in integral calculus, one should be very
        surprised by this result, for it says that if
        $f_{1},\dots,f_{n}$ are independent, then:
        \begin{equation}
            \int_{\Omega}\prod_{k=1}^{n}f_{k}\diff{\mu}=
            \prod_{k=1}^{n}\int_{\Omega}f_{k}\diff{\mu}
        \end{equation}
        This is almost never true for a given set of functions,
        but if they are independent then the result holds.
        \subsection{Covariance}
            The covariance of $f_{1}$ and $f_{2}$ is:
            \begin{equation}
                E\Big(\big(f_{1}-E(f_{1})\big)
                    \big(f_{2}-E(f_{2})\big)\Big)
                =\int_{\Omega}\big(f_{1}-E(f_{1})\big)
                    \big(f_{2}-E(f_{2})\big)\diff{\mu}
            \end{equation}
            We can simplify this down to:
            \begin{equation}
                E(f_{1}f_{2})-E(f_{1})E(f_{2})
            \end{equation}
            If $f_{1}$ and $f_{2}$ are independent, then:
            \begin{equation}
                Cov(f_{1},f_{2})=0
            \end{equation}
            The converse is not true. It does not imply that
            $f_{1}$ and $f_{2}$ are independent.
            For let:
            \begin{equation}
                \Omega=\{1,2,3\}
            \end{equation}
            Let $\mathcal{A}=\mathcal{P}(\Omega)$ and let
            $\mu$ be the counting measure on $\Omega$. That is:
            \begin{equation}
                \mu(A)=\frac{\Card(A)}{3}
            \end{equation}
            Then $(\Omega,\mathcal{A},\mu)$ is a probability
            measure. Define $f_{1}$ and $f_{2}$ as follows:
            \begin{align}
                f_{1}(\omega)&=
                \begin{cases}
                    1,&\omega=1\\
                    0,&\omega=0\\
                    1&\omega=2
                \end{cases}\\
                f_{1}(\omega)&=
                \begin{cases}
                    1,&\omega=1\\
                    0,&\omega=0\\
                    \minus{1}&\omega=2
                \end{cases}
            \end{align}
            Then we compute and get:
            \begin{equation}
                E(f_{1})=\int_{\Omega}f_{1}\diff{\mu}=0
            \end{equation}
            And also:
            \begin{equation}
                E(f_{2})=\frac{2}{3}
            \end{equation}
            But if we multiply, we see that
            $f_{1}f_{2}=f_{1}$, and therefore:
            \begin{equation}
                E(f_{1}f_{2})=E(f_{1})=0
            \end{equation}
            But then:
            \begin{equation}
                E(f_{1}f_{2})-E(f_{1})E(f_{2})=0
            \end{equation}
            And thus $f_{1}$ and $f_{2}$ are uncorrelated.
            But they are dependent. We may expect this since
            $f_{2}=f_{1}^{2}$. Let's compute the associated
            $\sigma\textrm{-Algebras}$. We have:
            \begin{align}
                f_{1}^{\minus{1}}(\{1\})
                &=\{1\}\\
                f_{2}^{\minus{1}}&=\{1,3\}
            \end{align}
            But then:
            \begin{align}
                \mu(f_{1}^{\minus{1}}(\{1\})
                &=\frac{1}{3}\\
                \mu(f_{2}^{\minus{1}}(\{1\})&=\frac{2}{3}
            \end{align}
            But the product measure is:
            \begin{equation}
                \mu_{(f_{1},f_{2})}(\{1\})=\frac{1}{3}
            \end{equation}
            And this is not the product of the two measure, and
            therefore it they are not independent.
            If $Cov(f_{1},f_{2})=0$, we say that $f_{1}$ and
            $f_{2}$ are uncorrelated.
            \begin{theorem}
                If $f_{1},\dots,f_{n}$ are random variables
                that are pairwise uncorrelated, then:
                \begin{equation}
                    Var(\sum_{k=1}^{n}f_{k})=
                    \sum_{k=1}^{n}Var(f_{k})
                \end{equation}
            \end{theorem}
            \begin{proof}
                For:
                \begin{equation}
                    \int_{\Omega}
                        \Big(\sum_{k=1}^{n}f_{k}-
                            E(\sum_{k=1}^{n}f_{k}\Big)\diff{\mu}
                    =\sum_{i,j}\int_{\Omega}
                        (f_{i}-E(f_{i}))(f_{j}-E(f_{j}))\diff{\mu}
                \end{equation}
                But the $f_{i}$ are pairwise uncorrelated, and
                thus this product is zero if $i\ne{j}$. Thus, we
                get:
                \begin{equation}
                    \int_{\Omega}
                        \Big(\sum_{k=1}^{n}f_{k}-
                            E(\sum_{k=1}^{n}f_{k}\Big)\diff{\mu}
                    =\sum_{k=1}^{n}Var(f_{k})
                \end{equation}
            \end{proof}
    \section{Laws of Large Numbers}
        Consider a fair coin and toss it $n$ times. We would
        expect that, as $n$ gets large, the number of times
        heads occurs and the number of times heads occurs is
        roughly the same. That is:
        \begin{equation}
            \frac{|\textrm{Heads}|-|\textrm{Tails}|}{n^{2}}
            \rightarrow{0}
        \end{equation}
        And also:
        \begin{equation}
            \frac{|\textrm{Heads}|\times|\textrm{Tails}|}{n}
            \rightarrow\frac{1}{2}
        \end{equation}
        We want to build a more rigorous notion from this idea
        and create a mathematical model out of this. We use
        probability spaces as this model. Let
        $(\Omega,\mathcal{A},\mu)$ be a probability space and
        let $f_{j}:\Omega\rightarrow\mathbb{R}$ be random variables
        take on the values $\minus{1}$ and $1$, and such that
        they are independent. Then the associated
        $\sigma\textrm{-Algebra}$ are:
        \begin{equation}
            \mathcal{A}_{f_{j}}=
            \{\emptyset,f^{\minus{1}}(\{\minus{1}\}),
                f^{\minus{1}}(\{1\}),\Omega\}
        \end{equation}
        The measure on the space is such that:
        \begin{equation}
            \mu\Big(f^{\minus{1}}\big(\{\minus{1}\}\big)\Big)=
            \mu\Big(f^{\minus{1}}\big(\{1\}\big)\Big)=
            \frac{1}{2}
        \end{equation}
        Define a new function by:
        \begin{equation}
            F_{N}(\omega)=\frac{1}{N}\sum_{k=1}^{N}f_{k}(\omega)
        \end{equation}
        Then $F_{N}(\omega)$ is the number of times 1 occurs
        minus the number of times -1 occurs, divided by $N$.
        It seems likely that this function should converge to
        zero for large $N$. Recall that there are three different
        types of convergence. We say
        $g_{n}\rightarrow{g}$ almost everywhere if there is a
        set of measure 0 such that $g_{n}\rightarrow{g}$ on the
        complement of this set. We say that
        $g_{n}\rightarrow{g}$ almost uniformly if there is a set
        of arbitrarily small measure $\varepsilon$ such that
        $g_{n}\rightarrow{g}$ uniformly on the complement.
        Finally, $g_{n}\rightarrow{g}$ in measure if for all
        $\delta>0$:
        \begin{equation}
            \mu\Big(\{\omega:|g_{n}(\omega)-g(\omega)|\geq\delta\}
            \Big)\rightarrow{0}
        \end{equation}
        We have seen the almost uniform convergence is the
        strongest and implies the other two. By Egorov, since
        $\mu(\Omega)=1$ in a probability space,
        convergence almost everywhere implies convergence almost
        uniformly. Lastly, convergence in measure implies there
        is a subsequence that converges almost uniformly.
        \begin{ldefinition}{Strong Law of Large Numbers}
            A sequence that obeys the Strong Law of Large Numbers
            in a probability space $(\Omega,\mathcal{A},\mu)$
            is a sequence $f_{n}$ such that:
            \begin{equation}
                \frac{1}{N}\sum_{n=1}^{N}
                    \Big[f_{n}(\omega)-E(f_{n})\Big]\rightarrow{0}
            \end{equation}
            $\mu$ almost everywhere.
        \end{ldefinition}
        \begin{ldefinition}{Weak Law of Large Numbers}
            A sequence that obeys the Weak Law of Large Numbers
            in a probability space $(\Omega,\mathcal{A},\mu)$
            is a sequence $f_{n}$ such that:
            \begin{equation}
                \frac{1}{N}\sum_{n=1}^{N}
                    \Big[f_{n}(\omega)-E(f_{n})\Big]\rightarrow{0}
            \end{equation}
            Where the convergence is in measure.
        \end{ldefinition}
        \begin{ltheorem}{Khinchin's Weak Law of Large Numbers}
            If $(\Omega,\mathcal{A},\mu)$ is a probability space,
            if $f_{j}$ is a sequence of random variables that
            are pair-wise uncorrelated such that:
            \begin{equation}
                \frac{1}{n^{2}}\sum_{j=1}^{n}Var(f_{k})
                \rightarrow{0}
            \end{equation}
            Then $f_{j}$ obeys the Weak Law of Large Numbers.
        \end{ltheorem}
        \begin{proof}
            For:
            \begin{equation}
                \int_{\Omega}\Big(\frac{1}{n}\sum_{k=1}^{n}\big(
                    f_{k}(\omega)-E(f_{k})\big)\Big)^{2}\diff{\mu}
                =\frac{1}{n^{2}}\sum_{k=1}^{n}Var(f_{k})
            \end{equation}
            Let:
            \begin{equation}
                \Omega_{\delta,n}=
                \{\omega:|\frac{1}{n}\sum_{k=1}^{n}
                    \big(f_{k}(\omega)-E(f_{k})\big)|\geq\delta\}
            \end{equation}
            But by the Chebyshev inequality, we have:
            \begin{equation}
                \int_{\Omega}\Big(\frac{1}{n}\sum_{k=1}^{n}\big(
                    f_{k}(\omega)-E(f_{k})\big)\Big)^{2}\diff{\mu}
                \geq\int_{\Omega_{\delta}}
                    \Big(\frac{1}{n}\sum_{k=1}^{n}\big(
                    f_{k}(\omega)-E(f_{k})\big)\Big)^{2}\diff{\mu}
                \geq\delta^{2}\int_{\Omega_{\delta}}\diff{\mu}
            \end{equation}
            But then:
            \begin{equation}
                \mu(\Omega_{\delta,n})\leq
                \frac{1}{\delta^{2}}\frac{1}{n}^{2}
                \sum_{j=1}^{n}V(f_{k})
            \end{equation}
            But this last part tends to zero. Therefore, etc.
        \end{proof}
        \begin{lexample}
            If all of the $f_{i}$ have the same distribution, or
            if they are uniformly bounded, then the theorem applies.
            This can be used to show that our model for a fair
            coin toss obeys the weak law of large numbers.
        \end{lexample}
        Suppose $g_{n}(\omega)\rightarrow{g}(\omega)$ almost
        everywhere. Then, for all $\delta>0$ there is an
        $N$ such that, for all $n>N$, we have:
        \begin{equation}
            |g_{n}(\omega)-g(\omega)|<k^{\minus{1}}
        \end{equation}
        For some $k$. Consider the negation of this claim. Then
        there exists $k\in\mathbb{N}$ such that, for all
        $N\in\mathbb{N}$ there is an $n>N$ such that:
        \begin{equation}
            |g_{n}(\omega)-g(\omega)|\geq{k}^{\minus{1}}
        \end{equation}
        COnsider the following set:
        \begin{equation}
            B=\bigcup_{n=1}^{\infty}\bigcap_{N=1}^{\infty}
                \bigcup_{k=N}^{\infty}\big\{\omega:
                |g_{n}(\omega)-g(\omega)|\geq{k}^{\minus{1}}\big\}
        \end{equation}
        This is the set of $\omega$ such that
        $g_{n}(\omega)\not\rightarrow{g}(\omega)$. We wish to show
        that $\mu(B)=0$. This will happen if and only if for all
        $k\in\mathbb{N}$:
        \begin{equation}
            \mu\Big(\bigcap_{N=1}^{\infty}\bigcup_{n=N}^{\infty}
                \big\{\omega:|g_{n}(\omega)-g(\omega)|
                \geq{k}^{\minus{1}}\big\}\Big)=0
        \end{equation}
        Consider a collection of set $A_{n}$ and define:
        \begin{equation}
            \overline{A}=\bigcap_{N=1}^{\infty}
                \bigcup_{n=N}^{\infty}A_{n}
        \end{equation}
        If the $A_{n}$ are independent, then $\overline{A}$ is
        a terminal event, and thus by the Kormogorov zero-one law,
        eighet $\mu(\overline{A})=1$ or $\mu(\overline{A})=0$.
        \begin{theorem}
            If:
            \begin{equation}
                \sum_{n=1}^{\infty}\mu)A_{n})<\infty
            \end{equation}
            Then:
            \begin{equation}
                \mu\Big(\bigcap_{N=1}^{\infty}
                    \bigcup_{N=n}^{\infty}A_{n}\Big)=0
            \end{equation}
        \end{theorem}
        \begin{proof}
            For:
            \begin{equation}
                \mu\Big(\bigcap_{N=1}^{\infty}
                    \bigcup_{N=n}^{\infty}A_{n}\Big)
                \leq\mu\Big(\bigcup_{N=n}^{\infty}A_{n}\Big)
                \leq\sum_{n=N}^{\infty}\mu(A_{n}
            \end{equation}
            But this sum converges, and thus the tail end can
            be made arbitrarily small.
        \end{proof}
        \begin{ltheorem}{Borel-Cantelli Lemma}
            If $A_{n}$ are pair-wise independent and are such
            that:
            \begin{equation}
                \sum_{k=1}^{\infty}\mu(A_{n})=\infty
            \end{equation}
            Then:
            \begin{equation}
                \mu\Big(\bigcap_{N=1}^{\infty}
                    \bigcup_{N=n}^{\infty}A_{n}\Big)=1
            \end{equation}
        \end{ltheorem}
        \begin{proof}
            For if:
            \begin{equation}
                \mu\Big(\bigcup_{N=1}^{\infty}\bigcap_{n=N}^{\infty}
                    A_{n}^{C}\big)=0
            \end{equation}
            Then, for all $N$:
            \begin{equation}
                \mu\Big(\bigcap_{n=N}^{\infty}A_{n}^{C}\Big)=0
            \end{equation}
            So it suffices to show that this is true. For let
            $N\in\mathbb{N}$, and define:
            \begin{equation}
                B=\bigcap_{n=N}^{\infty}A_{n}^{C}
            \end{equation}
            Also define:
            \begin{equation}
                B_{M}=\bigcap_{n=N}^{M}A_{n}^{C}
            \end{equation}
            It then follows from continuity from below that:
            \begin{equation}
                \mu(B)=\underset{M\rightarrow\infty}{\lim}\mu(B_{M})
                =\underset{M\rightarrow\infty}{\lim}
                    \mu\Big(\bigcap_{n=N}^{M}A_{n}^{C}\Big)
            \end{equation}
            But from independence, we obtain:
            \begin{equation}
                \mu(B)=\underset{M\rightarrow\infty}{\lim}
                    \prod_{n=N}^{M}\mu\big(A_{n}^{C}\big)
                =\underset{M\rightarrow\infty}{\lim}
                    \prod_{n=N}^{M}\mu\big(1-A_{n}\big)
            \end{equation}
            Using the exponential function, we note that
            $1-x\leq\exp(\minus{x})$, and so:
            \begin{equation}
                \mu(B)\leq               
                \underset{M\rightarrow\infty}{\lim}
                    \prod_{n=N}^{M}\exp\big(\minus\mu(A_{n})\big)
                =\underset{M\rightarrow\infty}{\lim}
                    \exp\Big(\sum_{n=N}^{M}\mu(A_{n})\Big)=0
            \end{equation}
        \end{proof}
        The independence of the $A_{n}$ is indeed necessary for
        this theorem. For let $A_{n}=A_{0}$, and let
        $\mu(A_{0})=\frac{1}{2}$. Then the sum will indeed
        diverge, but the measure of final set is still
        $\frac{1}{2}$.
        The Borel-Cantelli lemma thus complements the
        Kormogrov Zero-One law by giving the precise criterion for
        when the measure is either one or zero. Given a sequence
        of random events, the terminal event has measure one
        if and only if the sum of the individual measures converges,
        and is equal to one otherwise.
        \begin{ltheorem}{Borel's Strong Law of Large Numbers}
            If $f_{n}$ is a sequence of random variables such that:
            \begin{equation}
                \int_{\Omega}|f_{n}|^{4}\diff{\mu}\leq{M}
            \end{equation}
            For all $n\in\mathbb{N}$, then $f_{n}$ obeys the
            strong law of large numbers.
        \end{ltheorem}
        \begin{proof}
            It suffices to show that, for all $\varepsilon>0$:
            \begin{equation}
                \mu\Big(\bigcap_{N=1}^{\infty}
                    \bigcup_{N=n}^{\infty}
                    \big\{\omega:|\frac{1}{n}\sum_{k=1}^{n}
                        f_{k}(\omega)|\geq\varepsilon\Big)=0
            \end{equation}
            Denote the sequence of centered random variables by:
            \begin{equation}
                \overset{\circ}{f}_{n}(\omega)=
                f_{n}(\omega)=E(f_{n}(\omega))
            \end{equation}
            To show this, we need to show that:
            \begin{equation}
                \sum_{n=1}^{\infty}\mu\Big(
                    \big\{\omega:|\frac{1}{n}\sum_{k=1}^{n}
                        \overset{\circ}{f}_{n}(\omega)|
                        \geq\varepsilon\big\}\Big)<\infty
            \end{equation}
            Define:
            \begin{equation}
                \Omega_{n,\varepsilon}=
                \Big\{\omega:\big|\frac{1}{n}\sum_{k=1}^{n}
                    \overset{\circ}{f}_{n}(\omega)\big|
                    \geq\varepsilon\Big\}
            \end{equation}
            But then:
            \begin{equation}
                \int_{\Omega}\big|\frac{1}{n}\sum_{k=1}^{n}
                    \overset{\circ}{f}_{n}\big|^{4}\diff{\mu}
                \geq\int_{\Omega_{n,\varepsilon}}
                \big|\frac{1}{n}\sum_{k=1}^{n}
                    \overset{\circ}{f}_{n}\big|^{4}\diff{\mu}
                \geq\varepsilon^{4}
                    \mu\big(\Omega_{\varepsilon,n}\big)
            \end{equation}
            Combining this together, we have:
            \begin{equation}
                \mu\big(\Omega_{\varepsilon,n}\big)
                \leq\frac{1}{\varepsilon^{4}}\frac{1}{n^{4}}
                \int_{\Omega}\Big(\sum_{k=1}^{n}
                \overset{\circ}{f}_{k}(\omega)\Big)^{4}\diff{\mu}
                =\frac{1}{\varepsilon^{4}}\frac{1}{n^{4}}
                \sum_{i,j,k,\ell}\int_{\Omega}
                \overset{\circ}{f}_{i}\overset{\circ}{f}_{j}
                \overset{\circ}{f}_{k}\overset{\circ}{f}_{\ell}
                \diff{\mu}
            \end{equation}
            But the $f_{n}$ are independent, and thus the
            $\overset{\circ}{f}_{n}$ are independent. But then
            $\mathcal{A}_{\overset{\circ}{f}_{n}}$ are independent,
            and thus $\overset{\circ}{f_{i}}$ is independent
            from the product
            $\overset{\circ}{f}_{j}\overset{\circ}{f}_{k}\overset{\circ}{f}_{\ell}$. But if they are independent, then:
            \begin{equation}
                \int_{\Omega}
                \overset{\circ}{f}_{i}\overset{\circ}{f}_{j}
                \overset{\circ}{f}_{k}\overset{\circ}{f}_{\ell}
                \diff{\mu}=
                \int_{\Omega}
                \overset{\circ}{f}_{i}\diff{\mu}
                \int_{\Omega}\overset{\circ}{f}_{j}
                \overset{\circ}{f}_{k}\overset{\circ}{f}_{\ell}
                \diff{\mu}=0
            \end{equation}
            There are two cases left, when the indices are equal
            in pairs, and when all of the indices are equal. In
            the cases where all are equal, we have:
            \begin{equation}
                \sum_{i=1}^{n}
                \int_{\Omega}
                |\overset{\circ}{f}_{i}|^{4}\diff{\mu}
                \leq{M}n
            \end{equation}
            For the case of pairs, we have $n^{2}-n$ possibilities,
            and thus:
            \begin{equation}
                \sum_{i,j}\int_{\Omega}
                |\overset{\circ}{f}_{i}^{2}
                \overset{\circ}{f}_{j}^{2}|\diff{\mu}
                \leq{M}(n^{2}-n)
            \end{equation}
            Therefore, we have:
            \begin{equation}
                \frac{1}{\varepsilon^{4}}\frac{1}{n^{4}}
                \sum_{i,j,k,\ell}\int_{\Omega}
                \overset{\circ}{f}_{i}\overset{\circ}{f}_{j}
                \overset{\circ}{f}_{k}\overset{\circ}{f}_{\ell}
                \diff{\mu}
                \leq\frac{M}{\varepsilon^{4}}
                \frac{1}{n^{2}}
            \end{equation}
            But:
            \begin{equation}
                \sum_{n=1}^{\infty}\frac{1}{n^{2}}
                =\frac{\pi^{2}}{6}<\infty
            \end{equation}
            Thus, the measure is zero.
        \end{proof}
        The $|f_{n}|^{4}$ are called the fourth moments of the
        $f_{n}$. There are sequences that obey the weak law but
        not the strong law. Borel's theorem shows that uniformly
        bounded sequences of random variables automatically obey
        the strong law of strong numbers, since a uniformly
        bounded sequence will have uniformly bounded fourth
        moments. To find a sequence that obeys the weak law but
        not the strong law, we will need to consider sequences
        that take on arbitrarily large values.
        \begin{lexample}
            Let $f_{n}$ be a sequence of random variables such that
            the following are true:
            \begin{subequations}
                \begin{align}
                    \mu\Big(\{\omega:f_{n}(\omega)=n\}\Big)
                    &=\frac{P_{n}}{2}\\
                    \mu\Big(\{\omega:f_{n}(\omega)=\minus{n}\}\Big)
                    &=\frac{P_{n}}{2}\\
                    \mu\Big(\{\omega:f_{n}(\omega)=0\}\Big)
                    &=1-P_{n}
                \end{align}
            \end{subequations}
            We need to find a sequence $P_{n}$ such that the
            $f_{n}$ will obey the weak law but not the strong law.
            Choosing the $P_{n}$ to be small will most likely
            result in the sequence obeying the strong law. Indeed,
            if $P_{n}=0$, then the $f_{n}$ will obey the strong
            law. In fact, if:
            \begin{equation}
                P_{n}\leq\frac{1}{n^{2}}
            \end{equation}
            Then $f_{n}$ will obey the strong law. This is a
            consequence of the Borel-Cantelli lemma. If $P_{n}$
            is to large, it may not be true that the $f_{n}$ obeys
            the weak law. For example, suppose:
            \begin{equation}
                P_{n}=\frac{1}{n}
            \end{equation}
            We cannot apply Khinchin's theorem, since:
            \begin{equation}
                \frac{1}{n^{2}}\sum_{j=1}^{n}V(f_{j})=
                \frac{n(n+1)}{2n^{2}}
            \end{equation}
            And this does not converge to zero. Let:
            \begin{equation}
                P_{n}=\frac{1}{n\ln(n+2)}
            \end{equation}
            Let's now show that $f_{n}$ will obey the weak law.
            It does. But it does not obey the strong law. We will
            need to use the Borel-Cantelli lemma. But the sum:
            \begin{equation}
                \sum_{n=1}^{\infty}\frac{1}{n\ln(n+2)}=\infty
            \end{equation}
            Therefore:
            \begin{equation}
                \mu\Big(\bigcap_{N=1}^{\infty}\bigcup_{n=N}^{\infty}
                    \{\omega:|f_{n}(\omega)|\}\Big)=1
            \end{equation}
            This contradicts the strong law of large numbers. For
            suppose not. Then, for almost every $\omega$, and for
            all $N$, there is an $N>N$ such that
            $|f_{n}(\omega)|=n$. Then thre exists a sequence
            $n_{k}$ such that $|f_{n_{k}}(\omega)|=n_{k}$.
            But:
            \begin{equation}
                \frac{1}{n_{k}}\sum_{j=0}^{n_{k}}f_{j}\rightarrow{0}
            \end{equation}
            And therefore:
            \begin{equation}
                \frac{1}{n_{k}-1}\sum_{j=0}^{n_{k}}f_{j}
                    \rightarrow{0}
            \end{equation}
            Taking the difference, we get:
            \begin{equation}
                \frac{1}{n_{k}}f_{n_{k}}(\omega)\rightarrow{0}
            \end{equation}
            But $|f_{n_{k}}(\omega)|=n_{k}$, a contradiction.
            So the $f_{n}$ do not obey the strong law.
        \end{lexample}
        \subsection{Borel Numbers}
            Let $0\leq{x}\leq{1}$ and suppose $x$ has the
            representation $x=0.x_{1}x_{2}\dots$ and exclude
            numbers with two representations. For example,
            $1=0.999\dots$. The measure of the set of these numbers
            is zero. Let $0\leq{a}\leq{9}$. Let $C_{n}(x)$ be
            the number of $a$ among the first $n$ digits. Then:
            \begin{equation}
                \frac{C_{n}(x)}{n}\rightarrow\frac{1}{10}
            \end{equation}
            For almost every $x$. Let $\Omega=[0,1]$ and
            $\mathcal{B}$ be the Borel $\sigma\textrm{-Algebra}$.
            Also, let $\mu$ be the Lebesgue measure. Consider
            the functions:
            \begin{equation}
                f_{j}(x)=
                \begin{cases}
                    1,&x_{j}=a\\
                    0,&x_{j}\ne{a}
                \end{cases}
            \end{equation}
            Then:
            \begin{equation}
                \frac{C_{n}(x)}{n}=\frac{1}{n}\sum_{k=1}^{n}
                    f_{k}(x)
            \end{equation}
            If the $f_{k}$ obey the strong law of large numbers,
            then:
            \begin{equation}
                \frac{1}{n}\sum_{k=1}^{n}\big(f_{k}-E(f_{k})\big)
                \rightarrow{0}
            \end{equation}
            $\mu$ almost everywhere. We have that $f_{j}$ are
            bounded, and thus it suffices to show that they are
            also independent. Define:
            \begin{equation}
                \mathcal{A}_{f_{i}}=
                \{\emptyset,A_{j},A_{j}^{C},\Omega\}
            \end{equation}
            Where:
            \begin{equation}
                A_{j}=\{x:f_{j}(x)=1\}
            \end{equation}
            THe $A_{j}$ are the set of elements $x$ such that
            $x=0.x_{1}x_{2}\dots{a}x_{j+1}x_{j+2}\dots$ Using this
            there are $10^{j-1}$ options for the first $j-1$
            digits. This set is covered by $10^{j-1}$ intervals,
            each of length $10^{\minus{j}}$. Thus, the Lebesgue
            measure of $A_{j}$ is $\frac{1}{10}$. We now need to
            show that, for distinct $j,k$, that the measure of the
            intersection is $\frac{1}{100}$. Suppose $j<k$. Then
            $A_{j}\cap{A}_{k}$ is the set of numbers with $a$ in
            the $j^{th}$ decimal and $a$ in the $k^{th}$ decimal.
            There are $10^{j-1}$ ways to choose the first
            $j-1$ digits, and $10^{k-j-1}$ ways to choose the
            next $k-j-1$ digits. Total, there are
            $10^{k-2}$ digits to choose. So we can cover this
            set with $10^{k-2}$ intervals, each of lenght
            $10^{\minus{k}}$. Thus, the measure of the intersection
            is $10^{\minus{2}}$. Theefore, the $f_{j}$ are
            independent. By Borel's Strong Law of Large Numbers,
            the $f_{j}$ obey the strong law of large numbers.
            \par\hfill\par
            Let $\Omega=\mathbb{Z}_{n}$, let $\mathcal{A}$ be
            the power set, and let $\mu$ be the counting
            measure on $\Omega$. Taking the product
            $\Omega^{n}$, and considering the product measure,
            we see that every point has measure $10^{\minus{n}}$.
            Thus, if we consider the infinite product, points will
            have measure zero. This isn't too strange since the
            Lebesgue measure is such that points have measure
            zero. Let $\tilde{\Omega}$ be the infinite product
            and let $B_{n}\in\Omega^{n}$. Then
            $B_{n}\times\Omega_{n+1}\times\dots$ is contained in
            $\tilde{\Omega}$. Let $\tilde{\mathcal{B}}$ be the
            smallest $\sigma\textrm{-Algebra}$ on the product
            space that contains all of these types of sets, and let
            $\tilde{\mu}$ be the extension measure. Then
            $f_{j}(\omega)=\omega_{j}$ are independent by
            construction of the product measure, and also:
            \begin{equation}
                \mu(f_{j}=a)=\frac{1}{10}
            \end{equation}
            There is a map $\tilde{\Omega}\mapsto[0,1]$ by sending
            $(\omega_{1},\dots)$ to $0.\omega_{1}\omega_{2}\dots$.
            The image measure of the product measure $\tilde{\mu}$
            is the Lebesgue measure. So we have an equivalent
            model of $[0,1]$ with the Lebesgue measure.
    \section{Central Limit Theorem}
        We now wish to discuss convergence of measures and
        distributions. We restrict ourself to
        Lebesgue-Stieljes measures on the Borel
        $\sigma\textrm{-Algebra}$ of $\mathbb{R}$. We does it
        mean for a sequence of measures $\mu_{n}$ to converge
        to a measure $\mu$? For all $B\in\mathcal{B}$:
        \begin{equation}
            \mu_{n}(B)\rightarrow\mu(B)
        \end{equation}
        This is reminiscent of point-wise convergence of functions
        of a real variable, but turns out to be too much. What if
        we restrict ourselves to sets of the form $[a,b)$? Let:
        \begin{equation}
            f_{n}(x)=
                \begin{cases}
                    0,&|x|\geq\frac{1}{n}\\
                    n(1-|x|),&|x|<\frac{1}{n}
                \end{cases}
        \end{equation}
        And define:
        \begin{equation}
            \mu_{n}([a,b)]=\int_{a}^{b}\rho_{n}(x)\diff{x}
        \end{equation}
        Then by the Caratheodory extension theorem, there is a
        measure $\nu_{n}$ that agrees with $\mu_{n}$ on all
        such intervals. Then $\nu_{n}$ converges to the
        Dirac measure, which is an example of an atomic measure:
        \begin{equation}
            \delta(B)=
                \begin{cases}
                    1,&0\in{B}\\
                    0,&0\notin{B}
                \end{cases}
        \end{equation}
        However:
        \begin{equation}
            \mu_{n}([0,b)]\rightarrow\frac{1}{2}
        \end{equation}
        And:
        \begin{equation}
            \mu_{n}([a,0)]\rightarrow\frac{1}{2}
        \end{equation}
        However:
        \begin{align}
            \delta([0,b))&=1\\
            \delta([a,0))&=0
        \end{align}
        This leads us to the correct definition of measure:
        \begin{ldefinition}{Convergence of Measure}
            A sequence of measure $\nu_{n}$ converges to a
            measure $\nu$ if, for all measure sets $B$ such
            that $\nu(\partial{B})=0$, it is true that
            $\nu_{n}(B)\rightarrow\nu(B)$.
        \end{ldefinition}
        Given:
        \begin{equation}
            \int_{\mathbb{R}}\chi_{[a,b)}\diff{\nu_{n}}
            \rightarrow\int_{\mathbb{R}}\chi_{[a,b)}\diff{\nu}
        \end{equation}
        We have that $\nu(\{a\})=\nu(\{b\})=0$, and thus
        $\chi_{[a,b)}$ is continuous $\nu$ almost everywhere.
        Suppose $\nu_{n}$ and $\nu$ are probability
        Lebesgue-Stieltjes measure on $\mathbb{R}$. Then we
        get the equivalent form:
        \begin{theorem}
            If for every continuous bounded function $g(\omega)$,
            we have that:
            \begin{equation}
                \int_{\mathbb{R}}g\diff{\mu}_{n}\rightarrow
                \int_{\mathbb{R}}g\diff{\mu}
            \end{equation}
            Then $\mu_{n}\rightarrow\mu$.
        \end{theorem}
        \begin{theorem}
            If for every continuous function with bounded support,
            if:
            \begin{equation}
                \int_{\mathbb{R}}g\diff{\mu}_{n}\rightarrow
                \int_{\mathbb{R}}g\diff{\mu}
            \end{equation}
            Then $\nu_{n}\rightarrow\nu$.
        \end{theorem}
        \begin{theorem}
            If:
            \begin{equation}
                \int_{\mathbb{R}}\exp(itu)\diff{\nu_{n}}\rightarrow
                \int_{\mathbb{R}}\exp(itu)\diff{\nu}
            \end{equation}
            Then $\nu_{n}\rightarrow\nu$.
        \end{theorem}
        Suppose $(\Omega,\mathcal{A},\mu)$ is a probability space,
        and suppose $f_{n}:\Omega\rightarrow\mathbb{R}$ is a
        sequence of random variables that are
        $\mathcal{A}-\mathcal{B}$ measure. Consider the
        distributions $\mu_{g_{n}}$. If $g_{n}\rightarrow{g}$ in
        measure, then the distribuctions converge to $\mu_{g}$.
        \begin{theorem}
            If $(\Omega,\mathcal{A},\mu)$ is a probability space,
            if $h_{n}:\Omega\rightarrow\mathbb{R}$ is a
            sequence of random variables, if $\mu_{h_{n}}$ are the
            distributions of $g_{n}$, and if $h_{n}\rightarrow{h}$
            in measure, then $\mu_{h_{n}}\rightarrow\mu_{h}$.
        \end{theorem}
        \begin{proof}
            For let $g$ be a continuous function with compact
            support. Then, applying the measure transformation
            theorem, we have:
            \begin{equation}
                \Big|\int_{\mathbb{R}}g\diff{\mu_{n}}-
                    \int_{\mathbb{R}}\diff{\mu_{h}}\Big|
                =\Big|\int_{\Omega}g(h_{n})\diff{\mu}-
                    \int_{\Omega}g(h)\diff{\mu}\Big|
                \leq\int_{\Omega}|g_{n}(h)-g(h)|\diff{\mu}
            \end{equation}
            But $g$ is continuous on a compact set, and is
            therefore uniformly continuous. Thus, for all
            $\varepsilon>0$ there is a $\delta>0$ such that, for
            all $|u'-u''|<\delta$, we have that
            $|g(u')-g(u'')|<\varepsilon$. Define the following:
            \begin{align}
                E_{1,n,\varepsilon}
                &=\{\omega:|h_{n}(\omega)-h(\omega)|\geq\delta\}\\
                E_{2,n,\varepsilon}
                &=\{\omega:|h_{n}(\omega)-h(\omega)|<\delta\}
            \end{align}
            Then:
            \begin{align}
                \int_{\Omega}|g_{n}(h)-g(h)|\diff{\mu}
                &=\int_{E_{1,n,\varepsilon}}|g_{n}(h)-g(h)|\diff{\mu}
                +\int_{E_{2,n,\varepsilon}}
                    |g_{n}(h)-g(h)|\diff{\mu}\\
                &\leq{2}M\mu(E_{1,n,\varepsilon})+
                    \varepsilon\mu(E_{2,n,\varepsilon})
            \end{align}
            And this converges to $\varepsilon$.
        \end{proof}
        The converse of this theorem is not true in general,
        since vastly different functions can have the same
        distributions. There is a special case, however, where the
        converse holds. Consider a function $h$ such that it's
        distribution is the Dirac distribution. That is:
        \begin{equation}
            \mu(\{\omega:h(\omega)=a\})=
            \mu_{h}(\{a\})=\delta_{a}(\{a\})=1
        \end{equation}
        Then $h(\omega)=a$ $\mu$ almost everywhere, or if we are
        in a probabilty space, almost surely.
        \begin{theorem}
            If $h_{n}$ is a sequence of random variables such that
            $\mu_{h_{n}}\rightarrow\delta_{a}$, where $\delta_{a}$
            is the Diract measure centered at $a$, then
            $h_{n}\rightarrow{a}$ almost surely.
        \end{theorem}
        \begin{proof}
            For:
            \begin{align}
                \mu(\{\omega:|h_{n}(\omega)-a|\geq\delta\})
                &=\mu_{h_{n}}
                    (\mathbb{R}\setminus(a-\delta,a+\delta)\})
                &=1-\mu_{h_{n}}((a-\delta,a+\delta))\\
                &\rightarrow{1}-\delta_{a}((a-\delta,a+\delta))\\
                &=0
            \end{align}
        \end{proof}
        Thus, the weak law of large numbers can be restated by
        saying that, if:
        \begin{equation}
            \mu_{\frac{1}{n}\sum_{j=1}^{n}f_{j}}\rightarrow
            \delta_{0}
        \end{equation}
        Then $f_{j}$ obeys the weak law of large numbers.
        \subsection{Convergence of Distributions}
            A distribution is an arbitrary probability
            Lebesgue-Stieljes measure. That is, a Lebesgue-Stieljes
            measure such that the measure of the entire space is
            one. We say that a sequence of distribuctions
            $\nu_{n}$ converges to a measure $\nu$ if any of
            the following equivalent statements holds:
            \begin{enumerate}
                \item $\nu_{n}([a,b))\rightarrow\nu([a,b))$
                      for all $a<b$.
                \item $\nu_{n}((\minus\infty,c))\rightarrow%
                       \nu(\minus\infty,c))$ for all $c$ such
                       that $\nu(\{c\})=0$. This requirement
                       implies that $\nu$ is continuous at $c$.
                       That is, if $F_{\nu}$ is the cumulative
                       distribution function, then $F_{\nu}$ is
                       continuous at $c$.
                \item For every bounded continuous function $h$,
                      $\int_{\mathbb{R}}h\diff\nu_{n}\rightarrow%
                       \int_{\mathbb{R}}h\diff{\nu}$.
                \item For every continuous function with compact
                      support:
                      $\int_{\mathbb{R}}h\diff\nu_{n}\rightarrow%
                       \int_{\mathbb{R}}h\diff{\nu}$.
                \item $\int_{\mathbb{R}}\exp(itu)\diff{\nu_{n}}%
                       =\int_{\mathbb{R}}\exp(itu)\diff{\nu}$
            \end{enumerate}
            \begin{theorem}
                A sequence of random variables $f_{j}$ obeys
                the weak law of large numbers if and only if:
                \begin{equation}
                    \mu_{\frac{1}{n}\sum_{j=1}^{n}f_{j}}
                    \rightarrow\delta_{0}
                \end{equation}
            \end{theorem}
            \begin{proof}
                For:
                \begin{equation}
                    \mu\Big(\big\{\omega:\Big|\frac{1}{n}
                        \sum_{j=1}^{n}\overset{\circ}{f}_{k}(\omega)
                        \Big|\geq\delta\big\}\Big)=
                    \mu_{\frac{1}{n}\sum_{k=1}^{n}
                         \overset{\circ}{f}_{j}}
                         \big((\minus\delta,\delta)^{C}\big)
                \end{equation}
            \end{proof}
            \begin{theorem}
                If $f_{j}$ is a sequence of random variables such
                that the second moments are finite, then the
                first moments are finite.
            \end{theorem}
            \begin{proof}
                For:
                \begin{equation}
                    \int_{\Omega}|f_{j}|\diff{\mu}\leq
                    \int_{\Omega}(1+|f_{j}|^{2})\diff{\mu}
                    =\int_{\Omega}\diff{\mu}+
                    \int_{\Omega}|f_{j}|^{2}\diff{\mu}=
                    1+\int_{\Omega}|f_{j}|^{2}\diff{\mu}
                \end{equation}
                Therefore, etc.
            \end{proof}
            \begin{ftheorem}{Central Limit Theorem}
                  {Measure_Theory_Central_Limit_Theorem}
                If $f_{j}$ are independent and identically
                distributed, with standard deviation $\sigma$,
                then:
                \begin{equation}
                    \mu_{\frac{1}{\sigma\sqrt{n}}
                        \sum_{j=1}^{n}\overset{\circ}{f}_{j}}
                    \rightarrow\nu_{0,1}
                \end{equation}
                Where $\nu_{0,1}$ is the Gaussian distribution:
                \begin{equation}
                    \nu_{0,1}(B)=\frac{1}{\sqrt{2\pi}}
                    \int_{B}\exp(\minus{u}^{2}/2)\diff{u}
                \end{equation}
            \end{ftheorem}
            \begin{proof}
                We will use the Fourier transform to prove this.
                We have:
                \begin{equation}
                    \int_{\mathbb{R}}\exp(iut)
                        \diff{\nu_{0,1}}=
                    \int_{\mathbb{R}}\exp(itu)
                        \exp(\minus\frac{u^{2}}{2})\diff{u}
                        =\exp(\minus{t}^{2}/2)
                \end{equation}
                That is, the Fourier transform of a Gaussian
                is itself. We will use this to make the
                computation easier. Using the measure
                transformation theorem, we have:
                \begin{equation}
                    \int_{\mathbb{R}}\exp(iut)
                    \mu_{\frac{1}{\sigma\sqrt{n}}
                        \sum_{j=1}^{n}\overset{\circ}{f}_{j}}
                        \diff{\mu}
                    =\int_{\Omega}\exp\Big(
                        \frac{i}{\sigma\sqrt{n}}\sum_{j=1}^{n}
                        \overset{\circ}{f}_{j}(\omega)t\Big)
                        \diff{\mu}
                \end{equation}
                We invoke independence to get:
                \begin{subequations}
                    \begin{align}
                        \int_{\Omega}\exp\Big(
                            \frac{i}{\sigma\sqrt{n}}\sum_{j=1}^{n}
                            \overset{\circ}{f}_{j}(\omega)t\Big)
                            \diff{\mu}
                        &=\int_{\Omega}\prod_{j=1}^{n}
                            \exp\Big(\frac{i}{\sigma\sqrt{n}}
                            \overset{\circ}{f}_{j}\Big)\diff{\mu}\\
                        &=\prod_{j=1}^{n}\int_{\Omega}
                            \exp\Big(\frac{i}{\sigma\sqrt{n}}
                            \overset{\circ}{f}_{j}\Big)\diff{\mu}
                    \end{align}
                \end{subequations}
                But the distributions are identically distributed,
                and thus we have:
                \begin{equation}
                    \int_{\Omega}\exp\Big(
                        \frac{i}{\sigma\sqrt{n}}\sum_{j=1}^{n}
                        \overset{\circ}{f}_{j}(\omega)t\Big)
                        \diff{\mu}=
                    \Big[\int_{\Omega}\exp\Big(
                        iu\frac{t}{\sqrt{n}}\Big)
                        \diff{\mu}\Big]^{n}
                \end{equation}
                We now need to prove that for an arbitrary
                Lebesgue-Stieltjes Measure on the Borel
                $\sigma\textrm{-Algebra}$ of $\mathbb{R}$,
                such that:
                \begin{equation}
                    \int_{\mathbb{R}}\diff{\mu}=0\quad\quad
                    \int_{\mathbb{R}}u\diff{\mu}=0\quad\quad
                    \int_{\mathbb{R}}u^{2}\diff{\mu}=1
                \end{equation}
                Then:
                \begin{equation}
                    \Big[\int_{\mathbb{R}}\exp\Big(
                        iu\frac{t}{\sqrt{n}}\Big)\diff{\mu}\Big]^{n}
                    \rightarrow\exp\big(\minus{t}^{2}/2\big)
                \end{equation}
                Consider the function:
                \begin{equation}
                    \varphi_{\nu}(t)=
                    \int_{\mathbb{R}}\exp(iut)\diff{\nu}
                \end{equation}
                In analysis this is the Fourier transform,
                whereas in probability this is called the
                characteristic function of $\nu$. We are
                tasked with showing that:
                \begin{equation}
                    \Big[\varphi_{\mu}
                        \big(\frac{t}{\sqrt{n}}\big)\Big]^{n}
                    \rightarrow\exp\big(\minus{t}^{2}/2\big)
                \end{equation}
                If $\mu$ is a Lebesgue-Stieltjes measure, and
                if the second moment if finite, and if:
                \begin{equation}
                    \varphi_{\nu}(t)=
                    \int_{\mathbb{R}}\exp(itu)\diff{\nu}
                \end{equation}
                then the first two derivatives of $\varphi_{\nu}$
                exist and are continuous. Moreover:
                \begin{equation}
                    \varphi_{\nu}(t)=
                    \varphi_{\nu}(0)+
                    \varphi_{\nu}'(0)t+
                    \varphi_{\nu}''(0)t^{2}+h(t)
                \end{equation}
                Where $h$ is such that:
                \begin{equation}
                    \underset{t\rightarrow{0}}{\lim}
                    \frac{h(t)}{t^{2}}=0
                \end{equation}
                First, it is continuous. For let $t_{k}$ be
                sequence such that $t_{k}\rightarrow{t}$ and let
                $g_{k}=\exp(it_{k}u)$. Then $|g_{k}|=1$, and is
                therefore summable. Moreover, $g_{k}$ tends to
                $\exp(itu)$. Thus, by the dominated convergence
                theorem:
                \begin{equation}
                    \underset{n\rightarrow\infty}{\lim}
                    \varphi_{\nu}(t_{k})
                    =\underset{n\rightarrow\infty}{\lim}
                    \int_{\mathbb{R}}\exp(it_{k}u)\diff{\mu}
                    =\int_{\mathbb{R}}
                        \underset{n\rightarrow\infty}{\lim}
                        \exp(it_{k}u)\diff{\mu}
                    =\varphi_{\nu}(t)
                \end{equation}
                And thus we have continuity. For differentiability,
                suppose $\Delta{t}_{k}$ is a sequence that
                tends to zero, and consider:
                \begin{equation}
                    \frac{\varphi_{\nu}(t+\Delta{t}_{k})-
                          \varphi_{\nu}(t)}{\Delta{t}_{k}}=
                    \int_{\mathbb{R}}
                    \frac{\exp(iu\Delta{t}_{k})-1}{\Delta{t}_{k}}
                    \exp(iut)\diff{\mu}
                \end{equation}
                Again, we want to apply the dominated convergence
                theorem. Thus we need to find a summable
                majorant. Consider $f(s)=(\exp(s)-1)/s$. On the
                real axis, this function has finite limit at
                zero and has zero limit at infinity, and therefore
                $f(s)$ is bounded on the real axis by some $K$.
                Thus, $K\exp(iut)$ serves as a summable majorant.
                Applying the dominated convergence theorem shows
                that the limit exists, and thus
                $\varphi_{\nu}$ is differentiable. We obtain:
                \begin{equation}
                    \varphi_{\nu}'(t)=
                    \int_{\mathbb{R}}iu\exp(iut)\diff{\nu}
                \end{equation}
                Moreover, this is differentiable and:
                \begin{equation}
                    \varphi_{\nu}''(t)=
                    \minus\int_{\mathbb{R}}u^{2}\exp(iut)
                        \diff{\mu}
                \end{equation}
                From Taylor, we have:
                \begin{equation}
                    h(t)=\varphi_{\nu}(t)-\varphi_{\nu}(0)
                        -\varphi_{\nu}'(0)t-\varphi_{\nu}''(0)
                            \frac{t^{2}}{2}
                \end{equation}
                Thus $h''(t)$ exists and is continuous,
                $h(0)=0$, $h'(0)=0$, and $h''(0)=0$. By the
                mean value theorem, we have:
                \begin{equation}
                    h(t)=h'(t_{1})t
                \end{equation}
                For some $t_{1}\in(0,t)$. Moreover:
                \begin{equation}
                    h(t)=h''(t_{2})t^{2}
                \end{equation}
                Where $0<t_{1}<t_{2}<t$. Thus:
                \begin{equation}
                    \frac{h(t)}{t^{2}}=h''(t_{2})
                \end{equation}
                And from the continuity of $h''(t)$, this
                converges to zero as $t$ tends to zero. Thus
                we have that $\varphi_{\nu}(0)=1$,
                $\varphi_{\nu}'(0)=0$, and
                $\varphi_{\nu}''(0)=\minus{1}$. Now we need to 
                finally justify the following limit:
                \begin{equation}
                    \Big[\varphi_{\mu}\big(\frac{t}{\sqrt{n}}\big)
                        \Big]^{n}\rightarrow\exp(\minus{t}^{2}/2)
                \end{equation}
                We have:
                \begin{equation}
                    \varphi_{\nu}(t)=1-\frac{t^{2}}{2}+h(t)
                \end{equation}
                Where $h(t)/t^{2}\rightarrow{0}$ as
                $t\rightarrow{0}$. Thus:
                \begin{equation}
                    \Big[\varphi_{\nu}\big(\frac{t}{\sqrt{n}}
                        \big)\Big]^{n}=
                    \Big[1-\frac{t^{2}}{2n}+h(\frac{t}{\sqrt{n}})
                        \Big]^{n}
                \end{equation}
                Define:
                \begin{equation}
                    w_{n}(t)=h(t/\sqrt{n})-\frac{t^{2}}{2n}
                \end{equation}
                Then we have:
                \begin{equation}
                    \Big[\varphi_{\nu}\big(\frac{t}{\sqrt{n}}
                        \big)\Big]^{n}
                    =\Big(\Big[1+w_{n}(t)\Big]^{w_{n}(t)}
                        \Big)^{\frac{n}{w_{n}(t)}}
                \end{equation}
                The inner part is the definition of $e$, so we
                now need to show that $n/w_{n}(t)$ converges to
                $\minus{t}^{2}/2$.
            \end{proof}
\end{document}