\documentclass[crop=false,class=book,oneside]{standalone}                      %
%----------------------------------Preamble------------------------------------%
\input{preamble.tex}                                                           %
%---------------------------------tikz Path------------------------------------%
\makeatletter                                                                  %
    \def\input@path{{../../../tikz/}}                                          %
\makeatother                                                                   %
%----------------------------------GLOSSARY------------------------------------%
\makeglossaries                                                                %
\loadglsentries{glossary}                                                      %
\loadglsentries{acronym}                                                       %
%--------------------------------Main Document---------------------------------%
\begin{document}
    \ifx\ifmain\undefined
        \pagenumbering{roman}
        \title{Functional Analysis}
        \author{Ryan Maguire}
        \date{\vspace{-5ex}}
        \maketitle
        \tableofcontents
        \clearpage
        \chapter*{Functional Analysis}
        \addcontentsline{toc}{chapter}{Functional Analysis}
        \markboth{}{FUNCTIONAL ANALYSIS}
        \vspace{10ex}
        \setcounter{chapter}{1}
        \pagenumbering{arabic}
    \else
        \chapter{Functional Analysis}
    \fi
    \section{Metric Spaces}
        \subsection{Basic Definitions}
            Functional analysis is concerned with normed spaces.
            This is a vector space $V$ with a function, called
            a norm, from $V$ to $[0,\infty)$. This is usually
            written $\norm{\mathbf{x}}$ for an element
            $\mathbf{x}\in{V}$. This norm must satisfy the
            folllowing for all $\mathbf{x}$, $\mathbf{y}\in{V}$:
            \begin{enumerate}
                \item $\norm{\mathbf{x}}=0$ if and only
                      if $\mathbf{x}=\mathbf{0}$
                      \hfill[Definiteness]
                \item $\norm{c\mathbf{x}}=|c|\norm{\mathbf{x}}$
                      for all $c\in\mathbb{R}$.
                      \hfill[Positiveness]
                \item $\norm{\mathbf{x}+\mathbf{y}}%
                       \leq\norm{\mathbf{x}}+\norm{\mathbf{y}}$
                      \hfill[Triangle Inequality]
            \end{enumerate}
            \begin{example}
                $\mathbb{R}$ with $\norm{x}=|x|$ and
                $\mathbb{R}^{n}$ with $\norm{\mathbf{x}}_{2}$
                defined by:
                \begin{equation*}
                    \norm{\mathbf{x}}_{2}
                    =\sqrt{\sum_{k=1}^{n}x_{k}^{2}}
                \end{equation*}
                are some of the most commonly used normed spaces.
                $\mathbb{R}^{n}$ can be thought of a the set
                of vectors in $n$ dimensions and the norm
                $\norm{\mathbf{x}}_{2}$ can be thought of as
                the length of $\mathbf{x}$ using the
                Pythagorean Theorem. There are other norms one
                can define on $\mathbb{R}^{n}$. A common
                one is the $p$ norm, defined by:
                \begin{equation*}
                    \norm{\mathbf{x}}_{p}
                    =\Big(\sum_{k=1}^{n}x_{k}^{p}\Big)^{1/p}
                \end{equation*}
                Together, $(\mathbb{R}^{n},\norm{}_{p})$ defines
                a normed space for all $p\geq{1}$. Another type
                of norm on $\mathbb{R}^{n}$ is the
                $p\textrm{-adic}$ norm.
            \end{example}
            A common family of sets,
            which we will deal with frequently, are the
            $\ell^{p}$ spaces.
            \begin{definition}
                $\ell^{p}$ is the set of all sequences
                $x:\mathbb{R}\rightarrow\mathbb{R}$ such that
                the sequence of partial sums
                $S:\mathbb{N}\rightarrow\mathbb{R}$ defined by
                $S_{N}=\sum_{n=1}^{N}|x_{n}|^{p}$ is bounded.
            \end{definition}
            \begin{definition}
                The $p$ norm on $\ell^{q}$ is the function
                $\norm{}_{p}:\ell^{q}\rightarrow\mathbb{R}$
                defined by:
                \begin{equation*}
                    \norm{x}_{p}
                    =\Big(\sum_{k=1}^{\infty}x_{k}^{p}\Big)^{1/p}
                \end{equation*}
            \end{definition}
            \begin{theorem}
                If $p\geq{1}$, then $(\ell^{p},\norm{}_{p})$
                is a normed space.
            \end{theorem}
            \begin{definition}
                The supremum norm on $\mathbb{R}^{n}$,
                $\norm{}_{\infty}:%
                 \mathbb{R}^{n}\rightarrow\mathbb{R}$,
                is the function:
                \begin{equation*}
                    \norm{\mathbf{x}}_{\infty}
                    =\max\{|x_{1}|,\hdots,|x_{n}|\}
                \end{equation*}
            \end{definition}
            \begin{theorem}
                If $n\in\mathbb{N}$, then
                $(\mathbb{R}^{n},\norm{}_{\infty})$ is a
                normed space.
            \end{theorem}
            \begin{definition}
                $\ell^{\infty}$ is the set of sequences
                $x:\mathbb{N}\rightarrow\mathbb{R}$ such that
                $x$ is bounded.
            \end{definition}
            \begin{definition}
                The supremum norm on $\ell^{\infty}$
                is the function
                $\norm{}_{\infty}:%
                 \ell^{\infty}\rightarrow\mathbb{R}$
                defined by:
                \begin{equation*}
                    \norm{x}_{\infty}
                    =\sup\{|x_{n}|:n\in\mathbb{N}\}
                \end{equation*}
            \end{definition}
            \begin{theorem}
                If $\norm{}_{\infty}$ is the
                supremum norm on $\ell^{\infty}$, then 
                $(\ell^{\infty},\norm{}_{\infty})$
                is a normed space.
            \end{theorem}
            From the fact that $\ell^{\infty}$ is a normed space
            we have that the set of convergent sequences,
            again with the $\norm{}_{\infty}$ norm, is also
            a normed space. The set of null sequences, which
            is the set of sequences that converge to zero,
            is also a normed space.
            A stranger normed space
            is the set of all bounded continuous functions
            $f:S\rightarrow\infty$ with norm
            $\norm{f}=\sup\{|f(x)|\}$. Furthermore, the
            set of all integrable functions with
            bounded integrals, with norm
            $(\int_{S}|f|^{p})^{1/p}$. If you allow integral
            to mean Lebesgue Integrable, then this becomes
            a special space denoted $L^{p}(S)$.
            \begin{definition}
                The Sobolev Space, denoted $W^{n,p}([a,b])$
                is the set of functions
                $f:[a,b]\rightarrow\mathbb{R}$ such that:
                \begin{equation*}
                    \int_{a}^{b}\sum_{k=0}^{n}
                    |f^{(k)}(x)|^{p}\diff{x}<\infty
                \end{equation*}
            \end{definition}
            \begin{definition}
                The $p$ norm on the Sobolev space
                $W^{n,q}([a,b])$ is the function
                $\norm{}_{p}:W^{n,q}([a,b])\rightarrow\mathbb{R}$
                defined by:
                \begin{equation*}
                    \Big(\int_{a}^{b}\sum_{k=0}^{n}
                    |f^{(k)}(x)|^{p}\diff{x}\Big)^{1/p}
                \end{equation*}
            \end{definition}
            \begin{theorem}
                If $p\geq{1}$, then
                $(W^{n,p}([a,b]),\norm{}_{p})$ is a
                normed space.
            \end{theorem}
            A lot of the things we wish to
            prove don't rely on the fact that all of these
            spaces are vector spaces. Really, we only care about
            the properties that the norm on the space has.
            What matters is that there's a set and a notion
            of distance on the set. This abstraction is the
            fundamental concept of a metric space.
            \begin{definition}
                A metric space is a set $S$ and a function
                $d:{X}\times{X}\rightarrow[0,\infty)$ such that:
                \begin{enumerate}
                    \item For all $x$, $y\in{X}$, $d(x,y)=0$
                          if and only if $x=y$.
                          \hfill[Definitenes]
                    \item For all $x$, $y\in{X}$,
                          $d(x,y)=d(y,x)$
                          \hfill[Symmetry]
                    \item For all $x$, $y\in{X}$,
                          $d(x,z)\leq{d(x,y)+d(y,z)}$
                          \hfill[Triangile Inequality]
                \end{enumerate}
            \end{definition}
            It turns out
            that we can actually write the following:
            \begin{definition}
                A metric space is a set $X$ and a function
                $d:{X}\times{X}\rightarrow\mathbb{R}$
                such that:
                \begin{enumerate}
                    \item $d(x,y)=0$ if and only if
                          $x=y$.
                    \item $d(x,z)\leq{d(x,y)+d(z,y)}$
                \end{enumerate}
            \end{definition}
            By writing the triangle inequality in this
            way, symmetry comes for free
            (The fact that $d(x,y)=d(y,x)$), as well
            as positivity (The fact that $d(x,y)\geq{0}$).
            Since it's easier to prove two things are
            true, rather than four things, it's nice to
            take this as the definition of a metric space,
            and then prove that the two definitions are
            equivalent.
            In a metric space $(X,d)$, $d$ is often called the
            \textit{distance function} or
            \textit{metric function}. It is meant to be an
            abstract mimicry of the absolute value function
            that is used with real numbers. Definiteness
            says the only point that is zero meters from a
            point $x$ is $x$ itself. Symmetry says the distance
            walking from $x$ to $y$ is the same as the distance
            walking from $y$ to $x$. The last rule stems from
            Euclidean geometry. It says walking from $x$ to $z$
            is shorter than (or equal to) walking from
            $x$ to $y$ and then $y$ to $z$. In Euclidean
            geometry equality is achieved only when
            $y$ lies between $x$ and $z$. In
            abstract metric spaces there may be no such
            thing as a \textit{line} between two points,
            so we need to be careful.
            \begin{example}
                $\mathbb{R}^{n}$ (for $1\leq{p}<\infty$):
                \begin{equation*}
                    d_{p}(\mathbf{x},\mathbf{y})=
                    \big(
                        \sum_{k=1}^{n}|x_{k}-y_{k}|
                    \big)^{1/p}
                    =\norm{\mathbf{x}-\mathbf{y}}_{p}
                \end{equation*}
            \end{example}
            \begin{example}
                In $\ell^{p}$, which are sequences for
                which
                $\sum_{k=1}^{\infty}|x_{k}|^{p}<\infty$,
                $d_{p}(x,y)$ forms a metric, as well
                as
                $d_{\infty}(x,y)=\sup\{|x_{k}-y_{k}|\}$,
                which is called the supremum norm.
            \end{example}
            \begin{example}
                $C(S,\mathbb{R})$, which is the
                set of continuous functions from
                $S$ to $\mathbb{R}$, letting
                $L^{p}(S)$ be the set of of functions
                such that:
                \begin{equation*}
                    \int_{S}|x(t)|^{p}<\infty
                \end{equation*}
                Then the following is a metric:
                \begin{equation*}
                    d_{p}(x,y)=
                    \bigg(
                        \int_{S}|x(t)-y(t)|^{p}dt
                    \bigg)^{1/p}
                \end{equation*}
                Also,
                $d_{\infty}(x,y)=\sup\{|x(t)-y(t)|\}$,
                which is called the supremum norm.
            \end{example}
            \begin{example}
                Let $C$ be the set of sequences such that
                $x_{n}\rightarrow{0}$. Then, with
                $d_{p}$, this forms a metric space.
                If $C_{0}$ is set of sequences with
                only finitely many non-zero terms,
                then
                $C_{0}\subset{C}\subset{\ell^{\infty}}$.
                Is there a sequence $x\in{C}$ such
                that, for all $1\leq{p}<\infty$,
                $x\notin{\ell^{p}}$.
            \end{example}
            Since the image of the metric function
            lies in $\mathbb{R}$,
            we may speak of \textit{convergence}
            in metric spaces.
            \begin{definition}
                A convergent sequence in a metric space
                $(X,d)$ is a sequence
                $x:\mathbb{N}\rightarrow{X}$ such that there
                is an $a\in{X}$ such that
                $d(a,x_{n})\rightarrow{0}$.
            \end{definition}
            \begin{definition}
                A limit of a sequence
                $x$ in a metric space $(X,d)$ is an
                $a\in{X}$ such that
                $d(x_{n},a)\rightarrow{0}$.
            \end{definition}
            Much like convergence in real numbers, limits
            in metric spaces are unique.
            \begin{theorem}
                \label{thm:Funct:Limit_of_Metric_Sequence_Unique}
                If $(X,d)$ is a metric space,
                $x:\mathbb{N}\rightarrow{X}$
                is a convergence sequence in $X$,
                and if $a$ and $b$ are limits of $x$,
                then $a=b$.
            \end{theorem}
            \begin{proof}
                For suppose not. As
                $(X,d)$ is a metric space, $d(a,b)>0$.
                Let $\varepsilon=\frac{d(a,b)}{4}$.
                Then, as $d(a,x_{n})\rightarrow{0}$
                and $\varepsilon>0$, there is an
                $N_{1}\in\mathbb{N}$ such that, for all
                $n>N_{1}$, $d(a,x_{n})<\varepsilon$. But,
                as $d(b,x_{n})\rightarrow{0}$ and
                $\varepsilon>0$, there is an $N_{2}$ such
                that, for all $n>N_{2}$,
                $d(b,x_{n})<\varepsilon$.
                Let $n=\max\{N_{1},N_{2}\}+1$.
                But then:
                \begin{equation*}
                    d(a,b)\leq{d(a,x_{n})+d(b,x_{n})}
                    <2\varepsilon=\frac{d(a,b)}{2}
                \end{equation*}
                A contradiction. Therefore, $a$ is unique.
            \end{proof}
            \begin{theorem}
                If $(X,d)$ be a metric space and if
                $x$, $y$, $z\in{X}$, then
                $|d(x,z)-d(y,z)|\leq{d(x,y)}$
            \end{theorem}
            \begin{proof}
                Suppose $d(x,z)\geq{d(y,z)}$.
                If $d(x,z)<d(y,z)$, the proof is
                symmetric. Thus we have:
                \begin{equation*}
                    |d(x,z)-d(y,z)|
                    =d(x,z)-d(y,z)
                    \leq{(d(x,y)+d(y,z))-d(y,z)}
                    =d(x,y)
                \end{equation*}
                Therefore,
                $|d(x,z)-d(y,z)|\leq{d(x,y)}$.
            \end{proof}
            \begin{theorem}
                If $(X,d)$ is a metric space
                and $x_{n}\rightarrow{a}$, then
                for all $b\in{X}$,
                $d(x_{n},b)\rightarrow{d(a,b)}$.
            \end{theorem}
            \begin{proof}
                For
                $|d(x_{n},b)-d(a,b)|\leq{d(x_{n},a)}%
                 \rightarrow{0}$.
            \end{proof}
            \begin{theorem}
                If $(V,\norm{})$ is a normed space
                and if $d:{V}\times{V}\rightarrow[0,\infty)$
                is defined by
                $d(\mathbf{x},\mathbf{y})%
                 =\norm{\mathbf{x}-\mathbf{y}}$,
                then $(V,d)$ is a metric space.
            \end{theorem}
            \begin{proof}
                In order:
                \begin{enumerate}
                    \item If $\norm{\mathbf{x}-\mathbf{y}}=0$,
                          then $\mathbf{x}=\mathbf{y}$.
                          Similarly,
                          $\norm{\mathbf{x}-\mathbf{x}}%
                           =\norm{\mathbf{0}}=0$.
                    \item $d(\mathbf{x},\mathbf{y})%
                           =\norm{\mathbf{x}-\mathbf{y}}%
                           =\norm{(-1)(\mathbf{y}-\mathbf{x})}%
                           =|-1|\norm{\mathbf{y}-\mathbf{x}}%
                           =\norm{\mathbf{y}-\mathbf{y}}%
                           =d(y,x)$
                    \item The triangle inequality follows
                          from the triangle inequality that
                          norms have.
                \end{enumerate}
            \end{proof}
            There are metric spaces that have nothing to do
            with vector spaces or norms. Metric spaces are
            a more abstract object. Every normed space
            has an associated metric space since there
            is the ``induced'' metric.
            \begin{example}
                Let $X$ be a set and let
                $d(x,y)=\begin{cases}%
                            0,&x=y\\%
                            1,&{x}\ne{y}%
                        \end{cases}$
                This is the discrete metric on $X$.
            \end{example}
            \begin{example}
                Let $X=\{a,b,c\}$, and
                $d(a,b)=1$, $d(b,c)=2$. What value
                must $d(a,c)$ have if $d$ is a metric on $X$?
                Consider the following table:
                \begin{table}[H]
                    \captionsetup{type=table}
                    \centering
                    \begin{tabular}{|c|c|c|c|}
                        \hline
                        $X$&a&b&c\\
                        \hline
                        a&0&1&?\\
                        \hline
                        b&1&0&2\\
                        \hline
                        c&?&2&0\\
                        \hline
                    \end{tabular}
                \end{table}
                This obeys everything except the triangle
                inequality. We must pick $d(a,c)$
                such that this is upheld.
                So we need the following:
                \begin{align*}
                    d(a,b)&\leq{d(a,c)+d(c,b)}&
                    d(a,c)&\leq{d(a,b)+d(b,c)}&
                    d(b,c)&\leq{d(b,a)+d(a,c)}\\
                    \Rightarrow{1}&\leq{2+d(a,c)}&
                    \Rightarrow{d(a,c)}&\leq{3}&
                    \Rightarrow{2}&\leq{1+d(a,c)}
                \end{align*}
                So we need $1\leq{d(a,c)}\leq{3}$.
                Pick $d(a,c)=2$.
                This makes $(X,d)$ a metric space.
            \end{example}
            \begin{example}
                Let $X=\mathbb{R}$ and $d(x,y)=|x-y|$.
                Then $(X,d)$ is a metric space.
            \end{example}
            \begin{example}
                $\mathbb{R}$ with
                $d(x,y)=|f(x)-f(y)|$, where
                $f:\mathbb{R}\rightarrow\mathbb{R}$
                is injective, is a metric space.
                Let $f$ be a real-valued function. Then
                from the triangle inequality
                \begin{equation*}
                    |f(x)-f(y)|\leq|f(x)-f(z)|+|f(z)-f(y)|
                \end{equation*}
                Therefore $d$ obeys the triangle inequality.
                It also obeys symmetry, for:
                \begin{equation*}
                    |f(x)-f(y)|=|(-1)(f(y)-f(x))|=|f(y)-f(x)|
                \end{equation*}
                The absolute value function is doing
                most of the work.
                But finally we require that
                $|f(x)-f(y)|=0$ if and only if
                $x=y$. But $|f(x)-f(y)|=0$ if and only
                if $f(x)=f(y)$. So we require that $f$
                is injective. If $f$ is not injective,
                then there exists $x_{1}$, $x_{2}$
                such that
                $x_{1}\ne{x_{2}}$ and yet
                $f(x_{1})=f(x_{2})$. But then
                $|f(x_{1})-f(x_{2})|=0$, contradicting the
                fact that this is a metric. If $f$ is
                injective, then this is a metric. Note
                injective functions need not be
                continuous, and can be very crazy.
            \end{example}
            \begin{example}
                $\mathbb{R}$ with
                $d(x,y)=|\tan^{-1}(x)-\tan^{-1}(y)|$ is a
                metric. Moreover, $d(x,y)<\pi$ for all
                $x,y\in\mathbb{R}$. Thus, we have found
                a metric that makes $\mathbb{R}$ a bounded
                set. As a fun fact, $x_{n}=n$ is a Cauchy
                sequence in this metric space, but
                this sequence does not converge to anything.
                Thus we've found a metric on
                $\mathbb{R}$ such that
                $(\mathbb{R},d)$ is not complete.
            \end{example}
            \begin{example}
            Can $d(x,y)=f(x-y)$ be a metric on $\mathbb{R}$
            if $f$ is differentiable? Not everywhere.
            $f$ can not be differentiable at the origin for
            $d(x,y)=f(x-y)$ to be a metric function, however
            $f$ can be differentiable everywhere else. Use
            $f(x)=|x|$ as an example.
            If $f(x-y)$ is a metric, $f$
            must be an even function. But
            then $f'(0)=0$. But $f(x-y)$ also must obey
            the triangle inequality. Therefore:
            \begin{equation*}
                f(2x)\leq{f(x)+f(x)}=2f(x)    
            \end{equation*}
            Define $h(x)$ by:
            \begin{equation*}
                h(x)=
                \left\{
                    \begin{array}{cr}
                    \frac{f(x)}{x},&x\ne{0}\\
                    0,&x=0
                    \end{array}\right.
            \end{equation*}
            Then, from
            the previous statement, $h(2x)\leq{h(x)}$.
            But then:
            \begin{equation*}
                h\Big(\frac{1}{2^{n}}\Big)\leq
                h\Big(\frac{1}{2^{n+1}}\Big)
            \end{equation*}
            But from L'H\^{o}pital's Rule,
            $h(x)\rightarrow{f'(0)}$ as $x\rightarrow{0}$.
            Therefore $h(1)\leq{f'(0)}$. But $h(1)>0$ since
            $f(x-y)$ is a metric, a contradiction.
            Therefore, $f$ can not be differentiable at
            the origin.
            \end{example}
        \subsection{Topology}
            \begin{definition}
                The open ball of radius $r>0$
                about a point $x$ in a metric space
                $(X,d)$ is the set
                $B_{r}(x)=\{y\in{X}:d(x,y)<r\}$
            \end{definition}
            The picture for this is a ``circle'' around the
            point $x$ or radius $r$. However, this circle
            can look very strange for weird metrics.
            \begin{example}
                If $X$ is a set and $d$ is the discrete metric,
                then $B_{r}(x)$ is either the point $x$
                (If $r\leq{1}$), or it is the entire set $X$.
            \end{example}
            \begin{example}
                With $X=\mathbb{R}$ and $d$ the standard metric
                $d(x,y)=|x-y|$, we have $B_{r}(x)$ is simply
                the open interval $(x-r,x+r)$.
            \end{example}
            \begin{example}
                \label{EXAMPLE:FUNCTIONAL:UNIT_BALLS_EXAMPLE}
                Let $X=\mathbb{R}^{2}$ and define
                $d_{p}(x,y)%
                 =(|x_{1}-y_{1}|^{p}+|x_{2}-y_{2}|^{p})^{1/p}$.
                For $p=2$, an open ball is a circle around
                the point $(x,y)$ of radius $r$. For $p=1$,
                we have ``diamonds'' around the point $x$.
                And for $p=\infty$ we have a square
                around $x$.
                Let $X=\mathbb{R}^{2}$ and let $d$ be the metric
                such that you can only travel parallel to the
                $y$ axis, or along the $x$ axis.
                Consider the unit balls in $(X,d)$
                about the following points:
                \begin{enumerate}
                    \begin{multicols}{4}
                        \item[a.] $(0,0)$
                        \item[b.] $(0,1)$
                        \item[c.] $(0,\frac{1}{2})$
                        \item[d.] $(\frac{1}{2},\frac{1}{2})$
                    \end{multicols}
                \end{enumerate}
                If $\mathbf{x}_{1}=(x_{1},y_{1})$ and
                $\mathbf{x}_{2}=(x_{2},y_{2})$, then we have:
                \begin{equation*}
                    d(\mathbf{x}_{1},\mathbf{x}_{2})=
                    \begin{cases}
                        |y_{2}-y_{1}|,&x_{1}=x_{2}\\
                        |x_{2}-x_{1}|+|y_{1}|+|y_{2}|,
                        &x_{1}\ne{x_{2}}
                    \end{cases}
                \end{equation*}
            About the point $(0,0)$, the unit ball
            is simply points
            $(x,y)$ such that $|x|+|y|<1$. This is a ``diamond.''
            About $(0,1)$, first note that to get to any point
            whose $x$ coordinate is not $0$, you first must travel
            the entirety of the $y$ axis. Since this length is
            already $1$, you can't go left or right
            on the $x$ axis.
            The unit ball is the line segment on the $y$ axis
            between $(0,0)$ and $(0,2)$. For the third one, if
            the $x$ coordinate changes, we have
            $0.5+|y|+|x|<1$, which implies
            $|y|+|x|<0.5$. This is again a diamond, but a
            smaller one. If the $x$ coordinate does not
            change, we have $|y-0.5|<1$. This is another
            line segment. Repeat the same arguments for the
            fourth coordinate. The diagrams are show in
            Fig.~\ref{FUNCTIONAL:HOMEWORK:2:PROBLEM:4:FIGURES}.
        \begin{figure}[H]
            \centering
            \captionsetup{type=figure}
            \subimport{../../../tikz/}{Open_Ball_in_Metric_Space_I}
            \caption{Figures for Example
                     \ref{EXAMPLE:FUNCTIONAL:UNIT_BALLS_EXAMPLE}.}
            \label{FUNCTIONAL:HOMEWORK:2:PROBLEM:4:FIGURES}
        \end{figure}
            \end{example}
            If you have a vector space and a norm on it,
            then the open balls about a point will have the
            property of convexity. Convexity is a vector space
            property, given two points the ``line'' between the
            two remains in the set. Metric spaces have no such
            notion. Since the balls of $\norm{}_{p}$ are not
            convex with $p<1$, we have that $\norm{}_{p}$ is
            a metric on $\mathbb{R}^{n}$
            if and only if $p\geq{1}$.
            \begin{definition}
                An open subset of a metric space
                $(X,d)$ is a set $S\subset{X}$ such that,
                for all $x\in{S}$, there is an
                $r>0$ such that
                $B_{r}(x)\subset{S}$.
            \end{definition}
            \begin{example}
                If $(X,d)$ is a metric space, then
                $X$ is open and $\emptyset$ is open
                (Vacuously true).
            \end{example}
            \begin{theorem}
                If $(X,d)$ is a metric space, $x\in{X}$,
                and $r>0$, then $B_{r}(x)$ is an open
                subset of $X$.
            \end{theorem}
            \begin{proof}
                If $z\in{B_{r}(x)}$, let $t=d(x,z)$.
                Then $0\leq{t}<r$. Let $r'=r-t$.
                But if $y\in{B_{r'}(z)}$, then
                $d(x,y)\leq{d(x,z)+d(y,z)}<t+r'=t+r-t=r$.
                Therefore $B_{r'}(z)\subset{B_{r}(x)}$.
            \end{proof}
            \begin{theorem}
                A finite intersection of open sets is open.
            \end{theorem}
            \begin{proof}
                If $\mathcal{U}_{1},\hdots,\mathcal{U}_{n}$
                are open and if
                $x\in\cap_{k=1}^{n}\mathcal{U}_{k}$, then there
                exists $r_{1},\hdots,r_{n}$ such that
                $B_{r_{i}}(x)\subset\mathcal{U}_{i}$. Let
                $r=\min\{r_{1},\hdots,r_{n}\}$. Then
                $B_{r}(x)\subset\cap_{k=1}^{n}\mathcal{U}_{i}$
            \end{proof}
            \begin{theorem}
                Arbitrary unions of open sets are open.
            \end{theorem}
            Infinite intersections need not be open.
            The proof above would fail since the
            $r_{i}$ can form a sequence tending to zero.
            But indeed, let $X=\mathbb{R}$ and let
            $d(x,y)=|x-y|$, and take
            $\mathcal{U}_{n}=(-\frac{1}{n},\frac{1}{n})$.
            Then all of the $\mathcal{U}_{n}$ are open,
            yet the intersection, which is the set $\{0\}$,
            is not open. All of this mumbo-jumbo creates
            the more general notion of a topological space.
            \begin{definition}
                A topological space is a set $X$ and a
                subset $\tau\subset\mathcal{P}(X)$ such that:
                \begin{enumerate}
                    \item $\emptyset,X\in\tau$
                    \item Finite intersections of sets in $\tau$
                          are also sets in $\tau$.
                    \item Arbitrary unions of sets in $\tau$
                          are also sets in $\tau$.
                \end{enumerate}
            \end{definition}
            Here, $\mathcal{P}(X)$ denotes the \textit{power set}
            of $X$. This is the set of all subsets of $X$.
            The notion of a topological space generalizes the
            notion of a metric space. There is no notion of
            distance in such spaces, and things can be weird.
            There are topological spaces that have no metric
            associated with them.
            \begin{definition}
                An open subset of a topological space
                $(X,\tau)$ is a set $\mathcal{U}\in\tau$.
            \end{definition}
            \begin{definition}
                An interior point of a subset $S$ of a topological
                space $(X,\tau)$ is a point $x\in{S}$ such that
                there is an open subset $\mathcal{U}\subseteq{S}$
                such that $x\in\mathcal{U}$.
            \end{definition}
            \begin{definition}
                The interior of a subset $S$ of a topological
                space $(X,\tau)$, denoted $\Int(S)$, is the set
                of all interior points of $S$.
            \end{definition}
            \begin{theorem}
                If $S$ is an open subset of
                $(X,\tau)$, then $\Int(S)=S$.
            \end{theorem}
            \begin{definition}
                A function from a metric space
                $(X,d_{X})$ to a metric space $(Y,d_{Y})$
                continuous at $x\in{X}$ is a function
                $f:X\rightarrow{Y}$ such that
                for all $\varepsilon>0$ there is
                a $\delta>0$ such that for all
                $x_{0}\in{X}$ such that
                $d_{X}(x,x_{0})<\delta$, we have
                $d_{Y}(f(x),f(x_{0})<\varepsilon$
            \end{definition}
            \begin{theorem}
                If $(X,\tau)$ is a topological space and
                $S\subseteq{X}$, and if $\mathcal{O}$ is the set
                of all open sets $\mathcal{U}$ such that
                $\mathcal{U}\subseteq{S}$, then
                $\Int(S)=%
                 \bigcup_{\mathcal{U}\in\mathcal{O}}\mathcal{U}$.
            \end{theorem}
            \begin{definition}
                A nowhere dense subset of a topological space
                $(X,\tau)$ is a subset $S\subseteq{X}$ such that
                $\Int(S)=\emptyset$.
            \end{definition}
            \begin{theorem}
                If $(X,d)$ is a metric space,
                $y\in{X}$, then
                $f:X\rightarrow\mathbb{R}$ defined by
                $f(x)=d(x,y)$ is uniformly continuous.
            \end{theorem}
            A surprising theorem, and the entire
            basis of the study of topology, goes as
            follows:
            \begin{theorem}
                If $(X,d_{x})$ and $(Y,d_{Y})$
                are metric spaces, then
                $f:X\rightarrow{Y}$ is continuous
                at $x\in{X}$ if and only if
                for all open subsets
                of $S\subset{Y}$ such that
                $f(x)\in{S}$, $f^{-1}(S)$ is an
                open subset of $X$.
            \end{theorem}
            This allows us to talk about continuous
            functions without a notion of metric.
            Thus, for topological spaces, this is
            the \textit{definition} of continuity.
            When the space we're discussing is a
            metric space, this theorem shows that the
            definition from topology and the defintition
            from real analysis are in fact equivalent.
            \begin{theorem}
                A function $f:X\rightarrow{Y}$ between
                metric spaces is continuous at a point
                $x\in{X}$ if and only if for all
                sequences $x_{n}$ such that
                $d_{X}(x,x_{n})\rightarrow{0}$, we have
                $d_{Y}(f(x),f(x_{n})\rightarrow{0}$.
            \end{theorem}
            We now have three different ways to talk
            about continuity. Topological spaces can be
            nastier, however. We saw in
            Thm.~\ref{thm:Funct:Limit_of_Metric_Sequence_Unique}
            that the limit of a convergent sequence in a
            metric space is unique.
            This is not true in a topological space and there
            are topological spaces with sequences
            which converge to every point in the
            space simultaneously. Indeed, it may be impossible
            to distinguish two points in a topological
            space. The ability to
            ``Separate,'' points is special.
            Hausdorff spaces can, but
            we'll save that for topology.
        \subsubsection{Closed Sets}
            \begin{definition}
                A limit point of a subset
                $S\subset{X}$ of a metric space
                $(X,d)$ is a point $a\in{X}$ such
                that there is a sequence
                $x:\mathbb{N}\rightarrow{S}$ such that
                $d(a,x_{n})\rightarrow{0}$.
            \end{definition}
            \begin{definition}
                A closed subset of a metric space $(X,d)$
                is a set $S$ such that for all $x\in{X}$ such
                that $x$ is a limit point of $S$, $x\in{S}$.
            \end{definition}
            This says that if $S$ is closed, and
            $x$ is a sequence in $S$ such
            that $x_{n}\rightarrow{a}$, then
            $a\in{S}$.
            \begin{example}
                In $\mathbb{R}$, with the standard
                metric, $(a,b)$ is open,
                $\mathbb{R}$ is open (and closed),
                $[a,b]$ is closed,
                $[a,\infty)$ is closed,
                $[a,b)$ is neither closed nor open.
            \end{example}
            \begin{example}
                If $X=(0,1)$, and
                $d(x,y)=|x-y|$, then
                $(0,1)$ is closed. This is because
                there is no sequence that converges
                to a point in the space whose limit
                is not in the space. There are no sequences
                in $X$ which converge to zero or one since,
                as far as $X$ is concerned,
                neither or these points exist.
            \end{example}
            \begin{theorem}
                If $(X,d)$ is a metric space,
                then a subset $S\subset{X}$ is open
                if and only if $X\setminus{S}$ is closed.
            \end{theorem}
            \begin{proof}
                Suppose $S$ is open, and let
                $x_{n}$ be a sequence in $S^{c}$.
                Suppose $x_{n}\rightarrow{x}$ and
                $x\in{S}$. But $S$ is open, and thus
                there is an $\varepsilon>0$ such that
                $B_{\varepsilon}(x)\subset{S}$.
                But $x_{n}\rightarrow{x}$, and thus
                this is an $N\in\mathbb{N}$ such that
                for all $n>N$, $d(x,x_{n})<\varepsilon$.
                But then for all $n>N$,
                $x_{n}\in{B_{\varepsilon}(x)}$. But
                $x_{n}\in{S^{c}}$, a contradiction.
                Therefore, $S^{c}$ is closed. On the
                other hand, if $S^{c}$ is closed
                and there is an $x\in{S}$ such that
                for all $r>0$,
                $B_{r}(x)\cap{S}\ne\emptyset$, then
                for all $n\in\mathbb{N}$ there is
                an $x_{n}\in{S^{c}}$ such that
                $d(x,x_{n})<\frac{1}{n}$. But then
                $x_{n}\rightarrow{x}$, and therefore
                $x\in{S^{c}}$. But $x\in{S}$,
                a contradiction. Thus, $S$ is open.
            \end{proof}
            In topology we take the definition of
            closed sets to be the compliment of open
            sets. This theorem shows that the
            topological definition is equivalent when we
            consider metric spaces.
            \begin{definition}
                The closure of a subset
                $S$ of a metric space
                $(X,d)$, denoted $\overline{S}$,
                is the set of all
                limit points of $S$.
            \end{definition}
            \begin{theorem}
                If $(X,d)$ is a metric space, if
                $S\subset{X}$, and if
                $\Delta$ is the set of all closed subsets
                $\mathcal{C}\subset{X}$ such that
                $S\subset\mathcal{C}$, then:
                $\overline{S}=
                 \bigcap_{\mathcal{C}\in\Delta}
                 \mathcal{C}$
            \end{theorem}
            Thus we may loosely say that
            the closure of a set $S$ is the
            ``Smallest,'' closed set that contains $S$.
            \begin{definition}
                The closed ball of radius $r>0$ about
                a point $x$ in a metric space
                $(X,d)$ is the set:
                \begin{equation*}
                    \overline{B}_{r}(x)=
                    \{y\in{X}:d(x,y)\leq{r}\}
                \end{equation*}
            \end{definition}
            There exists metric spaces $(X,d)$
            such that
            $\overline{B}_{r}(x)\ne\overline{B_{r}(x)}$.
            For take the discrete metric, $r=1$.
            Then the closure of $B_{1}(x)$ is simply
            the point $x$. However, the closed ball
            $\overline{B}_{1}(x)$ is the entire space.
            Metric spaces can be very weird like this.
            They have a property, that given a nested
            sequence of closed balls whose radius
            tends to zero, there is precisely one
            point that lies in the intersection. However,
            if the radius does not tend to zero it is
            possible that the intersection is empty.
            This is very counter-intuitive.
            \begin{definition}
                A dense subset of a metric space $(X,d)$
                is a set $S\subset{X}$ such that
                $\overline{S}=X$.
            \end{definition}
            A subset $S$ is dense in $X$ if every point
            in $X$ can be approximated arbitrarily well
            by points in $S$. For any point $a\in{X}$
            there is a sequence $x\in{S}$
            such that $x_{n}\rightarrow{a}$. The
            classic example is $\mathbb{Q}$ and
            $\mathbb{R}$. Every real number can be
            approximated arbitrary well by a rational
            number. To see this, just take the continued
            fraction of a real number and stop once
            the approximation is less than
            $\varepsilon$. When we say $\mathbb{Q}$ is
            dense in $\mathbb{R}$, we of course mean with
            respect to the standard metric on $\mathbb{R}$.
            $\mathbb{Q}$ is \textbf{not} dense in
            $\mathbb{R}$ with respect to the discrete metric.
            Indeed, if $d$ is the discrete metric on $X$,
            then $S\subset{X}$ is dense in $X$ if and only if
            $S=X$.
            \begin{example}
                $\mathbb{Q}$ is dense in $\mathbb{R}$
                with respect to $d_{p}$ for all
                $p\geq{1}$. This includes
                $d(x,y)=|x-y|$.
            \end{example}
            \begin{example}
                The set of polynomials on the interval
                $[a,b]$ are dense in the set of
                continuous functions on $[a,b]$ with
                respect to the $d_{\infty}$ metric.
                This comes from Weierstrass's Theorem.
            \end{example}
            \begin{example}
                The set of polynomials on $[a,b]$
                is dense in the set of continuous
                functions on $[a,b]$ with respect to
                the $d_{p}$ metric, for $p\geq{1}$. This
                is because:
                \begin{align*}
                    d_{p}(P,x)&=
                    \Big(
                        \int_{a}^{b}|P(t)-x(t)|^{p}\diff{t}
                    \Big)^{1/p}
                    &
                    &=\Big(
                        d_{\infty}(P,x)^{p}\int_{a}^{b}\diff{t}
                    \Big)^{1/p}\\
                    &\leq\Big(\int_{a}^{b}
                        |\max\{P(t)-x(t)\}|^{p}\diff{t}
                    \Big)^{1/p}
                    &
                    &=(b-a)^{1/p}d_{\infty}(P,x)
                \end{align*}
            \end{example}
            \begin{example}
                The continuous functions are not dense
                in the set of integrable functions,
                with respect to the supremum metric
                $d_{\infty}$. This is more or less
                because integrable functions can
                be discontinuous, or have jumps. This
                means, with respect to $d_{\infty}$,
                that no continuous functions could
                approximate such a discontinuous function
                arbitrary well.
            \end{example}
            \begin{definition}
                A separable metric space
                is a metric space $(X,d)$ with
                a countable dense subset $S$.
            \end{definition}
            \begin{example}
                $\mathbb{R}$ is separable, with
                the standard metric, since
                $\mathbb{Q}$ is countable and also
                dense in $\mathbb{R}$.
            \end{example}
            \begin{example}
                The set of continuous functions on
                $[a,b]$ is separable. For
                take the set of polynomials with
                rational coefficients. This can
                be seen as a countable union of
                countably many elements. For let
                $P_{N}$ be the set of polynomials
                of degree $N$ with rational
                coefficients. This is countable,
                and the set of all polynomials with
                rational coefficients is simply the
                union of $P_{N}$ over all $N$. This
                is dense in the set of polynomials,
                and the set of polynomials is dense
                in $C[a,b]$, and thus
                the set of polynomials with rational
                coefficients is dense in $C[a,b]$. Thus
                $C[a,b]$ is separable.
            \end{example}
            \begin{example}
                $\ell^{p}$ is separable with the
                $d_{p}$ metric, simply use elements
                with rational entries. That is,
                sequences of rational numbers.
            \end{example}
            \begin{example}
                $\ell^{p}$ with the $d_{\infty}$ metric
                is NOT separable. Consider the real
                numbers in $(0,1)$.
            \end{example}
        \subsection{Completeness}
            \begin{definition}
                A complete metric space is a metric
                space $(X,d)$ such that every
                Cauchy sequence $x_{n}$
                in $X$ converges to a point in $X$
                with respect to $d$.
            \end{definition}
            Recall that a sequence $x_{n}$ is Cauchy if
            $\forall_{\varepsilon>0}\exists_{N\in\mathbb{N}}:%
             \forall_{n,m>N},d(x_{n},x_{m})<\varepsilon$.
            Convergence with respect to $d$ means that
            $d(x,x_{n})\rightarrow{0}$.
            \begin{example}
                $\mathbb{R}$ with the standard metric
                $d(x,y)=|x-y|$ is complete.
            \end{example}
            \begin{example}
                $(\mathbb{R}^{n},d_{p})$ is also complete
                for all $n\in\mathbb{N}$.
            \end{example}
            Completeness is both a property of the set
            and the metric itself. It is not a topological
            property.
            \begin{example}
                $(\mathbb{R},d)$, where
                $d(x,y)=|\tan^{-1}(x)-\tan^{-1}(y)|$
                is \textit{not} complete. For let
                $x_{n}=n$. This is a Cauchy sequence,
                as one can see from the graph
                of $\tan^{-1}(x)$. That is, because
                $\tan^{-1}(x)\rightarrow{\pi/2}$,
                $x_{n}=n$ is a Cauchy sequence in this
                metric. Being even more rigorous, let
                $\varepsilon>0$ and
                $N=\ceil{\tan(\pi/2-\varepsilon)}$.
                Then, for all $n,m>N$,
                $d(x_{n},x_{m})%
                 =|\tan^{-1}(n)-\tan^{-1}(m)|%
                 <|\pi/2-\tan^{-1}(\min\{n,m\})|%
                 <|\pi/2-(\pi/2-\varepsilon)|%
                 =\varepsilon$. But $x_{n}$ does not
                converge. For suppose not.,
                Suppose $x_{n}=n\rightarrow{x}$.
                Then for $n>x+1$,
                $d(x_{n},x)=|\tan^{-1}(n)-\tan^{-1}(x)|%
                 <|\tan^{-1}(x+1)-\tan^{-1}(x)|$,
                so $d(x_{n},x)\not\rightarrow{0}$.
                The sequence does not converge.
            \end{example}
            Let $X=\mathbb{R}\cup\{-\infty,\infty\}$.
            Let $d:X\times{X}\rightarrow\mathbb{R}$
            be defined by
            \begin{align*}
                d(x,y)
                &=|\tan^{-1}(x)-\tan^{-1}(y)|
                &
                d(x,\infty)
                &=\frac{\pi}{2}-\tan^{-1}(x)\\
                d(-\infty,x)
                &=\frac{\pi}{2}+\tan^{-1}(x)
                &
                d(-\infty,\infty)&=\pi
            \end{align*}
            Then $d$ is a metric on $X$, and moreover
            $(X,d)$ is complete. The counterexample
            we found for $(\mathbb{R},d)$ has been
            ``filled,'' in a sense. The hole is
            no longer there. The sequence $x_{n}=n$
            now converges to $\infty$. Somewhat
            unsurpringly, $\mathbb{R}$ is
            dense in $X$, with respect to
            $d$. Every element in $X$ is the limit of
            a sequence of elements in $\mathbb{R}$.
            \begin{definition}
                A completion of a metric space
                $(X,d)$ is a complete metric space
                $(\tilde{X},\tilde{d})$ such that
                $X\subset{\tilde{X}}$ and
                the restriction of
                $\tilde{d}$ onto $X$ is equal to $d$.
            \end{definition}
            \begin{theorem}
                Every metric space has a completion.
            \end{theorem}
            \begin{definition}
                An isometry between
                metric spaces
                $(X,d_{X})$ and
                $(Y,d_{Y})$ is a function
                $f:X\rightarrow{Y}$ such that
                $d_{X}(x,y)=d_{Y}(f(x),f(y))$
                for all $x,y\in{X}$.
            \end{definition}
            \begin{definition}
                Isometric metric spaces are metric spaces
                with an isometry between them.
            \end{definition}
            \begin{theorem}
                If $(X,d)$ is a metric space
                and $(\tilde{X}_{1},\tilde{d}_{1})$
                and $(\tilde{X}_{2},\tilde{d}_{2})$
                are completions of $(X,d)$, then
                $(\tilde{X}_{1},\tilde{d}_{1})$
                and $(\tilde{X}_{2},\tilde{d}_{2})$
                are isometric.
            \end{theorem}
            This says the completion of a metric space is
            unique up to isometry.
            The Lebesgue space $L^{p}(S)$
            can be defined to be the completion of
            $C(S)$ with respect to the $d_{p}$ metric.
            \begin{theorem}
                $(C(S),d_{\infty})$ is complete.
            \end{theorem}
            \begin{proof}
                Suppose $x_{n}$ is a Cauchy sequence
                and let $\varepsilon>0$. As $x_{n}$ is
                Cauchy, there exists $N\in\mathbb{N}$
                such that for all $n,m>N$,
                $\sup|x_{m}(t)-x_{n}(t)|<\frac{\varepsilon}{3}$.
                But then for all $t\in{S}$,
                $|x_{m}(t)-x_{n}(t)|<\frac{\varepsilon}{3}$,
                for all
                $n,m>N$. That is, if $x_{n}$ is
                a Cauchy sequence in $(C(S),d_{\infty})$,
                then it is a Cauchy sequence in
                $(\mathbb{R},d_{1})$. But
                $(\mathbb{R},d_{1})$ is complete, and
                therefore, for all $t\in{S}$, there is
                an $x(t)$ such that
                $x_{n}(t)\rightarrow{x(t)}$ with respect
                to the $d_{1}$ metric on $\mathbb{R}$. We
                now need to show that $x(t)$ is a continuous
                function. That is, that
                $x(t)\in{C(S)}$. Finally we need to show that
                $x_{n}\rightarrow{d}$ with respect to
                $d_{\infty}$. We need to show that
                for all $\varepsilon>0$ and all $t\in{S}$
                there is a $\delta>0$
                such that for all $|t-t_{0}|<\delta$,
                $|x(t)-x(t_{0})|<\varepsilon$. But for
                all $n,m>N$,
                $\sup\{x_{n}(t)-x_{m}(t)\}<\frac{\varepsilon}{3}$.
                Taking the limit on $m$, we have
                $|x(t)-x_{n}(t)|<\frac{\varepsilon}{2}$.
                But $x_{n}(t)$ is continuous, and thus
                there exists $\delta>0$ such that
                for all $|t-t_{0}|<\delta$,
                $|x_{n}(t)-x_{n}(t_{0})|<\frac{\varepsilon}{3}$.
                But
                $|x(t)-x(t_{0})|\leq%
                  |x(t)-x_{n}(t)|%
                 +|x_{n}(t)-x_{m}(t)|%
                 +|x(t_{0})-x_{n}(t_{0})$
                But
                $|x(t_{0})-x_{n}(t_{0})|<%
                 \sup\{|x(t)-x_{n}(t)|\}<\frac{\varepsilon}{3}$,
                and therefore
                $|x(t)-x(t_{0})|<\varepsilon$.
                So $x(t)$ is continuous.
            \end{proof}
            The Weierstrass Approximation Theorem says that,
            for closed finite intervals $S$,
            $(C(S),d_{\infty})$ is the completion
            of the set of polynomials with respect to
            the $d_{\infty}$ metric. On the other hand,
            $(C[0,1],d_{p})$ is not complete when
            $1\leq{p}<\infty$. For define the following:
            \begin{equation*}
                H(x)=
                \begin{cases}
                    0,&0\leq{x}\leq{\frac{1}{2}}\\
                    1,&\frac{1}{2}<x\leq{1}
                \end{cases}
            \end{equation*}
            This is discontinuous and cannot be
            approximated arbitrarily well
            by any continuous function. However, the
            \textit{area} underneath $H$ can be approximated
            arbitrarily well be continuous functions. For define:
            \begin{equation*}
                x_{n}(t)=
                \begin{cases}
                    0,&0\leq{x}\leq{\frac{1}{2}-\frac{1}{n}}\\
                    n(x-\frac{1}{2}+\frac{1}{n}),
                    &\frac{1}{2}-\frac{1}{n}\leq{x}
                     \leq{\frac{1}{2}}\\
                    1,&\frac{1}{2}<{x}\leq{1}
                \end{cases}
            \end{equation*}
            Then the area under $x_{n}(t)$
            is $\frac{1}{2}+\frac{1}{2n}$, and thus
            $d_{1}(x_{n}(t),x_{m}(t))%
             =|\frac{1}{2m}-\frac{1}{2n}|$,
            and therefore $x_{n}(t)$ is a Cauchy sequence.
            But $x_{n}(t)$ does not converge in
            $(C[0,1],d_{1})$. For suppose not, suppose
            $x_{n}(t)\rightarrow{x(t)}$, and
            $x(t)\in{C[0,1]}$.
            If $x(1/2)\geq{1/2}$, then, as $x(t)$ is
            continuous, there is a $\delta>0$ such that
            for all $|t-1/2|<\delta$,
            $x(t)>1/4$. But then
            $d(x_{n},x)=\int_{0}^{1}|x(t)-x_{n}(t)|dt%
            \geq\int_{1/2-\delta/2}^{1/2}|x(t)-x_{n}(t)|dt$.
            But $|x|=|(x-y)+y|\leq{|x-y|+|y|}$,
            and thus
            $|x|-|y|\leq{|x-y|}$. From this we have
            $d(x_{n}(t),x(t))\geq%
             \int_{1/2-\delta/2}^{1/2}(x(t)-x_{n}(t))dt%
             >\int_{1/2-\delta/2}^{1/2}\frac{1}{4}dt%
             -\int_{0}^{1/2}x_{n}(t)dt%
             =\frac{1}{4}\delta-\frac{1}{2n}%
             \rightarrow{\frac{1}{4}}\delta$.
            But then $d(x_{n}(t),x(t))\not\rightarrow{0}$.
            Therefore $x_{n}(t)$ does not converge.
            \begin{theorem}
                If $1\leq{p}<\infty$, then
                $(\ell^{p},d_{p})$ is complete.
            \end{theorem}
            \begin{proof}
                Let $x_{n}$ be a Cauchy sequence
                in $(e\ell^{p},d_{p})$,
                $x_{n}=x_{n}(1),x_{n}(2),\hdots,x_{n}(k),\hdots$
                Then, for $n,m\in\mathbb{N}$,
                $d_{p}(x_{n},x_{m})%
                 =(%
                    \sum_{k=0}^{\infty}|x_{n}(k)-x_{m}(k)|^{p}%
                  )^{1/p}$
                As $x_{n}$ is Cauchy, for all 
                $\varepsilon>0$ there is an $N\in\mathbb{N}$
                such that for all $n,m>N$,
                $d_{p}(x_{n},x_{m})<\varepsilon$.
                But then, for all $n,m>N$ and all
                $k\in\mathbb{N}$,
                $|x_{n}(k)-x_{m}(k)|^{p}<d_{p}(x_{n},x_{m})^{P}%
                 <\varepsilon^{p}$.
                But then
                $|x_{n}(k)-x_{m}(k)|<\varepsilon$. Therefore
                $x_{n}(k)$ is a Cauchy sequence in
                $(\mathbb{R},d)$, and this metric space is
                complete. Therefore, for all $k\in\mathbb{N}$,
                there is a $z_{k}$ such that
                $x_{n}(k)\rightarrow{z_{k}}$. We now need to
                show that $z_{k}$ is an element of
                $\ell^{p}$ and that
                $x_{n}\rightarrow{z_{k}}$ with respect to
                the $d_{p}$ metric. For let $N\in\mathbb{N}$.
                Then
                $\sum_{k=0}^{N}|x_{n}(k)-x_{m}(k)|^{p}%
                 \leq{\sum_{k=0}^{\infty}|x_{n}(k)-x_{m}(k)|^{p}}%
                 <\varepsilon^{p}$. Taking the limit on $m$,
                we have
                $\sum_{k=0}^{N}|z_{k}-x_{n}(k)|<\varepsilon^{p}$.
                The reason we have written a finite sum is to
                avoid getting into trouble with limits. An
                infinite sum is itself a limit, and taking
                limits of limits can get very messy very easily.
                For example,
                $f(n,m)=\frac{m}{n+m}$. Taking the limit on
                $m$ first results in $1$, whereas taking the
                limit on $n$ first gives you $0$.
                That is,
                $\lim_{n}\lim_{m}f(n,m)%
                 \ne\lim_{m}\lim_{n}f(n,m)$.
                You have to
                be careful when considering limits of limits.
                With this we have shown that
                $z_{k}-x_{n}(k)\in\ell^{p}$ for all
                $n\in\mathbb{N}$. But $x_{n}\in\ell^{p}$,
                and $\ell^{p}$ is closed under addition.
                Therefore $z_{k}\in\ell^{p}$. But also,
                for $n>N$, we have
                $d_{p}(x_{n},z)<\varepsilon$. Thus,
                $x_{n}$ converges.
            \end{proof}
            \begin{theorem}
                If $(X,d)$ is complete and $S$ is a closed
                subset of $X$, then $(S,d_{S})$ is complete,
                where $d_{S}$ is the restriction of
                $d$ onto $S$.
            \end{theorem}
            \begin{proof}
                Let $x_{n}$ be a Cauchy sequence in $S$. Then
                $x_{n}\rightarrow{x}$, $x\in{X}$,
                since $x_{n}$ is Cauchy in $X$
                and $X$ is complete. Since $S$ is closed,
                $x\in{S}$. Therefore, etc.
            \end{proof}
            \begin{theorem}
                If $(X,d)$ is complete and
                $S\subset{X}$ is not closed,
                then $(S,d_{S})$ is not complete.
            \end{theorem}
            \begin{proof}
                If $S$ is not closed then there
                is a convergent sequence $x_{n}\in{S}$
                whose limit it not in $S$. But
                then $x_{n}$ is a Cauchy sequence in
                $X$, and therefore is also a
                Cauchy sequence in $S$, but
                $x_{n}$ does not converge in $S$.
                Therefore $(S,d_{S})$ is not complete.
            \end{proof}
            Recall that $c_{0}$ is the set of sequences which
            tend to zero. That is, it is the set of
            null sequences.
            \begin{theorem}
                $c_{0}$ is a closed subset of
                $(\ell^{\infty},d_{\infty})$
            \end{theorem}
            \begin{proof}[proof 1]
                Let $x_{n}$ be a sequence in $c_{0}$
                that converges to $z\in\ell^{\infty}$
                with respect to $d_{\infty}$.
                Then
                $\sup\{|x_{n}(k)-z_{k}|\}\rightarrow{0}$.
                We need to show that $z\in{c_{0}}$.
                Let $\varepsilon>0$. Let $N_{1}\in\mathbb{N}$
                be such that
                $n>N$ implies
                $\sup\{|x_{n}(k)-z_{k}\}<\frac{\varepsilon}{2}$.
                But $x_{n}\in{c_{0}}$ for all $n$, and thus
                $x_{n}(k)\rightarrow{0}$ as $k\rightarrow\infty$.
                Thus, there is an $N_{2}\in\mathbb{N}$
                such that $n>N_{2}$ implies
                $|x_{n}(k)<\varepsilon$.
                But then for $n>\max\{N_{1},N_{2}\}$,
                $|z_{k}|\leq|z_{k}-x_{n}(k)|+|x_{n}(k)|%
                 <\varepsilon$.
            \end{proof}
            \begin{proof}[Proof 2]
                We can also show that
                $c_{0}^{C}$ is open.
                Let $x\in{c_{0}^{C}}$. Then there is
                an $r>0$ and a subsequence
                $x_{k_{n}}$ of $x$ such that
                $x_{k_{n}}>r$ for all $n$.
                But then $B_{r/2}(x)$ is
                an open ball contained in $c_{0}^{C}$.
                For if $y\in{B_{r/2}(x)}$, then
                $d_{\infty}(x,y)%
                 =\sup\{|x_{n}-y_{n}|\}<r<2$,
                and thus
                $|y_{k_{n}}-x_{k_{n}}|<r/2$,
                and there for $|y_{k_{n}}|>r/2$.
                Thus, $y$ is not a null sequence and
                $c_{0}^{C}$ is open. So
                $c_{0}$ is closed.
            \end{proof}
            Let $X$ be the set of sequences with only
            finitely many nonzero terms.
            Then $(X,d_{\infty}$ is not complete.
            Let $x_{1}=(1,0,0,\hdots)$,
            $x_{2}=(1,1/2,0,0,\hdots)$,
            $x_{n}=(1,1/2,\hdots,1/n,0,0,\hdots)$.
            Then
            $d_{\infty}(x_{n},x_{m})=1/\max\{n,m\}\rightarrow{0}$.
            But clearly
            $x_{n}\rightarrow(1,1/2,\hdots,1/n,\hdots)$, which
            is an element of $c_{0}$, but not an element
            of $X$. Thus $X$ is not closed, and therefore is
            not complete. Returning to $C[0,1]$, when we had
            that sequence of continuous functions that clearly
            converged to a discontinuous functions, we still
            needed to show that there is no continuous function
            that the $x_{n}(t)$ converged to. Here we've embedded
            $X$ into a bigger space, shown that the
            sequence converges to something outside of $X$,
            in our case an element of
            $c_{0}\setminus{X}$, and then used the uniqueness
            of limits to show that the limit does
            not converge in $X$.
        \subsection{Banach's Fixed Point Theorem}
            If $(X,d)$ is a complete metric space,
            and if $T:X\rightarrow{X}$ satisfies
            the property that, for all $x$ and $y$
            in $X$, $d(T(x),T(y))<kd(x,y)$ for
            some $k<1$, then $T$ has a unique
            point $x$, called a fixed point,
            such that $T(x)=x$.
            \begin{definition}
                A contraction of a metric
                space $(X,d)$ is a function
                $T:{X}\rightarrow{X}$ such that there
                exists a $k\in(0,1)$ such that
                for all $x,y\in{X}$,
                $d(T(x),T(y))<kd(x,y)$.
            \end{definition}
            \begin{definition}
                A fixed point of a function
                $f:X\rightarrow{X}$ is a point
                $x\in{X}$ such that
                $f(x)=x$.
            \end{definition}
            \begin{theorem}[%
                Banach's Fixed Point Theorem%
            ]
                If $(X,d)$ is a complete
                metric space and $T:X\rightarrow{X}$
                is a contraction, then there is
                a unique fixed point $x\in{X}$
                with respect to $T$.
            \end{theorem}
            \begin{definition}
                A Lipschitz continuous function is a
                function $f:[a,b]\rightarrow\mathbb{R}$
                such that there is an $L\in\mathbb{R}$
                such that
                $|f(x)-f(y)|<L|x-y|$ for all
                $x,y\in[a,b]$.
            \end{definition}
            This says that the slopes of the
            secant lines of the
            function are bounded. The square root
            function $y=\sqrt{x}$ is an example
            of a function that is not Lipschitz. The
            slopes of secant lines go to infinity
            as the points tend towards the origin.
            \begin{theorem}[Picard's Theorem]
                If $f:[a,b]\times\mathbb{R}%
                    \rightarrow\mathbb{R}$
                is Lipschitz continuous,
                Then there is a unique function
                $x:[a,b]\rightarrow\mathbb{R}$
                such that
                $\frac{dx}{dt}=f(t,x(t))$ and $x(a)=a$.
            \end{theorem}
            \begin{proof}
                We prove Picard by using the
                Banach Fixed Point Theorem. First
                we write the problem as an integral
                equation.
                If $\dot{x}=f(t,x(t))$, then:
                \begin{equation*}
                    x(t)
                    =\int_{a}^{t}\frac{dx}{dt}dt
                    =x_{0}+\int_{a}^{t}f(t,x(t))dt
                \end{equation*}
                Let $(X,d)$ be $C[a,b]$ with the
                supremum norm $d_{\infty}$. Then
                $(x,d)$ is a complete metric space.
                Let $T:{X}\rightarrow{X}$ be defined
                by:
                \begin{equation*}
                    Tx=x_{0}+\int_{a}^{t}f(t,x(t))dt
                \end{equation*}
                All we need to do is show that $T$ is
                a contraction. Applying the
                Banach Fixed Point theorem then
                shows that there is a unique
                fixed point of $T$, thus showing
                that there is a unique solution
                to our original initial value problem.
                If $x,y\in{X}$, then:
                \begin{align*}
                    d(Tx,Ty)
                    &=\sup\{|Tx(t)-Ty(t)|\}\\
                    &=\sup\{
                        (x_{0}+
                         \int_{a}^{t}f(t,x(t))dt)
                       -(x_{0}+
                         \int_{a}^{t}f(t,y(t))dt)
                    \}\\
                    &=\sup\{
                        \int_{a}^{t}f(t,x(t))dt)-
                        \int_{a}^{t}f(t,y(t))dt)
                    \}\\
                    &\leq\int_{a}^{t}|
                        f(t,x(t))-f(t,y(t))|dt
                \end{align*}
                But from the Lipschitz continuity
                of $f$, we have:
                \begin{align*}
                    d(Tx,Ty)&\leq
                    L\int_{a}^{t}|x(t)-y(t)|dt\\
                    &\leq{L}(t-a)d(x,y)\\
                    &\leq{L}(b-a)d(x,y)
                \end{align*}
                So $T$ is a contraction for
                $L(b-a)<1$. Usually we can
                extend this solution by taking
                $b$ as the initial condition and
                stepping forward one interval
                at a time. We'll take a different
                approach. We have that
                $d(Tx,Ty)\leq{L}(b-a)d(x,y)$. From
                this, we obtain:
                \begin{align*}
                    d(T^{2}x,T^{2}y)
                    &\leq{L}\int_{a}^{b}d(Tx,Ty)dt\\
                    &\leq{L}\int_{a}^{t}
                        L(t-a)d(x,t)dt\\
                    &=\frac{L^{2}}{2}(t-a)^{2}d(x,y)\\
                    &\leq
                    \frac{L^{2}}{2}(b-a)^{2}d(x,y)
                \end{align*}
                Applying induction, we have:
                \begin{equation*}
                    d(T^{n}x,T^{n}y)
                    \leq\frac{L^{n}}{n!}(b-a)^{n}
                \end{equation*}
                But this tends to zero, and thus
                there is an $N$ such that,
                for all $n>N$, $T^{n}$ is a
                contraction. But then, by the
                Banach Fixed Point Theorem, there
                is a unique point $x$ such that
                $T^{n}x=x$. But then
                $Tx=T^{n}(Tx)$, and thus
                $Tx$ is a fixed point of
                $T^{n}$. But the fixed point of
                $T^{n}$ is unique, and $x$ is a
                fixed point. Therefore
                $Tx=x$. Therefore, etc.
            \end{proof}
            Without Lipschitz continuous you may
            lose uniqueness, but you still have
            existence. This is Peano's theorem.
            An example is $\dot{x}=\sqrt{x}$
            with $x(0)=0$.
            This has solutions $x(t)=0$ and
            $(t)=t^{2}/4$. Now back to compactness.
            \subsubsection{Compactness}
                \begin{definition}
                    A metric space $(X,d)$ is
                    sequentially compact if every
                    sequence in $X$ has a convergent
                    subsequence.
                \end{definition}
                In topology there is a difference
                between sequential compactness
                and regular compactness, but in
                metric spaces they turn out
                to be the same.
                A subset of $S$ of $X$ is
                compact if every sequence in
                $S$ has a subsequence which converges.
                That is, $(S,d)$ is compact.
                \begin{theorem}
                    A subset $S$ of a compact
                    metric space $(X,d)$ is compact
                    if and only if $S$ is closed.
                \end{theorem}
                \begin{proof}
                    For let $x_{n}$ be a sequence
                    in $S$. Then $x_{n}$ is a
                    sequence in $X$ and thus there
                    is a convergent subsequence
                    $x_{k_{n}}$ with a limit $x$.
                    But $x_{k_{n}}$ is in $S$ and
                    $S$ is closed, and therefore
                    $x$ is in $S$. Thus, $S$
                    is compact. Conversely, if
                    $S$ is compact, suppose it is
                    not closed. Then there is a point
                    $y\in{X}$ such that $y$ is a
                    limit point of $S$ but not
                    contained in $S$. Let
                    $x_{n}$ be a sequence that
                    converges to $y$. Then, as
                    $S$ is compact, there is
                    a convergent subsequence. But
                    the limit of this subsequence
                    is $y$, a contradiction as
                    $y\notin{S}$. Therefore $S$
                    is closed.
                \end{proof}
                \begin{theorem}
                    If $(X,d)$ is a compact metric
                    space, then
                    $(X,d)$ is complete.
                \end{theorem}
                \begin{proof}
                    If $x_{n}$ is Cauchy in $X$,
                    then there is a convergent
                    subsequence $x_{k_{n}}$
                    in $X$. But if $x_{k_{n}}$
                    converges to $x$, then
                    $x_{n}$ converges to $x$ as
                    well, as $x_{n}$ is Cauchy.
                    Therefore, $(X,d)$ is complete.
                \end{proof}
                \begin{theorem}[Heine-Borel Theorem]
                    A subset of
                    $\mathbb{R}^{n}$ is
                    compact if and only if
                    it is closed and bounded.
                \end{theorem}
                \begin{example}
                    The closed unit ball
                    of $\ell^{p}$ is not compact,
                    if $1\leq{p}\leq{\infty}$.
                    Let $x_{n}(m)$ be the sequence
                    (of sequences) such that
                    $x_{n}(m)=1$ if $n=m$, and
                    zero otherwise. Then
                    $d_{p}(x_{n},x_{m})=2^{1/p}$,
                    so $x_{n}$ has no subsequence
                    which is Cauchy. But then there
                    is no convergent subsequence
                    either, and therefore
                    $\ell^{p}$ is not compact.
                \end{example}
                \begin{example}
                    The closed unit ball in
                    $(C[0,1],d_{\infty})$ is
                    not compact. For let
                    $x_{n}(t)=t^{2^{n}}$. Then
                    (Do some calculus) the maximum of
                    $d(x_{n},x_{n+1})$ is always
                    $1/4$. So this has no subsequence
                    which is Cauchy, and thus no
                    convergent subsequence exists.
                \end{example}
                \begin{definition}
                    A metric space $X$ is totally
                    bounded if for all
                    $\varepsilon>0$ there is a finite
                    number of points $x_{n}$ such
                    that $B_{\varepsilon}(x_{n})$
                    covers the entirety of $X$.
                \end{definition}
                \begin{theorem}
                    A compact metric space is
                    totally bounded.
                \end{theorem}
                \begin{proof}
                    Suppose not. Then there is an
                    $\varepsilon>0$ such that
                    no finite collection
                    $B_{\varepsilon}(x_{n})$
                    is a covering of $X$. Let
                    $x_{1}\in{X}$. Then
                    $B_{\varepsilon}(x_{1})$ is not
                    $X$. Thus there is an $x_{2}$
                    such that
                    $x_{2}\notin%
                     B_{\varepsilon}(x_{1})$.
                    But also
                    $B_{\varepsilon}(x_{1})\cup%
                     B_{\varepsilon}(x_{2})$ is
                    not the entirety of $X$.
                    Continuing we have that there
                    is a sequence $x_{n}$ such that,
                    for all $n\ne{m}$,
                    $d(x_{n},x_{m})\geq{\varepsilon}$.
                    So there is no convergent
                    subsequence. But $X$ is
                    compact, a contradiction.
                    Therefore, etc.
                \end{proof}
                There are metric spaces that are
                bounded but not totally bounded.
                For let
                $X=\mathbb{R}$ and $d$ be the
                discrete metric. Then, for
                $\varepsilon=1/2$, the is no
                finite covering. Every point needs
                it's own ball, so the covering is
                uncountable.
                \begin{theorem}
                    If $(X,d)$ is complete and
                    totally bounded, then it
                    is compact.
                \end{theorem}
                \begin{proof}
                    Let $x_{n}$ be a sequence
                    in $X$. Let $\varepsilon=1$. Then
                    there are finitely many points
                    $y_{k}$ such that
                    $B_{\varepsilon}(y_{k})$ covers
                    $X$. Then one of these
                    balls has infinitely many of
                    the $x_{n}$. Similarly, for
                    $\varepsilon=\frac{1}{n}$, there
                    is a finite number of points
                    $y_{k}$ such that
                    $B_{\frac{1}{n}}(y_{k})$ covers
                    $X$. Thus there is a point with
                    infinitely many of the $x_{n}$
                    in it. So, we can find a
                    subsequence such that, for
                    $n,m>N$,
                    $d(x_{k_{n}},x_{k_{m}})<%
                     \frac{1}{N}$. But $(X,d)$ is
                    complete, and therefore
                    $x_{k_{n}}$ converges. Therefore
                    $x_{n}$ has a convergent
                    subsequence. Thus, $(X,d)$ is
                    compact.
                \end{proof}
                \begin{theorem}
                    Compact spaces are separable.
                \end{theorem}
                \begin{proof}
                    If $X$ is compact, then
                    it is totally bounded. But
                    then, for $\varepsilon=1/n$
                    there is a finite covering of
                    $X$ with balls of radius
                    $\varepsilon$. Then,
                    taking all of the
                    centers of all of the points
                    for all $n$ (Countable union
                    of finite points is countable),
                    we obtain a countable dense
                    subset.
                \end{proof}
                \begin{example}
                    There are ``infinite dimension''
                    sets that are also compact. Two
                    in particular worth mentioning.
                    The first is the hilbert Cube.
                    It's a subset of $\ell^{2}$
                    whose elements are such that
                    $|x_{n}|<1/n$. That is, elements
                    are sequences whose $n^{th}$
                    elements are less than
                    $1/n$. This is compact.
                    Arzela-Ascoli. Peano.
                \end{example}
    \section{Normed and Inner Product Spaces}
        \subsection{Basic Definitions}
            We're finally going to put some structure on these
            sets, and talk about vector spaces. In a metric
            space, the only thing you can really talk about
            is the distance between points. In a vector space
            we have a lot more structure. We will start off
            with vector spaces over the reals $\mathbb{R}$.
            The main properties are that there is a
            $\mathbf{0}$ element, addition is well defined
            and is both associative and commutative,
            there is a notion of scalar multiplication that
            is associative, and the distributive law holds.
            \begin{example}
                $\mathbb{R}^{n}$, with it's usual notion
                of addition, and with scalar multiplication
                defined over $\mathbb{R}$, is a vector space.
            \end{example}
            \begin{definition}
                A norm on a vector space $X$ over $\mathbb{R}$
                is a function $\norm{}:X\rightarrow\mathbb{R}$
                such that:
                \begin{enumerate}
                    \item For all $\mathbf{x}\in{X}$,
                          $\norm{\mathbf{x}}\geq{0}$ and
                          $\norm{\mathbf{x}}=0$ if and only
                          if $\mathbf{x}=\mathbf{0}$.
                          \hfill[Positive Definiteness]
                    \item For all $\mathbf{x}\in{X}$ and
                          $c\in\mathbb{R}$,
                          $\norm{c\mathbf{x}}%
                           =|c|\norm{\mathbf{x}}$
                          \hfill[Homogeneity]
                    \item For all $\mathbf{x},\mathbf{y}\in{X}$,
                          $\norm{\mathbf{x}+\mathbf{y}}%
                           \leq\norm{\mathbf{x}}%
                           +\norm{\mathbf{y}}$
                          \hfill[Triangle Inequality]
                \end{enumerate}
            \end{definition}
            We have seen before that
            $d(\mathbf{x},\mathbf{y})%
             =\norm{\mathbf{x}-\mathbf{y}}$
            defines a metric, and thus $(X,d)$ is a metric space.
            Thus, for every vector space there is an associated
            metric space, the metric $d$ called the
            \textit{induced} metric.
            \begin{definition}
                A normed vector space is a vector space
                $X$ over $\mathbb{R}$ with a norm
                $\norm{}$ on $X$.
            \end{definition}
            \begin{example}
                $\mathbb{R}^{n}$ with
                $\norm{\mathbf{x}}_{p}$, for $p\geq{1}$,
                is a normed vector space.
            \end{example}
            \begin{example}
                $\ell^{p}$ with $\norm{x}_{p}$ is
                also a normed vector space.
            \end{example}
            \begin{example}
                $C[a,b]$ equipped with the supremum norm,
                $\norm{x(t)}_{\infty}$,
                is a normed vector space.
            \end{example}
        \subsubsection{Inner Product Spaces}
            \begin{definition}
                An inner product on a vector space
                $X$ over $\mathbb{R}$ is a function
                $\langle\rangle:X\rightarrow\mathbb{R}$
                such that:
                \begin{enumerate}
                    \item For all $x\in{X}$,
                          $\langle{\mathbf{x},\mathbf{x}}%
                           \rangle\geq{0}$
                          and
                          $\langle\mathbf{x},\mathbf{x}\rangle=0$
                          if and only
                          if $\mathbf{x}=\mathbf{0}$.
                          \hfill[Positive Definiteness]
                    \item For all $\mathbf{x},\mathbf{y}\in{X}$,
                          $\langle\mathbf{x},\mathbf{y}\rangle%
                           =\langle\mathbf{y},\mathbf{x}\rangle$
                          \hfill[Symmetry]
                    \item For all
                          $\mathbf{x},\mathbf{y},\mathbf{z}%
                           \in{X}$
                          and all $\alpha,\beta\in\mathbb{R}$,
                          $\langle\alpha\mathbf{x}%
                           +\beta\mathbf{y},\mathbf{z}\rangle%
                           =\alpha\langle\mathbf{x},\mathbf{z}%
                           \rangle+\beta\langle\mathbf{y},%
                           \mathbf{z}\rangle$
                          \hfill[Linearity]
                \end{enumerate}
            \end{definition}
            \begin{example}
                $\mathbb{R}^{2}$ with
                $\langle(x_{1},x_{2}),(y_{1},y_{2})\rangle%
                 =x_{1}y_{1}+x_{2}y_{2}$ is an inner product.
                Replacing this with $\mathbb{R}^{n}$ and doing
                $\sum_{k=1}^{n}x_{k}y_{k}$ is also an inner
                product. This is the usual dot product that one
                sees in a vector calculus course. In $\ell^{2}$,
                $\sum_{k=1}^{\infty}x_{k}y_{k}$ is an inner
                product as well. Note also that
                $\sum|x_{i}y_{i}|$ converges since
                $|x_{i}y_{i}|\leq\frac{1}{2}|x_{i}^{2}|%
                 +\frac{1}{2}|y_{i}|^{2}$.
            \end{example}
            \begin{example}
                In $C[a,b]$, let
                $\langle{x(t),y(t)}\rangle%
                 =\int_{a}^{b}x(t)y(t)dt$. This defines an
                inner product.
            \end{example}
            \begin{definition}
                An inner product space is a vector space
                $X$ over $\mathbb{R}$ with an inner product
                $\langle\rangle$.
            \end{definition}
            \begin{theorem}[Cauchy-Schwarz Inequality]
                If $X$ is an inner product space
                and $x,y\in{X}$, then
                $|\langle{x,y}\rangle<\norm{x}\norm{y}$
            \end{theorem}
            \begin{proof}
                For all $y\in\mathbb{R}$,
                $\langle{x+ty,x+ty}\rangle%
                 =\langle{x,x}\rangle%
                 +2t\langle{x,y}\rangle%
                 +t^{2}\langle{y,y}\rangle%
                 =\norm{x}^{2}+2t\langle{x,y}\rangle%
                 +t^{2}\norm{y}^{2}$. Thus we have a
                quadratic in $t$. But this is always positive,
                and thus the discriminant must be non-positive. Therefore
                $(2\langle{x,y})^{2}-4\norm{x}^{2}\norm{y}^{2}%
                 \leq{0}$
                and thus
                $|\langle{x,y})|\leq\norm{x}\norm{y}$.
            \end{proof}
            \begin{theorem}
                If $X$ is a vector space over $\mathbb{R}$
                and $\langle\rangle$ is an inner product,
                then
                $\norm{\mathbf{x}}%
                 =\sqrt{\langle\mathbf{x},\mathbf{y}\rangle}$
                is a norm on $X$.
            \end{theorem}
            \begin{proof}
                Positivity, homogeneity, and definiteness are
                pretty easy. The only tricky thing to check is
                the triangle inequality. We have that
                $\norm{x+y}=\langle{x+y,x+y}\rangle$,
                and this simplify to
                $\norm{x}^{2}+2\langle{x,y}\rangle+\norm{y}^{2}$.
                But from the Cauchy-Schwartz inequality, we
                have $\langle{x,y}\rangle\leq\norm{x}\norm{y}$.
                Thus
                $\norm{x+y}^{2}\leq\norm{x}^{2}%
                 +2\norm{x}\norm{y}+\norm{y}^{2}%
                 =(\norm{x}+\norm{y})^{2}$. Taking square roots
                 completes the theorem.
            \end{proof}
            In $\mathbb{R}^{n}$, the Cauchy-Schwartz inequality
            says that the dot product of two vectors is less
            than or equal to the product of the magnitude
            of the two vectors.
            This is obvious from the fact that the dot product
            of two vector is the product of the magnitudes and
            the \textit{cosine} of the angle between them.
            Since the cosine of a number is less than or equal
            to one, this would complete the theorem.
            In $\ell^{p}$ and $L^{p}$ spaces, this is the
            special case of the H\"{o}lder inequality for
            when $p=q=2$.
        \subsubsection{Convergence in Normed Spaces}
            In a metric space, convergence meant that
            $d(x_{n},x)\rightarrow{0}$. In a normed space
            we have the induced metric, and thus we may define
            convergence as $\norm{x_{n}-x}\rightarrow{0}$.
            \begin{definition}
                A convergent sequence in a normed space $X$
                is a sequence $x_{n}$ such that there is an
                $x\in{X}$ such that
                $\norm{x_{n}-x}\rightarrow{0}$.
            \end{definition}
            Since
            $\norm{y}=\norm{(y-x)+x}\leq\norm{y-x}+\norm{x}$,
            it follows that
            $|\norm{x}-\norm{y}|\leq\norm{x-y}$.
            But then if $x_{n}\rightarrow{x}$, then
            $|\norm{x_{n}}-\norm{x}|\leq\norm{x_{n}-x}$,
            and $\norm{x_{n}-x}\rightarrow{0}$. Therefore
            $\norm{x_{n}}\rightarrow\norm{x}$. That is,
            the norm function is a continuous function.
            Similarly, if $x_{n}\rightarrow{x}$, then
            $\langle{x_{n},y}\rangle\rightarrow%
             \langle{x,y}\rangle$.
            In fact, if $x_{n}\rightarrow{x}$ and
            $y_{n}\rightarrow{y}$, then
            $\langle{x_{n},y_{n}}\rangle%
             \rightarrow\langle{x,y}\rangle$. To see this, we
            have
            $\langle{x_{n},y_{n}}\rangle-\langle{x,y}\rangle%
             =\langle{x_{n}-x,y}\rangle+\langle{x,y-y_{n}}\rangle$
            and therefore
            $|\langle{x_{n},y_{n}}\rangle-\langle{x,y}\rangle%
             \leq\norm{x_{n}-x}\norm{y_{n}}%
             +\norm{x}\norm{y-y_{n}}$. But $\norm{x-x_{n}}\rightarrow{0}$
            and $\norm{y-y_{n}}\rightarrow{0}$. But also
            $\norm{y_{n}}=\norm{(y_{n}-y)+y}\leq\norm{y_{n}-y}+\norm{y}$,
            which is bounded. Therefore
            $\langle{x_{n},y_{n}}-\langle{x,y}\rangle\rightarrow{0}$.
            So inner product spaces and normed spaces are metric spaces
            and we can define everything we did for metric spaces and all
            of the previous results remain true. That is, the notions and
            theorems pertaining to convergence, completeness, compactness,
            the notion of open and closed. All of these still make sense in
            these new spaces.
        \subsubsection{Banach Spaces and Hilbert Spaces}
            \begin{definition}
                A Banach Space is a normed vector space $X$ that is
                complete with respect to the induced metric.
            \end{definition}
            \begin{definition}
                A Hilbert Space is an inner product space $X$ that is
                complete with respect to the induced metric.
            \end{definition}
        \subsubsection{Linear Operators}
            Let $X$ and $Y$ be normed spaces. A mapping
            $T:X\rightarrow{Y}$ is called a linear operator if, for
            all $x,y\in{X}$, and for all $\alpha,\beta\in\mathbb{R}$,
            $T(\alpha{x}+\beta{y})=\alpha{T(x)}+\beta{T(y)}$. Usually, with
            operators, we simply write $Tx$ and $Ty$. Similar to how
            we write matric multiplication over vectors. In $\mathbb{R}^{n}$,
            every $n\times{n}$ matrix defines a linear operator.
            \begin{definition}
                A linear operator from a normed vector space $X$ to
                a normed vector space $Y$ is a function
                $T:X\rightarrow{Y}$ such that, for all $x,y\in{X}$
                and for all $\alpha,\beta\in\mathbb{R}$,
                $T(\alpha{x}+\beta{y})=\alpha{Tx}+\beta{Ty}$.
            \end{definition}
            \begin{definition}
                A bounded linear operator from a normed vector space
                $X$ to a normed vector space $Y$ is a linear operator
                $T:X\rightarrow{Y}$ such that there is a $K\in\mathbb{R}$
                such that for all $x\in{X}$, $\norm{Tx}\leq{K}\norm{x}$
            \end{definition}
            In a just world, ``bounded'' would mean
            $\norm{Tx}\leq{K}$. However, the only linear mapping that does
            this is the zero mapping. For if $\norm{Tx}=1$,
            then $\norm{T(2x)}=2$, and so on, and thus no linear mapping
            is bounded (With the exception of the zero mapping).
            Boundedness of a norm $T:X\rightarrow{Y}$ depends on
            the norms of the space.
            \begin{theorem}
                Bounded linear operators are continuous.
            \end{theorem}
            \begin{proof}
                If $x_{n}\rightarrow{x}$, then
                $\norm{Tx_{n}-Tx}=\norm{T(x_{n}-x)}$. But
                $T$ is bounded, and thus there is a $K$ such that
                $\norm{T(x_{n}-x)}\leq{K}\norm{x_{n}-x}$. But
                $\norm{x_{n}-x}\rightarrow{0}$. Therefore, etc.
            \end{proof}
            The converse is also true.
            \begin{theorem}
                If $T$ is a continuous linear operator,
                than there exists a $\delta>0$ such that for
                all $x\in{B}_{\delta}(0)$,
                $\norm{Tx-T0}<1$. But from linearity,
                $T0=0$, and thus $\norm{Tx}<1$. Then for any
                $z\in{Z}$, we have
                $\norm{\frac{\delta}{2}\frac{z}{\norm{z}}}=\frac{\delta}{2}$,
                and thus $\norm{T(\frac{\delta}{2}\frac{z}{\norm{z}})}<1$.
                Letting $K=\delta$, we have
                $\norm{Tx}<K\norm{x}$. Thus, $T$ is bounded.
            \end{theorem}
            Continuity at 0 implies uniform continuity since
            if $x_{n}-y_{n}\rightarrow{0}$, then
            $\norm{Tx_{n}-Ty_{n}}=\norm{T(x_{n}-y_{n})}%
             \leq{K}\norm{x_{n}-y_{n}}\rightarrow{0}$.
            The set of bounded linear operators form a vector space,
            where addition is $(S+t)(x)=(Sx)+(Tx)$, and scalar multiplication
            is defined by $(\alpha{T})(x)=\alpha(Tx)$. We must show that
            when you add two bounded linear operators, the result is a
            bounded linear operator.
            \begin{theorem}
                If $T_{1}:X\rightarrow{Y}$ and $T_{2}:X\rightarrow{Y}$
                are bounded linear operators, then $T_{1}+T_{2}$ is a
                bounded linear operator.
            \end{theorem}
            \begin{proof}
                For let $T_{1}$ and $T_{2}$ be bounded. Then there are
                $K_{1},K_{2}$ such that, for all $x\in{X}$,
                $\norm{T_{1}x}\leq{K_{1}}\norm{x}$ and
                $\norm{T_{2}x}\leq{K_{2}}\norm{x}$. But then
                $\norm{(T_{1}+T_{2})x}=\norm{T_{1}x+T_{2}x}%
                 \leq\norm{T_{1}x}+\norm{T_{2}x}%
                 \leq{K_{1}}\norm{x}+K_{2}\norm{x}$. Let $K=K_{1}+K_{2}$.
            \end{proof}
            \begin{theorem}
                If $T:X\rightarrow{Y}$ is a bounded linear operator, and
                $\alpha\in\mathbb{R}$, then $\alpha{T}$ is a bounded
                linear operator.
            \end{theorem}
            \begin{proof}
                For
                $\norm{\alpha{Tx}}=|\alpha|\norm{Tx}%
                 \leq|\alpha|K\norm{x}=K\norm{\alpha{x}}$.
            \end{proof}
            We write $B(X,Y)$ to denote the set of bounded linear
            operators from $X$ to $Y$. That is, linear operators
            $T:X\rightarrow{Y}$.
            We can define a norm on $B(X,Y)$ as follows:
            $\norm{T}_{B}%
             =\sup_{x\in{X},x\ne{0}}\{\frac{\norm{Tx}}{\norm{x}}\}$.
            This is the ``Smallest $K$,'' used as a bounded for the linear
            operator $T$. This shows that
            $\norm{Tx}_{Y}\leq\norm{T}_{B}\norm{x}_{X}$.
        \subsection{Lecture 7: October 22, 2018}
            \subsubsection{Bounded Linear Operators}
                A bounded linear operator is a function
                $T:X\rightarrow{Y}$ between normed spaces
                $X$ and $Y$ such that $T$ is linear, and
                there exists a $K\in\mathbb{R}$ such that,
                for all $x\in{X}$,
                $\norm{Tx}_{Y}\leq{K}\norm{x}_{X}$. The
                norm of $T$, $\norm{T}$, is then defined
                as the smallest such $K$. Equivalently:
                \begin{equation*}
                    \norm{T}=
                    \sup\Big\{\frac{\norm{Tx}_{Y}}{\norm{x}_{X}}:
                              x\in{X},x\ne{0}\Big\}
                    =\sup\{\norm{Tx}_{Y}:\norm{x}_{X}=1\}
                \end{equation*}
                The set of all bounded linear operators
                from a normed space $X$ to a normed space
                $Y$ is denoted $B(X,Y)$. This is a vector
                space with addition defined as
                $(T+S)x=(Tx)+(Sx)$ and $(aT)x=a(Tx)$.
                \begin{theorem}
                    $\norm{T}$ defines a norm on
                    $B(X,Y)$.
                \end{theorem}
                \begin{proof}
                    For $\norm{T}\geq{0}$ and
                    $\norm{Tx}=0$ if and only if
                    $Tx=0$ for all $x\in{X}$, and thus
                    $T$ is the zero operator. If
                    $\alpha\in\mathbb{R}$, then:
                    \begin{align*}
                        \norm{\alpha{T}}
                        &=\sup\Big\{
                            \frac{\norm{\alpha{T}x}_{Y}}{\norm{x}_{X}}:
                            x\in{X},x\ne{0}\Big\}\\
                        &=|\alpha|\sup\Big\{
                            \frac{\norm{Tx}_{Y}}{\norm{x}_{X}}:
                            x\in{X},x\ne{0}\Big\}\\
                        &=|\alpha|\norm{T}
                    \end{align*}
                    Finally, if $S,T\in{B}(X,Y)$, then:
                    \begin{align*}
                        \norm{S+T}&=\sup\Big\{
                            \frac{\norm{(S+t)x}_{Y}}{\norm{x}_{X}}:
                            x\in{X},x\ne{0}\}\\
                        &=\sup\Big\{
                            \frac{\norm{Sx+Tx}_{Y}}{\norm{x}_{X}}:
                            x\in{X},x\ne{0}\Big\}\\
                        &\leq\sup\Big\{
                            \frac{\norm{Sx}_{y}+\norm{Tx}_{Y}}
                                 {\norm{x}_{X}}:
                            x\in{X},x\ne{0}\Big\}\\
                        &\leq\norm{T}+\norm{S}
                    \end{align*}
                \end{proof}
                \begin{theorem}
                    If $Y$ is a Banach space, and 
                    if $X$ is a normed space, then
                    $B(X,Y)$ is a Banach space.
                \end{theorem}
                \begin{proof}
                    For let $T_{n}$ be a Cauchy sequence
                    in $B(X,Y)$ and let $\varepsilon>0$.
                    Then there exists $N_{0}\in\mathbb{N}$
                    such that for all $n,m>N_{0}$,
                    $\norm{T_{n}-T_{m}}<\varepsilon$. That is,
                    for all $n,m>N_{0}$:
                    \begin{align*}
                        \sup\Big\{
                            \frac{\norm{T_{n}x-T_{m}x}_{Y}}
                                 {\norm{x}_{X}}:
                            x\in{X},x\ne{0}\Big\}
                        &\leq\varepsilon\\
                        \Rightarrow
                        \frac{\norm{T_{n}x-T_{m}y}_{Y}}
                             {\norm{x}_{X}}
                        &\leq\varepsilon
                    \end{align*}
                    That is, $T_{n}x$ is a Cauchy sequence
                    in $Y$ for any fixed value $x\in{X}$.
                    But $Y$ is a Banach space, and is therefore
                    complete. But then if $T_{n}x$ is a Cauchy
                    sequence in $Y$ it has a limit $y\in{Y}$.
                    Let $Tx=\lim_{n\rightarrow\infty}T_{n}x$
                    for all $x\in{X}$.
                    Then $T\in{B(X,Y)}$. For:
                    \begin{equation*}
                        T(x+y)
                        =\lim_{n\rightarrow\infty}T_{n}(x+y)
                        =\lim_{n\rightarrow\infty}(T_{n}x+T_{n}y)
                        =Tx+Ty
                    \end{equation*}
                    And similarly $(\alpha{T})x=\alpha{T}x$.
                    Lastly, $T$ is bounded. For all $n,m>N$ we have
                    $\norm{T_{n}x-T_{m}x}_{Y}/\norm{x}_{X}<\varepsilon$.
                    Taking the limit on $m$, we have
                    $\norm{Tx-T_{n}x}_{Y}/\norm{x}_{X}\leq\varepsilon$
                    for all $n>N_{0}$. Thus,
                    $\norm{T_{n}x-Tx}_{X}\leq\varepsilon\norm{x}_{X}$.
                    But
                    $\norm{Tx-T_{n}x}_{Y}=\norm{T_{n}x-(T_{n}-Tx)}_{Y}$,
                    and therefore
                    $\norm{Tx}\leq\varepsilon\norm{x}_{X}+\norm{T_{n}x}$,
                    and $\norm{T_{n}x}\leq\norm{T_{n}}$, and therefore
                    $\norm{Tx}\leq\varepsilon\norm{X}_{X}+%
                     \norm{T}\norm{x}_{X}$. But then
                    $\norm{Tx}_{Y}\leq%
                     (\varepsilon+\norm{T_{n}})\norm{x}_{X}$.
                    But $T_{n}$ is bounded, and therefore
                    $T$ is bounded. Finally, we must show that
                    $T_{n}\rightarrow{T}$ in $B(X,Y)$ with respect
                    to the norm $\norm{T_{n}-T}$. That is, we must
                    show that $\norm{T-T_{n}}\rightarrow{0}$. This
                    follows since
                    $\norm{Tx-T_{n}x}_{Y}/\norm{x}_{X}<\varepsilon$
                    for $n>N_{0}$, and therefore
                    $\norm{T-T_{n}}<\varepsilon$. Therefore, etc.
                \end{proof}
            \subsubsection{Dual Spaces}
                So if $Y$ is a Banach space, and $X$ is any normed
                space, then $B(X,Y)$ is a Banach space. One of the
                most important cases is $Y=(\mathbb{R},||)$, where
                $||$ is the normal absolute value ``norm.''
                $B(X,\mathbb{R})$ is a Banach space, and it is
                called the continuous dual space of $X$, written
                $X'$. Elements of $X'$ are called bounded linear
                functionals. These are bounded linear operators
                whose range of the operator is the real numbers.
                The characterization, or the representation, or
                realization, of these dual spaces is a major
                topic in functional analysis. A lot of these
                theorems are do to a mathematician by the name
                of Riesz.
                \begin{example}
                    A functional takes an element of a normed
                    space $X$ and spits out a real number. For
                    example, if $X$ is the space of continuous
                    functions, then the following are
                    functionals:
                    \begin{align*}
                        f_{1}(x)&=\int_{0}^{1}x(t)t^{2}\diff{t}
                        &
                        f_{2}(x)&=x(0.5)
                        &
                        f_{3}(x)&=0
                    \end{align*}
                \end{example}
                Let $X=(\mathbb{R}^{2},\ell^{1}$. What does
                $X'$ look like? That is, what is the dual
                space of $X$? let $f:X\rightarrow\mathbb{R}$
                be defined by
                $f(x_{1},x_{2})=2x_{1}-5x_{2}$. Then
                $f\in{X'}$ and $\norm{f}=5$. More generally,
                every element of $\mathbb{R}^{2}$ defines
                and element of $X'$. Given
                $(a,b)\in\mathbb{R}^{2}$, we define
                $f(x_{1},x_{2})=ax_{1}+bx_{2}$. $f$ is then linear,
                and:
                \begin{align*}
                    |f(x_{1},x_{2})|
                    &=|ax_{1}+bx_{2}|\\
                    &\leq|a||x_{1}|+|b||x_{2}|\\
                    &\leq\max\{|a|,|b|\}(|x_{1}|+|x_{2}|)\\
                    &=\norm{(a,b)}_{\infty}
                    \norm{(x_{1},x_{2})}_{\ell^{1}}
                \end{align*}
                And therefore $f$ is bounded, as $\norm{(a,b)}_{\infty}$
                is a bound. That is, $\norm{f}\leq\norm{(a,b)}_{\infty}$.
                By choosing $x=(x_{1},x_{2})$, where $x_{1}=1$ and
                $x_{2}=0$ if $|b|\leq|a|$, and $x_{1}=0$ and
                $x_{2}=1$ otherwise, we ge
                $|f|=\max\{(a,b)\}=\norm{(a,b)}_{\infty}$.
                Therefore $\norm{f}=\norm{(a,b)}_{\infty}$. On
                the other hand, if $f\in{X'}$, let
                $a=f(1,0)$ and $b=f(0,1)$. Then, for all
                $(x_{1},x_{2})\in\mathbb{R}^{2}$:
                \begin{equation*}
                    f(x_{1},x_{2})
                    =f(x_{1}(1,0)+x_{2}(0,1))
                    =x_{1}f(1,0)+x_{2}f(0,1)
                    =ax_{1}+bx_{2}
                \end{equation*}
                So the dual of $(\mathbb{R}^{2},\ell^{1})$
                looks very much like $(\mathbb{R}^{2},\ell^{\infty})$.
                In fact, $(\mathbb{R}^{2},\ell^{1})'$ and
                $(\mathbb{R}^{2},\ell^{\infty})$ are isometric
                and isomorphic. That is, we really can't tell them
                apart and we can consider them as the same thing.
                More generally,
                $(\mathbb{R}^{n},\ell^{n})'=(\mathbb{R}^{n},\ell^{\infty})$.
                Even more general, if $p$ and $q$ are exponential
                conjugates of each other (That is,
                $\frac{1}{q}+\frac{1}{p}=1$), then
                $(\mathbb{R}^{n},\ell^{p})'=(\mathbb{R}^{n},\ell^{p})$
                for all $1\leq{p}\leq\infty$. Saying $p=\infty$ is
                equivalent to saying $q=1$. Setting $p=q=2$, we have
                $(\mathbb{R}^{n},\ell^{2})'=(\mathbb{R}^{n},\ell^{2})$.
                This is true of any Hilbert space: The dual of any
                Hilbert Space $\mathcal{H}$ is itself. That is,
                $\mathcal{H}'=\mathcal{H}$. This is one of the
                Riesz Representation Theorems. In infinite dimensions,
                $(\ell^{p})'=\ell^{q}$, where $p$ and $q$ are such that
                $\frac{1}{p}+\frac{1}{q}=1$, and $1\leq{p}<\infty$.
                Now, we cannot allow $p=\infty$. For
                $(\ell^{\infty})'$ is not equal to $\ell^{1}$.
                \begin{theorem}
                    If $1\leq{p}<\infty$ and
                    $\frac{1}{p}+\frac{1}{q}=1$, then
                    $(\ell^{p})'=\ell^{q}$.
                \end{theorem}
                \begin{proof}
                    If $(f_{1},f_{2},\hdots)\in\ell^{q}$, then let
                    $f:\ell^{p}\rightarrow\mathbb{R}$ be defined by
                    $f(x_{1},x_{2},\hdots)=\sum_{k=1}^{\infty}x_{k}f_{k}$.
                    This converges from H\"{o}lder's inequality:
                    \begin{equation*}
                        \sum_{k=1}^{\infty}|x_{k}f_{k}|
                        \leq
                        \Big(\sum_{k=1}^{\infty}f_{k}^{q}\Big)^{1/q}
                        \Big(\sum_{k=1}^{\infty}x_{i}^{p}\Big)^{1/p}
                    \end{equation*}
                    And therefore
                    $|fx|=\norm{(f_{1},f_{2},\hdots)}_{q}%
                     \norm{(x_{1},x_{2},\hdots)}_{p}$. That is,
                    Moreover $f$ is linear. Therefore
                    $f\in(\ell^{p})'$ and
                    $\norm{f}\leq\norm{(f_{1},f_{2},\hdots)}_{q}$.
                    On the other hand, let
                    $x_{i}=|f_{i}|^{q/p}\sgn(f_{i}$. Then
                    \begin{equation*}
                        fx=\sum_{k=1}^{\infty}f_{k}x_{k}
                        =\sum_{k=1}^{\infty}|f_{k}|^{q/p+1}
                    \end{equation*}
                    But $\frac{1}{p}+\frac{1}{q}=1$, and thus
                    $\frac{q}{p}+1=q$. Thus:
                    \begin{align*}
                        |fx|
                        &=\sum_{k=1}^{\infty}|f_{k}|^{q}\\
                        &=\norm{(f_{1},f_{2},\hdots)}_{q}^{q}\\
                        &=\norm{(f_{1},f_{2},\hdots)}_{q}
                            \norm{(f_{1},f_{2},\hdots)}_{q}^{q-1}\\
                        &=\norm{(f_{1},f_{2},\hdots)}_{q}
                            \Big(\sum_{k=1}^{\infty}|f_{k}^{q}
                            \Big)^{\frac{q-1}{q}}
                        &=\norm{(f_{1},f_{2},\hdots)}_{q}
                            \Big(\sum_{k=1}^{\infty}|x_{k}|^{p}
                            \Big)^{1/p}\\
                        &=\norm{(f_{1},f_{2},\hdots)}_{q}\norm{x}_{p}
                    \end{align*}
                    Therefore, $\norm{f}=\norm{(f_{1},f_{2},\hdots)}_{q}$.
                    Thus, for all $y\in\ell^{q}$ there is a bounded
                    linear operator $f\in\ell^{p}$ such that
                    $\norm{y}_{\ell^{q}}=\norm{f}_{(\ell^{p})'}$. That
                    is, every $(f_{i})\in\ell^{q}$ defines an element
                    of $(\ell^{p})'$ by
                    $fx=\sum_{k=1}^{\infty}f_{k}x_{k}$, for any
                    $(x_{i})\in\ell^{p}$. So $\ell^{q}$ can
                    be \textit{embedded} into to $(\ell^{p})'$.
                    Now we need to show that this embedding is
                    the entirety of $(\ell^{p})'$. If
                    $f\in(\ell^{p})'$, let
                    $f_{i}=f(e_{i})$, where $e_{i}$ is the
                    sequence $(0,0,\hdots,1,0,0,\hdots)$, where
                    the 1 occurs in the $i^{th}$ spot. We need
                    to show that $(f_{i})\in\ell^{q}$ and
                    $fx=\sum_{k=1}^{\infty}f_{k}x_{k}$
                    for all $x\in\ell^{p}$. If $x\in\ell^{p}$, then:
                    \begin{align*}
                        x=\sum_{k=1}^{\infty}x_{k}e_{k}
                        \Rightarrow
                        fx=f\Big(\sum_{k=1}^{\infty}x_{k}e_{k}\Big)
                        =\sum_{k=1}^{\infty}f(x_{k}e_{k})
                        =\sum_{k=1}^{\infty}x_{k}f(x_{k})
                        =\sum_{k=1}^{\infty}x_{k}f_{k}
                    \end{align*}
                    Choosing $x_{k}=|f_{k}|^{q/p}\sgn(f_{k})$ and apply
                    H\"{o}lder.
                \end{proof}
        \subsection{Lecture 8: October 29, 2018}
            \subsubsection{Review}
                If $X$ is a vector space, an inner product on
                $X$ is a mapping
                $\langle,\rangle:X\times{X}\rightarrow\mathbb{R}$
                such that:
                \begin{enumerate}
                    \item $\langle{x,x}\rangle\geq{0}$ and
                          $\langle{x,x}\rangle=0$ if and only if
                          $x=0$
                    \item $\langle{x,y}\rangle=\langle{y,x}\rangle$
                    \item $\langle{ax+by,z}\rangle%
                           =a\langle{x,z}\rangle+b\langle{y,z}\rangle$
                \end{enumerate}
                Think of the dot product for vectors. This is a
                generalization of this concept. Every inner product
                on a vector space $V$ induce a norm on $V$:
                \begin{equation*}
                    \norm{x}=\sqrt{\norm{x,x}}
                \end{equation*}
                An inner product that is complete with respect to
                the induced norm is called a Hilbert Space. A mapping
                $f:X\rightarrow\mathbb{R}$ is bounded if there is a
                $K\in\mathbb{R}$ such that, for all $x\in{X}$,
                $|f(x)|\leq{L\norm{x}}$. $f$ is linear if
                $f(ax+by)=af(x)+bf(y)$, for all $x,y\in{X}$ and all
                $a,b\in\mathbb{R}$. The smallest such $K$ that works
                is called the norm of $f$, denoted $\norm{f}$. For
                all $x\in{X}$, $|f(x)|\leq\norm{f}\norm{x}$. The
                vector space of all bounded linear functionals on
                $X$ is the dual space $X'$. This is also a Banach
                space with the functional norm $\norm{f}$. One
                question that arises is, how do we know that there
                are bounded linear functionals on a space $X$? In
                the case that $X$ is a Hilbert space, this is rather
                easy, but for a more general Banach space this is not
                that trivial. For any normed space $X$ we can at least
                one bounded linear functional because the zero mapping
                $f(x)=0$ is such a functional. The question is then
                does every normed space have a bounded linear functional
                on it? The answer is yes, and this is related to
                the Hahn-Banach Theorem. As said before, in the
                Hilbert case this is rather easy.
                \begin{theorem}
                    If $X$ is a Hilbert space, then there is a
                    non-trivial bounded linear functional
                    $f:X\rightarrow\mathbb{R}$.
                \end{theorem}
                \begin{proof}
                    If $X$ is an inner product and
                    $z\in{X}$, let $f(x)=\langle{x,z}\rangle$ for
                    all $x\in{X}$. Then $f$ is linear since the
                    inner product is linear. But moreover, from
                    Cauchy-Schwarz we have:
                    \begin{equation*}
                        |f(x)|=|\langle{x,z}\rangle|
                        \leq\norm{x}\norm{z}
                    \end{equation*}
                    And thus $\norm{f}\leq\norm{z}$
                    But $|f(z)|=\norm{z}$, so
                    $\norm{f}=\norm{z}$. $f$ is a bounded
                    linear functional.
                \end{proof}
                Riesz's Representation theorem says that this is it.
                All bounded linear functionals look like this. Thus,
                if $H$ is a Hilbert space, it's dual $H'$ is the space
                of all functions that look like
                $f(x)=\langle{x,y}\rangle$ for some $y\in{X}$. More
                precisely, if $H$ is a Hilbert space and $f\in{H'}$,
                then there is a $y\in{H}$ such that, for all
                $x\in{X}$, $f(x)=\langle{x,y\rangle}$.
            \subsubsection{Riesz's Representation Theorem}
                \begin{theorem}
                    If $H$ is a Hilbert space, and
                    $f:H\rightarrow\mathbb{R}$ is a bounded
                    linear functional, then there is a unique
                    $y\in{H}$ such that, for all $x\in{X}$,
                    $f(x)=\langle{x,y}\rangle$. Moreover,
                    $\norm{f}=\norm{y}$.
                \end{theorem}
                \begin{proof}
                    Let $f\in{H'}$ and let $N=\nul(f)$. That is,
                    $N$ is the null space of the functional $f$
                    which is the set of all points
                    $x\in{X}$ such that $f(x)=0$. The Null space
                    actually defines a closed vector space, which
                    is a subspace of $H$. If $N=H$, then
                    $f(x)=0$, and thus let $y=0$. Otherwise, let $z$
                    be a non-zero elements such that,
                    for all $x\in{N}$, $\langle{x,y}\rangle=0$.
                    For all $x,z$, $f(x)z-f(z)x\in{N}$, for:
                    \begin{equation*}
                        f(f(x)z-f(z)x)
                        =f(f(x)z)-f(f(z)z)
                        =f(x)f(z)-f(z)f(z)=0
                    \end{equation*}
                    Therefore:
                    \begin{align*}
                        \langle{f(x)z-f(z)x,z}\rangle&=0\\
                        \Rightarrow
                        |f(x)|\norm{z}^{2}-|fz|\langle{x,y}\rangle&=0\\
                        \Rightarrow
                        f(x)
                        &=\langle{x,\frac{f(z)z}{\norm{z}^{2}}}\rangle
                    \end{align*}
                    Therefore, let $y=\frac{f(z)}{\norm{z}^{2}}z$.
                    This is unique since if for all $x\in{H}$,
                    $\langle{x,y_{1}}\rangle=\langle{x,y_{2}}\rangle$,
                    then $y_{1}=y_{2}$. Finally:
                    \begin{equation*}
                        \norm{y}
                        =\frac{|f(z)|}{\norm{z}^{2}}\norm{z}
                        =\frac{|f(z)|}{\norm{z}}
                        \leq\norm{f}
                    \end{equation*}
                    But also:
                    \begin{equation*}
                        |f(x)|=|\langle{x,z}\rangle|
                        \leq\norm{x}\norm{z}
                    \end{equation*}
                    Thus, $\norm{f}\leq\norm{z}$. But
                    $\norm{z}\leq\norm{f}$. Therefore,
                    $\norm{f}=\norm{z}$.
                \end{proof}
                Much like in $\mathbb{R}^{n}$, there is a notion of
                orthogonality in a general inner product space.
                \begin{definition}
                    Orthognal elements in an inner product space $X$
                    are elements $x,y\in{X}$ such that
                    $\langle{x,y}\rangle=0$.
                \end{definition}
                There's also a notion of convexity for a general
                vector space.
                \begin{definition}
                    A convex subset of a vector space $V$ space is a
                    subset $S\subset{V}$ such that, for all
                    $x,y\in{V}$ and for all $\lambda\in\mathbb{R}$,
                    $\lambda{x}+(1-\lambda)y\in{S}$.
                \end{definition}
                \begin{theorem}
                    If $S$ is a subset of $V$, then $S$ is convex.
                \end{theorem}
                Recall that for a general metric space $X$,
                if $S\subset{X}$, we defined
                $dist(x,S)=\inf\{d(x,s):s\in{S}\}$. We proved that,
                if $S$ is compact, then there is an $s\in{S}$ such
                that $dist(x,S)=d(x,s)$. We showed that, without
                compactness, this may not be true. Indeed, even
                complete spaces may lack this property. If
                $X$ is a Hilbert space, however, this property is
                guaranteed.
                \begin{theorem}
                    If $H$ is a Hilbert space and if $S\subset{H}$
                    is a closed convex subset of $H$, then there is
                    a unique $s\in{S}$ such that
                    $dist(x,S)=\norm{x-s}$.
                \end{theorem}
                \begin{proof}
                    As $dist(x,S)=\inf\{d(x,s):s\in{S}\}$, there is
                    a sequence $x_{n}\in{S}$ such that
                    $\norm{x-x_{n}}\rightarrow{dist(x,S)}$. Then, by
                    Appolonius:
                    \begin{align*}
                        \norm{x-x_{n}}^{2}+\norm{x-x_{m}}^{2}
                        &=\frac{1}{2}\norm{x_{n}-x_{m}}^{2}
                        +\frac{1}{2}
                        \norm{\frac{1}{2}(x_{n}+x_{m})-x}^{2}\\
                        &\geq\frac{1}{2}\norm{x_{n}+x_{m}}^{2}
                        +2dist(x,S)
                    \end{align*}
                    But $\norm{x-x_{n}}\rightarrow{dist(x,S)}$ and
                    $\norm{x-x_{m}}\rightarrow{dist(x,S)}$, so:
                    \begin{equation*}
                        \frac{1}{2}\norm{x_{n}-x_{m}}^{2}
                        \leq\norm{x-x-{m}}^{2}
                        +\norm{x-x_{m}}^{2}-2dist(x,S)
                    \end{equation*}
                    Which can be made arbitrarily small. Therefore,
                    $x_{n}$ is Cauchy. But $H$ is a Hilbert space, and
                    is therefore complete, and thus $x_{n}$ converges.
                    Let $s$ be the limit. As $S$ is closed, $s\in{S}$.
                    Moreover, from construction,
                    $\norm{x-s}=dist(x,S)$. If there is another point
                    $v$, then $\norm{x-s}=\norm{x-v}$. From
                    Appolonius:
                    \begin{equation*}
                        \norm{x-s}^{2}+\norm{x-v}^{2}
                        \geq\frac{1}{2}\norm{s-v}+2dist(x,S)^{2}
                    \end{equation*}
                    But $\norm{x-s}=\norm{x-v}=dist(x,S)$, and
                    thus $\norm{s-v}=0$. Therefore, $s=v$.
                \end{proof}
                Is $S$ is a closed subspace of $H$, then it's
                automatically convex. In this case, $x-s\perp{S}$,
                where $z\perp{S}$ means that, for all
                $s\in{S}$, $\langle{s,z}\rangle=0$. For if
                $z\in{S}$, then $s+tz\in{S}$ for all $t$. Thus:
                \begin{equation*}
                    \norm{s+tz-x}\geq{dist(x,S)}=\norm{s-x}^{2}
                \end{equation*}
                And therefore:
                \begin{equation*}
                    \rangle{s-x,s-x}\rangle+2t\langle{s-x,z}
                    +t^{2}\langle{z,z}\geq{s-x}^{2}
                    \Rightarrow{t^{2}\norm{z}^{2}+2t\langle{s-x,z}}
                    \geq{0}
                \end{equation*}
                Looking at the discriminant of this polynomial, we
                have:
                \begin{equation*}
                    \langle{s-x,z}\rangle=0
                \end{equation*}
                Therefore, $s-x\perp{S}$. You obtain $s\in{S}$
                by ``dropping the perpendicular of $x$,'' onto
                $S$. That is, $s$ is the orthogonal projection
                of $x$ onto $S$. $s=P_{S}x$ where
                $P_{S}:H\rightarrow{H}$ is the orthogonal
                projection. This has a few nice properties:
                \begin{enumerate}
                    \item It is idempotent: $P_{S}^{2}=P_{S}$.
                    \item Self adjoint:
                          $\langle{P_{S}x,y}\rangle%
                           =\langle{x,P_{S}y}\rangle=$
                    \item Linear.
                    \item Bounded and $\norm{P_{S}}=1$.
                \end{enumerate}
                If $S$ is a subset of an inner product space $X$,
                we write $S^{\perp}=\{x\in{X}:\langle{x,s}\rangle=0\}$.
                This is often read aloud as ``$S$ perp'' or
                ``$S$ perpendicular.''
                \begin{theorem}
                    If $S\subset{X}$, then $S^{\perp}$ is a
                    closed subspace.
                \end{theorem}
                \begin{theorem}
                    $S\subset(S^{\perp})^{\perp}$
                \end{theorem}
                The direct sum of two subsets of a Hilbert space
                $H$ is
                $S_{1}\oplus{S_{2}}=\{ax+by:x\in{S_{1}},y\in{S_{2}}\}$.
                \begin{theorem}
                    If $H$ is a Hilbert space and $S$ is a closed
                    subspace of $H$, then $H=S\oplus{S^{\perp}}$
                \end{theorem}
                \begin{proof}
                    For $x=P_{S}(x)+(x-P_{S}(x))$, and thus there is
                    an element in $S$ and an element in $S^{\perp}$
                    such that $x$ is the sum of those two elements.
                    This is the only representation. For if
                    $x=s_{1}+s_{1}^{\perp}$ and
                    $x=s_{2}+s_{2}^{\perp}$, then stuff.
                \end{proof}
                If $X$ and $Y$ are normed spaces, and if
                $f\in{B(X,Y)}$, then
                $\{x\in{X}:f(x)=0\in{Y}\}$ is called the null
                space of $f$.
                \begin{theorem}
                    If $X$ and $Y$ are normed spaces, and if
                    $f\in{B(X,Y)}$, then $\nul(f)$ is a closed
                    linear subspace of $X$.
                \end{theorem}
                \begin{proof}
                    Obvious since $f$ is linear and continuous.
                \end{proof}
                In a Hilbert space $H$, then
                $H=\nul(f)\oplus\nul(f)^{\perp}$. Thus, if
                $\nul(f)\ne{H}$, then $\nul(f)^{\perp}\ne\{0\}$.
                That is, there exists a $z\in\nul(f)^{\perp}$ that
                is non-zero. This is the $z$ we used to prove the
                Riesz representation theorem. Riesz's
                Theorem thus says that every Hilbert space is its own dual.
        \subsection{Lecture 9: November 5, 2018}
            \subsubsection{Adjoint}
                If $H$ is a Hilbert space and
                $f\in{H'}$, then there is a $z\in{H}$
                such that $f(x)=\langle{x,z}\rangle$ for
                all $x\in{H}$. Moreover, $\norm{f}=\norm{z}$.
                The adjoint of $T\in{B(H,H)}$ is an
                operator $T^{*}:H\rightarrow{H}$ such that
                $\langle{Tx,y}\rangle=\langle{x,T^{*}}\rangle$.
                There is always such an operator for any
                $T\in{B(H,H)}$. $T^{*}$ is also bounded and
                linear. By Riesz there is a $z=T^{*}y$ such
                that $f(x)=\langle{x,T^{*}y}\rangle$. Then
                $\norm{T^{*}y}=\norm{z}=\norm{f}\leq\norm{T}\norm{y}$.
                Thus, $\norm{T^{*}}\leq\norm{T}$. Therefore
                $T^{*}\in{B(H,H)}$. $T^{*}$ is called the
                ajdoint of $T$.
                \begin{example}
                    Consider $\mathbb{R}^{n}$ with the usual
                    inner product. Let $T$ be the matrix
                    $(T_{ij})$. Then:
                    \begin{equation*}
                        (Tx)_{i}=\sum_{j=1}^{n}T_{ij}x_{j}    
                    \end{equation*}
                    and:
                    \begin{equation*}
                        \langle{Tx,y}\rangle
                        =\sum_{i=1}^{n}(Tx)_{i}y_{i}
                        =\sum_{i=1}^{n}\sum_{j=1}^{n}
                        T_{ij}x_{j}y_{i}
                        =\sum_{j=1}^{n}\sum_{i=1}^{n}
                        T_{ij}y_{i}x_{j}
                    \end{equation*}
                    If $T^{*}$ is the adjoint, then:
                    \begin{equation*}
                        \langle{x,T^{*}y}\rangle
                        =\sum_{j=1}^{n}
                        \Big(\sum_{i=1}^{n}
                             T^{*}_{ji}y_{i}\Big)x_{j}
                    \end{equation*}
                    And thus $T^{*}_{ji}=T_{ij}$.
                    That is, the adjoint
                    of $T$ is the transpose of $T$. If we were in
                    $\mathbb{C}^{n}$ we would use the complex
                    conjugate of the transpose of $T$.
                    In general, if $T=T^{*}$
                    we say that $T$ is \textit{self-adjoint}.
                    This is also called symmetric or Hermitian.
                \end{example}
                \begin{example}
                    As another example, consider
                    $H=\ell^{2}$ and let
                    $T(x_{1},x_{2},\hdots)=(x_{2},x_{3},\hdots)$.
                    This is linear, and:
                    \begin{equation*}
                        \norm{T(x_{1},x_{2},\hdots)}
                        =\norm{(x_{2},x_{3},\hdots)}
                        =\sqrt{\sum_{n=2}^{\infty}x_{n}^{2}}
                        \leq
                        \sqrt{\sum_{n=1}^{\infty}x_{n}^{2}}
                        =\norm{(x_{1},x_{2},\hdots)}
                    \end{equation*}
                    Therefore $T$ is bounded and
                    $\norm{T}\leq{1}$. But
                    $T(0,1,0,0,\hdots)=(1,0,0,\hdots)$
                    showing that $\norm{T}\geq{1}$.
                    Thus, $\norm{T}=1$. Then, from
                    the definition of $T$:
                    \begin{align*}
                        \langle{Tx,y}\rangle
                        &=\langle{(x_{2},x_{3},\hdots),
                                  (y_{1},y_{2},\hdots)}\rangle\\
                        &=x_{2}y_{1}+x_{3}y_{2}+\hdots\\
                        &={x_{1}}\cdot{0}+x_{2}y_{1}+
                        x_{3}y_{2}+\hdots\\
                        &=\langle{(x_{1},x_{2},\hdots),
                                  (0,y_{1},y_{2},\hdots)}\rangle
                    \end{align*}
                    And therefore
                    $T^{*}(y_{1},y_{2},\hdots)=(0,y_{1},y_{2},\hdots)$.
                    Also $\norm{T^{*}}=1$. In general,
                    if $T\in{B(H,H)}$
                    then $\norm{T^{*}}=\norm{T}$.
                \end{example}
                \begin{theorem}
                    If $T\in{B(H,H)}$, then
                    $T^{**}=T$.
                \end{theorem}
                \begin{theorem}
                    $\norm{T}=\norm{T^{*}}$
                \end{theorem}
                \begin{proof}
                    For $\norm{T}\leq\norm{T^{*}}$ and
                    $\norm{T^{*}}\leq\norm{T^{**}}$, but
                    $T=T^{**}$, and therefore
                    $\norm{T}=\norm{T^{*}}$.
                \end{proof}
                \begin{example}
                    Let $x=C[0,1]$ and let
                    $\langle{x,y}\rangle=\int_{0}^{1}x(t)y(t)\diff{t}$.
                    Let $K:X\times{X}\rightarrow\mathbb{R}$
                    be continuous and define $T$ by:
                    \begin{equation*}
                        Tx(t)=\int_{0}^{1}K(t,s)x(s)\diff{s}
                    \end{equation*}
                    Then for all $x\in{X}$, $Tx\in{X}$ as well,
                    since $K$ is continuous.
                    Moreover, from Cauchy-Schwarz:
                    \begin{equation*}
                        \norm{Tx}^{2}
                        =\int_{0}^{1}
                        \Big[\int_{0}^{1}
                             K(t,s)x(d)\diff{s}\Big]^{2}\diff{t}
                        \leq\int_{0}^{1}
                        \Big[\int_{0}^{1}K(t,s)^{2}\diff{s}
                        \int_{0}^{1}x(s)^{2}\diff{s}\Big]\diff{t}
                    \end{equation*}
                    But
                    $\int_{0}^{1}x(s)^{2}\diff{s}=\norm{x}^{2}$.
                    So:
                    \begin{equation*}
                        \norm{Tx}^{2}\leq
                        \norm{x}^{2}\int_{0}^{1}\int_{0}^{1}
                        K(t,s)\diff{s}\diff{t}
                    \end{equation*}
                    Therefore $T$ is bounded and:
                    \begin{equation*}
                        \norm{T}\leq
                        \sqrt{\int_{0}^{1}\int_{0}^{1}
                              K(t,s)\diff{s}\diff{t}}
                    \end{equation*}
                    Computing the adjoint:
                    \begin{align*}
                        \langle{Tx,y}\rangle
                        &=\int_{0}^{1}Tx(t)y(t)\diff{t}\\
                        &=\int_{0}^{1}\Big(
                        \int_{0}^{1}K(t,s)x(s)\diff{s}\Big)
                        y(t)\diff{t}\\
                        &=\int_{0}^{1}\Big(
                        \int_{0}^{1}K(t,s)y(t)\diff{t}\Big)
                        x(s)\diff{s}\\
                        &=\int_{0}^{1}\Big(
                        \int_{0}^{1}K(s,t)y(s)\diff{s}\Big)
                        x(t)\diff{t}\\
                        &=\int_{0}^{1}Ty(s)x(s)\diff{s}\\
                        &=\langle{Ty,x}\rangle
                    \end{align*}
                    We may swap the order of integration since
                    $K$ is continuous on a compact set.
                \end{example}
                \begin{theorem}
                    $\norm{T^{*}T}=\norm{T}^{2}$
                \end{theorem}
                \begin{proof}
                    For:
                    \begin{equation*}
                        \norm{T^{*}Tx}\leq\norm{T^{*}}
                        \norm{Tx}\leq
                        \norm{T^{*}}\norm{T}\norm{x}
                    \end{equation*}
                    And therefore $\norm{T^{*}T}\leq\norm{T}^{2}$.
                    On the other hand:
                \end{proof}
                \begin{theorem}
                    If $T$ is self-adjoint, then
                    $\norm{T}=\sup\{|\langle{Tx,x}\rangle|:\norm{x}=1\}$.
                \end{theorem}
                \begin{proof}
                    \label{thm:Funct:Norm_of_Self_%
                           Adjoint_Operator}
                    Let
                    $\alpha=\sup\{sup\{|\langle{Tx,x}\rangle|:\norm{x}=1\}$.
                    Then:
                    \begin{equation*}
                        |\langle{Tx,x}\rangle|
                        \leq\norm{Tx}\norm{x}\leq
                        \norm{T}\norm{x}^{2}
                    \end{equation*}
                    Taking the supremum over $\norm{x}=1$,
                    we have $\alpha\leq\norm{T}$.
                    But if $\norm{x}=\norm{y}=1$, then:
                    \begin{align*}
                        |\langle{Tx,y}\rangle|
                        &=|\frac{1}{4}\langle{T(x+y),x+y}\rangle
                        -\frac{1}{4}\langle{T(x-y),x-y}\rangle\\
                        &=|\frac{1}{4}\norm{x+y}^{2}
                        \langle{T}\frac{x+y}{\norm{x+y}},
                               \frac{x+y}{\norm{x+y}}\rangle
                        -\frac{1}{4}\norm{x-y}
                        \langle{T}\frac{x-y}{\norm{x-y}},
                               \frac{x-y}{\norm{x-y}}\rangle
                    \end{align*}
                    Since
                    $\langle{Tx,y}\rangle=\langle{x,Ty}\rangle$,
                    as $T$ is self-adjoint.
                    And from the definition of $\alpha$:
                    \begin{equation*}
                        |\langle{Tx,y}\rangle|
                        \leq\frac{\alpha}{4}
                        \big(\norm{x+y}^{2}+\norm{x-y}^{2}\big)
                        \leq\frac{\alpha}{4}
                        \big(2\norm{x}^{2}+2\norm{y}^{2}\big)
                        =\alpha
                    \end{equation*}
                    Let $y=Tx/\norm{Tx}$, we get:
                    \begin{equation*}
                        \langle{Tx,\frac{Tx}{\norm{Tx}}}\rangle
                        \leq\alpha
                    \end{equation*}
                    And therefore $\norm{T}\leq\alpha$. But also
                    $\alpha\leq\norm{T}$. Thus, $\norm{T}=\alpha$.
                \end{proof}
                Thm.~\ref{thm:Funct:Norm_of_Self_Adjoint_Operator}
                can fail if $T$ is not self-adjoint.
                In $\mathbb{R}^{2}$, let
                $T(x_{1},x_{2})=(0,x_{1})$. Then:
                \begin{equation*}
                    \norm{Tx}^{2}=x_{1}^{2}\leq
                    x_{1}^{2}+x_{2}^{2}
                    =\norm{x}^{2}
                \end{equation*}
                And therefore $\norm{T}\leq{1}$.
                But $T(1,0)=(0,1)$, and
                thus $\norm{T}=1$. But if $(x_{1},x_{2})$
                lies on the unit circle, then
                $|x_{1}x_{2}|\leq0.5$. Thus:
                \begin{equation*}
                    |\langle{Tx,x}\rangle|
                    =|\langle(x_{1},x_{2}),(0,x_{1})\rangle
                    =|x_{1}x_{2}|\leq\frac{1}{2}
                \end{equation*}
                Therefore
                $|\langle{Tx,x}\rangle|\leq{0.5}<\norm{T}$ for
                all $x\in\mathbb{R}^{2}$ such that $\norm{x}=1$.
            \subsubsection{Compact Operators}
                Compact operators can be defined in a more
                general spaces
                than that of Hilbert or Banach spaces.
                They can be defined
                on Topological spaces, but we won't go that far.
                For now we will simply define them on a
                general metric space.
                \begin{definition}
                    A compact mapping from a metric space $X$
                    to a metric
                    space $Y$ is a function $T:X\rightarrow{Y}$
                    such that
                    for all bounded subsets $S$ of $X$, the image
                    $T(S)$ is pre-compact in $Y$. That is,
                    $\overline{T(S)}$ is compact
                    (The closure of $T(S)$ is compact).
                \end{definition}
                \begin{theorem}
                    If $T:X\rightarrow{Y}$ is a linear compact
                    operator between normed spaces $X$ and $Y$,
                    then $T$ is continuous.
                \end{theorem}
                \begin{proof}
                    For let $S=\overline{B_{1}(\mathbf{0})}$.
                    This is bounded, so
                    $\overline{T(S)}$ is compact, and therefore bounded.
                    Let $M$ be such a bound.
                    Thus, for all $s\in\overline{S}$
                    such that $\norm{s}=1$,
                    $\norm{Ts}\leq{M}$, and therefore $\norm{T}\leq{M}$.
                    Thus $T$ is bounded and linear, and is therefore
                    continuous.
                \end{proof}
                \begin{example}
                    Every linear mapping
                    $T:\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}$
                    is compact.
                    As a another example, let
                    $X=C[0,1]$ and equip this with the supremum norm.
                    Define $T$ as:
                    \begin{equation*}
                        Tx(t)=\int_{0}^{1}K(t,s)x(s)\diff{s}
                    \end{equation*}
                    Where $K:[0,1]\times[0,1]\rightarrow\mathbb{R}$
                    is continuous. This is a compact operator. For if
                    $S$ is a bounded subset then there exists
                    an $M$ such
                    that for all $x\in{S}$, $\norm{x}\leq{M}$. Thus:
                    \begin{align*}
                        \norm{Tx}=\sup|Tx(t)|
                        &=\sup\Big|\int_{0}^{1}
                        K(t,s)x(s)\diff{s}\Big|\\
                        &\leq\sup\int_{0}^{1}
                        |K(t,s)||x(s)|\diff{s}\\
                        &\leq\kappa\int_{0}^{1}|x(s)|\diff{s}\\
                        &\leq\kappa\norm{x}
                    \end{align*}
                    Where $\kappa=\sup|K(t,s)|$. $\kappa$ exists
                    since $K(t,s)$ is continuous on a compact
                    set and is therefore
                    bounded. So $T(S)$ is uniformly bounded. To apply
                    Arzela-Ascoli we need to show that
                    $T(S)$ is equicontinuous. That is, for all
                    $\varepsilon>0$ there is a $\delta>0$ such that,
                    for all $x\in{S}$, if $|t_{2}-t_{1}|<\delta$
                    then $|Tx(t_{2})-Tx(t_{1})|<\varepsilon$. If we
                    can show that $T$ satisfies this, then
                    $\overline{T(S)}$ is compact,
                    and thus $T$ is compact.
                    Let's show this. If $x\in{S}$, then:
                    \begin{align*}
                        |Tx(t_{2})-Tx(t_{1})|
                        &=\Big|\int_{0}^{1}K(t_{1},s)x(s)\diff{s}
                        -\int_{0}^{1}K(t_{2},s)x(s)\diff{s}\Big|\\
                        &=\Big|\int_{0}^{1}(K(t_{2},s)-
                        K(t_{1},s))x(s)\diff{s}\Big|\\
                        &\leq
                        \int_{0}^{1}|K(t_{2},s)-K(t_{1},s)||x(s)|
                        \diff{s}\\
                        &\leq{M}\int_{0}^{1}|K(t_{2},s)-K(t_{1},s)|
                        \diff{s}
                    \end{align*}
                    But as $K$ is uniformly continuous,
                    there is a $\delta>0$
                    such that, for all $s\in[0,1]$,
                    $|t_{2}-t_{1}|<\delta$ implies
                    $|K(t_{2},s)-K(t_{1},s)|<\varepsilon/M$.
                    Thus, $T(S)$ is equicontinuous. We can replace the
                    supremum norm with $L^{2}$ and $T$ is still compact.
                    Indeed, it is true for $L^{p}$ if we replace the
                    use of Cauchy-Schwarz with the more general
                    H\"{o}lder's Inequality. From this we have that
                    $T$ is a compact self-adjoint operator.
                \end{example}
        \subsection{Lecture 10: November 19, 2018}
            \subsubsection{Compact Linear Operators}
                A linear operator $T:X\rightarrow{Y}$
                is compact if $\overline{T(S)}$ is compact
                for all bounded $S\subset{X}$. Example,
                $Tx(t)=\int_{0}^{1}K(t,s)x(s)\diff{s}$,
                where $K$ is continuous on
                $[0,1]^{2}$, is a compact operator
                $T:C[0,1]\rightarrow{C[0,1]}$. If
                $K(x,t)=K(t,x)$ for all
                $(x,t)\in[0,1]^{2}$, then
                $T$ is a self-adjoint operator on
                $L^{2}[0,1]$. $L^{2}[0,1]$ can be seen
                as the completion of $C[0,1]$ with respect
                to the $L^{2}$ norm.
                \begin{theorem}
                    A linear operator
                    $T:X\rightarrow{Y}$ is compact if and only
                    if for all bounded sequences
                    $x:\mathbb{N}\rightarrow{X}$,
                    $Tx$ has a congergent subsequence in $Y$ 
                \end{theorem}
                We're interested mainly in the case of $Y=X$,
                and when $T:X\rightarrow{X}$ is bounded in
                linear. This is the set of all operators
                $B(X,X)$. We rewrite this as $B(X)$. That is,
                $B(X)$ is the set of all bounded linear operators
                from $X$ to itself. Recall that if
                $Y$ is a Banach space, and if $X$ is a normed
                space, then $B(X,Y)$ is a Banach space. Thus,
                if $X$ is a Banach space, then $B(X)$ is a
                Banach space. But we can also multiply elements
                in $B(X)$ by using function composition.
                If $S,T\in{B(X)}$, then $ST$ is defined by
                $(ST)(x)=S(Tx)$. But then:
                \begin{equation*}
                    \norm{(ST)x}=\norm{S(Tx)}
                    \leq\norm{S}\norm{Tx}
                    \rightarrow
                    \norm{ST}\leq\norm{S}\norm{T}
                \end{equation*}
                A Banach space with such a multiplication
                property is called a Banach Algebra. The set of
                compact linear operators on $X$ is often denoted
                $C(X)$. Thus, $C(X)\subset{B(X)}$. It's
                a two-sided closed ideal in $B(X)$. That is,
                if $S,T\in{C(X)}$, and if $a$ and $b$ are scalars,
                then $aS+bT\in{C(X)}$, $ST\in{C(X)}$,
                and $TS\in{C(X)}$. Finally, if
                $F:\mathbb{N}\rightarrow{C(X)}$ is a sequence of
                compact operators, and if $F_{n}\rightarrow{T}$,
                then $T\in{C(X)}$.
                \begin{definition}
                    Orthogonal Elements in an inner product
                    space $(X,\langle\rangle)$ are elements
                    $x,y\in{X}$ such that
                    $\langle{x,y}\rangle=0$.
                \end{definition}
                \begin{definition}
                    An orthonormal subset of an inner
                    product space $(X,\langle\rangle)$
                    is a subset $S\subset{X}$ such that for
                    all $x,y\in{S}$ such that $x\ne{y}$,
                    $\langle{x,y}\rangle=0$ and for all
                    $x\in{S}$, $\norm{x}=1$.
                \end{definition}
                \begin{theorem}
                    If $x\in{X}$ and
                    $\varphi:\mathbb{N}\rightarrow{X}$
                    is a sequence such that
                    $A=\{\varphi_{n}:n\in\mathbb{N}\}$ is an
                    orthonormal subset of $X$, then for all
                    $x\in{X}$:
                    \begin{equation*}
                        \norm{x}=
                        \sum_{n=1}^{N}\langle{x,\varphi_{n}}
                        \rangle^{2}
                        +\norm{x-\sum_{n=1}^{N}
                               \langle{x,\varphi_{n}}\rangle
                               \varphi_{n}}^{2}
                    \end{equation*}
                \end{theorem}
                \begin{proof}
                    If $m\in\mathbb{Z}_{N}$, then:
                    \begin{equation*}
                        \langle{x-\sum_{n=1}^{N}
                                \langle{x,\varphi_{n}}\rangle
                                \varphi_{n},\varphi_{m}}\rangle
                        =\langle{x,\varphi_{m}}\rangle
                        -\sum_{n=1}^{N}\langle{x,\varphi_{n}}\rangle
                        \langle{\varphi_{n},\varphi_{m}}\rangle
                    \end{equation*}
                    But $A$ is an orthonormal subset of $X$,
                    and thus if $n\ne{m}$ then
                    $\langle{\varphi_{n},\varphi_{m}}\rangle=0$
                    ad if $n=m$ then
                    $\langle{\varphi_{n},\varphi_{m}}\rangle%
                     =\norm{\varphi_{n}}=1$. In terms of the
                    Kronecker-Delta function,
                    $\langle{\varphi_{n},\varphi_{m}}\rangle%
                     =\delta_{nm}$. So we have:
                    \begin{equation*}
                        \langle{x-\sum_{n=1}^{N}
                                \langle{x,\varphi_{n}}\rangle
                                \varphi_{n},\varphi_{m}}\rangle
                        =0
                    \end{equation*}
                    But for all $N\in\mathbb{N}$,
                    $x=(x-\sum_{n=1}^{N}%
                     \langle{x,\varphi_{n}}\rangle\varphi_{n})+%
                     \sum_{n=1}^{N}\langle{x,\varphi_{n}}\rangle%
                     \varphi_{n}$,
                    and these two are orthogonal.
                    Therefore, from Pythagoras:
                    \begin{align*}
                        \norm{x}^{2}&=
                        \norm{x-\sum_{n=1}^{N}
                              \langle{x,\varphi_{n}}\varphi_{n}
                              \rangle}^{2}+
                        \norm{\sum_{n=1}^{N}
                              \langle{x,\varphi_{n}}\varphi_{n}
                              \rangle}^{2}\\
                        &=\sum_{n=1}^{N}\langle{x,\varphi_{n}}
                        \norm{\varphi_{n}}^{2}\rangle
                        +\norm{\sum_{n=1}^{N}
                        \langle{x,\varphi_{n}}\varphi_{n}
                        \rangle}^{2}
                    \end{align*}
                    But $\norm{\varphi_{n}}^{2}=1$ for all $n$.
                    Therefore, etc.
                \end{proof}
                \begin{theorem}[Bessel's Inequality]
                    $\sum_{n=1}^{N}\langle{x,\varphi_{n}}\rangle%
                     \leq\norm{x}^{2}$
                \end{theorem}
                \begin{example}
                    $A=\{e_{n}:n\in\mathbb{N}\}$ is an orthonormal
                    subset of $\ell^{2}$.
                    $A=\{\sin(nt)/\sqrt{\pi}:n\in\mathbb{N}\}$
                    is an orthonormal subset of
                    $C[0,1]$ with the $L^{2}$ inner product.
                \end{example}
                \begin{definition}
                    A basis of an innert product space $X$
                    is an orthonormal subset $A\subset{X}$
                    such that there is no orthonormal subset
                    $B\subset{X}$ such that $A\subset{B}$.
                \end{definition}
                \begin{theorem}
                    If $(X,\langle\rangle)$ is an inner product
                    space, then there is an $A\subset{X}$
                    such that $A$ is an orthonormal subset of $X$.
                \end{theorem}
                \begin{theorem}
                    If $X$ is an inner product space, and if
                    $\varphi:\mathbb{N}\rightarrow{X}$ is a sequence
                    such that $S=\{\varphi_{n}:n\in\mathbb{N}\}$
                    is a basis of $X$, then for all $x\in{X}$
                    there is a sequence
                    $a:\mathbb{N}\rightarrow\mathbb{R}$
                    such that
                    $x=\sum_{n=1}^{\infty}a_{n}\varphi_{n}$
                \end{theorem}
            \subsubsection{Summability}
                What does $\sum_{\alpha\in{A}}z_{\alpha}$
                if $z_{\alpha}\in\mathbb{R}$ for all
                $\alpha\in{A}$?
                \begin{definition}
                    We say
                    $\sum_{\alpha\in{A}}z_{\alpha}$ is summable
                    to $z\in\mathbb{R}$ if for all
                    $\varepsilon>0$ there is a subset
                    $B\subset{A}$ such that, for all
                    $C\subset{A}$ such that
                    $B\subset{C}$,
                    $|\sum_{\alpha\in{C}}z_{\alpha}-z|<\varepsilon$.
                \end{definition}
                It turns out that, if
                $\sum_{\alpha\in{A}}z_{\alpha}$ is summable, then
                only countably many $z_{\alpha}$ are non-zero.
                For all $n$, the set
                $\{z_{\alpha}:z_{\alpha}>1/n\}$ must be finite.
                The set of all non-zero elements is the
                union over all of these $n$, which is the
                countable union of finite sets, which is
                thus countable.
                \begin{theorem}
                    If $(X,\langle\rangle)$ is an inner
                    product space, if
                    $\varphi:\mathbb{N}\rightarrow{X}$ is a
                    sequence such that
                    $S=\{\varphi_{n}:n\in\mathbb{N}\}$ is a
                    basis of $X$, then:
                    \begin{equation*}
                        x=\sum_{n=1}^{\infty}
                        \langle{x,\varphi_{n}}\rangle
                        \varphi_{n}
                    \end{equation*}
                \end{theorem}
                \begin{example}
                    If $L^{2}[0,\pi]$, let
                    $\varphi_{n}(t)=\sin(nt)\sqrt{2/\pi}$. Then
                    $A=\{\varphi_{n}(t):n\in\mathbb{N}\}$.
                \end{example}
        \subsection{Lecture 11: November 26, 2018}
            Let $T$ be a linear operator on a vector space
            $T$. We say $\lambda\in\mathbb{C}$ is an
            eigenvalue of $T$ if there exists $x\ne{0}$ in
            $X$ such that $Tx=\lambda{x}$. The corresponding
            $x$ is called the eigenvector or eigenfunction.
            We're interested in the case of compact
            self-adjoint operators $T$ on a Hilbert space
            $\mathscr{H}$.
            \begin{theorem}
                There is a sequence of real eigenvalues
                $\lambda_{n}$ of $T$, finite or infinite,
                such that $0$ is the only possible
                accumulation point of $\lambda_{n}$, and
                corresponding basis of
                orthogonormal eigenvectors $x_{n}$.
            \end{theorem}
            \begin{proof}
                We'll prove this in steps. First, either
                $\norm{T}$ or $-\norm{T}$ is an eigenvalue. This
                is because:
                \begin{equation*}
                    \norm{T}=
                    \underset{\norm{x}=1}{\sup}
                    \{|\langle{Tx,x}\rangle|\}
                \end{equation*}
                This comes from the fact that
                $T=T^{*}$ for self-adjoint operators. Thus,
                either
                $\norm{T}=\langle{Tx,x}\rangle$ or
                $-\norm{T}=\langle{Tx,x}\rangle$. Choose a
                sequence $x_{n}$ such that
                $\norm{x_{n}}=1$ and
                $\langle{Tx_{n},x_{n}}\rangle%
                 \rightarrow\pm\norm{T}$. By choosing a
                subsequence, we may assume that
                $Tx_{n}$ converges. We can not assume that
                $x_{n}$ converges, however. Thus,
                $Tx_{n}\rightarrow{y}$. Then:
                \begin{align*}
                    \norm{Tx_{n}-\lambda{x}_{n}}^{2}
                    &=\langle{Tx_{n}-\lambda{x}_{n},
                              Tx_{n}-\lambda{x}_{n}}\rangle\\
                    &=\norm{Tx_{n}}^{2}
                    -2\lambda\langle{Tx_{n},x}\rangle
                    +\lambda^{2}\norm{x}^{2}\\
                    &\leq
                    \norm{T}^{2}\norm{x}^{2}
                    -2\lambda\langle{Tx_{n},x}\rangle
                    +\lambda^{2}\norm{x}^{2}
                \end{align*}
                And this converges to zero as $n$ tends to
                infinity. Thus,
                $Tx_{n}-\lambda{x}_{n}\rightarrow{0}$. It
                follows that
                $\lambda{x}_{n}=Tx_{n}-(Tx_{n}-\lambda{x}_{n})$,
                and this converges to $y$. Therefore
                $x_{n}\rightarrow{y}/\lambda$. Note that
                $\lambda$ is only equal to zero if
                $T$ is the zero operator. In this case the
                problem is trivial. Thus we may assume
                $\lambda\ne{0}$. Therefore
                $Tx_{n}\rightarrow{Ty}/\lambda$, but
                $Tx_{n}\rightarrow{y}$ as well. Thus
                $y=Ty/\lambda$. Let $\lambda_{1}=\lambda$
                and $\varphi_{1}=y/\norm{y}$. Then
                $\norm{\phi_{1}}=1$ and
                $T\varphi_{1}=\lambda_{1}\phi_{1}$. Moreover,
                let $T_{1}=T$ and let $H_{1}=\mathscr{H}$.
                Let $H_{2}=\{x\in{H}_{1}:x\perp\varphi_{1}\}$.
                Define $T_{2}:H_{2}\rightarrow{H}_{1}$ by
                $T_{2}x=T_{1}x=Tx$. This is the restriction
                of $T$ to $H_{2}$. Then
                $T_{2}H_{2}\subseteq{T}_{2}H_{2}$. For if
                $x\in{H}_{2}$, then
                $\langle{T}_{2}x,\varphi_{1}\rangle%
                 \langle{T}_{1}x,\varphi_{1}\rangle$. But
                $T$ is self-adjoint, and
                $T_{1}=T$, and therefore
                $T_{1}$ is self-adjoint. But then:
                \begin{equation*}
                    \langle{T}_{2}x,\varphi_{1}\rangle
                    =\langle{T}_{1}x,\varphi_{1}\rangle
                    =\langle{x},T_{1}\varphi_{1}\rangle
                    =\lambda_{1}\langle{x}_{1},
                        \varphi_{1}\rangle
                \end{equation*}
                Therefore $T_{2}x\in{H}_{2}$, and therefore
                $T_{2}:H_{2}\rightarrow{H}_{2}$. $T_{2}$
                is self-adjoint, for:
                \begin{align*}
                    \langle{T}_{2}x,y\rangle
                    =\langle{T}_{1}x,y\rangle
                    =\langle{x},T_{1}y\rangle
                    =\langle{x},T_{2}y\rangle
                \end{align*}
                $T_{2}$ is compact since if $x_{n}$ is
                bounded in $H_{2}$, then it's bounded in
                $H_{1}$, and thus
                $T_{2}x_{n}=T_{1}x_{n}$, which has a convergent
                subsequence,
                and therefore $T_{2}$ is compact. $H_{2}$ is
                a subspace of $\mathscr{H}$ and is closed since
                $x_{n}\in{H}_{2}$ and
                $x_{n}\rightarrow{x}$ in $\mathscr{H}$ then:
                \begin{equation*}
                    \langle{x},\varphi_{1}\rangle=
                    \langle\lim{x}_{n},\varphi_{1}\rangle=
                    \lim\langle{x}_{n},\varphi_{1}\rangle=
                    0
                \end{equation*}
                Thus $H_{2}$ is closed and is therefore complete,
                and thus $H_{2}$ is a Hilbert space. As before
                there is a $\varphi_{2}$ and a $\lambda_{2}$
                such that $\varphi_{2}\in{H}_{2}$,
                $\norm{\varphi_{2}}=1$, and:
                \begin{equation*}
                    T_{2}\varphi_{2}=\lambda_{2}\varphi_{2}
                \end{equation*}
                But then $T\varphi_{2}=\lambda_{2}\varphi_{2}$,
                where $\lambda_{2}=\pm\norm{T_{2}}$. Moreover,
                $|\lambda_{2}|<|\lambda_{1}|$. Continuing in
                this manner, let
                $H_{n}=\{x\in\mathscr{H}:%
                 x\perp\varphi_{1},\hdots,\varphi_{n-1}\}$
                and $T_{n}:H_{n}\rightarrow{H}_{n}$ be the
                restriction of $T$ onto $H_{n}$. We obtain a
                $\varphi_{n}$ such that
                $\norm{\varphi_{n}}$ and
                $T\varphi_{n}=\lambda\varphi_{n}$. Moreover
                $|\lambda_{n}|\leq|\lambda_{n-1}|$. Thus,
                $|\lambda_{n}|$ forms a monotonically
                decreasing sequence and either there is an
                $N\in\mathbb{N}$ such that
                $\lambda_{N}=0$, in which case for all $n>N$,
                $\lambda_{n}=0$ as well, or for all
                $n\in\mathbb{N}$, $|\lambda_{n}|>0$. In the first
                case it is clear that $\lambda_{n}\rightarrow{0}$.
                In the second case we have
                $\lambda_{n}\varphi_{n}=T\varphi_{n}$ has
                a convergent subsequence for $T$ is compact.
                But $|\lambda_{n}|$ is a monotonically
                decreasing sequence bounder below by zero,
                and therefore converges. Let $c$ be the limit.
                Then $\lambda_{n}x_{n}\rightarrow{cx}$.
                Moreover $\norm{\varphi_{n}-\varphi_{m}}^{2}=2$
                since $\varphi_{n}$ and $\varphi_{m}$ are
                orthogonal when $n\ne{m}$. Therefore
                $\varphi_{n}$ is not a Cauchy sequence. Thus,
                for $\lambda_{n}\varphi_{n}$ to converge,
                $c=0$.
            \end{proof}
            \begin{theorem}[Hilbert-Schmidt Theorem]
                If $x\in\mathscr{H}$, then:
                \begin{equation*}
                    Tx=\sum_{n=1}^{\infty}
                        \lambda_{n}\langle{x},\varphi_{n}\rangle
                        \varphi_{n}
                \end{equation*}
            \end{theorem}
            \begin{proof}
                For define $y_{m}$ as:
                \begin{equation*}
                    y_{m}=x-\sum_{n=1}^{m-1}
                    \langle{x},\varphi_{n}\rangle
                    \varphi_{n}
                \end{equation*}
                Then:
                \begin{equation*}
                    \langle{y}_{m},\varphi_{k}\rangle
                    =\langle
                    x-\sum_{n=1}^{m-1}\rangle{x},\varphi_{n}
                    \rangle\varphi_{n},\varphi_{k}\rangle
                    =\langle{x},\varphi_{k}\rangle-
                    \sum_{n=1}^{m-1}\langle{x},\varphi_{n}
                    \rangle\langle\varphi_{n},\varphi_{k}\rangle
                    =0
                \end{equation*}
                If $\lambda_{N}=0$, then $Ty=T_{n}y=0$ by
                setting $m=N$. Thus:
                \begin{equation*}
                    0=Ty
                    =Tx-\sum_{n=1}^{N-1}
                    \langle{x},\varphi_{n}\rangle{T}\varphi_{n}
                    =Tx-\sum_{n=1}^{N-1}
                    \lambda_{n}\langle{x},\varphi_{n}\rangle
                    \varphi_{n}
                \end{equation*}
                If $\lambda_{n}\ne{0}$ for all $n\in\mathbb{N}$,
                then $y_{m}\in{H}_{m}$ and therefore:
                \begin{equation*}
                    x=(x-y_{m})+y_{m}
                \end{equation*}
                But $x-y_{m}$ is orthogonal to $y_{m}$, and
                therefore by Pythagoras:
                \begin{equation*}
                    \norm{x}^{2}=
                    \norm{x-y_{m}}^{2}+\norm{y_{m}}^{2}
                \end{equation*}
                Therefore $\norm{y_{m}}\leq\norm{x}$. Also:
                \begin{equation*}
                    \norm{Ty_{m}}=\norm{T_{m}y_{m}}
                    \leq\norm{T_{m}}\norm{y_{m}}
                    =|\lambda_{m}|\norm{y_{m}}
                \end{equation*}
                And therefore:
                \begin{equation*}
                    \norm{Tx-\sum_{n=1}^{m-1}
                    \lambda_{n}\rangle{x},\varphi_{n}\rangle
                    \varphi_{n}}\leq|\lambda_{m}|\norm{x}
                    \rightarrow{0}
                \end{equation*}
            \end{proof}
            \begin{theorem}
                If $T$ is a compact self-adjoint operator
                on a Hilbert Space $\mathscr{H}$, then there is
                an orthogonal basis for $\mathscr{H}$
                consisting of eigenvector of $T$.
            \end{theorem}
            \begin{proof}
                For any $x\in\mathscr{H}$:
                \begin{equation*}
                    Tx=\sum_{n=1}^{\infty}\lambda_{n}
                    \rangle{x},\varphi_{n}\rangle\varphi_{n}
                \end{equation*}
                This sum may be infinite.
                Let $\{\psi_{\alpha}\}_{\alpha\in{A}}$ be
                and orthogonal basis of $\nul(T)$. Then
                $T\psi_{\alpha}=0$ for all $\alpha\in{A}$,
                and thus $0$ is an eigenvalue for all
                $\psi_{\alpha}$. But also:
                \begin{equation*}
                    \lambda_{n}\langle\varphi_{n},
                    \psi_{\alpha}\rangle
                    =\langle\lambda_{n}\varphi_{n},
                    \psi_{\alpha}\rangle
                    =\langle{T}\varphi_{n},\psi_{\alpha}\rangle
                    =\langle\varphi_{n},T\psi_{\alpha}\rangle
                    =0
                \end{equation*}
                Thus, for all $\alpha\in{A}$ and all
                $n\in\mathbb{N}$,
                $\varphi_{n}\perp\psi_{\alpha}$. Then by
                Hilbert-Schmidt, for every $x\in\mathscr{H}$:
                \begin{equation*}
                    T\big(x-\sum_{n=1}^{\infty}
                    \langle{x},\varphi_{n}\rangle\varphi_{n}\big)
                    =Tx-\sum_{n=1}^{\infty}\lambda_{n}
                    \langle{x},\varphi_{n}\rangle\varphi_{n}=0
                \end{equation*}
                Thus:
                \begin{equation*}
                    x=\sum_{n=1}^{\infty}\lambda_{n}
                    \langle{x},\varphi_{n}\rangle\varphi_{n}+
                    \sum_{\alpha\in{A}}
                    \langle{x},\psi_{\alpha}\rangle\psi_{\alpha}
                \end{equation*}
            \end{proof}
    \section{More Stuffs}
        \subsection{Lecture 12: December 3, 2018}
            Cauchy-Schwarz says that:
            \begin{equation}
                \Big(\int_{a}^{b}x(s)y(s)\diff{s}\Big)^{2}
                \leq\Big(\int_{a}^{b}x^{2}(s)\diff{s}\Big)
                \Big(\int_{a}^{b}y^{2}(s)\diff{s}\Big)
            \end{equation}
            So:
            \begin{align}
                |Tx(t)|^{2}&=
                \Big(\int_{0}^{t}x(s)\diff{s}\Big)^{2}\\
                &\leq\Big(\int_{0}^{t}1^{2}\diff{s}\Big)
                \Big(\int_{0}^{t}x(s)^{2}\diff{s}\Big)\\
                &=t\int_{0}^{t}x(s)^{2}\diff{s}\\
                &\leq{t}\int_{0}^{1}x(s)^{2}\diff{s}\\
                &=t\norm{x}_{2}^{2}
            \end{align}
            So then:
            \begin{equation}
                \int_{0}^{1}Tx(t)^{2}\diff{t}
                \leq\int_{0}^{1}t\norm{x}_{2}^{2}\diff{t}
            \end{equation}
            This then implies:
            \begin{align}
                \norm{Tx}^{2}&\leq
                \frac{1}{2}\norm{x}_{2}^{2}\\
                \Rightarrow
                \norm{T}&\leq\frac{1}{\sqrt{2}}
            \end{align}
            Letting $x(t)=1$, we have $Tx=t$. Thus:
            \begin{equation}
                \norm{Tx}^{2}=\int_{0}^{1}t^{2}\diff{t}
                =\frac{1}{3}
            \end{equation}
            And thus $\norm{Tx}=1/\sqrt{3}$. So
            $1/\sqrt{3}\leq\norm{T}\leq{1}/\sqrt{2}$.
            If $x(t)=1-t$, then $Tx(t)=t-t^{2}/2$. So
            $\norm{Tx}=\sqrt{2/15}$. We're getting closer to
            the answer. Letting $x(t)=\cos(\pi{t}/2)$ gives
            us the norm. We now have to show this. Write:
            \begin{equation*}
                x(t)=\sum_{n=1}^{\infty}b_{n}\sin(n\pi{t})
            \end{equation*}
            Then:
            \begin{equation*}
                Tx(t)=\sum_{n=1}^{\infty}
                \frac{b_{n}}{n\pi}\big[1-\cos(n\pi{t})\big]
                =\Big(\sum_{n=1}^{\infty}\frac{b_{n}}{n\pi}\Big)
                -\Big(\sum_{n=1}^{\infty}\frac{b_{n}}{n\pi}
                \cos(n\pi{t})\Big)
            \end{equation*}
            But:
            \begin{align*}
                \norm{x}^{2}&=\sum_{n=1}^{\infty}\frac{b_{n}^{2}}{2}
                &
                \norm{Tx}^{2}&=
                \Big(\sum_{n=1}^{\infty}\frac{b_{n}}{n\pi}\Big)^{2}
                +\sum_{n=1}^{\infty}\frac{b_{n}^{2}}{2n^{2}\pi^{2}}
            \end{align*}
            Let's maximize $\norm{Tx}^{2}$ subject to
            $\norm{x}^{2}$. Using Lagrange multipliers we get:
            \begin{equation*}
                \frac{2}{n\pi}\Big(\sum_{k=1}^{\infty}
                \frac{b_{k}}{k\pi}\Big)
                +\frac{b_{n}}{n^{2}\pi^{2}}
                =\lambda^{2}b_{n}
            \end{equation*}
            Letting $A=\sum_{k=1}^{\infty}b_{k}/k\pi$, we obtain:
            \begin{equation*}
                b_{n}=\frac{2n\pi}{\lambda^{2}n^{2}\pi^{2}-1}A
            \end{equation*}
            So then:
            \begin{equation*}
                A=\sum_{n=1}^{\infty}
                \frac{2}{\lambda^{2}n^{2}\pi^{2}-1}A
            \end{equation*}
            And thus:
            \begin{equation*}
                \sum_{n=1}^{\infty}
                \frac{2}{\lambda^{2}n^{2}\pi^{2}-1}=1
            \end{equation*}
            And this is the expansion of cotangent:
            \begin{equation*}
                1-\frac{1}{\lambda}\cot\big(\frac{1}{\lambda}\big)
                =1
            \end{equation*}
            And therefore:
            \begin{equation*}
                \lambda=\frac{\pi}{2},\frac{3\pi}{2},\frac{5\pi}{2},
                \hdots
            \end{equation*}
            Finally:
            \begin{equation*}
                x(t)=\sum_{n=1}^{\infty}
                b_{n}\sin(n\pi{t})=
                \sum_{n=1}^{\infty}
                \frac{2n\pi}{4n^{2}-1}A\sin(n\pi{t})
                =\sqrt{2}\cos\big(\frac{\pi}{2}t\big)
            \end{equation*}
            But before we can do all of this we need to show
            that there is such a maximum. That is, the step that
            involves Lagrange multipliers is valid. We want
            an $x$ such that $\norm{x}=1$ and
            $\norm{Tx}=\norm{T}$. Certainly there is a sequence
            such that $\norm{x_{n}}=1$ and
            $\norm{Tx_{n}}\rightarrow\norm{T}$. Since $T$ is
            compact we may assume, taking subsequences as
            necessary, that
            $Tx_{n}\rightarrow{y}$. If $T$ is self adjoint
            and $x_{n}\rightarrow{x}$, then
            $Tx=y$. It would be nice if Bolzano-Weiestrass worked
            and we could say
            $\norm{x_{n}}$ bounded implies that
            $x_{n}\rightarrow{x}$, but this is not always
            true in infinite dimensions. We'll need to
            weaken our notion of convergence for this. We
            could simply say that everything converges to
            everything, which is the chaotic topology, but
            this is not very useful for it loses uniqueness.
            \begin{definition}
                A weakly convergent sequence in an inner product
                space $X$ is a sequence
                $x:\mathbb{N}\rightarrow{X}$ such that there
                is an $a\in{X}$ such that
                for all $z\in{X}$,
                $\langle{x_{n},z}\rangle\rightarrow%
                 \langle{a,z}\rangle$. We write
                 $x_{n}\overset{w}{\rightarrow}{a}$
            \end{definition}
            For example, if $X$ is a Hilbert space and
            $e_{n}$ is an orthonormal basis, then
            $e_{n}\rightarrow{0}$ since, for all $z$:
            \begin{equation*}
                \norm{z}^{2}=\sum_{n=1}^{\infty}
                \langle{e_{n},z}\rangle^{2}
            \end{equation*}
            And thus $\langle{e_{n},z}\rangle\rightarrow{0}$.
            But $\langle{0,z}\rangle=0$, so
            $e_{n}\overset{w}{\rightarrow}{0}$. Normal convergence
            is also called strong convergence. Strong convergence
            implies weak convergence. For if
            $x_{n}\rightarrow{a}$, then
            $\langle{a-x_{n},z}\rangle\rightarrow{0}$ for all
            $z$, and thus $x_{n}\overset{w}{\rightarrow}{a}$.
            Moreover, weak limits are unique. If
            $T$ is a bounded linear operator on a Hilbert space
            $H$, and if $x_{n}$ converges weakly to $a$,
            then $Tx_{n}\overset{w}{\rightarrow}{Ta}$.
            If $x$ converges weakly to $a$ and if $T$ is a
            compact linear operator on $H$, then
            $Tx_{n}$ converges strongly to $Tx$.
            \begin{theorem}
                If $H$ is a Hilbert space,
                $T$ is a compact operator on $H$,
                and if $x:\mathbb{N}\rightarrow{H}$ is a
                weakly convergent sequence such that
                $x_{n}\overset{w}{\rightarrow}{a}$,
                then the sequence
                $y:\mathbb{N}\rightarrow{H}$ defined by
                $y_{n}=Tx_{n}$ is such that 
                $y_{n}\rightarrow{Tx}$. That is,
                $Tx_{n}$ converges strongly to $Ta$.
            \end{theorem}
            \begin{proof}
                For suppose not. As $T$ is compact and
                $x_{n}$ is bounded, there is
                a convergent subsequence. Let $a_{1}$ be the
                limit. If the limit is not unique then there
                is another convergent subsequence with
                a different limit $a_{2}$. But
                $Tx_{n}$ converges weakly to
                $a$, and strong convergence implies weak
                convergence. Therefore $a_{1}=a_{2}=a$, a
                contradiction. Therefore, $Tx_{n}$ converges
                strongly to $Ta$.
            \end{proof}
            \begin{theorem}
                If $x_{n}\overset{w}{\rightarrow}{a}$
                and $\norm{x_{n}}\rightarrow{C}$,
                then $\norm{a}\leq{C}$.
            \end{theorem}
            \begin{proof}
                For:
                \begin{align*}
                    \norm{a}^{2}&=|\langle{a,a}\rangle|\\
                    &=\underset{n\rightarrow\infty}{\lim}
                    |\langle{x_{n},a}\rangle|\\
                    &\leq\underset{n\rightarrow\infty}{\lim}
                    \norm{x_{n}}\norm{a}\\
                    &=C\norm{a}
                \end{align*}
                Dividing by $\norm{a}$ gives the result.
            \end{proof}
            We now prove that there exists $x$ such that
            $\norm{x}=1$ and $\norm{Tx}=\norm{T}$. This works
            for any compact linear operator on a Hilbert space.
            We can always find a sequence, by definition, such
            that $\norm{x_{n}}=1$ and
            $\norm{Tx_{n}}\rightarrow\norm{T}$. But since
            $x_{n}$ is bounded by 1 there is a weakly
            convergent subsequence (Still to be proved).
            This is ``Bolzano-Weierstrass,'' of infinite
            dimensions. Then $x_{n}$ converges weakly to
            $x$, and thus $Tx_{n}$ converges strongly to
            $Tx$. Then $\norm{Tx}=\norm{T}$. Finally,
            $\norm{x}\leq\lim\norm{x_{n}}=1$, and
            $\norm{T}=\norm{Tx}\leq\norm{T}\norm{x}$,
            so $\norm{x}\geq{1}$, and therefore
            $\norm{x}=1$. Let's show $T:L^{2}\rightarrow{L}^{2}$
            defined by $Tx(t)=\int_{0}^{t}x(s)\diff{s}$ is
            compact. Suppose $x_{n}$ is bounded in $L^{2}$ with
            bound $M$. That is, $\norm{x_{n}}\leq{M}$.
            Then:
            \begin{align*}
                |Tx_{n}(t)|^{2}&=
                \Big|\int_{0}^{t}x_{n}(s)^{2}\diff{s}\Big|^{2}\\
                &\leq
                \Big(\int_{0}^{1}|x_{n}(s)|\diff{s}\Big)^{2}\\
                &\leq\Big(\int_{0}^{1}\diff{s}\Big)
                \Big(\int_{0}^{1}|x_{n}(s)|^{2}\diff{s}\Big)\\
                &=\norm{x_{n}}^{2}\\
                &\leq{M}^{2}
            \end{align*}
            Taking the supremum over $t\in[0,1]$ gives:
            \begin{equation*}
                \norm{Tx_{n}}_{\infty}\leq{M}
            \end{equation*}
            So $Tx_{n}$ is bounded in $C[0,1]$. Also, if
            $0\leq{t}_{1}$ and $t_{2}\leq{1}$, then:
            \begin{align*}
                |Tx_{n}(t_{2})-Tx_{n}(t_{1})|^{2}
                &\leq\Big(\int_{t_{1}}^{t_{2}}|x_{n}(s)|
                \diff{s}\Big)^{2}\\
                &\leq\Big(\int_{t_{1}}^{t_{2}}\diff{s}\Big)
                \Big(\int_{t_{1}}^{t_{2}}|x_{n}(s)|^{2}
                \diff{s}\Big)\\
                &=(t_{2}-t_{1})\int_{t_{1}}^{t_{2}}
                |x_{n}(s)|^{2}\diff{s}\\
                &\leq(t_{2}-t_{1})\norm{x_{n}}^{2}\\
                \leq{M}^{2}(t_{2}-t_{1})
            \end{align*}
            So $|Tx_{n}(t_{2})-Tx_{n}(t_{1})|$ can be made
            arbitrarily small for $t_{2}$ and $t_{1}$ close enough,
            independent on the $n$. That is, a
            $\delta$ may be chosen independent of $n$. This is
            the criterion for equicontinuity.
            The compactness of $[0,1]$ then gives uniform
            equicontinuity. Arzela-Ascoli then says there is a
            subsequence $Tx_{n}\rightarrow{y}$,
            with $y\in{c}[0,1]$. That is,
            $\norm{Tx-y}_{\infty}\rightarrow{0}$. But then:
            \begin{align*}
                \norm{Tx_{n}-y}^{2}
                &=\int_{0}^{1}|Tx_{n}(t)-y(t)|^{2}\diff{t}\\
                &\leq\int_{0}^{1}
                \norm{Tx_{n}-y}_{\infty}^{2}\diff{t}\\
                =\norm{Tx_{n}-y}_{\infty}^{2}
            \end{align*}
            And this converges to zero. Thus,
            $Tx_{n}\rightarrow{y}$.
            \begin{theorem}[Baire Category Theorem]
                If $(X,d)$ is a complete metric and
                $C_{n}$ is a sequence of closed sets such
                that $X=\cup_{n=1}^{\infty}C_{n}$, then
                there is an $N\in\mathbb{N}$ such that
                $C_{N}$ contains an open subset.
            \end{theorem}
            \begin{proof}
                Let $r_{1}\in(0,1)$, $x_{1}\in{X}$.
                If $B_{r_{1}}(X_{1})\subset{C_{1}}$ then we're
                done. Otherwise $B_{r_{1}}(x_{1})\setminus{C_{1}}$
                is a non-empty set, so there exists
                $x_{n}\in{B}_{r_{2}}(x_{1})$, where
                $r_{2}\in(0,1/2)$. By induction, choose
                $r_{n}\in(0,1/n)$ and $x_{n}$ such that
                $x_{n}\in{B}_{r_{n-1}}(x_{n-1})$ and
                $\overline{B_{r_{n}}(x_{n})}\subset%
                 B_{r_{n-1}}(x_{n-1})$. For $n<m$,
                $x_{m}\in\overline{B_{r_{n}}(x_{n})}$,
                so $d(x_{n},x_{m})<1/n$. Then $x_{n}$ is
                Cauchy, but $X$ is complete so there is a limit
                $x$. Since $\overline{B_{r_{n}}(x_{n})}$ is
                closed, $x\in\overline{B_{r_{n}}(x_{n})}$. But
                $X=\cup_{n=1}^{\infty}C_{n}$ and thus there is
                an $N$ such that $x\in{N}$. But then
                $B_{r_{n}}(x)\subset{C}_{N})$, so $C_{N}$ contains
                an open subset.
            \end{proof}
            The Baire Category Theorem is used to prove the
            Uniform Boundedness Theorem, which is also called
            the Banach-Steinhau theorem.
            \begin{theorem}[Uniform Boundedness Theorem]
                If $H$ is a Hilbert space, and if
                $x_{n}\overset{w}{\rightarrow}{a}$, then
                $\norm{x_{n}}$ is bounded.
            \end{theorem}
            \begin{proof}
                Let
                $C_{k}=\{y\in{H}:|\langle{x_{n},y}\rangle\leq{k}\}$.
                Then $C_{k}$ is closed since $y_{j}\in{C_{k}}$
                and $y_{j}\rightarrow{y}$ implies that:
                \begin{align*}
                    |\langle{x_{n},y_{j}}\rangle|&\leq{k}\\
                    \Rightarrow
                    \underset{j\rightarrow\infty}{\lim}
                    |\langle{x_{n},y_{j}}\rangle|&\leq{k}\\
                    \Rightarrow
                    |\langle{x_{n},y}\rangle|&\leq{k}
                \end{align*}
                Moreover $H=\cup_{k=1}^{\infty}C_{k}$. By
                the Baire Category Theorem there is a
                $k\in\mathbb{N}$ such that
                $C_{k}$ contains an open subset. Let $z_{0}\in{H}$
                and $r\in\mathbb{R}$ be such that
                $B_{r}(z_{0})\subset{C_{k}}$. Let
                $y\in{H}$, $y\ne{0}$, and set $z=z_{0}+\alpha{y}$.
                where:
                \begin{equation*}
                    \alpha=\frac{r}{2\norm{y}}
                \end{equation*}
                Then $\norm{z-z_{0}}<r$, so
                $z\in{C}_{k}$. That is,
                $|\langle{x_{n},z}\rangle|\leq{k}$ for all
                $n$. Thus we have:
                \begin{align*}
                    |\langle{x_{n},y}\rangle|&=
                    |\langle{x_{n},\frac{z-z_{0}}{\alpha}}\rangle|\\
                    &\leq
                    \frac{1}{\alpha}\Big(
                    |\langle{x_{n},z\rangle}|+
                    |\langle{x_{n},z_{0}}\rangle|\\
                    &\leq
                    \frac{1}{\alpha}(k+k)\\
                    &=\frac{4k}{r}\norm{y}
                \end{align*}
                This is true of any $y\in{H}$. Choosing $y=x_{n}$,
                we get:
                \begin{equation*}
                    \norm{x_{n}}^{2}\leq\frac{4k}{r}\norm{x_{n}}
                \end{equation*}
                Dividing by $\norm{x_{n}}$ shows boundedness.
            \end{proof}
        \subsection{Lecture 13: December 10, 2018}
            \begin{equation}
                Tx(t)=\int_{0}^{1-t}x(s)\diff{s}
            \end{equation}
            If $\norm{x}_{2}\leq{M}$, then:
            \begin{equation}
                |Tx(t)|^{2}=
                \Big|\int_{0}^{1-t}x(s)\diff{s}\Big|^{2}
                \leq\Big(\int_{0}^{1-t}\diff{s}\Big)
                \Big(\int_{0}^{1-t}x(s)^{2}\diff{s}\Big)
                =(1-t)\norm{x}_{2}^{2}
            \end{equation}
            So we have:
            \begin{equation}
                \norm{Tx}^{2}=\int_{0}^{1}Tx(t)^{2}\diff{t}
                \leq\int_{0}^{1}(1-t)\norm{x}_{2}^{2}\diff{t}
                =\frac{1}{2}\norm{x}_{2}^{2}
                \leq\frac{1}{2}M^{2}
            \end{equation}
            So:
            \begin{align}
                |Tx(t_{1})-Tx(t_{2})|^{2}
                &=\Big|\int_{0}^{1-t_{1}}x(s)\diff{s}-
                \int_{0}^{1-t_{2}}x(s)\diff{s}\Big|^{2}\\
                &=\Big|\int_{1-t_{2}}^{1-t_{1}}x(s)
                \diff{s}\Big|^{2}\\
                &\leq\int_{1-t_{2}}^{1-t_{1}}\diff{s}
                \int_{1-t_{2}}^{1-t_{1}}x(s)^{2}\diff{s}\\
                &=|t_{1}-t_{2}|\norm{x}_{2}^{2}\\
                &\leq{M}^{2}|t_{1}-t_{2}|
            \end{align}
            This shows equicontinuity, and thus Arzela-Ascoli
            shows that $T$ is compact.
            \begin{theorem}[Banach-Alaoglu-Hilbert Theorem]
                If $H$ is a Hilbert space and
                $x:\mathbb{N}\rightarrow{H}$ is a bounded sequence,
                then there is a weakly convergent subsequence.
            \end{theorem}
            The next question would be ``What about Banach Space?''
            If $X$ is a normed space, we say $x_{n}$ converges
            weakly to $x$, denoted
            $x_{n}\overset{w}{\rightarrow}{x}$ if for all
            bounded linear functional $f\in{X'}$,
            $f(x_{n})\rightarrow{f(x)}$.
            For example let $X=\ell^{1}$. We have proven that the
            dual of $\ell^{1}$ is $\ell^{\infty}$ and elements
            of the dual take the form:
            \begin{equation}
                f(x)=\sum_{n=1}^{\infty}z_{i}x_{i}
            \end{equation}
            Where $z_{i}\in\ell^{\infty}$. Thus, $z_{i}$ is
            bounded and $x_{i}$ is absolutely convergent,
            since $x_{i}\in\ell^{1}$, and thus the product
            is absolutely convergent. That is,
            $x_{i}z_{i}\in\ell^{1}$. The problem with this is
            that we don't know that $X'\ne\{0\}$ for a given
            Banach space. There are plenty these, enough to
            separate points, thanks to the Hahn-Banach theorem.
            This says that if $f$ is a bounded linear functional
            on a subspace $M$ of $X$, then there exists
            $F\in{X'}$ such that
            $\norm{F}_{X'}=\norm{f}_{M'}$ and
            $F(x)=f(x)$ for all $x\in{M}$. So given $m\in{M}$,
            then $\alpha{m}\in{M}$ for al $\alpha\in\mathbb{R}$.
            Define $f(m)=k$, for some $k\in\mathbb{R}$. Then
            $f(\alpha{m})=\alpha{k}$ and
            $|f(\alpha{m})|=|\alpha||f(m)|%
             =|k|\norm{\alpha{m}}/\norm{m}$. And
             $|f(\alpha{m})|/\norm{\alpha{m}}=|k|/\norm{m}$.
            Thus $f\in{M'}$. Thus, Hahn-Banach can be exteneded
            to all of $X$. So we can for all $x\in{X}$ and for
            all $r\in\mathbb{R}$, there is a bounded linear
            function $f\in{X'}$ such that
            $f(x)=r$. If $m_{1}$ and $m_{2}$ are independent
            (That is, $m_{1}\ne\alpha{m}_{2}$ for any real
            number $\alpha$), then let
            $M=\{am_{1}+bm_{2}:a,b\in\mathbb{R}\}$. Define
            $f(am_{1}+bm_{2})=a\norm{m_{1}}$.
            Then $f\in{M'}$ so this
            can be extended to all of $X$ by the Hahn-Banach
            theorem. But $f(m_{1})=1$ and
            $f(m_{2})=0$, so $f$ separates points. Thus,
            if $x_{n}$ converges weakly to $x$ in a
            Banach space, and if $x_{n}$ also converges weakly
            to $y$, then $x=y$. The uniform boundedness theorem
            also holds if $X$ is complete. The
            Banach-Alaoglu-Hilbert theorem fails in a general
            Banach space. For example, $\ell^{1}$. We now talk
            about weak* convergence. In $X'$, we say
            $f_{n}$ converges weak* to $f$ if
            $f_{n}(x)\rightarrow{f(x)}$ for all
            $x\in{X}$. Banach-Alaoglu holds if weak is replaced
            with weak* and if $X'$ is separable. The double
            dual, $X''$, is the dual of $X'$. $X$ is
            embedded in $X''$. That is, $X$ embeds naturally
            in $X''$ as follows: Define
            $C:X\rightarrow{X''}$ as follows. If $x\in{X}$,
            $Cx(f)=f(x)$ for all $f\in{X'}$. It's easy to
            show that $\norm{cx}=\norm{x}$. Indeed,
            $C$ is an isometry on $X$ to $X''$. If $C$ is
            onto, we say that $X$ is reflexive. There are
            Banach spaces that are not reflexive that can
            be isometrically embedded into their second dual,
            but the canonical map is not such an embedding.
            \begin{theorem}
                If $H$ is a Hilbert space, then $H$ is reflexive.
            \end{theorem}
            \begin{theorem}
                If $X$ is reflexive, then
                $X=X''$.
            \end{theorem}
            \begin{theorem}
                If $X$ is reflexive, weak* convergence
                implies weak convergence.
            \end{theorem}
            From topology, a subbasis for a topological space
            is a collection of sets such that every open set can
            be written as arbitrary unions and finite intersections
            of the sets. Choose as the subbasis:
            \begin{equation}
                \{f^{-1}(-\infty,a):a\in\mathbb{R},f\in{X'}\}
            \end{equation}
            If $X'$ is separable, then this space is metrizable.
            For let $A$ be a countable dense subset, and define
            the metric $d$ as:
            \begin{equation}
                d(f,g)=\sum_{x\in{A}}
                \frac{|f(x)-g(x)|}{1+|f(x)-g(x)|}2^{-n}
            \end{equation}
            \begin{theorem}[Banach-Alaoglu]
                If $X$ is a normed vector space, and if
                $\tau$ is the weak* topology, then
                $\overline{B_{1}(0)}$ is a compact subset of
                $(X',\tau)$.
            \end{theorem}
            Adams Sobolev Spaces.
    \section{Old Notes}
        \subsection{Summary of Lectures}
            The boundary of a circle
            in $\mathbb{R}^{2}$ is nowhere dense,
            with respect to the metric on $\mathbb{R}^{2}$.
            Any open ball about any point on the circle contains
            points not on the circle, and thus it has empty
            interior. Something about $\varepsilon$ nets.
            \subsubsection{Normed Spaces and Banach Spaces}
                There are notions of subspace,
                linear combination, independence, spanning,
                dimension, basis, Hamel basis, and
                \textit{convexity}.
                Open and closed balls are convex. 
                A subspace of a Banach space is complete
                iff closed. Schauder basis.
                A Schauder basis implies separable.
                If $\{x_{1},\hdots,x_{n}\}$ is independent,
                then there exists a $c>0$ such that, for all
                $\boldsymbol{\alpha}$,
                $|\boldsymbol{\alpha}\cdot\mathbf{x}|%
                 \geq{c}\norm{\boldsymbol{\alpha}}$.
                Finite dimensional subspaces are complete,
                as are closed subspaces. In finite dimensional
                normed spaces, a space is compact if and only
                if it is closed and bounded.
                Riesz's Lemma says that if $Z$ is a subspace
                of a normed space $X$, and if $Y$ is a proper
                closed subspace of $Z$, then there is a $z\in{Z}$
                such that $\norm{z}=1$ and $D(z,Y)\geq{1/2}$.
                A corollary of this is that $B_{1}(0)$ is compact
                if and only if $X$ is finite dimensional.
            \subsubsection{Linear Operators}
                Identity, zero, differentiation,
                and integration. Domain/Range
                of a linear operator, the null space.
                Inverse of a linear operator is linear.
                $(ST)^{-1}=T^{-1}S^{-1}$.
                In finite dimension all linear operators are
                continuous. An operator is bounded
                if and only if it is continuous. If a linear
                operator is continuous at some point, then it
                is continuous everywhere. An operator is bounded
                if and only if its null space is closed.
                There is something called the extension of a
                bounded linear operator. $B(X,Y)$ is the set of
                bounder linear operators
                from $X$ to $Y$. This is complete if and only if
                $Y$ is complete. A functional is a mapping from a
                vector space $X$ into the real numbers $\mathbb{R}$.
                For continuous linear functional, continuity at
                $0$ implies continuity everywhere.
                There is something called the dual space
                $X'$, which is itself a Banach space.
            \subsubsection{Inner Product and Hilbert Spaces}
                If $x_{n}\rightarrow{x}$ and $y_{n}\rightarrow{y}$,
                then
                $\langle{x_{n},y_{n}}\rangle\rightarrow\langle{x,y}\rangle$.
                There's a notion of orthogonal sets,
                and orthonormality. If $(e_{n})$ is orthonormal basis,
                then $x=\sum\langle{x,e_{k}}\rangle{e_{k}}$
                for all $x$. Gram-Schmidt procedure.
                $\sum\alpha_{k}e_{k}$ converges if and only if
                $\sum|\alpha_{k}|^{2}$ converges.
                A set $M$ is total in a Hilbert space $H$ is the
                span of the closure of $M$ is equal
                to $H$. If $M$ is complete, then it is
                total if and only if $M^{\perp}=0$.
                Parseval's theorem. Legendre, Hermite, and Laguerre
                polynomials are things.
                Self adjoint, unitary, and normal operators.
                $T^{*}=T$, $T^{*}=T^{-1}$, and $T^{*}T=TT^{*}$. If
                $X$ is a vector space over the complex numbers,
                and if $T$ is self adjoint, then
                $\langle{Tx,x}\rangle$ is a real number for all $x$.
            \subsubsection{Compact Linear Operators}
                If $T$ is compact and linear, then it is bounded and
                continuous. An operator is compact and linear
                if and only if
                for all bounded sequences $x_{n}$,
                $Tx_{n}$ has a convergent subsequence. Compact linear
                operators form a vector space. The rank of an
                operator is
                the dimension of its image. If $T$ is linear,
                bounded, and of finite rank, then it is compact.
                If $T_{n}$ is a sequence of compact linear
                operators, if $Y$ is
                complete, and if $\norm{T_{n}-T}\rightarrow{0}$, then
                $T$ is compact. A sequence $x_{n}$ converges weakly to
                $x$ if, for all $y$,
                $\langle{x_{n},y}\rangle\rightarrow\langle{x,y}\rangle$.
                If $x_{n}$ converges weakly to $x$, then
                and if $T$ is a compact linear operator, then
                $Tx_{n}\rightarrow{Tx}$. If $H$ is a Hilbert space,
                $T$ is a compact self-adjoint operator, and if
                $x_{n}$ converges weakly to $x$, then
                $\langle{Tx_{n},x_{n}}\rangle\rightarrow\langle{Tx,x}\rangle$.
                If $T:H\rightarrow{H}$ is compact and linear, then so
                is its adjoint. The Hilbert-Schmidt theorem says that
                compact self-adjoint operators on a Hilbert space $H$
                have an orthonormal basis of eigenvectors. All of this
                has applications to integral operators and
                Sturm-Liouville Theory.
            \subsubsection{Fundamental Theorems}
                Zorn's Lemma. Hahn-Banach Theorem. Sublinear functionals.
                If $X$ is a normed space, and $Z$ is a subspace, and if
                $f\in{Z'}$, then $f$ be extended to $X$ such that
                $\norm{f}_{X}=\norm{f}_{Z}$.
                This extends Hilbert spaces by Riesz.
                If $X$ is a normed space
                and $x\ne{0}$, then there is an $f\in{X'}$ such that
                $\norm{f}=1$ and $f(x_{0})=\norm{x_{0}}$.
                For all $x$,
                $\norm{x}=\sup\{\norm{f(x)}/\norm{f}:f\in{X'},f\ne{0}\}$.
                There's a thing called bounded variation.
                If $x\in{X}$ and
                $g_{x}(f)=f(x)$ for $f\in{X'}$, then
                $g_{x}\in{X''}$ and $\norm{g_{x}}=\norm{x}$.
                Reflexive implies complete.
                Finite and Hilbert implies reflexive.
                $X'$ separable implies $X$ is separable.
                $X$ separable and reflexive implies
                $X'$ is separable.
                Strong convergence implies weak convergence.
                The converse is not true. If $X$ is finite
                dimensional, then weak convergence
                implies strong convergence. Weak convergence implies
                $\norm{x_{n}}$ is bounded. If
                $x_{n}\rightarrow{x}$ weakly, and if
                $\norm{x_{n}}\rightarrow\norm{x}$, then
                $x_{n}\rightarrow{x}$ strongly.
                Open mapping theorem.
                Closed graph theorem.
                Differentiation is a closed operator on
                $C^{1}[a,b]\rightarrow{C[a,b]}$.
    \section{Notes from Dartmouth}
    \section{Metric Spaces}
        \subsection{Basic Definitions}
            \begin{ldefinition}{Pseudo-Metric}
                  {Funct_Analysis_Pseudo_Metric}
                A pseudo-metric on a set $X$ is a function
                $\rho:X\times{X}\rightarrow[0,\infty)$ such that,
                for all $x,y,z\in{X}$, it is true that:
                \begin{align}
                    \rho(x,y)&=\rho(y,x)
                    \tag{Symmetry}\\
                    \rho(x,z)&\leq\rho(x,y)+\rho(y,z)
                    \tag{Triangle Inequality}
                \end{align}
            \end{ldefinition}
            \begin{ldefinition}{Pseudo-Metric Space}
                  {Funct_Analysis_Pseudo_Metric_Space}
                A pseudo-metric space, $(X,\rho)$, is a set
                $X$ and a pseudo-metric $\rho$ on $X$.
            \end{ldefinition}
            \begin{theorem}
                There exist pseudo-metric spaces $(X,\rho)$
                such that for all $x\in{X}$, $\rho(x,x)>0$.
            \end{theorem}
            \begin{proof}
                For let $X=\mathbb{R}$ and define
                $\rho:\mathbb{R}^{2}\rightarrow[0,\infty)$ by:
                \begin{equation}
                    \rho(x,y)=1+|x|+|y|
                \end{equation}
                Then $\rho$ is a pseudo-metric. For it is symmetric,
                since:
                \begin{equation}
                    \rho(x,y)=1+|x|+|y|=1+|y|+|x|=\rho(y,x)
                \end{equation}
                Moreover, it obeys the triangle inequality:
                \begin{subequations}
                    \begin{align}
                        \rho(x,z)&=1+|x|+|z|\\
                        &\leq{1}+|x|+|z|+2|y|+1\\
                        &=(1+|x|+|y|)+(1+|y|+|z|)\\
                        &=\rho(x,y)+\rho(y,z)
                    \end{align}
                    Thus, $\rho$ is a pseudo-metric. However, for
                    all $x\in\mathbb{R}$:
                    \begin{equation}
                        \rho(x,x)=1+|x|+|x|=1+2|x|\geq{1}>0
                    \end{equation}
                    Thus, there are no $x\in{X}$ such that
                    $\rho(x,x)=0$. Therefore, etc
                \end{subequations}
            \end{proof}
            If we require $\rho(x,x)=0$ for all $x\in{X}$, we
            can still have the case where elements cannot be
            distinguished from. That is, there may be
            $x,y\in{X}$ such that $x\ne{y}$, but
            $\rho(x,y)=0$.
            \begin{theorem}
                There exist pseudo-metric spaces $(X,\rho)$
                such that for all $x\in{X}$, $\rho(x,x)=0$,
                and there are distinct elements $x,y\in{X}$
                such that $\rho(x,y)=0$.
            \end{theorem}
            \begin{proof}
                For let $X$ have at least two distinct elements,
                and let $\rho:X^{2}\rightarrow[0,\infty)$
                be defined by:
                \begin{equation}
                    \rho(x,y)=0
                \end{equation}
                Then $\rho$ is a pseudo-metric. Symmetry and
                the triangle inequality are both trivial.
                However, since there are at least two distinct
                elements in $X$, we have unique points such that
                $\rho(x,y)=0$. Therefore, etc.
            \end{proof}
            \begin{ldefinition}{Metric Space}
                  {Funct_Analysis_Metric_Space}
                A metric space is a pseudo-metric space $(X,d)$
                such that:
                \begin{equation}
                    d(x,y)=0\Longleftrightarrow{x}=y
                    \tag{Definiteness}
                \end{equation}
            \end{ldefinition}
            \begin{ldefinition}{Semi-Norm}
                  {Funct_Analysis_Semi_Norm}
                A semi-norm on a vector space $V$ over a field
                $\mathbb{F}\subseteq\mathbb{C}$ is a function
                $\norm{\cdot}:V\rightarrow[0,\infty)$ such that, for
                all $v\in{V}$ and $\alpha\in\mathbb{F}$,
                it is true that:
                \begin{align}
                    \norm{\alpha{v}}
                    &=|\alpha|\norm{v}
                    \tag{Homogeneity}\\
                    \norm{v+w}
                    &\leq\norm{v}+\norm{w}
                    \tag{Triangle Inequality}
                \end{align}
            \end{ldefinition}
            \begin{theorem}
                If $V$ is a vector space over a field
                $\mathbb{F}\subseteq\mathbb{C}$, and if
                $\norm{\cdot}$ is semi-norm on $V$, then:
                \begin{equation}
                    \norm{\mathbf{0}}=0
                \end{equation}
            \end{theorem}
            \begin{proof}
                For:
                \begin{equation}
                    \norm{\mathbf{0}}
                    =\norm{0\mathbf{0}}
                    =|0|\norm{\mathbf{0}}=0
                \end{equation}
                Therefore, etc.
            \end{proof}
            \begin{ldefinition}{Norm}
                  {Funct_Analysis_Norm}
                A norm on a vector space $V$ over a
                field $\mathbb{F}\subseteq\mathbb{C}$
                is a semi-norm $\norm{\cdot}$ such that:
                \begin{equation}
                    \norm{\mathbf{x}}\Longrightarrow\mathbf{x}
                    =\mathbf{0}
                \end{equation}
            \end{ldefinition}
            \begin{lexample}
                Suppose that $\norm{\cdot}_{0}$ is a semi-norm
                on a vector space $V$. Define the following:
                \begin{equation}
                    N=\{v\in{V}:\norm{v}_{0}=0\}
                \end{equation}
                If follows from the definition of a semi-norm that
                $N$ is a subspace of $V$. Thus we can define a
                function on the quotient space
                $\norm{\cdot}:V/N\rightarrow[0,\infty)$ by:
                \begin{equation}
                    \norm{v+N}=\norm{v}_{0}
                \end{equation}
                We can then verify that this is well defined and
                that $\norm{\cdot}$ is a norm on $V/N$.
            \end{lexample}
            If $\norm{\cdot}$ is a norm on $V$, then we get an
            associated metric $\rho$ via:
            \begin{equation}
                \rho(v,u)=\norm{v-u}
            \end{equation}
            \begin{lexample}
                Let $(X,\mathcal{M},\mu)$ be a measure space.
                That is, $X$ is a set, $\mathcal{M}$ is
                $\sigma\textrm{-Algebra}$, and $\mu$ is a measure
                on $X$. Let $1\leq{p}<\infty$. Then:
                \begin{equation}
                    \mathcal{L}^{p}(X)=
                    \{f:X\rightarrow\mathbb{C}:
                    f\textrm{ is measurable and}
                    \int_{X}|f|^{p}\diff{\mu}<\infty.\}
                \end{equation}
                The set $\mathcal{L}^{p}(X)$ is a vector space.
                We define the semi-norm on $\mathcal{L}^{p}(X)$
                to be:
                \begin{equation}
                    \norm{f}_{p}=
                    \Big(\int_{X}|f|^{p}\diff\mu\Big)^{1/p}
                \end{equation}
                This is not a norm, since there are many
                functions such that $\norm{f}_{p}=0$, yet $f\ne{0}$.
                However, if $\norm{f}_{p}=0$, then $f=0$ $\mu$
                almost-everywhere. So we create equivalence
                classes by s comparing functions that are $\mu$
                almost-everywhere. The final thing to check is
                the triangle-inequality. It is not obvious and is a
                consequence of Minkowski's Inequality. We get
                a normed vector space by considering $N$ to be
                the set of functions $f$ such that $\norm{f}_{p}=0$,
                and we define:
                \begin{equation}
                    L^{p}(X)=\mathcal{L}^{p}(X)/N
                \end{equation}
                The analyst Halmos said that the only important
                values of $p$ are 1, 2, and $\infty$. If
                $f:X\rightarrow\mathcal{C}$ is
                measurable, then we define:
                \begin{equation}
                    \norm{f}_{\infty}=
                    \inf\{c\geq{0}:\mu\Big(\{x:|f(x)|>c\}\big)=0\}
                \end{equation}
                With the convention that $\inf\{\emptyset\}=\infty$.
                This defines a semi-norm on:
                \begin{equation}
                    \mathcal{L}^{\infty}(X)
                    =\{f:\norm{f}_{\infty}<\infty\}
                \end{equation}
                Homogeneity pops out rather quickly, but the
                triangle-inequality is still tricky. We call
                $\norm{\cdot}_{\infty}$ the essential supremum
                of $f$.
            \end{lexample}
            \begin{theorem}
                If $f:X\rightarrow\mathbb{C}$ is measurable, and if:
                \begin{equation}
                    E=\{p:\norm{f}_{p}<\infty,p\in[1,\infty)\}
                \end{equation}
                Then $E$ is connected.
            \end{theorem}
            \begin{lexample}
                Let $X$ be a finite set, let
                $\mathcal{M}=\mathcal{P}(X)$, and let $\mu$ be the
                counting measure on $X$. A function on
                $X$ is an n-tuple $x=(x_{1},\dots,x_{n})$. Then:
                \begin{equation}
                    \norm{x}_{p}=
                    \begin{cases}
                        \Big(\sum_{k=1}^{n}|x_{k}|^{p}\Big)^{1/p},
                        &1\leq{p}<\infty\\
                        \max\{|x|,x\in{X}\}
                    \end{cases}
                \end{equation}
                $\norm{\cdot}_{p}$ is a norm on $X$.
            \end{lexample}
            \begin{lexample}
                Let $X=\mathbb{N}$, the set of natural numbers. Let
                $\mathcal{M}=\mathcal{P}(X)$, and let $\mu$ be
                the counting measure. Then functions are sequences
                $a:\mathbb{N}\rightarrow\mathbb{R}$, or
                $a:\mathbb{N}\rightarrow\mathbb{C}$. Then:
                \begin{equation}
                    \norm{x}_{p}=
                    \begin{cases}
                        \Big(\sum_{n=1}^{\infty}
                            |x_{n}|^{p}\Big)^{1/p},
                        &1\leq{p}<\infty\\
                        \max\{|x|,x\in{X}\}
                    \end{cases}
                \end{equation}
                This defines a norm. Recall that a series is
                absolutely convergent if $\sum|a_{n}|<\infty$.
                Given an absolutely convergent series, the original
                series is also convergent. For this space we use
                the following notation:
                \begin{equation}
                    \ell^{p}=
                    \{a:\mathbb{R}\rightarrow\mathbb{R}:
                        \sum_{n=1}^{\infty}|a_{n}|^{p}<\infty\}
                \end{equation}
            \end{lexample}
            In general, if $X$ is a set then we can equip $X$
            with the counting measure and then if $f$ is any
            bounded function on $f$, then:
            \begin{equation}
                \norm{f}_{\infty}=\sup\{|f(x):x\in{X}\}
            \end{equation}
            For the space of sequences, we write $\ell^{\infty}(X)$.
            \begin{lexample}
                If $X$ is any set, then we define the following
                metric:
                \begin{equation}
                    \rho(x,y)=
                    \begin{cases}
                        1,&x\ne{y}\\
                        0,&x=1
                    \end{cases}
                \end{equation}
                This is often called the discrete metric. It is
                indeed a metric, and $(X,\rho)$ is a metric space.
            \end{lexample}
            \begin{lexample}
                Let $(X,\rho)$ be a metric space. If $Y\subseteq{X}$
                is a non-empty subset of $X$, we can define a new
                metric on $Y$
                be restricting $\rho$ to $Y\times{Y}$. We call
                this the metric subspace.
            \end{lexample}
            \begin{ldefinition}{Strongly Equivalent Metrics}
                  {Funct_Analysis_Strongly_Equivalent_Metrics}
                Strongly equilalent metrics on a set $X$ are metrics
                $\rho_{1}$ and $\rho_{2}$ such that there
                exists $c,d\in\mathbb{R}^{+}$ such that, for
                all $x,y\in{X}$:
                \begin{equation}
                    c\rho_{1}(x,y)\leq
                        \rho_{2}(x,y)\leq{d}\rho_{1}(x,y)
                \end{equation}
            \end{ldefinition}
            The definition of strongly equivalent metrics is indeed
            symmetric. For since $c,d\in\mathbb{R}^{+}$,
            $c^{-1}$ and $d^{-1}$ are well defined and positive,
            and thus:
            \begin{equation}
                \frac{1}{d}\rho_{2}(x,y)\leq\rho_{1}(x,y)
                \leq\frac{1}{c}\rho_{1}(x,y)
            \end{equation}
            \begin{theorem}
                If $p,q\in[1,\infty)$, then $\norm{\cdot}_{p}$ and
                $\norm{\cdot}_{q}$ are strongly equivalent.
            \end{theorem}
            \begin{proof}
                It suffices to show that for all
                $p\in[1,\infty)$ there exist
                $c,d\in\mathbb{R}^{+}$ such that:
                \begin{equation}
                    c\norm{x}_{p}\leq\norm{x}_{2}\leq{d}\norm{x}_{p}
                \end{equation}
                Also note that:
                \begin{equation}
                    \partial{\overline{B}_{1}(\mathbf{0})}=
                    \{\mathbf{x}\in\mathbb{R}^{n}:\norm{x}_{2}=1\}
                \end{equation}
                Is a closed and bounded subset of $\mathbb{R}^{n}$.
            \end{proof}
            \begin{lexample}
                Let $(X,\rho)$ be a metric space on $X$ and define:
                \begin{equation}
                    d(x,y)=\frac{\rho(x,y)}{1+\rho(x,y)}
                \end{equation}
                Then $(X,d)$ is a metric space. Definiteness
                and symmetry come rather immediately from the
                definition and the fact that $\rho$ is a metric.
                The only thing to check is the
                triangle inequality.
            \end{lexample}
            \begin{ldefinition}{Open Ball in a Metric Space}
                  {Funct_Analysis_Open_Ball}
                The open ball about a point $x$ in a metric space
                $(X,\rho)$ of radius $r\in\mathbb{R}$ is the set:
                \begin{equation}
                    B_{r}(x)=\{y\in{X}:\rho(x,y)<r\}
                \end{equation}
            \end{ldefinition}
            \begin{ldefinition}{Open Subsets of a Metric Space}
                  {Funct_Analysis_Open_in_Metric_Space}
                An open subset of a metric space $(X,\rho)$ is a set
                $\mathcal{U}\subseteq{X}$ such that for all
                $x\in\mathcal{U}$ there is an $\varepsilon>0$
                such that:
                \begin{equation}
                    B_{\varepsilon}(x)\subseteq\mathcal{U}
                \end{equation}
            \end{ldefinition}
            \begin{theorem}
                Open balls are open.
            \end{theorem}
            \begin{ldefinition}{Neighborhoods}
                  {Funct_Analysis_Neighborhoods}
                A neighborhood of a point $x$ in a metric space
                $(X,\rho)$ is a subset $D\subseteq{X}$ such
                that there is an open subset
                $\mathcal{U}\subseteq{D}$ such that
                $x\in\mathcal{U}$.
            \end{ldefinition}
            \begin{theorem}
                IF $(X,\rho)$ is a metric space, then $X$ is
                an open subset.
            \end{theorem}
            \begin{theorem}
                If $(X,\rho)$ is a metric space, then
                $\emptyset$ is an open subset.
            \end{theorem}
            \begin{theorem}
                If $\mathcal{U}_{I}$ is a collection of open subsets
                of a metric space $(X,\rho)$, and if
                $\mathcal{U}$ is defined by:
                \begin{equation}
                    \mathcal{U}=\bigcup_{i\in{I}}\mathcal{U}_{i}
                \end{equation}
                Then $\mathcal{U}$ is an open subset.
            \end{theorem}
            \begin{theorem}
                If $(X,\rho)$ is a metric space, and if
                $\mathcal{U}$ and $\mathcal{V}$ are open
                subsets, then the set $\mathcal{D}$ defined by:
                \begin{equation}
                    \mathcal{D}=\mathcal{U}\cap\mathcal{V}
                \end{equation}
                Is an open subset of $X$.
            \end{theorem}
            \begin{theorem}
                If $(X,\rho)$ is a metric space and
                $(\mathcal{E},\rho_{\mathcal{E}})$ is a subspace
                of $(X,\rho)$, then a set
                $\mathcal{U}\subseteq\mathcal{E}$
                is open in $\mathcal{E}$ if and only if there is
                an open set $\mathcal{U}$ in $X$ such that:
                \begin{equation}
                    \mathcal{V}=\mathcal{U}\cap\mathcal{E}
                \end{equation}
            \end{theorem}
            \begin{ldefinition}{Topology}
                  {Funct_Analysis_Topology}
                A topology on a set $X$ is a subset
                $\tau\subseteq\mathcal{P}(X)$ such that:
                $\emptyset\in\tau$ and $X\in\tau$, for any finite
                subset $\mathcal{C}\subseteq\tau$, it is true that:
                \begin{subequations}
                    \begin{equation}
                        \bigcap_{C\in\mathcal{C}}C\in\tau
                    \end{equation}
                    And for any subset $\mathcal{O}\subseteq\tau$
                    it is true that:
                    \begin{equation}
                        \bigcup_{\mathcal{U}\in\mathcal{O}}
                        \mathcal{U}\in\tau
                    \end{equation}
                \end{subequations}
                That is, $\tau$ is closed to finite intersections
                and arbitrary unions.
            \end{ldefinition}
            \begin{lexample}
                If $(X,\rho)$ is a metric space, then the set:
                \begin{equation}
                    \tau=\{\mathcal{U}\subseteq{X}:
                        \mathcal{U}\textrm{ is open}\}
                \end{equation}
                is a topology on $X$. This is the \textit{metric}
                topology. Not every topological space can be
                formed from a metric space. If
                $(\mathcal{E},\rho_{\mathcal{E}})$ is a subspace
                of $(X,\rho)$, then the \textit{subspace topology},
                or the relative topology, is the set:
                \begin{equation}
                    \tau_{\mathcal{E}}
                    =\{\mathcal{E}\cap\mathcal{U}:
                        \mathcal{U}\in\tau\}
                \end{equation}
                This is a topology on $\mathcal{E}$.
            \end{lexample}
            \begin{ldefinition}{Closed Subsets}
                  {Funct_Analysis_Closed_in_Metric}
                A closed subset of a metric space $(X,\rho)$
                is a set $\mathcal{C}\subseteq{X}$ such that
                $X\setminus\mathcal{C}$ is an open subset of $X$.
            \end{ldefinition}
            \begin{theorem}
                If $(X,\rho)$ is a metric space, then $X$ is closed.
            \end{theorem}
            \begin{theorem}
                If $(X,\rho)$ is a metric space, then
                $\emptyset$ is closed.
            \end{theorem}
            \begin{theorem}
                If $(X,\rho)$ is a metric space, and if
                $\mathcal{C}_{i}$ is a finite collection of
                closed subsets, then the set
                $\mathcal{C}$ defined by:
                \begin{equation}
                    \mathcal{C}=\cup_{i=1}^{n}\mathcal{C}_{i}
                \end{equation}
                is a closed subset of $X$.
            \end{theorem}
            \begin{ldefinition}{Closure of a Set}
                  {Funct_Analysis_Closure_in_Metric}
                If $(X,\rho)$ is a metric space and
                $\mathcal{E}\subseteq{X}$, then the closure
                of $\mathcal{E}$ is the set:
                \begin{equation}
                    \overline{\mathcal{E}}=
                    \bigcap\{\mathcal{F}\subseteq{X}:
                        \mathcal{E}\subseteq\mathcal{F}
                        \land\mathcal{F}\textrm{ is closed.}\}
                \end{equation}
            \end{ldefinition}
            The closure of a set $\mathcal{E}$ is the
            smallest closed set that contains $\mathcal{E}$.
            \begin{theorem}
                If $(X,\rho)$ is a metric space, and if
                $\mathcal{E}$ is a non-empty subset of $X$, then
                $x\in\overline{\mathcal{E}}$ if and only if for
                all $\varepsilon>0$:
                \begin{equation}
                    B_{\varepsilon}(x)\cap\mathcal{E}\ne\emptyset
                \end{equation}
            \end{theorem}
            \begin{ldefinition}
                  {Convergent Sequences in a Metric Space}
                  {Funct_Analysis_Convergent_Seq_in_Metric}
                A convergent sequence in a metric space
                $(X,\rho)$ is a sequence
                $a:\mathbb{N}\rightarrow{X}$ such that there is
                an $x\in{X}$ such that for all $\varepsilon>0$
                there exists an $N\in\mathbb{N}$ such that, for
                all $n\in\mathbb{N}$ and $n>N$, it is true that
                $d(x,a_{n})<\varepsilon$. We write
                $a_{n}\rightarrow{x}$.
            \end{ldefinition}
            \begin{ldefinition}{Limits of Convergent Sequences}
                  {Funct_Analysis_Limit_of_Conv_Seq_in_Metric}
                A limit of a convergent sequence
                $a:\mathbb{N}\rightarrow{X}$ in a metric space
                $(X,\rho)$ is a point $x\in{X}$ such that
                $a_{n}\rightarrow{x}$.
            \end{ldefinition}
            \begin{theorem}
                If $(X,\rho)$ is a metric space, if
                $a:\mathbb{N}\rightarrow{X}$ is a convergent
                sequence, and if $x$ and $y$ are limits of $a$,
                then $x=y$.
            \end{theorem}
            \begin{ldefinition}{Equivalent Metrics}
                  {Funct_Analysis_Equivalent_Metric}
                Equivalent metrics on a set $X$ and metrics
                $\rho$ and $d$ such that they generate the
                same topology.
            \end{ldefinition}
            \begin{theorem}
                If $X$ is a set and if $\rho$ and $d$ are
                equivalent metrics on $X$, then for all $x\in{X}$
                and for all $r\in\mathbb{R}^{+}$ there exists
                $r_{1},r_{2}\in\mathbb{R}^{+}$ such that:
                \begin{align}
                    B_{r_{1}}^{\rho}(x)&\subseteq{B}_{r}^{d}(x)\\
                    B_{r_{2}}^{d}(x)&\subseteq{B}_{r}^{\rho}(x)
                \end{align}
            \end{theorem}
            \begin{lexample}
                If $(X,\rho)$ is a metric space and if $d$
                is defined by:
                \begin{equation}
                    d(x,y)=\frac{\rho(x,y)}{1+\rho(x,y)}
                \end{equation}
                The $d$ is a metric on $X$. Moreover, $d$
                is equivalent to $\rho$. This shows that the
                notion of boundedness is not a topological one,
                but a metric property. For, given any metric
                $\rho$, $d$ is bounded. For all $x,y\in{X}$,
                $0\leq{d}(x,y)<1$. Letting $\rho$ be the
                standard metric on $\mathbb{R}$,
                $\rho(x,y)=|x-y|$, we see that the topology
                generated by this unbounded metric is equivalent
                to the topology generated by the metric:
                \begin{equation}
                    d(x,y)=\frac{|x-y|}{1+|x-y|}
                \end{equation}
            \end{lexample}
            \begin{theorem}
                If $(X,\rho)$ is a metric space, and if
                $d:X^{2}\rightarrow[0,\infty)$ is defined by:
                \begin{equation}
                    d(x,y)=\frac{\rho(x,y)}{1+\rho(x,y)}
                \end{equation}
                Then $\rho$ and $d$ are equivalent.
            \end{theorem}
            \begin{proof}
                For let $r>0$. For all $x,y\in{X}$,
                $d(x,y)\leq\rho(x,y)$, and therefore:
                \begin{equation}
                    B_{r}^{\rho}(x)\subseteq{B}_{r}^{d}(x)
                \end{equation}
                If $\rho(x,y)\leq{1}$, then
                $\rho(x,y)\leq{2}d(x,y)$. Let
                $r_{1}=\min\{r/2,1\}$, then:
                \begin{equation}
                    B_{r/2}^{d}(x)\subseteq{B}_{r}^{\rho}(x)
                \end{equation}
                Therefore, etc.
            \end{proof}
            \begin{ldefinition}
                  {Continuous Functions Between Metric Spaces}
                  {Funct_Analysis_Cont_Func_Metric}
                A continuous function from a metric space
                $(X,\rho)$ to a metric space $(Y,d)$ is a function
                $f:X\rightarrow{Y}$ such that, for all $x\in{X}$
                and for all $\varepsilon>0$, there is a
                $\delta>0$ such that:
                \begin{equation}
                    f\big(B_{\delta}^{\rho}(x)\big)\subset
                    B_{\varepsilon}^{d}\big(f(x)\big)
                \end{equation}
            \end{ldefinition}
            \begin{theorem}
                If $(X,\rho)$ and $(Y,d)$ are metric spaces, and
                if $f:X\rightarrow{Y}$ is a function, then
                the following are equivalent:
                \begin{enumerate}
                    \item $f$ is continuous at $x_{0}\in{X}$.
                    \item If $x_{n}\rightarrow{x_{0}}$ then
                          $f(x_{n})\rightarrow{f}(x_{0})$
                    \item If $\mathcal{V}$ is a neighborhood of $f(x_{0})$,
                          then $f^{-1}(\mathcal{V})$ is a neighborhodd of
                          $x_{0}$.
                \end{enumerate}
            \end{theorem}
            \begin{theorem}
                If $(X,\rho)$ and $(Y,d)$ are metric spaces, and if
                $f:X\rightarrow{Y}$ is a function that is
                continuous at $x_{0}\in{X}$, then for
                all $\mathcal{V}\subseteq{Y}$ such that
                $\mathcal{V}$ is open and $f(x_{0})\in\mathcal{V}$,
                then $f^{-1}(\mathcal{V})$ is an open subset
                of $x_{0}$.
            \end{theorem}
            \begin{ldefinition}{Uniformly Continuous Functions}
                  {Funct_Analysis_Uni_Cont_Func_Metric}
                A uniformly continuous function from a metric space
                $(X,\rho)$ to a metric space $(Y,d)$ if a function
                $f:X\rightarrow{Y}$ such that for all
                $\varepsilon>0$ there exists a $\delta>0$ such
                that, for all $x,y\in{X}$ such that
                $\rho(x,y)<\delta$, it is true that
                $d(f(x),f(y))<\varepsilon$.
            \end{ldefinition}
            \begin{theorem}
                If $f:X\rightarrow{Y}$ is uniformly continuous,
                then $f$ is continuous.
            \end{theorem}
            The converse is false. For define $f(x)=x^{2}$.
        \subsection{Completeness}
            \begin{ldefinition}{Cauchy Sequences}
                  {Funct_Analysis_Cauchy_Seq_Metric}
                A Cauchy sequence in a metric space $(X,d)$ is a
                sequence $a:\mathbb{N}\rightarrow{X}$ such that,
                for all $\varepsilon>0$ there is an
                $N\in\mathbb{N}$ such that, for all
                $n,m\in\mathbb{N}$ such that $n,m>N$,
                it is true that $d(x_{n},x_{m})<\varepsilon$.
            \end{ldefinition}
            \begin{lexample}
                Let $X=(0,2)$ with the usual metric, and let
                $a:\mathbb{N}\rightarrow{X}$ be defined by:
                \begin{equation}
                    a_{n}=\frac{1}{n}
                \end{equation}
                Then $a$ is a Cauchy sequence since:
                \begin{equation}
                    |a_{n}-a_{m}|=\frac{|n-m|}{nm}
                    <\frac{2}{\min(n,m)}
                \end{equation}
                And this converges to zero. However the sequence
                doesn't converge, since we took zero away.
            \end{lexample}
            \begin{lexample}
                Let $X=C([0,3])$ and let:
                \begin{equation}
                    \norm{f}_{1}=\int_{0}^{3}|f(x)|\diff{x}
                \end{equation}
                Then $\norm{\cdot}_{1}$ is a norm on the set of
                continuous functions, and thus induces a metric.
                Let $f_{n}$ be defined by:
                \begin{equation}
                    f_{n}(x)=
                    \begin{cases}
                        1,&x\leq{x}<2-\frac{1}{n}\\
                        Bob,\\
                        0,&x\geq{2}
                    \end{cases}
                \end{equation}
                Then $f_{n}$ is Cauchy, but does not converge.
            \end{lexample}
            \begin{ldefinition}{Complete Metric Spaces}
                  {Funct_Analysis_Complete_Metric_Space}
                A complete metric space is a metric space
                $(X,d)$ such that, for all Cauchy sequences
                $a:\mathbb{N}\rightarrow{X}$, $a$ is a convergent
                sequence.
            \end{ldefinition}
            \begin{theorem}
                If $(X,d)$ is a metric space, if
                $a:\mathbb{N}\rightarrow{X}$ is a Cauchy
                sequence, and if there is a convergent subseqence
                of $a$, then $a$ is a convergent sequence.
            \end{theorem}
            \begin{theorem}
                A normed vector space $(V,\norm{\cdot})$ is
                complete if and only if every absolutely
                convergent series converges.
            \end{theorem}
            \begin{proof}
                Suppose $V$ is complete and let $u_{n}$
                be absolutely convergent. That is, the sequence
                of partial sums:
                \begin{equation}
                    S_{N}=\sum_{n=1}^{N}\norm{u_{n}}
                \end{equation}
                Converges in $\mathbb{R}$. But then:
                \begin{equation}
                    \underset{N\rightarrow\infty}{\lim}
                    \sum_{k=N}^{\infty}\norm{u_{k}}=0
                \end{equation}
                Define:
                \begin{equation}
                    s_{n}=\sum_{k=1}^{n}u_{k}
                \end{equation}
                But if $m\geq{n}$, then:
                \begin{equation}
                    \norm{s_{n}-s_{m}}
                    \leq\sum_{k=n+1}^{m}\norm{u_{k}}
                    \leq\sum_{k=n+1}^{\infty}\norm{u_{k}}
                \end{equation}
                Thus, if $\varepsilon>0$ there is an $N\in\mathbb{N}$
                such that, for all $n\geq{N}$, it is true that:
                \begin{equation}
                    \sum_{k=n+1}^{\infty}\norm{u_{k}}<\varepsilon
                \end{equation}
                Therefore $s_{n}$ is a Cauchy sequence, and therefore
                there is an $s\in{V}$ such that $s_{n}\rightarrow{s}$.
                Proving the converse, suppose $u_{n}$ is a Cauchy
                sequence. Then there is an $N_{1}\in\mathbb{N}$ such
                that, for all $n,m\geq{N}_{1}$, we have
                $\norm{u_{n}-u_{m}}<1/2$. But then there is also an
                $N_{2}\in\mathbb{N}$ such that $N_{2}>N_{1}$, and for
                all $n,m>N_{2}$, $\norm{u_{n}-u_{m}}$. Continuing
                inductively, we find a sequence $u_{n_{k}}$ such that:
                \begin{equation}
                    \norm{u_{n_{k+1}}-u_{n_{k}}}<\frac{1}{2^{k}}
                \end{equation}
                Let $v_{k}=u_{n_{k+1}}-u_{n_{k}}$, and note that:
                \begin{equation}
                    \sum_{n=1}^{\infty}\norm{v_{n}}<\infty
                \end{equation}
                But then there is a $v\in{V}$ such that:
                \begin{equation}
                    \sum_{n=1}^{\infty}v_{n}=v
                \end{equation}
                But:
                \begin{align}
                    v&=\underset{N\rightarrow\infty}{\lim}
                        \sum_{k=1}^{N}v_{k}\\
                    &=\underset{N\rightarrow\infty}{\lim}
                        v_{n_{N+1}}-u_{n_{1}}
                \end{align}
                Therefore $u_{n_{k}}\rightarrow{v}+u_{n_{1}}$.
                But $u_{n}$ is Cauchy and thus if there is a
                convergent subsequence, then it is a convergent
                sequence. Therefore, $(X,\norm{\cdot})$ is complete.
            \end{proof}
            \begin{theorem}
                If $(X,\mathcal{M},\mu)$ is a measure space, and
                if $1\leq{p}\leq\infty$.
            \end{theorem}
            \begin{proof}
                Suppose that $f_{n}$ is a sequence of functions
                in $L^{P}(X,\mathcal{M},\mu)$ such that:
                \begin{equation}
                    \sum_{k=1}^{\infty}\norm{f_{n}}_{p}=B<\infty
                \end{equation}
                Define $G,G_{n}:X\rightarrow[0,\infty]$ be
                defined by:
                \begin{align}
                    G(x)&=\sum_{k=1}^{\infty}|f_{k}(x)|\\
                    G_{n}(x)&=\sum_{k=1}^{n}|f_{k}(x)|
                \end{align}
                Then, from the triangle inequality, we have that:
                \begin{equation}
                    \norm{G_{n}}_{p}\leq
                    \sum_{k=1}^{n}\norm{f_{k}}_{p}
                    \leq{B}
                \end{equation}
                Thus, by the monotone convergence theorem, we have:
                \begin{equation}
                    \int_{X}G(x)^{p}\diff{\mu}
                    =\underset{n\rightarrow\infty}{\lim}
                    \int_{X}G_{n}(x)^{p}\diff{\mu}\leq{B}^{p}
                \end{equation}
                Therefore $G\in\mathcal{L}^{p}(X)$, and thus
                $G(x)<\infty$ $\mu$ almost-everywhere. But then the
                original series converges $\mu$ almost everywhere.
                Define $F$ be:
                \begin{equation}
                    F(x)=
                    \begin{cases}
                        \sum_{n=1}^{\infty}f_{n}(x),
                        &|\sum_{n=1}^{\infty}f_{n}(x)|<\infty\\
                        0,&\textrm{Otherwise}
                    \end{cases}
                \end{equation}
                Then $|F(x)|\leq{G}(x)$, and thus
                $F\in\mathcal{L}^{p}(X)$. Moreover:
                \begin{equation}
                    \big|F(x)-\sum_{k=1}^{n}f_{k}(x)\big|^{p}
                    \leq{2}^{p}G(x)^{p}
                \end{equation}
                Therefore, by the Lebesgue Dominated Convergence
                Theorem, we have that:
                \begin{equation}
                    \norm{F-\sum_{k=1}^{n}f_{k}(x)}^{p}_{p}
                    \rightarrow{0}
                \end{equation}
                Therefore, $F\in{L}^{p}(X,\mathcal{M},\mu)$.
            \end{proof}
            \begin{ldefinition}
                  {Supremum Norm of Bounded Continuous Function}
                  {Funct_Analysis_Sup_Norm_of_Bound_Cont_Funcs}
                The supremum norm on set $C_{b}(X)$ of bounded
                continuous functions on a metric space $(X,d)$ is:
                \begin{equation}
                    \norm{f}_{\infty}=\sup_{x\in{X}}|f(x)|
                \end{equation}
            \end{ldefinition}
            From this, we can see thatn $f_{n}\rightarrow{f}$ if
            and only if $f_{n}\rightarrow{f}$ uniformly on $X$.
            \begin{theorem}
                $C_{b}(X)$ is complete in the supremum norm.
            \end{theorem}
            \begin{proof}
                Suppose that $f_{n}$ is a Cauchy sequence in
                $C_{b}(X)$. For for all $f_{n}$ and $x\in{X}$,
                $f_{n}(x)$ is a Cauchy sequence in $\mathbb{C}$.
                But $\mathbb{C}$ is complete, and thus there is a
                $c_{x}\in\mathbb{C}$ such that
                $f_{n}(x)\rightarrow{c}_{x}$. Let
                $f(x)=c_{x}$ for all $x\in{X}$. Then
                $f_{n}\rightarrow{f}$. For, let $\varepsilon>0$.
                Then there exists $N\in\mathbb{N}$ sch that, for
                all $n,m>{N}$ implies that:
                \begin{equation}
                    |f_{n}(x)-f_{m}(x)|<\varepsilon/2
                \end{equation}
                But then:
                \begin{equation}
                    |f_{n}(x)-f(x)|=
                    \underset{m\rightarrow\infty}{\lim}
                    |f_{n}(x)-f_{m}(x)|
                    \leq\frac{\varepsilon}{2}<\varepsilon
                \end{equation}
                But the uniform limit of continuous functions is
                continuous. Therefore, etc.
            \end{proof}
            \begin{theorem}
                If $(X,d)$ is a complete metric space, if
                $(E,d')$ is a subspace of $(X,d)$, and if
                $E$ is closed, then $(E,d')$ is complete.
            \end{theorem}
            \begin{proof}
                Suppose $E$ is closed and suppose $(x_{n})$ is a
                Cauchy sequence in $E$. Then $x_{n}$ is a Cauchy
                sequence in $X$, but $X$ is complete. Therefore
                there is an $x\in{X}$ such that
                $x_{n}\rightarrow{x}$. But $E$ is closed,
                and therefore $x\in{E}$. Now suppose $E$ is
                complete. Suppose $x_{n}$ is a sequence in $E$
                and that $x_{n}\rightarrow{y}$ in $X$. But
                convergent sequences are Cauchy sequences, and
                thus $x_{n}$ is a Cauchy sequence. But $E$ is
                complete and therefore $y\in{E}$.
                Therefore, $E$ is closed.
            \end{proof}
            \begin{ldefinition}{Bounded Metric Spaces}
                  {Funct_Analysis_Bounded_Metric_Space}
                A bounded metric space is a metric space
                $(X,d)$ such that there exists an $x\in{X}$
                and an $r>0$ such that:
                \begin{equation}
                    X\subseteq{B}_{r}^{(X,d)}(x)
                \end{equation}
            \end{ldefinition}
            \begin{ldefinition}{Diameter of a Metric Space}
                  {Funct_Analysis_Diam_of_Metric_Space}
                The diameter of a bounded metric space $(X,d)$ is:
                \begin{equation}
                    \diam(X)=\sup_{x\in{X}}\{d(x,y):x,y\in{X}\}
                \end{equation}
            \end{ldefinition}
            Every bounded metric space is contained in some
            open ball.
            \begin{theorem}
                If $(X,d)$ is a metric space, and then it
                is complete if and only if for any sequence
                of non-empty closed sets
                $F:\mathbb{N}\rightarrow\mathcal{P}(X)$ such that
                $F_{n+1}\subseteq{F}_{n}$ and
                $\diam(F_{n})\rightarrow{0}$,
                there is an $x\in{X}$ such that:
                \begin{equation}
                    \{x\}=\cap_{n=1}^{\infty}F_{n}
                \end{equation}
            \end{theorem}
            \begin{proof}
                For suppose $(X,d)$ is complete, and let
                $F:\mathbb{N}\rightarrow\mathcal{P}(X)$ be a
                sequence of non-empty subsets of $X$. Then, for
                all $n\in\mathbb{N}$, $F_{n}$ is non-empty, and
                thus there is a sequence
                $a:\mathbb{N}\rightarrow{X}$ such that, for all
                $n\in\mathbb{N}$, $x_{n}\in{F_{n}}$. But then:
                \begin{equation}
                    d(a_{n},a_{m})\leq\diam(F_{\max\{n,m\}})
                \end{equation}
                But $\diam(F_{n})\rightarrow{0}$, and therefore
                $a$ is a Cauchy sequence. But $(X,d)$ is complete,
                and therefore there is an $x\in{X}$ such that
                $a_{n}\rightarrow{x}$. Moreover, there is an
                $N\in\mathbb{N}$ such that $x\in\overline{F}_{N}$.
                But $F_{N}$ is closed, and thus $x\in{F}_{N}$.
                But for all $n>N$, $F_{n}\subseteq{F}_{N}$.
                Therefore:
                \begin{equation}
                    x\in\cap_{n=1}^{\infty}F_{n}
                \end{equation}
                If $y\in\cap_{n=1}^{\infty}F_{n}$, then
                $d(x,y)\leq\diam(F_{n})$ for all $n\in\mathbb{N}$.
                But $\diam(F_{n})\rightarrow{0}$, and thus
                $d(x,y)=0$. Therefore, $x=y$. Going the other
                way, suppose $X$ has the nested set property and
                let $a:\mathbb{N}\rightarrow{X}$ be a Cauchy
                sequence in $X$. Let
                $F:\mathbb{N}\rightarrow\mathcal{P}(X)$
                be defined by:
                \begin{equation}
                    F_{n}=\overline{\{a_{k}:k\geq{n}\}}
                \end{equation}
                Then, for all $n\in\mathbb{N}$, $F_{n}$
                is non-empty, and $F_{n+1}\subseteq{F}_{n}$.
                Moreover, $\diam(F_{n})\rightarrow{0}$. Thus, by
                the nested sequence property, there is an
                $x\in{X}$ such that $x\in\cap_{n=1}^{\infty}F_{n}$.
                But then:
                \begin{equation}
                    d(a_{n},x)\leq\diam(F_{n})\rightarrow{0}
                \end{equation}
                and therefore $a_{n}\rightarrow{x}$. Thus, $a$ is
                a Cauchy sequence and $(X,d)$ is complete.
            \end{proof}
        \subsection{Compactness}
            \begin{ldefinition}{Covers}
                  {Funct_Analysis_Covers}
                A cover of a subset $\mathcal{E}\subseteq{X}$ of
                a set $X$ is a subset
                $\mathcal{O}\subseteq\mathcal{P}(X)$ such that:
                \begin{equation}
                    \mathcal{E}\subseteq
                    \bigcup_{\mathcal{U}\in\mathcal{O}}
                        \mathcal{U}
                \end{equation}
            \end{ldefinition}
            \begin{ldefinition}{Sub-Cover}
                  {Funct_Analysis_Subcover}
                A sub-cover of a cover $\mathcal{O}$ of a subset
                $E\subseteq{X}$ of a set $X$ is a subset
                $\Delta\subseteq\mathcal{O}$ such that:
              \begin{equation}
                    \mathcal{E}\subseteq
                    \bigcup_{\mathcal{U}\in\Delta}
                        \mathcal{U}
                \end{equation}
            \end{ldefinition}
            \begin{ldefinition}{Open Covers}
                  {Funct_Analysis_Open_Cover}
                An open cover of a metric space $(X,d)$ is a cover
                $\mathcal{O}\subseteq\mathcal{P}(X)$ of $X$ such
                that, for all $\mathcal{U}\in\mathcal{O}$,
                $\mathcal{U}$ is open.
            \end{ldefinition}
            \begin{ldefinition}{Compact Sets}
                  {Funct_Analysis_Compact_Set}
                A compact metric space is a metric space $(X,d)$
                such that for any open cover $\mathcal{O}$ of
                $X$, there is a finite
                sub-cover $\Delta\subseteq\mathcal{O}$.
            \end{ldefinition}
            \begin{lexample}
                Let $X=[0,1)$ with the usual topology, and let:
                \begin{equation}
                    \mathcal{U}_{x}=[0,x)
                    \quad\quad
                    x\in(0,1)
                \end{equation}
                Then $\mathcal{O}=\{\mathcal{U}_{x}:x\in(0,1)\}$
                is an open cover of $X$, but there is no finite
                sub-cover. For given any finite sub-cover,
                there is a greatest $x$ such that
                $\mathcal{U}_{x}$ is contained in the sub-cover.
                But then for all $y\in(x,1)$, $y$ is not in
                the sub-cover. As a trivial example, any
                finite metric space is compact.
            \end{lexample}
            \begin{theorem}
                If $K$ is a subspace of $X$, then $K$ is compact
                if and only if every open cover of $K$ has a
                finite sub-cover.
            \end{theorem}
            \begin{proof}
                For suppose $(K,d_{K})$ is compact, and let
                $\mathcal{O}$ be an open cover of $K$. Then:
                \begin{equation}
                    \mathcal{O}_{K}=\{K\cup\mathcal{U}:
                        \mathcal{U}\in\mathcal{O}\}
                \end{equation}
                Is an open cover of $K$. But $K$ is compact,
                and thus there is a finite sub-cover
                $\Delta_{K}$. But then:
                \begin{equation}
                    \Delta=\{\mathcal{U}\in\mathcal{U}:
                             \mathcal{U}\cap{K}\in\Delta_{K}\}
                \end{equation}
                And this is a finite sub-cover.
            \end{proof}
            \begin{ldefinition}{Finite Intersection Property}
                  {Funct_Analysis_Finite_Intersect_Prop}
                A set with the finite intersection property
                in a metric space $(X,d)$ is a collection of sets
                $\mathscr{F}\subseteq\mathcal{P}(X)$ such that,
                for any sequence
                $F:\mathbb{Z}_{n}\rightarrow\mathscr{F}$, 
                it is true that $\cap_{k=1}^{n}F_{k}\ne\emptyset$.
            \end{ldefinition}
            \begin{theorem}
                A metric space $(X,d)$ is compact if and only if
                every collection $\mathscr{F}$ of closed sets in
                $X$ with the
                finite intersection property is such that:
                \begin{equation}
                    \bigcap_{\mathcal{C}\in\mathcal{F}}
                        \mathcal{C}\ne\emptyset
                \end{equation}
            \end{theorem}
            \begin{lexample}
                Let $F_{n}=[n,\infty)$, and let
                $\mathscr{F}=\{F_{n}:n\in\mathbb{N}\}$.
                Then $\mathscr{F}$ has the finite intersection
                property. However, the intersection over the
                entire set is empty, and hence $\mathbb{R}$
                (With the standard metric) is not compact.
            \end{lexample}
            \begin{ldefinition}{Totally Bounded Metric Space}
                  {Funct_Analysis_Totally_Bounded_Met_Space}
                A totally bounded metric space is a metric
                space $(X,d)$ such that, for all $\varepsilon>0$,
                there exists an $n\in\mathbb{N}$ and a sequence
                $a:\mathbb{Z}_{n}\rightarrow{X}$ such that:
                \begin{equation}
                    X=\cup_{k=1}^{n}B_{\varepsilon}^{(X,d)}(a_{k})
                \end{equation}
            \end{ldefinition}
            \begin{ldefinition}{$\varepsilon\textrm{-Nets}$}
                  {Funct_Analysis_epsilon_Net}
                An $\varepsilon\textrm{-Net}$ of a subspace
                $(\mathcal{E},d_{\mathcal{E}})$ of a
                metric space $(X,d)$ is a finite collection:
                \begin{equation}
                    E=\{B_{\varepsilon}^{(X,d)}(x_{k}):
                        k\in\mathbb{Z}_{n}\}
                \end{equation}
                Such that $E$ is an open cover of $\mathcal{E}$.
            \end{ldefinition}
            \begin{lexample}
                Let $X=\ell^{2}$ and let:
                \begin{equation}
                    e_{n}(x)=
                    \begin{cases}
                        1,&k=n\\
                        0,&k\ne{n}
                    \end{cases}
                \end{equation}
                Then $e_{n}\in\ell^{2}$ and $\norm{e}_{2}=1$,
                but for all $n\ne{m}$,
                $\norm{e_{n}-e_{m}}_{2}=\sqrt{2}$. Let:
                \begin{equation}
                    B_{1}=\{f\in\ell^{2}:\norm{f}_{2}\leq{1}\}
                \end{equation}
                Then $B_{1}$ is bounded, but if
                $\varepsilon=\sqrt{2}/2$ then no finite collection
                of $\varepsilon$ balls can cover $B_{1}$ since
                each ball can contain at most one of the
                $e_{n}$. Thus any cover is infinite.
            \end{lexample}
            \begin{theorem}
                A subset of $(\mathbb{R}^{n},\norm{\cdot}_{2})$
                is totally bounded if and only if it's bounded.
            \end{theorem}
            \begin{proof}
                Totally bounded implies bounded, so it suffices
                to show that if $\mathbb{R}^{n}$ is bounded then
                it is totally bounded. Let
                $\mathcal{E}\subseteq\mathbb{R}^{n}$ be bounded.
                Then there is an $r>0$ such that:
                \begin{equation}
                    \mathcal{E}\subseteq[-r,r]^{n}
                \end{equation}
                Then, compactness, stuff like that.
            \end{proof}
            This works for any norm on $\mathbb{R}^{n}$, since all
            norm's on $\mathbb{R}^{n}$ are strongly equivalent.
            \begin{ldefinition}{Sequential Compactness}
                  {Funct_Analysis_Seq_Compact}
                A sequentially compact metric space is a metric
                space $(X,d)$ such that, for all
                $a:\mathbb{N}\rightarrow{X}$, there is a convergent
                subsequence of $a$.
            \end{ldefinition}
            \begin{lexample}
                Let $X\subseteq\mathbb{R}$ be defined by:
                \begin{equation}
                    X=\{\frac{1}{n}:n\in\mathbb{N}\}\cup\{0\}
                \end{equation}
                Then $X$ is sequentially compact, with respect
                to the subspace metric.
            \end{lexample}
            \begin{theorem}
                IF $(X,d)$ is a metric space, then the following are
                equivalent:
                \begin{enumerate}
                    \item $X$ is compact.
                    \item $X$ is complete and totally bounded.
                    \item $X$ is sequentially compact.
                \end{enumerate}
            \end{theorem}
            \begin{proof}
                Suppose $(X,d)$ is not compact, and let
                $\mathcal{U}_{i}$ be an open cover with no finite
                subcover. If $X$ is totally bounded, then there is a
                finite covering of $1/2$ balls. But then at least
                one of these isn't covered by finitely many of the
                $\mathcal{U}_{i}$. Let $F_{1}$ be the closure of
                this. Then $F_{1}$ is totally bounded, and has
                a finite covering of $1/4$ balls. One of these
                must not be covered by finitely many of the
                $\mathcal{U}_{i}$. Let $F_{2}'$ be the closure
                of such a ball, and let $F_{2}=F_{1}\cap{F}_{2}'$.
                Then $F_{2}$ is closed, non-empty, and
                $\diam(F_{2})\leq{1/2}$. Continuing, we
                obtain a sequence of non-empty closed sets
                $F_{n}$ such that for all $n\in\mathbb{N}$,
                $F_{n+1}\subseteq{F}_{n}$ and
                $\diam(F_{n})<1/2^{n}$. Thus, if $X$ is complete,
                there is a unique $x$ that lies in the intersection
                of all of the $F_{n}$. But then there is a
                $\mathcal{U}_{i}$ such that $x\in\mathcal{U}_{i}$,
                and thus eventually $F_{n}\subset\mathcal{U}_{i}$,
                a contradiction. Thus, $X$ is compact. Now, suppose
                $X$ is compact and let $a:\mathbb{N}\rightarrow{X}$
                be a sequence in $X$. Let:
                \begin{equation}
                    F_{n}=\overline{\{x_{k}:l\geq{n}\}}
                    \quad\quad
                    \mathscr{F}=\{F_{n}:n\in\mathbb{N}\}
                \end{equation}
                Then $\mathscr{F}$ has the finite intersection
                property. Since $X$ is compact, the intersection of
                the $F_{n}$ is non-empty. Let $x$ be contained in
                the intersection. Then:
                \begin{equation}
                    B_{1}(x)\cap\{x_{k}:k\geq{1}\}\ne\emptyset
                \end{equation}
                Pick $n_{1}$ such that $x_{n_{1}}\in{B}_{1}(x)$.
                Then there is an $n_{2}>n_{1}$ such that
                $x_{n_{2}}\in{B}_{1/2}(x)$. Continuing, we obtain
                a subsequence $n_{k}$ such that
                $x_{k}\in{B}_{1/k}(x)$, and thus
                $x_{k}\rightarrow{x}$. Finally, we show that
                sequential compactness implies that $X$ is
                complete and totally bounded. For suppose $X$ is
                not totally bounded. Then there exists
                $\varepsilon>0$ such that $X$ has no finite
                covering of $\varepsilon$ balls. We can thus
                obtain a sequence $a:\mathbb{N}\rightarrow{X}$
                such that, for all $n\ne{m}$,
                $d(a_{n},a_{m})\geq\varepsilon$. But this
                has no convergence subsequence, for any convergent
                subsequence would be a Cauchy sequence. Moreover,
                $X$ is complete. For suppose not, and let
                $a:\mathbb{N}\rightarrow{X}$ be a Cauchy sequence
                and suppose it does not converge. But then there
                is no convergent subsequence, since Cauchy
                sequences with convergent subsequences converge.
                Thus, $X$ is complete.
            \end{proof}
            \begin{ltheorem}{Heine-Borel Theorem}
                  {Funct_Analysis_Heine_Borel}
                A subset $\mathcal{E}\subseteq\mathbb{R}^{n}$ is
                compact with respect to the standard topology if and
                only if $\mathcal{E}$ is closed and bounded.
            \end{ltheorem}
            This theorem does not generalize to other spaces. For
            consider $\ell^{2}$ and the closed unit ball about the
            origin. This is closed and bounded, but it is not
            compact. This is simply because it is not totally
            bounded, nor is it sequentially compact.
            \begin{ltheorem}{Extreme Value Theorem}
                  {Funct_Analysis_Extreme_Value_Theorem}
                If $(X,d)$ is a compact metric space and if
                $f:X\rightarrow\mathbb{R}$ is continuous, then
                $f$ attains it's maximum and minimum. In particular,
                if $f:X\rightarrow\mathbb{C}$ is continuous, then
                $f$ is bounded.
            \end{ltheorem}
            \begin{proof}
                Note that if $f:X\rightarrow\mathbb{C}$ is
                continuous, then $|f|:X\rightarrow\mathbb{R}$ is
                continuous, so we only need to prove the first
                statement. For if $X$ is compact, then $f(X)$ is
                compact, for $f$ is continuous. But then $f(X)$
                is closed and bounded. Let:
                \begin{equation}
                    M=\underset{x\in{X}}\sup\{f(x)\}
                \end{equation}
                Then, since $f(X)$ is bounded, $M\in\mathbb{R}$.
                But then there is a sequence
                $a:\mathbb{N}\rightarrow{X}$ such that
                $f(a_{n})\rightarrow{M}$. But if $X$ is
                compact, then it is sequentially compact, and
                thus there is an $x\in{X}$ an a subsequence
                $a_{k}$ such that $a_{k_{n}}\rightarrow{x}$.
                But then $f(x)=M$. Similarly for the minimum value.
            \end{proof}
        \subsection{Lebesgue Spaces}
            \begin{ldefinition}{Lebesgue Number}
                  {Funct_Analysis_Lebesgue_Number}
                A Lebesgue Number of an open cover
                $\mathcal{O}$ of a metric space $(X,d)$ is
                a non-zero number $d>0$ such that, for all
                $x\in{X}$, there exists
                a $\mathcal{U}\in\mathcal{O}$ such that:
                \begin{equation}
                    B_{d}^{(X,d)}(x)\subseteq\mathcal{U}
                \end{equation}
            \end{ldefinition}
            \begin{lexample}
                Let $X=\mathbb{R}$, and let $d$ be the
                standard metric. Let
                $\mathcal{O}=\{\mathcal{U}_{i}:i=1,2,3\}$ where:
                \begin{equation}
                    \mathcal{U}_{1}=(-\infty,1)
                    \quad\quad
                    \mathcal{U}_{2}=(0,2)
                    \quad\quad
                    \mathcal{U}_{3}=(1,\infty)
                \end{equation}
                Then $d=1/2$ is a Lebesgue number of this cover.
                Letting $X=(0,1)$ with the standard metric, for all
                $x\in{X}$ the is a $\delta_{x}>0$ such that:
                \begin{equation}
                    B_{\delta_{x}}^{(X,d)}(x)
                    \subseteq{X}
                \end{equation}
                And thus these open balls are a covering of the unit
                interval, but this covering has no Lebesgue number.
            \end{lexample}
            \begin{ltheorem}{Lebesgue Covering Lemma}
                  {Funct_Analysis_Lebesgue_Covering_Lemma}
                If $(X,d)$ is a compact metric space, and if
                $\mathcal{O}$ is an open covering of $X$, then
                $\mathcal{O}$ has a Lebesgue number.
            \end{ltheorem}
            \begin{proof}
                Suppose not. Suppose $(X,d)$ is compact, and suppose
                that $\mathcal{O}$ is a covering of $X$ with no
                Lebesgue number. But then, for all $n\in\mathbb{N}$,
                there is an $a_{n}$ such that, for all
                $\mathcal{U}\in\mathcal{O}$:
                \begin{equation}
                    B_{1/n}^{(X,d)}(a_{n})\not\subset\mathcal{U}
                \end{equation}
                But $X$ is compact, and thus there is a convergent
                subsequence such that $a_{k_{n}}\rightarrow{X}$. 
                But then there is a $\mathcal{U}\in\mathcal{O}$ such
                that $x\in\mathcal{U}$. But $\mathcal{U}$ is open,
                and thus there is an $r>0$ such that:
                \begin{equation}
                    B_{r}^{(X,d)}(x)\subseteq\mathcal{U}
                \end{equation}
                Let $N\in\mathbb{N}$ be such that, for all
                $k_{n}>N$, $d(x_{k_{n}},x)<r/2$. Let
                $n>N$ be such that $1/k_{n}<r/2$. But then:
                \begin{equation}
                    B_{1/k_{n}}(a_{k_{n}})\subseteq\mathcal{U}
                \end{equation}
                A contradiction.
            \end{proof}
            \begin{theorem}
                If $(X.d)$ is a compact metric space, if $(Y,\rho)$
                is a metric space, and if $f:X\rightarrow{Y}$ is a
                continuous function, then $f$ is
                uniformly continuous.
            \end{theorem}
            \begin{proof}
                For let $\varepsilon>0$. since $f$ is
                continuous, for all $x\in{X}$ there is a
                $\delta_{x}$ such that, for all $y\in{X}$ such
                that $d(x,y)<\delta_{x}$, it is true that
                $\rho(f(x),f(y))<\varepsilon/2$. But then:
                \begin{equation}
                    X\subseteq
                        \bigcup_{x\in{X}}B_{\delta_{x}}^{(X,d)}(x)
                \end{equation}
                But $X$ is compact, and thus this covering has a
                Lebesgue number. Let $\delta$ be such a
                Lebesgue number. But then if $d(x,y)<\delta$,
                then there is a $z\in{X}$ such that
                $x,y\in{B}_{\delta_{z}}(z)$. But then:
                \begin{equation}
                    \rho(f(x),f(x))\leq
                    \rho(f(x),f(z))+\rho(f(z),f(y))
                    <\varepsilon
                \end{equation}
            \end{proof}
        \subsection{Equicontinuity}
            \begin{ltheorem}{Arzela-Ascoli Theorem}
                  {Funct_Analysis_Arzela_Ascoli}
                If $X$ is a compact metric space, if
                $F_{n}\in{C}(X)$ is a sequence of equicontinuous
                point-wise bounded functions, then $F_{n}$ has a
                uniformly convergent subsequence.
            \end{ltheorem}
            \begin{theorem}
                If $X$ is a compact metric space and
                $\mathscr{F}\subseteq{C}(X)$ is a closed subset with
                respect to the uniform norm, and if
                $\mathscr{F}$ is equicontinuous on $X$ and point-wise
                bounded, then $\mathscr{F}$ is compact.
            \end{theorem}
            \begin{proof}
                It suffices to show that $\mathscr{F}$ is sequentially
                compact. Let $F_{n}$ be s sequence in $\mathscr{F}$.
                The by the Arzela-Ascoli theorem, there is a uniformly
                convergent subsequence $F_{k_{n}}$. But $\mathscr{F}$
                is closed, and thus the limit function is contained
                in $\mathscr{F}$. Thus, $\mathscr{F}$ is sequentially
                compact. But sequentially compact metric spaces are
                compact. Therefore, etc.
            \end{proof}
            \begin{theorem}
                If $X$ is a compact metric space and
                $\mathscr{F}\subseteq{C}(X)$ is a closed subset with
                respect to the uniform norm, and if
                $\mathscr{F}$ is equicontinuous on $X$ and point-wise
                bounded, then $\mathscr{F}$ is uniformly bounded.
            \end{theorem}
            \begin{proof}
                For $\mathscr{F}$ is compact by the previous theorem.
                But then $\mathscr{F}$ is bounded with respect to
                $\norm{\cdot}_{\infty}$. Therefore, $\mathscr{F}$ is
                uniformly bounded.
            \end{proof}
            \begin{theorem}
                If $X$ is a compact metric space and if
                $\mathscr{F}\subseteq{C}(X)$ is closed, equicontinuous,
                and uniformly bounded on $X$, then $\mathscr{F}$ is
                compact.
            \end{theorem}
            \begin{proof}
                For suppose $\mathscr{F}$ is compact. Then $\mathscr{F}$
                is closed and uniformly bounded. Thus it suffices to
                show that $\mathscr{F}$ is equicontinuous. Suppose not.
                Then there is a point $x\in{X}$ such that
                $\mathscr{F}$ is not equicontinuous at $x$. Thus,
                there exists an $\varepsilon>0$ such that, for all
                $\delta>0$, there are points $x,y$ such that
                $d(x,y)<\delta$, but $|f(x)-f(y)|\geq\varepsilon$
                for some $f\in\mathscr{F}$. Thus, for all
                $n\in\mathbb{N}$, there is an $x_{n}\in{X}$ such that
                $d(x,x_{n})<1/n$, and
                $|f_{n}(x)-f_{n}(x_{n})|\geq\varepsilon_{0}$. But if
                $\mathscr{F}$ is compact, then $f_{n}$ has a convergent
                subsequence $f_{k_{n}}$. Let $f$ be the limit.
                Since $\mathscr{F}$ is compact, $f\in\mathscr{F}$.
                But then $f_{k_{n}}(x_{k_{n}})\rightarrow{f}(x)$. But
                then there is an $N\in\mathbb{N}$ such that,
                for $k_{n}>N$,
                $\norm{f_{k_{n}}-f}_{\infty}<\varepsilon_{0}/3$.
                But then:
                \begin{align}
                    |f(x_{k_{n}})-f(x)|&=
                    |f(x_{k_{n}})-f_{k_{n}}(x_{k_{n}})
                    +f_{k_{n}}(x_{k_{n}})-f_{k_{n}}(x)
                    +f_{k_{n}}(x)-f(x)|\\
                    &\geq|f_{k_{n}}(x_{k_{n}})-f_{k_{n}}(x)|+
                    |f(x_{k_{n}})-f_{k_{n}}(x_{k_{n}})
                    +f_{k_{n}}(x)-f(x)|\\
                    &>\varepsilon
                \end{align}
                A contradiction.
            \end{proof}
        \subsection{Baire Spaces}
            \begin{ldefinition}{Baire Space}
                  {Funct_Analysis_Baire_Space}
            A Baire space is a metric space $(X,d)$ such that, for
            countable collection of open and dense sets, the
            intersection is also dense.
        \end{ldefinition}
            This is a topological property, and so Baire spaces can
            be defined for a more general topological space. The
            interior of a set in a topological space is:
            \begin{equation}
            \Int(A)=
            \bigcup\{\mathcal{U}\in\tau:\mathcal{U}\subseteq{A}\}
        \end{equation}
            \begin{theorem}
            A metric space $(X,d)$ is a Baire space if and only
            if given a countable collection $F_{n}$ of closed
            sets such that the union over all of $F_{n}$ has
            non-empty interior, then at least one of the $F_{n}$
            has non-empty interior.
            \end{theorem}
            \begin{theorem}
                There exist countable Baire spaces.
            \end{theorem}
            Suppose $\mathcal{U}\subseteq{X}$ is open at
            $x_{0}\in\mathcal{U}$. There there is a $\delta>0$ such
            $B_{\delta}(x)\subseteq\mathcal{U}$. Then:
            \begin{equation}
                \overline{B_{\delta/2}(x)}\subseteq{B}_{\delta}(x)
            \end{equation}
            Thus, $\overline{B_{\delta/2}(x)}\subseteq\mathcal{U}$
            and the diameter is less than $2\delta$.
            \begin{ltheorem}{Baire Category Theorem}
                  {Funct_Analysis_Baire_Category}
                Every complete metric space is a Baire space.
            \end{ltheorem}
            \begin{proof}
            Suppose $\mathcal{O}_{n}\subseteq{X}$ is open and
            dense for all $n\in\mathbb{N}$. Let $x_{0}\in{X}$ and
            $r_{0}>0$. It will suffice to show that:
            \begin{equation}
                B_{r_{0}}(x_{0})\cap\bigcap_{n\in\mathbb{N}}
                    \mathcal{O}_{n}\ne\emptyset
            \end{equation}
            Inductively, we create a sequence of points $x_{k}$
            and real numbers $r_{k}>0$ such that $r_{k}$ is strictly
            monotonically decreasing, and thus that:
            \begin{equation}
                \overline{B_{r_{k+1}}(x_{k+1})}
                \subseteq{B}_{r_{k}}(x_{k})\cap\mathcal{O}_{k+1}
            \end{equation}
        \end{proof}
            Consider the set of all lines through the origin with
            rational slope. The complete of any given line is the
            union of two open half planes, which are open and dense
            subsets of $\mathbb{R}^{2}$. Since we have only a countable
            collection of such lines, the intersection of the complement
            is dense in $\mathbb{R}^{2}$. Baire's Category Theorem
            holds even if $(X,d)$ is not complete, but is equivalent
            to a complete metric. For example, let $X=(0,1)$ and let
            $d(x,y)=|x-y|$ be the standard metric. This is not a
            complete space, but is homeomorphic to $\mathbb{R}$, 
            which is a complete metric space. Using this homeomorphism,
            we can find a metric $\tilde{d}$ on $(0,1)$ that is complete
            and which is equivalent to the original metric. Thus,
            $(0,1)$ is a Baire space.
            \begin{theorem}
            If $V$ is a non-empty open subset of a complete metric
            space $(X,d)$, then there is a metric $\tilde{d}$ such
            that $(V,\tilde{d})$ is complete.
        \end{theorem}
            Hence, $V$ is a Baire space. Then, given a set
            $F_{n}$ of closed subsets of $X$ such that:
            \begin{equation}
            V=\bigcup_{n=1}^{\infty}(V\cap{F}_{n})
        \end{equation}
            Then some $F_{n}\cap{V}$ has non-empty interior in $V$,
            and hence in $X$.
            \begin{theorem}
            If $X$ is a Baire space and if $f_{n}$ is a sequence
            of continuous function in $C(X)$ which converges
            point-wise to $f:X\rightarrow\mathbb{C}$, then
            the set:
            \begin{equation}
                \{x\in{X}:f\textrm{ is continuous at }x\}
            \end{equation}
            Is dense in $X$.
        \end{theorem}
            \begin{proof}
            Let $\varepsilon>0$ and define:
            \begin{align}
                A_{N}(\varepsilon)&=
                \{x:|f_{n}(x)-f_{m}(x)|\leq\varepsilon,
                    n,m\in\mathbb{N}\}\\
                &=\bigcap_{n,m\geq{N}}
                    \{x:|f_{n}(x)-f_{m}(x)|\leq\varepsilon\}
            \end{align}
            Then $A_{N}(\varepsilon)$ is closed. But also:
            \begin{equation}
                X=\bigcup_{N=1}^{\infty}A_{N}(\varepsilon)
            \end{equation}
            Thus, by the Baire category theorem, we have:
            \begin{equation}
                \mathcal{U}(\varepsilon)=
                \bigcup_{N=1}^{\infty}\Int(A_{N}(\varepsilon))
            \end{equation}
            Is non-empty and open. Moreover,
            $\mathcal{U}(\varepsilon)$ is dense. But then:
            \begin{equation}
                \mathcal{C}=\bigcap_{n=1}^{\infty}
                \mathcal{U}(\frac{1}{n})
            \end{equation}
            Is dense in $X$, and $f$ is continuous at all
            $x\in\mathcal{C}$.
        \end{proof}
            The Baire Category Theorem says that every complete metric
            space is a Baire space. The notion of Baire space is a
            topological property, and not a metric property. Thus, even
            if $(X,d)$ is not complete but is equivalent to a complete
            metric space $(X,\tilde{d})$, then $(X,d)$ is a Baire space.
            A topological space is called completely metrizable if there
            is a metric on the space that is complete and generates the
            topology. Given a complete metric space $(X,d)$, every
            non-empty open set $\mathcal{V}$ has a metric
            $d_{\mathcal{V}}$ such that $(\mathcal{V},d_{\mathcal{V}})$
            is complete, and is therefore a Baire space. Thus, if:
            \begin{equation}
                \mathcal{V}=\bigcup_{n\in\mathbb{N}}\Big(
                    \mathcal{V}\cap{F}_{n}\Big)
            \end{equation}
            Where $F_{n}$ is closed for all $n\in\mathbb{N}$, then
            for some $N\in\mathbb{N}$, $\mathcal{V}\cap{F}_{N}$ has
            interior.
            \begin{theorem}
            If $X$ is a Baire space, and if
            $F_{n}$ is a sequence of continuous functions that
            converges point-wise to $f:X\rightarrow\mathbb{C}$, then
            the set $\mathcal{D}$ defined by:
            \begin{equation}
                \mathcal{D}=
                    \{x\in{X}:\textrm{$f$ is continuous as $x$}\}
            \end{equation}
            Then $\mathcal{D}$ is dense in $X$.
        \end{theorem}
            \begin{proof}
            For let $\varepsilon>0$, and let:
            \begin{equation}
                A_{N}(\varepsilon)=
                \{x\in{X}:|f_{n}(x)-f_{m}(x)|\leq\varepsilon,n,m>N\}
            \end{equation}
            Then, for all $N\in\mathbb{N}$, $A_{N}(\varepsilon)$ is
            closed. Let $\mathcal{U}(\varepsilon)$ be defined by:
            \begin{equation}
                \mathcal{U}=\bigcup_{n\in\mathbb{N}}
                    \Int\Big(A_{N}(\varepsilon)\Big)
            \end{equation}
            Then $\mathcal{U}(\varepsilon)$ is open and dense. It
            is open for it is the union of open sets. For let
            $\mathcal{V}$ be a non-empty subset. Then:
            \begin{equation}
                \mathcal{V}=\bigcup_{n\in\mathbb{N}}
                    \Big(A_{n}(\varepsilon)\cap\mathcal{V}\Big)
            \end{equation}
            Hence there exists an $N\in\mathbb{N}$ such that:
            \begin{equation}
                A_{N}(\varepsilon)\cap\mathcal{V}\ne\emptyset
            \end{equation}
            And this has interior, and therefore:
            \begin{equation}
                \Int(A_{N}(\varepsilon))\cap\mathcal{V}\ne\emptyset
            \end{equation}
            Therefore,
            $\mathcal{V}\cap\mathcal{U}(\varepsilon)\ne\emptyset$.
            Now, define:
            \begin{equation}
                \mathcal{V}=\bigcap_{n\in\mathbb{N}}
                    \mathcal{U}\big(\frac{1}{n}\big)
            \end{equation}
            And therefore $\mathcal{C}$ is dense in $X$. We
            now want to show that $f$ is continuous for all
            $x\in\mathcal{C}$. For let $x_{0}\in\mathcal{C}$ and
            let $\varepsilon>0$. Let $k\in\mathbb{N}$ be such that
            $k^{\minus{1}}<\varepsilon$. Then
            $x_{0}\in\mathcal{U}(k^{\minus{1}})$ and thus there is
            an $N\in\mathbb{N}$ such that:
            \begin{equation}
                x_{0}\in\Int\big(A_{N}(k^{\minus{1}})\big)
            \end{equation}
            But $f_{N}$ is continuous, and thus there is a
            neighborhood $\omega$ of $x_{0}$ such that, for all
            $y\in\omega$:
            \begin{equation}
                |f_{N}(x_{0})-f_{N}(y)|<\varepsilon/3
            \end{equation}
            Shrink $\omega$ so that it resides inside of
            $\Int(A_{N}(k^{\minus{1}})$. Then:
            \begin{equation}
                |f_{n}(y)-f_{N}(y)|<k^{\minus{1}}
                \quad\quad
                n\geq{N}
            \end{equation}
            But then, use the Cauchy trick and you're down.
        \end{proof}
        \newpage
    \section{Normed Vector Spaces}
        \subsection{Basic Definitions}
            \begin{ldefinition}{Normed Vector Spaces}
                  {Funct_Analysis_Normed_Vector_Space}
                A normed vector space over a field
                $\mathbb{F}\subseteq\mathbb{C}$, denoted
                $(V,\norm{\cdot})$ is a vector space $V$ over
                $\mathbb{F}$ and a norm $\norm{\cdot}$ on $V$.
            \end{ldefinition}
        \subsection{Banach Spaces}
        \begin{ldefinition}{Banach Space}
              {Funct_Analysis_Banach_Space}
            A Banach space is a normed vector space
            $(V,\norm{\cdot})$ such that the metric $d$ induced by
            the norm $\norm{\cdot}$ is complete on $V$.
        \end{ldefinition}
        Normed spaces are special. Give $\mathbf{v}\in{V}$, and
        for $r>0$, we have:
        \begin{equation}
            B_{r}^{(V,\norm{\cdot})}(\mathbf{x})=
            B_{r}^{(V,\norm{\cdot})}(\mathbf{0})+\mathbf{x}
        \end{equation}
        That is, open balls about arbitrary points are merely
        translations of an open ball about the origin.
        \begin{equation}
            |\norm{\mathbf{v}}-\norm{\mathbf{u}}|
            \leq\norm{\mathbf{v}-\mathbf{u}}
        \end{equation}
        And thus the map $\mathbf{v}\mapsto\norm{\mathbf{v}}$ is
        continuous. The closure of an open ball is the closed ball.
        \begin{equation}
            \overline{B_{r}^{(V,\norm{\cdot})}(\mathbf{x})}
            =\{\mathbf{y}\in{V}:
                \norm{\mathbf{x}-\mathbf{y}}\leq{r}\}
        \end{equation}
        We can also multiply open balls by constants, to get
        the following:
        \begin{equation}
            \varepsilon{B}_{r}^{(V,\norm{\cdot})}(\mathbf{x})=
            B_{\varepsilon{r}}^{(V,\norm{\cdot})}(\mathbf{x})
        \end{equation}
        \begin{theorem}
            Suppose $X$ and $Y$ are normed vector spaces over
            $\mathbb{F}\subseteq\mathbb{C}$. Let $T:X\rightarrow{Y}$
            be a linear transformation. Then the following
            are equivalent:
            \begin{enumerate}
                \item $T$ is continuous.
                \item $T$ is continuous at some $x_{0}\in{X}$.
                \item There is an $\alpha>0$ such that
                      $\norm{Tx}\leq\alpha\norm{x}$.
            \end{enumerate}
        \end{theorem}
        \begin{proof}
            Suppose $T$ is continuous at $x_{0}$. Then there is a
            $\delta>0$ such that:
            \begin{equation}
                T\Big(\overline{B_{\delta}(x_{0})}\big)
                \subseteq{B}_{1}(T(x_{0}))
            \end{equation}
            But:
            \begin{align}
                T\Big(\overline{B_{\delta}(x_{0})}\big)
                &=T\Big(\overline{B_{\delta}(0)}\big)+T(x_{0})\\
                B_{1}(T(x_{0}))=
                B_{1}(0)+T(x_{0})
            \end{align}
            Now suppose $z\ne{0}$. Then:
            \begin{equation}
                \norm{T(z)}=
                \norm{\frac{\norm{z}}{\delta}
                      T\Big(\frac{\delta{z}}{\norm{z}}\Big)}
                \leq\frac{1}{\delta}\norm{z}
            \end{equation}
            Let $\alpha=\delta^{\minus{1}}$.
            Proving the next one:
            \begin{equation}
                \norm{T(x)-T(y)}=\norm{T(x-y)}
                \leq\alpha\norm{x-y}
            \end{equation}
            And so we have continuity.
        \end{proof}
        There are linear maps that are not bounded. Let
        $\ell_{1}^{0}=\Span\{e_{k}:x\in\mathbb{N}\}$. Map
        $e_{k}\rightarrow{k}e_{k}$. Let
        $\norm{\cdot}_{a}$ and $\norm{\cdot}_{b}$ be norms on
        $X$ that induce the same topology on $X$. Consider the map
        $id:(X,\norm{\cdot}_{a})\rightarrow(X,\norm{\cdot}_{b})$.
        Since the topologies are the same, $id$ is continuous.
        Then there is a $c\geq{0}$ such that:
        \begin{equation}
            \norm{x}_{b}\leq{c}\norm{x}_{a}
        \end{equation}
        We can go the other way as well, and thus we see that
        equivalence implies strongly equivalent. This is not true
        in a general metric space.
        \begin{ldefinition}{Operator Norm}
              {Funct_Analysis_Operator_Norm}
            Let $\mathscr{L}(X,Y)$ be the set of bounded linear
            transformation $T:X\rightarrow{Y}$. The operator
            norm on $T$ is:
            \begin{equation}
                \norm{T}=\sup\{\norm{T}(x):\norm{x}\leq{1}\}
            \end{equation}
        \end{ldefinition}
        \begin{theorem}
            The operator norm on $\mathscr{L}(X,Y)$ is a norm.
        \end{theorem}
        \begin{proof}
            For we have:
            \begin{equation}
                \norm{S\circ{T}}\leq\norm{S}\norm{T}
            \end{equation}
        \end{proof}
        \begin{ldefinition}{Algebra Over a Field}
              {Funct_Analysis_Algebra_Over_Field}
            An algebra over a field $\mathbb{F}$ is a vector
            space $A$ over $\mathbb{F}$ such that $A$ has a ring
            structure $(A,\times,+)$ such that:
            \begin{equation}
                \lambda(xy)=(\lambda{x})y
                =x(\lambda(y))
                \quad\quad
                x,y\in{A}
                \quad\lambda\in\mathbb{F}
            \end{equation}
        \end{ldefinition}
        \begin{lexample}
            $\mathbb{R}[x]$, $\mathbb{C}[x]$, $M_{n}(\mathbb{F})$,
            and $C_{b}(X)$.
        \end{lexample}
        \begin{ldefinition}{Normed Algebra}
              {Funct_Analysis_Normed_Algebra}
            A normed algebra is a normed space $(A,\norm{\cdot})$
            such that $A$ is an algebra and such that,
            for all $x,y\in{A}$:
            \begin{equation}
                \norm{xy}\leq\norm{x}\norm{y}
            \end{equation}
        \end{ldefinition}
        \begin{ldefinition}{Banach Algebra}
              {Funct_Analysis_Banach_Algebra}
            A Banach Algebra is a normed algebra $(A,\norm{\cdot})$
            such that $(A,\norm{\cdot})$ is a Banach space.
        \end{ldefinition}
        \begin{theorem}
            If $Y$ is a Banach space, then $\mathscr{L}(X,Y)$
            is a Banach space.
        \end{theorem}
        \begin{theorem}
            If $X$ is a Banach Algebra, then
            $\mathscr{L}(A)$ is a Banach algebra.
        \end{theorem}
        \begin{proof}
            For suppose $T_{n}$ is Cauchy in
            $\mathscr{L}(X,Y)$. Then for all $x\in{X}$,
            $T_{n}(x)$ is Cauchy in $Y$, and thus
            $T_{n}(x)$ converges to some $y\in{Y}$. Let
            $T:X\rightarrow{Y}$ be this limit function. Then
            $T:X\rightarrow{Y}$ is a linear map. But since
            $T_{n}$ is Cauchy, it is uniformly bounded. But then
            there is an $M\in\mathbb{R}^{+}$ such that:
            \begin{equation}
                \norm{T_{n}}\leq{M}
            \end{equation}
            And thus:
            \begin{equation}
                \norm{T{x}}=
                \underset{n\rightarrow\infty}{\lim}
                \norm{T_{n}x}\leq
                \lim\underset{n}{\sup}\norm{T_{n}}\norm{x}
                \leq{M}\norm{x}
            \end{equation}
            Therefore, etc.
        \end{proof}
    \section{Lecture 9, I Think}
        Let $X_{\lambda}$ be a Banach space for all
        $\lambda\in\Lambda$. Then the product is:
        \begin{equation}
            \prod_{\lambda\in\Lambda}X_{\lambda}
            =\{f:\Lambda\rightarrow\bigcup_{\lambda\in\Lambda}
            X_{\lambda}:f(\lambda)\in{X}_{\lambda}\}
        \end{equation}
        Generally, we think of the indexing set to be finite,
        $\Lambda=\{1,\dots,n\}$. The product space is then:
        \begin{equation}
            \prod_{\lambda=1}^{n}X_{\lambda}=
            X_{1}\times\dots{X}_{n}
        \end{equation}
        Functions are therefore $n$ tuples. There's no reason to
        expect that this will be a Banach space in any reasonable
        way. Thus we define the Banach Space Direct-Product.
        \begin{ldefinition}{Banach Space Direct Product}
              {Funct_Analysis_Banach_Space_Direct_Product}
            The Banach Space Direct Product of a set of Banach
            spaces $X_{\lambda}$ indexed over $\Lambda$ is:
            \begin{equation}
                \prod_{\lambda\in\Lambda}^{*}X_{\lambda}
                =\{f\in\prod_{\lambda\in\Lambda}X_{\lambda}:
                \underset{\lambda\in\Lambda}{\sup}
                f(\lambda)<\infty\}
            \end{equation}
        \end{ldefinition}
        Then $\norm{x}=\sup_{\lambda}\norm{x_{\lambda}}$ is a norm
        on the product space.
        \begin{ltheorem}{Open Mapping Theorem}
              {Funct_Analysis_Open_Mapping_Theorem}
            If $X$ and $Y$ are Banach spaces and if
            $T\in\mathcal{L}(X,Y)$ is surjective, then $T$ is an
            open map.
        \end{ltheorem}
        \begin{proof}
            It will suffice to find $r>0$ such that:
            \begin{equation}
                B_{r}^{Y}\subseteq
                T\Big(B_{1}^{X}(0)\Big)
            \end{equation}
            By homogeneity, $T(B_{\delta}^{X}(0))$ is a
            neighborhood of $0$ for all $\delta>0$. By linearity,
            $T(B_{\delta}(x))$ is a neighborhood of
            $T(x)$ for all $x\in{X}$ and for all $\delta>0$. But
            if $V\subseteq{X}$ is open, and $x\in{V}$, then there
            is a $\delta>0$ such that $B_{\delta}(x)\subseteq{V}$.
            Thus, $T(B_{\delta}(x))$ is a neighborhood of $T(x)$
            in $T(V)$. There is al an $r>0$ such that:
            \begin{equation}
                B_{r}^{Y}(0)\subseteq
                \overline{T(B_{1}^{X}(0)))}
            \end{equation}
            For let $\alpha\in(0,1)$. Note that:
            \begin{equation}
                T(B_{\alpha}(x))=\alpha{T}(B_{1}(0))
            \end{equation}
            And also:
            \begin{equation}
                B_{\alpha{r}}(0)\subseteq
                \overline{\alpha{T}(B_{1}(0))}
            \end{equation}
            Let $y\in{B}_{r}(0)$. Then there is a
            $y_{1}\in{T(B_{1}(0))}$ such that:
            \begin{equation}
                \norm{y-y_{1}}<\frac{r}{2}
            \end{equation}
            But then $y-y_{1}\in{B}_{r/2}(0)$. But then there is a
            $y_{2}\in{T}(B_{1/2}(0))$ such that:
            \begin{equation}
                \norm{y_{2}}<\frac{r}{4}
            \end{equation}
            That is:
            \begin{equation}
                \norm{y-y_{1}-y_{2}}<\frac{r}{2^{2}}
            \end{equation}
            Continuing we obtain a sequence $y_{n}$ such that:
            \begin{equation}
                y_{n}\in{T}(B_{1/2^{n}}(0))
            \end{equation}
            And such that:
            \begin{equation}
                \norm{y-\sum_{k=1}^{n}y_{k}}<
                \frac{r}{2^{n}}
            \end{equation}
            Note that there exists $x_{n}\in{X}$ such that
            $T(x_{n})=y_{n}$ and:
            \begin{equation}
                \norm{x_{n}}<\frac{1}{2^{n-1}}
            \end{equation}
            But then there is an $x\in{X}$ such that:
            \begin{equation}
                x=\sum_{n=1}^{\infty}x_{n}
            \end{equation}
            Since $X$ is complete and since this series converges.
            But $T$ is continuous, and therefore $T(x)=y$. But:
            \begin{equation}
                \norm{x}\leq\sum_{n=1}^{\infty}\norm{x_{n}}
                <\sum_{n=0}^{\infty}\frac{1}{2^{n}}=2
            \end{equation}
            That is,:
            \begin{equation}
                B_{r}(0)\subseteq{T}(B_{2}(0))
            \end{equation}
            And therefore:
            \begin{equation}
                B_{r/2}(0)\subseteq
                T_{1}(B_{1}(0))
            \end{equation}
            If $T$ is surjective, we can write:
            \begin{equation}
                Y=\bigcup_{n\in\mathbb{N}}
                \overline{T(B_{n}(0))}
            \end{equation}
            But $Y$ is a Banach space, and thus by the Baire
            category theorem, there is an $n\in\mathbb{N}$ such
            that $\overline{T(B_{n}(0))}$ has interior. Therefore,
            etc.
        \end{proof}
        \begin{lexample}
            Recall that:
            \begin{equation}
                \ell_{0}^{p}=
                \{x\in\ell^{p}:\exists{N\in\mathbb{N}},
                    \forall_{n>N},x_{n}=0\}
            \end{equation}
            Let $\ell_{0}^{p}$ be defined by:
            \begin{equation}
                \ell_{0}^{p}=\Span\{e_{n}:n\in\mathbb{N}\}
            \end{equation}
            Define $T:\ell_{0}^{2}\rightarrow\ell_{0}^{p}$ by:
            \begin{equation}
                T(e_{n})=\frac{1}{n}e_{n}
            \end{equation}
            Then $T$ is bounded and $\norm{T}\leq{1}$. Note that
            $T$ is bijective and has an inverse:
            \begin{equation}
                T^{\minus{1}}(e_{n})=ne_{n}
            \end{equation}
            And this is not bounded, so
            $T^{\minus{1}}\notin\mathcal{L}(\ell_{0}^{p})$.
            This can happen since $\ell_{0}^{P}$ is not complete.
        \end{lexample}
        \begin{ltheorem}{Inverse Mapping Theorem}
              {Funct_Analysis_Inverse_Mapping_Theorem}
            If $X$ and $Y$ are Banach spaces and
            $T\in\mathcal{L}(X,Y)$ is bijective, then
            $T^{\minus{1}}\in\mathcal{L}(Y,X)$.
        \end{ltheorem}
        \begin{proof}
            Note that $T^{\minus{1}}$ is linear. Since $T$ has to be
            open by the open mapping theorem, $T^{\minus{1}}$ is
            continuous. But continuous linear functions are bounded.
            Therefore, $T^{\minus{1}}$ is bounded.
        \end{proof}
        \begin{ltheorem}{Closed Graph Theorem}
              {Funct_Analysis_Closed_Graph_Theorem}
            If $X$ and $Y$ are Banach spaces and if
            $T:X\rightarrow{Y}$ is linear, then
            $T\in\mathcal{L}(X,Y)$ if and only if the graph of
            $T$ is closed in $X\times{Y}$.
        \end{ltheorem}
        \begin{proof}
            Note that $X\times{Y}$ is a Banach space and
            $(x_{n},y_{n})\rightarrow(x,y)$ if and only if
            $x_{n}\rightarrow{x}$ and $y_{n}\rightarrow{y}$.
            If $T$ is bounded, then the graph of $T$ is closed.
            Now, suppose that the graph of $T$ is closed. But
            then the graph is a Banach space. The projection map
            $P:T\rightarrow{X}$ given by $P((x,T(x))=x$ is
            a bounded bijection. Hence, $P^{\minus{1}}$ is bounded.
            Let $P_{2}$ be defined by $P_{2}(x,T(x))=T(x)$. Then
            $P_{2}$ is bounded. But:
            \begin{equation}
                T(x)=P_{2}\circ{P}_{1}^{\minus{1}}(x)
            \end{equation}
            And therefore $T$ is bounded.
        \end{proof}
        \begin{lexample}
            Suppose $T_{n}\in\mathcal{L}(X,Y)$ and suppose, for
            all $x\in{X}$:
            \begin{equation}
                T(x)=\underset{n\rightarrow\infty}{\lim}T_{n}(x)
            \end{equation}
            Then we have that $T$ is linear. Is
            $T\in\mathcal{L}(X,Y)$? We have:
            \begin{equation}
                \norm{T}=
                \norm{\underset{n\rightarrow\infty}{\lim}T_{n}}
                \leq\underset{n\rightarrow\infty}{\lim}\sup_{n}
                \norm{T_{n}}
            \end{equation}
        \end{lexample}
        \begin{ltheorem}{Principle of Uniform Boundedness}
              {Funct_Analysis_Prin_Uni_Bounded}
            If $X$ and $Y$ are Banach spaces, if
            $T_{\lambda}\in\mathcal{L}(X,Y)$ for all
            $\lambda\in\Lambda$, and if for all $x\in{X}$ we have:
            \begin{equation}
                \norm{\{\norm{T_{\lambda}(x)}:\lambda\in\Lambda\}}
                <\infty
            \end{equation}
            Then $\{\norm{T_{\lambda}}:\lambda\in\Lambda\}$ is
            bounded.
        \end{ltheorem}
        Recall that if $X_{\lambda}$ are Banach spaces for all
        $\lambda\in\Lambda$, where $\Lambda$ is some indexing
        set, then we can form the Banach Space direct product:
        \begin{equation}
            \prod_{\lambda\in\Lambda}^{*}X_{\lambda}
            =\Big\{x\in\prod_{\lambda\in\Lambda}X_{\lambda}:
                \norm{x}_{\lambda}<\infty\Big\}
        \end{equation}
        If $\lambda_{0}\in\Lambda$, we can define:
        \begin{equation}
            P_{\lambda}:\prod_{\lambda\in\Lambda}^{*}X_{\lambda}
            \rightarrow{X}_{\lambda_{0}}
        \end{equation}
        By defining $P_{\lambda_{0}}(x)=x(\lambda_{0})$.
        Now consider the case when $X_{\lambda}=Y$ for all
        $\lambda\in\Lambda$. We'll write:
        \begin{equation}
            Y_{\Lambda}=\prod_{\lambda\in\Lambda}Y
        \end{equation}
        The principle of uniform boundedness says that if
        $X$ and $Y$ are Banach spaces and if
        $T_{\lambda}\in\mathcal{L}(X,Y)$ for all
        $\lambda\in\Lambda$, and that for all $x\in{X}$, the set
        $\{\norm{T_{\lambda}(x)}:\lambda\in\Lambda\}$ is
        bounded, then $\{\norm{T_{\lambda}}:\lambda\in\Lambda\}$
        is bounded.
        \begin{ltheorem}{Principle of Uniform Boundedness}{}
            If $X$ and $Y$ are Banach spaces, if
            $T_{\lambda}\in\mathcal{L}(X,Y)$ for all
            $\lambda\in\Lambda$, and if for all $x\in{X}$ we 
            have that $\{\norm{T_{\lambda}(x)}:\lambda\in\Lambda\}$
            is bounded, then
            $\{\norm{T_{\lambda}}:\lambda\in\Lambda\}$ is bounded.
        \end{ltheorem}
        \begin{proof}
            For let:
            \begin{equation}
                Y_{\Lambda}=\prod_{\lambda\in\Lambda}Y
            \end{equation}
            And define $T:X\rightarrow{Y}_{\Lambda}$ by:
            \begin{equation}
                T(x)=T_{\lambda}(x)
            \end{equation}
            Then $T$ is well defined and linear. To see
            that $T$ is bounded, use the closed graph
            theorem. That is, suppose $x_{n}\rightarrow{x}$.
            We need to show that $T(x_{n})\rightarrow{T}(x)$.
            Let $\lambda\in\Lambda$. Then
            $P_{\lambda}(T(x_{n}))\rightarrow{P}_{\lambda}(y)$.
            Then $T_{n}(x_{n})(\lambda)\rightarrow{y}(\lambda)$.
            But $T(x_{n})(\lambda)=T_{\lambda}(x_{n})$, and this
            converges to $T_{\lambda}(x)$. But then
            $y(\lambda)=T_{\lambda}$ for all $\lambda$, and
            thus $y=T(x)$. Therefore
            $T\in\mathcal{L}(X,Y_{\Lambda})$. But if
            $\norm{x}\leq{1}$, then:
            \begin{equation}
                \norm{T_{\lambda}(x)}\leq
                \sup_{\lambda\in\Lambda}\norm{T_{\lambda}(x)}
                =\sup_{\lambda\in\Lambda}\norm{T(x)(\lambda)}
                =\norm{T(x)}\leq\norm{T}
            \end{equation}
            Thus, $\norm{T}\geq\norm{T_{\lambda}}$ for all
            $\lambda\in\Lambda$. Therefore, etc.
        \end{proof}
        \begin{ltheorem}{Banach-Stienhaus Theorem}
              {Funct_Analysis_Banach_Stienhaus}
            If $X$ and $Y$ are Banach spaces, and if
            $T_{n}\in\mathcal{L}(X,Y)$ converges point-wise to
            $T$, then $T\in\mathcal{L}(X,Y)$.
        \end{ltheorem}
        \begin{proof}
            By the principle of uniform boundedness,
            $T_{n}$ is uniformly bounded. Therefore $T$ is bounded.
        \end{proof}
    \section{Zorn's Lemma}
        A relation on a set $X$ is a subset
        $R\subseteq{X}\times{X}$. Given an element $(x,y)\in{R}$,
        we often write $xRy$ to denote this. Here we'll write
        $x\leq{y}$.
        \begin{ldefinition}{Ordered Sets}
              {Funct_Analysis_Ordered_Set}
            An ordered set is a set $X$ and a relation
            $\leq$ on $X$, denoted $(X,\leq)$ such that
            the following are true:
            \begin{enumerate}
                \item For all $x\in{X}$, $x\leq{x}$.
                \item For all $x,y\in{X}$ such that $x\leq{y}$ and
                      $y\leq{x}$, it is true that $x=y$.
                \item For all $x,y,z\in{X}$ such that $x\leq{y}$
                      and $y\leq{z}$, it is true that $x\leq{z}$.
            \end{enumerate}
        \end{ldefinition}
        \begin{ldefinition}{Majorants in Ordered Sets}
              {Funct_Analysis_Majorant_in_Ord_Set}
            A majorant of a subset $Y\subseteq{X}$ of
            and ordered set $(X,\leq)$ is an element $x\in{X}$
            such that, for all $y\in{Y}$, it is true that
            $y\leq{x}$.
        \end{ldefinition}
        \begin{lexample}
            Let $(X,d)$ be a metric space, and let
            $x_{0}\in{X}$. Define the following:
            \begin{equation}
                \mathscr{N}(x_{0})=
                \big\{\mathcal{V}\subseteq{X}:\mathcal{V}
                    \textrm{ is a neighborhood of $x_{0}$}\big\}
            \end{equation}
            We can order $\mathscr{N}$ by reverse containment.
            That is, We have the following relation:
            \begin{equation}
                \leq=\big\{(\mathcal{U},\mathcal{V})\in
                    \mathscr{N}(x_{0})\times\mathscr{N}(x_{0})
                    :\mathcal{V}\subseteq\mathcal{U}\big\}
            \end{equation}
            That is, we write $\mathcal{U}\leq\mathcal{V}$ if
            $\mathcal{V}$ is a subset of $\mathcal{U}$. Note
            that, for all $x_{0}$,
            $\mathscr{x_{0}}$ has a least element, or a minorant,
            but $X$ is such an element. But, if $\{x_{0}\}$ is
            not open, then there is no majorant.
        \end{lexample}
        \begin{ldefinition}{Totally Ordered Sets}
              {Funct_Analysis_Tot_Ord_Set}
            A totally ordered set is an ordered set
            $(X,\leq)$ such that, for all $x,y\in{X}$, either
            $x\leq{y}$ or $y\leq{x}$.
        \end{ldefinition}
        \begin{ldefinition}{Maximal Element}
              {Funct_Analysis_Maximal_Element}
            A maximal element of a subset $Y\subseteq{X}$ of
            a totally ordered set $(X,\leq)$ is an element $y$
            such that:
            \begin{equation}
                \{y'\in{Y}:y\leq{y}'\}=\{y\}
            \end{equation}
            Note that $y$ is not necessary a majorant for $Y$
            nor is $y$ necessarily unique.
        \end{ldefinition}
        \begin{ldefinition}{Inductively Ordered Sets}
              {Funct_Analysis_Induct_Ordered_Set}
            An inductively ordered set is an ordered set
            $)X,\leq)$ such that, for all totally ordered
            subsets $S\subseteq{X}$, there is a majorant
            $x\in{X}$ of $S$.
        \end{ldefinition}
        That is, there exists $x\in{X}$ such that, for all
        $y\in{S}$, $y\leq{x}$.
        \begin{lexample}
            Let $X=\mathbb{R}$ and consider the set
            $\mathscr{N}(0)$. Then $\mathscr{N}(0)$ is
            not inductively ordered.
        \end{lexample}
        \begin{ltheorem}{Zorn's Lemma}
              {Funct_Analysis_Zorns_Lemma}
            If $(X,\leq)$ is an inductively ordered set,
            then there is a maximal element $x\in{X}$.
        \end{ltheorem}
        For more review, see Folland's Real Analysis and
        Pedersen's Analysis Now. Zorn's lemma is used to
        prove that every vector space has a basis. We'll
        use this to discuss the notion of duals on
        normed vector spaces.
        \begin{ldefinition}{Dual of a Normed Vector Space}
              {Funct_Analysis_Dual_of_Normed_Vec_Space}
            The dual of a normed vector space $(X,\norm{\cdot})$
            is the set $X^{*}=\mathcal{L}(X,\mathbb{F})$
            with the operator norm.
        \end{ldefinition}
        \begin{theorem}
            If $(X,\norm{\cdot})_{X}$ is a normed vector space,
            then $(X^{*},\norm{\cdot})$ is a Banach space.
        \end{theorem}
        \begin{lexample}
            Let $X=\mathbb{F}^{n}$ and let $\{e_{1},\dots,e_{n}\}$
            be the standard basis. The we can define
            $e_{k}^{*}\in(\mathbb{F}^{n})^{*}$ by:
            \begin{equation}
                e_{k}^{*}(\alpha_{1}e_{1}+\cdots+\alpha_{n}e_{n})
                =\alpha_{k}
            \end{equation}
            Then $\{e_{1}^{*},\dots,e_{n}^{*}\}$ is a basis
            for $(\mathbb{F})^{*}$, and thus
            $(\mathbb{F}^{n})^{*}\simeq\mathbb{F}^{n}$.
        \end{lexample}
        \begin{lexample}
            Let $\{e_{\lambda}\}_{\lambda\in\Lambda}$ be
            a Hamel basis. Then we can define a linear basis:
            \begin{equation}
                e_{\lambda}^{*}:X\rightarrow\mathbb{F}
            \end{equation}
            But also, the set
            $\{\lambda\in\Lambda:e_{\lambda}^{*}\in{X}^{*}\}$
            is at most finite.
        \end{lexample}
        One question that arises is, given any normed vector
        space $X$, what can we say about the dual space $X^{*}$?
        We know that the zero operator is in there, but is there
        anything else?
        \begin{ldefinition}{Minkowski Functional}
              {Funct_Analysis_Minkowski_Functional}
            A Minkowski function on a normed vector space
            $(X,\norm{\cdot})$ over a field
            $\mathbb{F}\subseteq\mathbb{C}$ is a function
            $m:X\rightarrow\mathbb{R}$ such that the following are
            true:
            \begin{subequations}
                \begin{align}
                    m(x+y)&\leq{m}(x)+m(y)\\
                    m(tx)&=tm(x)\quad\quad{t}\geq{0}
                \end{align}
            \end{subequations}
        \end{ldefinition}
        \begin{lexample}
            If $\norm{\cdot}$ is a semi-norm on $X$ over
            $\mathbb{R}$ or $\mathbb{C}$, then $m(x)=\norm{x}$
            is a Minkowski functional on $X$. If we let
            $X=\ell_{\mathbb{R}}^{\infty}$, then:
            \begin{equation}
                m(x)=\underset{n\rightarrow\infty}{\lim}
                    \underset{k\leq{n}}{\sup}\{x_{n}\}
            \end{equation}
            Is also a Minkowski functional.
        \end{lexample}
        \begin{ltheorem}{Basic Extension Lemma}
              {Funct_Analysis_Basic_Extension_Lemma}
            If $m:X\rightarrow\mathbb{R}$ is a Minkowski functional
            on a vector space $X$ over $\mathbb{R}$ if
            $Y\subseteq{X}$ is a subspace, and if
            $\varphi:Y\rightarrow\mathbb{R}$ is a linear functional
            such that, for all $y\in{Y}$,
            $\varphi(y)\leq{m}(y)$, then there is a linear
            functional $\tilde{\varphi}:X\rightarrow\mathbb{R}$
            such that, for all $x\in{X}$,
            $\tilde{\varphi}(x)\leq{m}(x)$, and for all $y\in{Y}$,
            $\tilde{\varphi}(y)=\varphi(y)$.
        \end{ltheorem}
        If $X$ is a normed vector space, then we can identify
        $X$ with it's image $i(X)$ in $X^{**}$, where
        $i:X\rightarrow{X}^{**}$ is defined by:
        \begin{equation}
            i(x)(\phi)=\phi(x)
        \end{equation}
        If $X$ is a Banach space, then $i(X)$ is closed. The
        Otherwise, we call $\tilde{X}=\overline{i(X)}$ is the
        completion of $X$.
        \begin{ldefinition}{Reflexive Banach Space}
              {Funct_Analysis_Reflexive_Banach_Space}
            A reflexive banach space is a Banach space $X$ such
            that the natural map $i:X\rightarrow{X}^{**}$ is
            surjective.
        \end{ldefinition}
        If $X$ is reflexive, then $X$ is isometrically isomorphic
        to $X^{**}$. One might suspect that the converse is true,
        but that's not what the definition says. Indeed, the
        converse is not true. There are Banach spaces $X$ that
        are isometrically isomorphic to $X^{**}$ that are not
        reflexive.
        \begin{lexample}
            Let $X=\ell^{p}$, with $1<p<\infty$, and let
            $q$ be the conjugate exponent of $p$. If $y\in\ell^{q}$,
            there is a linear functional
            $\varphi_{q}^{y}\in(\ell^{p})^{*}$ defined by:
            \begin{equation}
                \varphi_{y}^{p}(x)=\sum_{n=1}^{\infty}x_{n}y_{n}
            \end{equation}
            From H\"{o}lder's inequality, this sum does indeed
            converge, and:
            \begin{equation}
                \norm{\varphi_{y}^{p}}\leq\norm{y}_{q}
            \end{equation}
            Moreover, equality is obtained:
            \begin{equation}
                \norm{\varphi_{y}^{p}}=\norm{y}_{q}
            \end{equation}
            and the map $y\mapsto\varphi_{y}^{p}$ is an
            isometric isomorphism of $\ell^{q}$ with
            $(\ell^{p})^{*}$. Consider
            $i(x)\in(\ell^{p})^{**}=(\ell^{q})^{*}$ by
            the identity above. Note that:
            \begin{equation}
                i(x)(\varphi_{y}^{p})=\varphi_{y}^{p}(x)
                =\varphi_{x}^{q}(y)
            \end{equation}
            And thus $i:X\rightarrow{X}^{**}$ is surjective, so
            $X$ is reflexive.
        \end{lexample}
        \begin{ldefinition}{Transpose of a Bounded Linear Operator}
              {Funct_Analysis_Tranpose_of_BLO}
            The transpose over a bounded linear operator
            $T:X\rightarrow{Y}$ between normed vector spaces $X$
            and $Y$ is the function $T^{*}:Y^{*}\rightarrow{X}^{*}$
            defined by:
            \begin{equation}
                T^{*}(\varphi)(x)=\varphi(T(x))
            \end{equation}
            For all $\varphi\in{Y}^{*}$.
        \end{ldefinition}
        \begin{theorem}
            If $X$ and $Y$ are normed vector spaced and if
            $T\in\mathcal{L}(X,Y)$, then
            $T^{*}\in\mathcal{L}(Y^{*},X^{*})$ and
            $\norm{T^{*}}=\norm{T}$.
        \end{theorem}
        \begin{proof}
            For:
            \begin{subequations}
                \begin{align}
                    \norm{T^{*}(\varphi)}&=
                    \underset{\norm{x}=1}{\sup}|T^{*}(\varphi)(x)|\\
                    &=\underset{\norm{x}=1}{\sup}|\varphi(T(x))|\leq
                    \underset{\norm{x}=1}{\sup}
                        \norm{\varphi}\norm{T}\norm{x}\\
                    &=\underset{\norm{x}=1}{\sup}
                        \norm{\varphi}{\norm{T}}\\
                    &\leq\norm{T}\norm{\varphi}
                \end{align}
            \end{subequations}
            Therefore, $\norm{T^{*}}\leq\norm{T}$. Let
            $\varepsilon>0$. Then there is an $x$ such that
            $\norm{x}=1$ and $\norm{T}<\norm{T(x)}+\varepsilon$.
            But there exists $\varphi\in{Y}^{*}$ such that
            $\norm{\varphi}=1$ and $\varphi(T(x))=\norm{T(x)}$.
            But then:
            \begin{subequations}
                \begin{align}
                    \norm{T^{*}}\geq\norm{T^{*}(\varphi)}
                    &\geq|T^{*}(\varphi(x))|\\
                    &=\norm{T(x)}\\
                    &>\norm{T}-\varepsilon
                \end{align}
            \end{subequations}
            By letting $\varepsilon$ tend to zero, we see that
            $\norm{T^{*}}\geq\norm{T}$. Thus,
            $\norm{T^{*}}=\norm{T}$.
        \end{proof}
        \begin{theorem}
            If $X$ and $Y$ are Banach Spaces, if $T:X\rightarrow{Y}$
            and $S:Y^{*}\rightarrow{X}^{*}$ are functions such
            that, for all $\varphi\in{Y}^{*}$ and for all $x\in{X}$,
            we have $S(\varphi)(x)=\varphi(T(x))$, then $S$ and $T$
            are bounded linear operators and $S=T^{*}$.
        \end{theorem}
        \begin{proof}
            Not if $\varphi\in{Y}^{*}$, then $S(\varphi)\in{X}^{*}$,
            and thus:
            \begin{subequations}
                \begin{align}
                    \varphi\big(T(x+\lambda{y})\big)&=
                    S(\varphi)(x+\lambda{y})\\
                    &=S(\varphi)(x)+\lambda{S}(\varphi)(y)\\
                    &=\varphi(T(x))+\lambda\varphi(T(y))\\
                    &=\varphi(T(x)+\lambda{T}(y))
                \end{align}
            \end{subequations}
            Since $\varphi\in{Y}^{*}$, we have:
            \begin{equation}
                T(x+\lambda{y})=T(x)+\lambda{T}(y)
            \end{equation}
            To see that $T$ is bounded, we use the closed graph
            theorem. Suppose $x_{n}\rightarrow{x}$ and
            $T(x_{n})\rightarrow{y}$. Then, for all
            $\varphi\in{Y}^{*}$, we have:
            \begin{subequations}
                \begin{align}
                    \varphi(y)&=\underset{n\rightarrow\infty}{\lim}
                        \varphi(T(x_{n}))\\
                        &=\underset{n\rightarrow\infty}{\lim}
                        S(\varphi)(x_{n})\\
                        &=S(\varphi)(x)\\
                        &=\varphi(T(x))
                \end{align}
            \end{subequations}
            Thus, by the closed graph theorem, $y=T(x)$.
        \end{proof}
        \subsection{Brushing Up on Topology}
            Let $(X,\tau)$ be a topological space. Recall that
            elements of $\tau$ are called open sets.
            \begin{ldefinition}{Basis of a Topology}
                  {Funct_Analysis_Basis_of_Topology}
                A basis of a topological space $(X,\tau)$ is a
                subset $\beta\subseteq\tau$ such that, for all
                $\mathcal{U}\in\tau$ and $x\in\mathcal{U}$, there
                is a $V\in\beta$ such that
                $x\in{V}\subset\mathcal{U}$.
            \end{ldefinition}
            We say that a subset $V\subseteq{X}$ is a neighborhood
            of $x$ if there is an open set
            $\mathcal{U}\in\tau$ such that
            $x\in\mathcal{U}\subseteq{V}$. We write
            $\mathscr{N}(x)$ for the collection of neighborhoods
            of $x$.
            \begin{ldefinition}{Neighborhood Basis}
                  {Funct_Analysis_Neighborhood_Basis}
                A neighborhood basis of a point $x$ in a topological
                space $(X,\tau)$ is a subset
                $\alpha\subseteq\mathscr{N}(x)$ such that
                for all $\mathcal{U}\in\mathscr{N}(x)$ there is
                a $V\in\alpha$ such that
                $x\in{V}\subseteq\mathcal{U}$.
            \end{ldefinition}
            \begin{lexample}
                In a metric space, the open balls form
                a basis for the metric topology. In $\mathbb{R}^{n}$
                every point has a neighborhood basis consisting of
                compact sets. Thus, we say that $\mathbb{R}^{n}$
                is locally compact. Thus it is useful to allow
                neighborhoods of point be be more than just open
                sets containing the point.
            \end{lexample}
            \begin{theorem}
                If $\alpha(x)$ is a neighborhood basis of
                $x$ consisting of open sets, and:
                \begin{equation}
                    \beta=\bigcup_{x\in{X}}\alpha(x)
                \end{equation}
                Then $\beta$ is a basis for $(X,\tau)$.
            \end{theorem}
            \begin{theorem}
                If $(X,\tau)$ a topological space, then
                $\beta\subseteq\tau$ is a basis if and only if,
                for all $\mathcal{U}\in\tau$, we have:
                \begin{equation}
                    \mathcal{U}=
                    \bigcup_{\underset{V\subseteq\mathcal{U}}
                        {V\in\beta}}V
                \end{equation}
            \end{theorem}
            \begin{ldefinition}{Separable Topological Space}
                  {Funct_Analysis_Sep_Top_Space}
                A separable topological space is a topological
                space $(X,\tau)$ such that there is a countable
                dense subset $\mathcal{D}\subseteq{X}$.
            \end{ldefinition}
            \begin{ldefinition}{First Countable Topological Space}
                  {Funct_Analysis_First_Count_Top_Space}
                A first countable topological space is a
                topological space $(X,\tau)$ such that, for
                all $x\in{X}$, there is a countable neighborhood
                basis $\alpha(x)\subseteq\tau$.
            \end{ldefinition}
            \begin{ldefinition}{Second Countable Topological Space}
                  {Funct_Analysis_Second_Count_Top_Space}
                A second countable topological space is a
                topological space $(X,\tau)$ such that there is
                a countable basis $\beta\subseteq\tau$.
            \end{ldefinition}
            \begin{lexample}
                Let $X$ be any set, and let $\tau=\mathcal{P}(X)$.
                This is called the discrete topology on $X$
                and is metrizable, for it is generated by the
                discrete metric on $X$. The topological space
                $(X,\tau)$ is not second countable if $X$ is an
                uncountable set.
            \end{lexample}
            \begin{theorem}
                If $(X,\tau)$ is a metrizable metric space, then
                $(X,\tau)$ is second countable if and only if
                $(X,\tau)$ is separable.
            \end{theorem}
            \begin{theorem}
                If $(X,\tau)$ is metrizable, then it is first
                countable.
            \end{theorem}
            \begin{theorem}
                If $X$ is a set and $S\subseteq\mathcal{P}(X)$,
                then there is a smallest topology, $\tau(S)$, such
                that $S\subseteq\tau(S)$.
            \end{theorem}
            \begin{proof}
                Since $\mathcal{P}(X)$ is a topology on $X$ such
                that $S\subseteq\mathcal{P}(X)$, the set of
                topologies on $X$ such that $S$ is contained in
                the topology is non-empty. Let $A$ denote this set,
                and define:
                \begin{equation}
                    \tau=\bigcap_{\tau_{A}\in{A}}\tau_{A}
                \end{equation}
                Then $\tau$ is a topology, and $S\subseteq\tau$.
                Moreover, for any topology $\tau'$ such
                that $S\subseteq\tau'$, we have $\tau\subseteq\tau$.
                Thus, $\tau$ is the smallest topology.
            \end{proof}
            \begin{theorem}
                If $\beta\subseteq\mathcal{P}(X)$ has a cover of
                $X$, then $\beta$ is a basis for
                $\tau(\beta)$ if and only if, given $\mathcal{U}$,
                $\mathcal{V}$ in $\tau(\beta)$, and
                $x\in\mathcal{U}\cap\mathcal{V}$, then there is an
                $\omega\in\beta$ such that
                $x\in\omega\subseteq\mathcal{U}\cap\mathcal{V}$.
            \end{theorem}
            \begin{theorem}
                If $X$ is a topological space, if
                $\beta\subseteq\mathcal{P}(X)$ is a cover of $X$
                such that $\beta$ is closed under intersection,
                then $\beta$ is a basis for the topological space
                $(X,\tau(\beta))$.
            \end{theorem}
            \begin{theorem}
                If $\rho\subseteq\mathcal{P}(X)$ is a cover of
                $X$, and if:
                \begin{equation}
                    \beta=\Big\{\bigcap_{i=1}^{n}V_{i}:
                        n\in\mathbb{N},V_{i}\in\rho\Big\}
                \end{equation}
                Then $\beta$ is a basis for $\tau(\rho)$. We call
                $\rho$ a sub-basis for $\tau(\rho)$.
            \end{theorem}
            \begin{ldefinition}{Topology Induced by Functions}
                  {Funct_Analysis_Top_Induced_by_Funcs}
                The topology induced on a set $X$ by a
                set $\mathcal{F}$ of functions $f$ from $X$ to
                a topological space $(Z_{F},\tau_{F})$ is the
                smallest topology on $X$ such that, for all
                $f\in\mathcal{F}$, $f$ is continuous.
            \end{ldefinition}
            \begin{theorem}
                If $X$ is a set, $\mathcal{F}$ is a set of functions
                from $X$ to a topological space $(Z_{F},\tau_{F})$,
                if $\tau$ is the topology induced by $\mathcal{F}$,
                then:
                \begin{equation}
                    \beta=\big\{f^{\minus{1}}(V):
                        f\in\mathcal{F},V\in\tau_{F}\big\}
                \end{equation}
                Is a sub-basis for $\tau$.
            \end{theorem}
            \begin{ldefinition}{Weak Topology}
                  {Funct_Analysis_Weak_Topology}
                The weak topology on a normed vector space
                $X$ is the topology on $X$ generated by 
                $X^{*}$.
            \end{ldefinition}
            That is, the weak topology is the smallest topology on
            $X$ such that every linear functional is continuous.
    \section{Stuff}
        Last time, we described the initial topology on a space
        $X$ generated by a family of functions $\mathscr{F}$
        $f:X\rightarrow(Z_{F},\tau_{F})$.
        \begin{ldefinition}{Initial Topology}
              {Funct_Analysis_Initial_Topology}
            The initial topology on a normed vector space
            $(X,\norm{\cdot})$ generated by a family of functions
            $\mathscr{F}=\{f:X\rightarrow(Z_{F},\tau_{F})\}$ such
            that $f$ is continuous for all $f\in\mathscr{F}$.
        \end{ldefinition}
        \begin{theorem}
            If $\mathcal{U}$ is defined by:
            \begin{equation}
                \mathcal{U}(\varphi,x_{0},\varepsilon)=
                \{x\in{X}:|\varphi(x)-\varphi(x_{0})|<\varepsilon\}
            \end{equation}
            For $\varphi\in{X}^{*}$, $x_{0}\in{X}$, $\varepsilon>0$,
            then $\mathcal{U}$ forms a sub-basis for the weak
            topology on $X$. Moreover, the sets:
            \begin{equation}
                \mathcal{U}(\{\varphi_{1},\dots,\varphi_{n}\},
                    x_{0},\varepsilon)\}
                    =\{x\in{X}:|\varphi_{k}(x)-\varphi_{k}(x_{0})|
                        <\varepsilon,k\in\mathbb{Z}_{n}\}
            \end{equation}
            Form an pen neighborhood basis at $x_{0}$ in the weak
            topology.
        \end{theorem}
        \begin{proof}
            It suffices to prove the second assertion. Since open
            balls in $\mathbb{F}$ form a basis for the topology,
            the collection:
            \begin{equation}
                \varphi^{\minus{1}}(B_{r}(c))=
                    \{x:|\varphi(x)-c|<r\}
            \end{equation}
            Form a sub-basis for the weak topology. But if
            $x_{0}\in\{x:|\varphi(x)-c|<r\}$, and if
            $\varepsilon=r-|\varphi(x_{0})-c|$, then
            $|\varphi(x)-\varphi(x_{0})|<\varepsilon$. Thus:
            \begin{equation}
                x_{0}\in\{x:|\varphi(x)-\varphi(x_{0})|<\varepsilon\}
                    \subseteq\{x:|\varphi(x)-c|<r\}
            \end{equation}
            Therefore, etc.
        \end{proof}
        \begin{theorem}
            If $X$ is a normed vector space, and $\tau$ is the
            normed topology and $\tau_{w}$ is the weak
            topology, then $\tau_{w}\subseteq\tau$.
        \end{theorem}
        \begin{theorem}
            If $X$ is a normed vector space, if $\tau$ is the
            normed topology, if $\tau_{w}$ is the weak topology,
            and if $X$ is finite dimensional, then
            $\tau=\tau_{w}$.
        \end{theorem}
        \begin{theorem}
            If $X$ is a normed vector space, $\tau$ is the normed
            topology, $\tau_{w}$ is th weak topology, and if
            $X$ is infinite dimensional, then
            $\tau_{w}\ne\tau$.
        \end{theorem}
        In any topology space $X$, we say that a sequence $x_{n}$
        converges to $x$ if the sequence is eventually contained in
        any neighborhood of $x$.
        \begin{theorem}
            If $x_{n}$ is a sequence in a normed vector space,
            then $x_{n}\rightarrow{x}$ weakly if and only if
            $\varphi(x_{n})\rightarrow\varphi(x)$ for all
            $\varphi\in{X}^{*}$.
        \end{theorem}
        \begin{proof}
            For suppose $x_{n}\rightarrow{x}_{0}$ weakly. Then,
            for all $\varepsilon>0$, let:
            \begin{equation}
                \mathcal{U}(\varphi,x_{0},\varepsilon)=
                    \{x:|\varphi(x)-\varphi(x_{0})|<\varepsilon\}
            \end{equation}
            This is a weak neighborhood of $x_{0}$. Hence,
            $x_{n}$ is eventually in $\mathcal{U}$. Thus,
            $\varphi(x_{n})$ is eventually in
            $B_{\varepsilon}(\varphi(x_{0}))$, and thus
            $\varphi(x_{n})\rightarrow\varphi(x_{0})$.
            Now suppose $\varphi(x_{n})\rightarrow\varphi(x_{0})$
            for all $\varphi\in{X}^{*}$. Let $V$ be a weak
            neighborhood of $x_{0}$. Then for some
            $\varphi_{1},\dots,\varphi_{n}$, and $\varepsilon>0$:
            \begin{equation}
                x_{0}\in\mathcal{U}(
                \{\varphi_{1},\dots,\varphi_{n}\},x_{0},
                    \varepsilon\})
                =\{x:|\varphi_{k}(x)-\varphi_{k}(x_{0})|
                    <\varepsilon,k\in\mathbb{Z}_{n}\}
                \subseteq{V}
            \end{equation}
            Thus, $x_{n}$ is eventually in $V$.
        \end{proof}
        \begin{theorem}
            Every weakly convergent sequence is bounded with
            respect to the normed topology.
        \end{theorem}
        \begin{proof}
            Suppose $x_{n}\rightarrow{x}$ weakly and let
            $i:X\rightarrow{X}^{**}$ be the canonical map. Then,
            for all $\varphi\in{X}^{*}$:
            \begin{equation}
                i(x_{n}(\varphi)=\varphi(x_{n})
            \end{equation}
            And this is bounded, and thus
            $i(x_{n})$ is uniformly bounded by the Uniform
            Boundedness Principle. But $i$ is isometric, and
            thus $\norm{x_{n}}$ is bounded.
        \end{proof}
        \begin{lexample}
            Let $X=\ell^{2}$. Recall that if $x\in\ell^{2}$, then:
            \begin{equation}
                \norm{x}_{2}=
                    \sqrt{\sum_{n=1}^{\infty}|x_{n}|^{2}}
            \end{equation}
            Let $e_{n}$ be the usual basis:
            \begin{equation}
                e_{n}(k)=\delta_{nk}=
                    \begin{cases}
                        1,&n=k\\
                        0,&n\ne{k}
                    \end{cases}
            \end{equation}
            We have seen that $(\ell^{2})^{*}$ is isometric to
            $\ell^{2}$. That is, if $y\in\ell^{2}$, then
            $y$ corresponds to $\varphi_{y}$ where:
            \begin{equation}
                \varphi_{y}(x)=\sum_{n=1}^{\infty}
                    x_{n}y_{n}
            \end{equation}
            And every $\varphi\in(\ell^{2})^{*}$ can be written
            this way. There is a neighborhood basis for
            $0\in\ell^{2}$ in the weak topology given by the
            sets of the form:
            \begin{equation}
                \mathcal{U}=\{x\in\ell^{2}:
                    \sum_{i=1}^{n}|\varphi_{y_{i}}(x)|^{2}
                    <\varepsilon\}
            \end{equation}
            Now define $T$ as follows:
            \begin{equation}
                T=\{\sqrt{n}e_{n}:n\in\mathbb{N}\}
            \end{equation}
            We want to find the \textit{weak} closure of $T$, which
            is the closure of $T$ with respect to the weak topology.
            That is:
            \begin{equation}
                W=\overline{T}^{w}=
                \bigcap\{F\subseteq\ell^{2}:
                    T\subseteq{F},F^{C}\in\tau_{w}\}
            \end{equation}
            Where $F^{C}$ denotes the complement with respect
            to the weak topology. Let's show that $0$ is an
            element of $W$. Suppose not. If $0\ne{W}$, then
            there is a $\mathcal{U}$ such that
            $\mathcal{U}\cap{T}=\emptyset$. But:
            \begin{equation}
                |\varphi_{y_{i}}(\sqrt{k}e_{k})|=
                \sqrt{k}|y_{i}(k)|
            \end{equation}
            Thus, for all $k\in\mathbb{N}$:
            \begin{equation}
                \sum_{i=1}^{n}|y_{i}(k)|^{2}\geq
                \frac{\varepsilon}{k}
            \end{equation}
            But then:
            \begin{equation}
                \sum_{i=1}^{n}\norm{y_{i}}_{2}^{2}
                =\sum_{i=1}^{n}\sum_{k=1}^{\infty}
                    |y_{i}(k)|^{2}
                =\sum_{k=1}^{\infty}\sum_{i=1}^{n}
                    |y_{i}(k)|^{2}\geq\sum_{k=1}^{\infty}
                        \frac{\varepsilon}{k}
            \end{equation}
            But this sum diverges, a contradiction as the functions
            are in $\ell^{2}$. Thus, $0\in{W}$.
        \end{lexample}
        Note that any sequence $x_{n}\subseteq{T}$ that converges
        to zero must be bounded. But $\{\sqrt{n}e_{n}:n\leq{n}\}$
        is weakly closed. But no sequence in $T$ can converge weakly
        to zero, and thus the weak topology is not metrizable.
        As it turns out, the weak topology is not even
        first countable. We like sequences and want to continue
        using this notion, but cannot use this for the weak
        topology on infinite dimensional normed spaces. We
        generalize by describing nets.
        \begin{ldefinition}{Directed Ordered Sets}
              {Funct_Analysis_Directed_Ord_Set}
            A directed ordered set is an ordered set
            $(\Lambda,\leq)$ such that, for all $x,y\in\Lambda$,
            there is a $z\in\Lambda$ such that
            $x\leq{z}$ and $y\leq{z}$.
        \end{ldefinition}
        \begin{lexample}
            There are two common examples. Any totally ordered
            set is automatically a directed ordered set,
            since we can just choose the max of any two elements.
            Moreover, if $(X,\tau)$ is a topologicaly space, then
            $(\tau,\subseteq)$ is a directed ordered set, since
            for $\mathcal{U}$ and $\mathcal{V}$, then
            $\mathcal{U}\cup\mathcal{V}$ is larger than both.
            Similarly with reverse containment, taking
            $\mathcal{U}\cap\mathcal{V}$ as the
            \textit{larger} set.
        \end{lexample}
        \begin{ldefinition}{Nets}
            A net on a set $X$ is a function from a directed
            set $\Lambda$ into $X$, $x:\Lambda\rightarrow{X}$.
        \end{ldefinition}
        As with sequences, we denote the image of $\lambda\in\Lambda$
        under a net $x$ by writing $x(\lambda)=x_{\lambda}$.
        We say that a net converges in a topological space if
        for any neighborhood $\mathcal{U}$ of the limit point
        $x_{0}$, there is a $\lambda_{0}\in\Lambda$ such that,
        for all $\lambda\geq\lambda_{0}$, $x_{\lambda}$, we have
        $x_{\lambda}\in\mathcal{U}$. An accumulation point of
        $x_{\lambda}$. There is also a generalized notion of
        accumulation point for nets.
        \begin{lexample}
            Every sequence is a net. Take $\Lambda=\mathbb{N}$.
        \end{lexample}
        \begin{theorem}
            If $(X,\tau)$ is a topological space, and
            $\mathcal{E}\subseteq{X}$, then
            $x\in\overline{\mathcal{E}}$ if and only if there is
            a net $a:\Lambda\rightarrow\mathcal{E}$ such that
            $a_{\lambda}\rightarrow{x}$.
        \end{theorem}
        \begin{proof}
            For suppose $x_{\lambda}\rightarrow{x}$ with
            $x_{\lambda}\in\mathcal{E}$. If
            $x\notin\overline{\mathcal{E}}$, there there is a
            $\mathcal{U}\subseteq\mathcal{O}(x)$ such that
            $\mathcal{U}\cap\mathcal{U}=\emptyset$. But
            $x_{\lambda}$ is eventually in $\mathcal{U}$,
            a contradiction. Thus, etc. This direction of
            the proof is identical for sequences, and indeed holds
            for sequences. Going the other way requires the use
            of the notion of nets. Suppose
            $x\in\overline{\mathcal{U}}$. Let
            $\Lambda$ be the set of neighborhoods of $x$. Then
            this is a directed set. Pick points to get a net.
        \end{proof}
    \section{More Normed Vector Space Stuff}
        Given a normed vector space $X$, the dual $X^{*}$ is
        a Banach space. It has a normed topology and a weak
        topology given by $X^{**}$. We can also give it the
        weak star topology: $\sigma(X^{*},X)$. This is the
        smallest (Or initial) topology determined by the
        functionals in $X$. That is, elements of $X^{*}$
        are continuous. In other words, for all
        $x\in{X}$, the mapping $\varphi\mapsto\varphi(x)$
        is continuous. Recall that a net $(\varphi_{\lambda})$
        in $X^{*}$ converges to $\varphi$ in the weak star
        topology if and only if it converges point-wise.
        That is, $\varphi_{\lambda}(x)\rightarrow\varphi(x)$
        for all $x\in{X}$.
        \begin{ltheorem}{Alaoglu's Theorem}
            If $X$ is a normed vector space, if
            $B{*}$ is the closed unit disc in the dual space:
            \begin{equation}
                B^{*}=\{\varphi\in{X}^{*}:
                    \norm{\varphi}\leq{1}\}
            \end{equation}
            Then $B^{*}$ is compact in the weak star
            topology.
        \end{ltheorem}
        \begin{proof}
            Define $D_{r}$ by:
            \begin{equation}
                D_{r}=\{z\in\mathbb{F}:|z|\leq{r}\}
            \end{equation}
            For all $r>0$. Define:
            \begin{equation}
                Z=\prod_{x\in{X}}D_{\norm{x}}
            \end{equation}
            Then, by Tychonoff's theorem, $Z$ is compact in
            the product topology. But this topology is the
            initial topology determined by the projection
            maps. And a net $(z_{\lambda})\rightarrow{z}$ in
            $Z$ if and only if $z_{\lambda}(x)\rightarrow{z}(x)$
            for all $x\in{X}$. Define
            $j:B^{*}\rightarrow{Z}$ by:
            \begin{equation}
                j(\varphi)(x)=\varphi(x)
            \end{equation}
            Then $j$ is an injective map. Moreover, $j$ has
            closed range in $Z$. For suppose $\varphi_{\lambda}$
            is a net and $j(\varphi_{\lambda})\rightarrow{z}$,
            for some $z\in{Z}$. But then, for all $x\in{X}$,
            $j(\varphi_{\lambda})(x)\rightarrow{z}(x)$, and thus
            $\varphi_{\lambda}(x)\rightarrow{z}(x)$. Thus $z$
            is linear, $z(x+y)=z(x)+z(y)$ and
            $z(\alpha{x})=\alpha{z}(x)$. Thus, $\varphi(x)=z(x)$
            and $|\varphi(x)|\leq\norm{x}$, and therefore
            $\varphi\in{B}^{*}$. Therefore $j(B^{*})$ is compact.
            Moreover, $j$ is homeomorphism between
            $B^{*}$ and $j(B^{*})$, and thus $B^{*}$ is compact.
        \end{proof}
    \section{Hilbert Spaces}
        \begin{ldefinition}{Sesqui-Linear Form}
            A Sesqui-Linear form on a vector space $V$ over a
            field $\mathbb{F}$ is a function
            $\langle{\cdot|\cdot}\rangle$ such that:
            \begin{equation}
                \langle{\alpha{x}+y|z}\rangle
                =\alpha\langle{x,z}\rangle+\langle{y,z}\rangle
                \quad\quad
                x,y,z\in{V}
                \quad
                \alpha\in\mathbb{F}
            \end{equation}
            And such that:
            \begin{equation}
                \langle{x,\alpha{y}+z}\rangle
                =\overline{\alpha}\langle{x|y}\rangle
                +\langle{x|z}\rangle
               \quad\quad
                x,y,z\in{V}
                \quad
                \alpha\in\mathbb{F}
            \end{equation}
        \end{ldefinition}
        \begin{ldefinition}{Self-Adjoint Sesqui-Linear Form}
            A self-adjoint sesqui-linear form on a vector space
            $V$ over a field $\mathbb{F}$ is a sesqui-linear
            form $\langle{\cdot|\cdot}\rangle$ such that:
            \begin{equation}
                \langle{x|y}\rangle=\overline{\langle{y|x}\rangle}
                \quad\quad
                x,y\in{V}
            \end{equation}
        \end{ldefinition}
        \begin{ldefinition}{Postive Sesqui-Linear Form}
            Positive if $\langle{x|x}\rangle\geq{0}$.
        \end{ldefinition}
        \begin{theorem}
            If $\mathbb{F}=\mathbb{C}$ and
            $\langle{\cdot|\cdot}\rangle$ is a sesqui-linear
            form, then:
            \begin{equation}
                \langle{x|y}\rangle
                =\frac{1}{4}\sum_{n=0}^{3}
                    i^{n}\langle{x+i^{n}y|x+i^{n}y}\rangle
            \end{equation}
        \end{theorem}
        \begin{theorem}
            If $\mathbb{F}=\mathbb{C}$, and
            $\langle{\cdot|\cdot}\rangle$ is a sesqui-linear
            form, then it is self-adjoint if and only if
            it is positive.
        \end{theorem}
        Thus, on a complex vector space, a positive sesqui-linear
        form is always self-adjoint.
        \begin{ldefinition}{Pre-Inner Product}
            A pre-inner product is a positive self-adjoint
            sesqui-linear form.
        \end{ldefinition}
        \begin{ldefinition}{Inner Product}
            An inner product is a pre-inner product such that
            $\langle{x|x}\rangle=0$ implies that $x=0$.
        \end{ldefinition}
        \begin{ldefinition}{Induced Semi-Norm}
            Given a pre-inner product, the induced norm is:
            \begin{equation}
                \norm{v}=\sqrt{\langle{x|y}\rangle}
            \end{equation}
        \end{ldefinition}
        If $\mathbb{F}=\mathbb{R}$, then:
        \begin{equation}
            \langle{x|y}\rangle=\norm{x+y}^{2}-\norm{x-y}^{2}
        \end{equation}
        \begin{ltheorem}{Cauchy-Schwarz Inequality}
            If $\langle{\cdot|\cdot}\rangle$ is a pre-inner product
            and $\norm{\cdot}$ is the induced semi-inner product,
            then:
            \begin{equation}
                |\langle{x|y}\rangle|\leq\norm{x}\norm{y}
            \end{equation}
            A similar result holds for $\mathbb{C}$.
        \end{ltheorem}
        \begin{proof}
            For:
            \begin{equation}
                0\leq\norm{\alpha{x}+y}^{2}
                =\langle{\alpha{x}+y|\alpha{x}+y}\rangle
                =|\alpha|^{2}\norm{x}^{2}
                +\alpha\langle{x|y}\rangle
                +\overline{\alpha\langle{x|y}\rangle}+\norm{y}^{2}
            \end{equation}
            We can simplify this further to obtain:
            \begin{equation}
                0\leq|\alpha|^{2}+2\Re(\alpha\langle{x|y}\rangle)
                    +\norm{y}^{2}
            \end{equation}
            Let $\tau\in\mathbb{F}$ be such that
            $\tau\langle{x|y}\rangle=|\langle{x|y}\rangle|$.
            Let $\alpha=t\tau$, with $t\in\mathbb{R}$. Then:
            \begin{equation}
                0\leq{t}^{2}\norm{x}^{2}+2t|\langle{x|y}\rangle
                    +\norm{y}^{2}
            \end{equation}
            But this is a quadratic with a positive discriminant,
            and thus by the quadratic formula:
            \begin{equation}
                4\langle{x|y}\rangle^{2}-4\norm{x}^{2}\norm{y}^{2}
                \leq{0}
            \end{equation}
            Therefore, etc. Smiley face.
        \end{proof}
        \begin{theorem}
            If $\langle{\cdot|\cdot}\rangle$ is a pre-inner
            product, then the induced semi-norm is a semi-norm.
        \end{theorem}
        \begin{proof}
            Apply Cauchy-Schwarz to obtain the triangle inequality.
        \end{proof}
        \begin{ldefinition}{Induced Norm}
            An induced norm is a semi-induced norm induced
            by an inner product.
        \end{ldefinition}
        \begin{theorem}
            Induced norms are norms.
        \end{theorem}
        \begin{ldefinition}{Inner Product Space}
            An inner product space Is s vector space
            $V$ over a field $\mathbb{F}$ with an
            inner product $\langle{\cdot|\cdot}\rangle$,
            denoted $(V,\langle{\cdot|\cdot}\rangle)$.
        \end{ldefinition}
        \begin{ldefinition}{Hilbert Space}
            A Hilbert space is an inner product space
            such that the induced norm is complete.
        \end{ldefinition}
        \begin{lexample}
            Let $\mathcal{H}=\mathbb{F}^{n}$ and define:
            \begin{equation}
                \langle{x|y}\rangle
                =\sum_{k=1}^{n}x_{k}\overline{y}_{k}
            \end{equation}
            Then $\langle{\cdot|\cdot}\rangle$ is an inner product.
            The induced norm is $\ell^{2}$, which is complete.
            Thus this is a Hilbert space. Extending this to
            sequences:
            \begin{equation}
                \langle{x|y}\rangle
                =\sum_{k=0}^{\infty}x_{k}\overline{y}_{k}
            \end{equation}
            Similarly, we can define:
            \begin{equation}
                \langle{f|g}\rangle
                =\int_{X}f(x)\overline{g}(x)\diff{\mu}
            \end{equation}
            For $f,g\in{L}^{2}(X,\mathcal{M},\mu)$. Remembering
            to identity functions that differ on only a set
            of measure zero, this is a Hilbert space.
        \end{lexample}
        \begin{theorem}
            If $H$ is an inner product space, and if
            $x,y\in{H}$, then:
            \begin{equation}
                \norm{x+y}^{2}+\norm{x-y}^{2}=
                2\norm{x}^{2}+2\norm{y}^{2}
            \end{equation}
        \end{theorem}
        This theorem characterizes inner product spaces.
        \begin{ltheorem}{Jordan von-Neumann Theorem}
            If $X$ is a complex Banach space such that:
            \begin{equation}
                \norm{x+y}^{2}+\norm{x-y}^{2}
                =2\norm{x}^{2}+2\norm{y}^{2}
            \end{equation}
            Then there is an inner product that induces
            the norm.
        \end{ltheorem}
        \begin{theorem}
            If $H$ is an inner product space and
            $x_{n}\rightarrow{x}$ and
            $y_{n}\rightarrow{y}$, then:
            \begin{equation}
                \langle{x_{n}|y_{n}}\rangle
                \rightarrow\langle{x|y}\rangle
            \end{equation}
        \end{theorem}
        \begin{proof}
            Recall that the norm is continuous, so
            $\norm{x_{n}}\rightarrow\norm{x}$ and
            $\norm{y}_{n}\rightarrow\norm{y}$. By Cauchy-Schwarz:
            \begin{equation}
                |\langle{x_{n}|y_{n}}\rangle-\langle{x|y}\rangle|
                \leq\langle{x_{n}-x|y_{n}}\rangle|+
                    |\langle{x|y-y_{n}}\rangle|
                \leq\norm{x-x_{n}}\norm{y_{n}}+
                    \norm{x}\norm{y-y_{n}}
                \rightarrow{0}
            \end{equation}
            Therefore, etc.
        \end{proof}
        \begin{ldefinition}{Orthogonal Elements}
              {Funct_Analysis_Orthogonal_Elements}
            Orthogonal elements of an inner product space
            $(H,\langle{\cdot|\cdot}\rangle)$ are points
            $x,y\in{H}$ such that
            $\langle{x|y}\rangle=0$.
        \end{ldefinition}
        We can define a similar notion for subsets of $H$.
        \begin{ltheorem}{Pythagoras' Theorem}
            If $x_{1},\dots,x_{n}$ are pairwise orthogonal
            elements of an inner product space $H$, then:
            \begin{equation}
                \sum_{k=1}^{n}\norm{x_{k}}^{2}=
                \norm{\sum_{k=1}^{n}x_{k}}^{2}
            \end{equation}
        \end{ltheorem}
        \begin{proof}
            For:
            \begin{align}
                \norm{\sum_{k=1}^{n}x_{k}}^{2}
                &=\langle{\sum_{k=1}^{n}x_{k}|\sum_{k=1}^{n}x_{k}}
                    \rangle\\
                &=\sum_{k=1}^{n}\sum_{j=1}^{n}
                    \langle{x_{k}|x_{j}}\rangle
                &=\sum_{k=1}^{n}\langle{x_{k}|x_{k}}\rangle\\
                &=\sum_{k=1}^{n}\norm{x_{k}}^{2}
            \end{align}
            Therefore, etc.
        \end{proof}
        \begin{theorem}
            I $C$ is a closed non-empty convex subset of a Hilbert
            space $H$, then for all $y$ there is a unique
            $x\in{C}$ such that $\dist(y,C)=\norm{x-y}$.
        \end{theorem}
        Next time, on Functional analysis:
        Direct sum, proof of the previous theorem, the fact
        that there may not be a further element. Stuff.
    \section{Even More Stuff}
        \begin{theorem}
            If $C$ is a closed non-empty convex subset of a
            Hilbert space $\mathcal{H}$, and if
            $h\in\mathcal{H}$, then there is a unique
            $c\in{C}$ such that:
            \begin{equation}
                \dist(h,C)=\norm{y-h}
            \end{equation}
            That is, there is a unique $x$ in $C$ that is
            closest to $h$.
        \end{theorem}
        \begin{proof}
            Translate $C$ by $C-h$, so we can assume $h=0$.
            Define the following:
            \begin{equation}
                \alpha=\inf\{\norm{x}:x\in{C}\}
            \end{equation}
            That is, $\alpha=\dist(0,C)$. Let $x_{n}$
            be a sequence in $C$ such that
            $\norm{x_{n}}\rightarrow\alpha$. Then, by the
            parallelogram law:
            \begin{equation}
                2(\norm{x_{n}}^{2}+\norm{x_{m}}^{2})=
                \norm{x_{n}+x_{m}}^{2}+
                \norm{x_{n}-x_{m}}^{2}
            \end{equation}
            But then, for all $n,m\in\mathbb{N}$:
            \begin{equation}
                \frac{x_{n}+x_{m}}{2}\in{C}
            \end{equation}
            Since $C$ is convex. Therefore:
            \begin{equation}
                2(\norm{x_{n}}^{2}+\norm{x_{m}}^{2})
                \geq4\alpha^{2}+\norm{x_{n}-x_{m}}^{2}
            \end{equation}
            And thus $x_{n}$ is Cauchy. Then
            $x_{n}\rightarrow{x}$ and $\norm{x}=\alpha$.
            Moreover, from convexity, $x$ is unique.
        \end{proof}
        \begin{ldefinition}{Orthogonal Complement}
            The orthogonal complement of a subset
            $S\subseteq{H}$ of an inner product space
            $H$ is the set:
            \begin{equation}
                S^{\perp}=\{y\in{H}:\forall_{x\in{A}},
                    \langle{x|y}\rangle=0\}
            \end{equation}
        \end{ldefinition}
        \begin{theorem}
            If $H$ is an inner product space and
            $S\subseteq{H}$, then $S^{\perp}$ is a closed
            subspace.
        \end{theorem}
        From linear algebra, if $W_{1}$ and $W_{2}$ are
        subsapces of a vector space $V$ such that
        $W_{1}+W_{2}=V$ and $W_{1}\cap{W}_{2}=\{0\}$, then
        we say $V=W_{1}\oplus{W}_{2}$. The map
        $P_{1}:V\rightarrow{V}$ defined by taking $v\in{V}$ to
        the unique $w_{1}\in{W}_{1}$ such that
        $v=w_{1}+w_{2}$ is called the projection of $H$
        onto $W_{1}$ along $W_{2}$.
        \begin{theorem}
            If $W$ is a closed subspace of a Hilbert space
            $\mathcal{H}$, then
            $\mathcal{H}=W\oplus{W}^{\perp}$. If
            $P_{W}:\mathcal{H}\rightarrow\mathcal{H}$ is the
            projection mapping of $H$ into $W$, then
            $P_{W}(h)$ is the closest element in $W$ to $h$.
        \end{theorem}
        \begin{proof}
            For let $h\in\mathcal{H}$ and let $x$ be the
            closed element in $W$ to $h$. Let
            $x^{\perp}=h-x$. Let $w\in{W}$ and
            $\varepsilon>0$. But then:
            \begin{subequations}
                \begin{align}
                    \norm{x^{\perp}}^{2}
                    &=\norm{h-x}^{2}\leq
                    \norm{h-(x+\varepsilon{w}}^{2}\\
                    &=\norm{h-x-\varepsilon{w}}^{2}\\
                    &=\norm{x^{\perp}-\varepsilon{w}}^{2}\\
                    &=\norm{x^{\perp}}^{2}-
                    2\varepsilon\Re(\langle{x^{\perp}|w}\rangle)
                    +\varepsilon^{2}\norm{w}^{2}
                \end{align}
            \end{subequations}
            Therefore:
            \begin{equation}
                2\varepsilon\Re(\langle{x^{\perp}|w}\rangle)
                =\varepsilon^{2}\norm{w}^{2}
            \end{equation}
            Thus, $x^{\perp}\in{W}^{\perp}$. But
            $\mathcal{H}=W+W^{\perp}$ and
            $W\cap{W}^{\perp}=\{0\}$.
            Therefore, $W\oplus{W}^{+}=\mathcal{H}$
        \end{proof}
        \begin{theorem}
            If $\mathcal{H}$ is a Hilbert space and if
            $S\subseteq\mathcal{H}$, then:
            \begin{equation}
                (S^{\perp})^{\perp}
                =\mathrm{Cl}_{\norm{\cdot}}\big(\Span(S)\big)
            \end{equation}
        \end{theorem}
        Note that given a point $h$ in an inner product space,
        $\varphi_{h}(x)=\langle{x|v}\rangle$ defins a linear
        function. By the Cauchy-Schwarz theorem:
        \begin{equation}
            |\varphi_{h}(h)|\leq\norm{h}\norm{v}
        \end{equation}
        And thus $\varphi_{v}\in{H}^{*}$ and
        $\norm{\varphi_{v}}\leq\norm{v}$. But:
        \begin{equation}
            |\varphi_{v}(v)|=\norm{v}^{2}
        \end{equation}
        And thus $\norm{\varphi_{v}}=\norm{v}$.
        \begin{ltheorem}{Riesz's Representation Theorem}
              {thm:Funct_Analysis_Riesz_Rep_Theorem}
            If $\mathcal{H}$ is a Hilbert space, then the
            map $\Phi:\mathcal{H}\rightarrow\mathcal{H}^{*}$
            given by $\Phi(v)=\varphi_{v}$ is a conjugate
            linear isometric map of $\mathcal{H}$ onto
            $\mathcal{H}^{*}$.
        \end{ltheorem}
        \begin{proof}
            By the previous remark, it suffices to show that
            $\Phi$ is surjective. Let $\varphi\in\mathcal{H}^{*}$.
            Let $W=\ker(\varphi)$. But $\varphi$ is continuous,
            and thus $W$ is a closed subspace of $\mathcal{H}$.
            If $\varphi$ is the zero function, let $v=0$. If not,
            then there exists a non-zero element
            $v\in(\ker(\varphi))^{\perp}$. Let
            $y=v/\norm{v}$.
        \end{proof}
        \begin{theorem}
            If $\mathcal{H}$ is a Hilbert space, then the
            bijection $\Phi:\mathcal{H}\rightarrow\mathcal{H}^{*}$
            mapping $v\mapsto\varphi_{v}$ is a homeomorphism
            of $\mathcal{H}$ with the weak topology onto
            $\mathcal{H}^{*}$ with the weak star topology.
        \end{theorem}
        \begin{proof}
            Recall every $\varphi\in\mathcal{H}^{*}$ is of the
            form $\varphi_{v}$ for some $v\in\mathcal{H}$.
            Thus $h_{\lambda}\mapsto{h}$ in the weak topology
            if and only if for all $v\in\mathcal{H}$:
            \begin{subequations}
                \begin{align}
                    \langle{h_{\lambda}|h}\rangle
                    &\rightarrow\langle{h|v}\rangle\\
                    \Longrightarrow
                    \langle{v|h_{\lambda}}\rangle
                    &\rightarrow\langle{v|h}\rangle\\
                    \Longrightarrow\varphi_{h_{\lambda}}(v)
                    &\rightarrow\varphi_{h}(v)
                \end{align}
            \end{subequations}
            But then $\varphi_{h_{\lambda}}\rightarrow\varphi_{h}$
            in the weak star topology.
        \end{proof}
        \begin{theorem}
            If $\mathcal{H}$ is a Hilbert space and if
            $B$ is the closed unit ball, then $B$ is
            weakly compact.
        \end{theorem}
        \begin{ldefinition}{Orthonormal Basis}
            An orthonormal basis of an inner product
            space $H$ is a subset $E\subseteq{H}$ such that, for
            all $e\in{E}$, $\norm{e}=1$, and for all distinct
            $e_{\alpha},e_{\beta}\in{E}$,
            $\langle{e_{\alpha}|e_{\beta}}\rangle=0$.
        \end{ldefinition}
        \begin{ltheorem}{Bessel's Inequality}
              {Funct_Analysis_Bessels_Inequality}
            If $H$ is an inner product, and if
            $e_{n}$ is an orthonormal sequence in $H$< then
            for all $x\in{H}$:
            \begin{equation}
                \sum_{n=1}^{\infty}|\langle{x|e_{n}}\rangle|^{2}
                    \leq\norm{x}^{2}
            \end{equation}
        \end{ltheorem}
        \begin{proof}
            For define the following:
            \begin{equation}
                x_{n}=x-\sum_{k=1}^{n}
                    \langle{x|e_{k}}\rangle{e}_{k}
            \end{equation}
            Note that $x_{n}\perp{e}_{k}$ for $k=1,\dots,n$.
            But, by the Pythagorean theorem:
            \begin{equation}
                \norm{x}^{2}=\norm{x_{n}}^{2}+
                    \sum_{k=1}^{n}|\langle{x|e_{k}}\rangle|^{2}
            \end{equation}
            Thus:
            \begin{equation}
                \norm{x}^{2}\geq\underset{n\in\mathbb{N}}{\sup}
                    \sum_{k=1}^{n}|\langle{x|e_{k}}\rangle|^{2}
                    =\sum_{k=1}^{\infty}|\langle{x|e_{k}}\rangle|^{2}
            \end{equation}
            Therefore, etc.
        \end{proof}
        \begin{ldefinition}{Orthogonal Projection}
              {def:Funct_Analysis_Orthogonal_Projection}
            The orthogonal projection of a closed subspace $W$
            of a Hilbert space $\mathcal{H}$ is the projection
            $P_{W}:\mathcal{H}\rightarrow\mathcal{H}$ of
            $W$ along $W^{\perp}$.
        \end{ldefinition}
        \begin{theorem}
            If $E$ is an orthonormal set in a Hilbert space
            $\mathcal{H}$, and if
            $\mathcal{E}=\mathrm{Cl}(\Span(E))$, then for all
            $h\in{H}$, the sum over
            $\langle{h|e_{n}}\rangle{e}_{n}$ converges and:
            \begin{equation}
                P_{\mathcal{E}}(h)=\sum_{n=1}^{\infty}
                    \langle{h|e_{n}}\rangle{e}_{n}
            \end{equation}
        \end{theorem}
    \section{Even MORE Stuff!}
        \begin{theorem}
            If $S$ and $T$ are self adjoint bounded operators
            on $\mathcal{H}$, and if $S\leq{T}$, then
            $ASA^{*}\leq{A}TA^{*}$ for all
            $A\in\mathscr{L}(\mathcal{H})$.
        \end{theorem}
        \begin{theorem}
            If $S,T$ are self-adjoint operators on $\mathcal{H}$,
            if $0\leq{S}\leq{T}$, then $\norm{S}\leq\norm{T}$.
        \end{theorem}
        \begin{proof}
            For suppose $0\leq{S}\leq{T}$, and let
            $[x,y]=\langle{Sx|y}\rangle$. Then $[\cdot,\cdot]$
            is a pre-inner product on $\mathcal{H}$, and thus
            if $\norm{x}=\norm{y}=1$, then by Cauchy-Schwarz:
            \begin{equation}
                |\langle{Sx|y}\rangle|^{2}=|[x,y]|^{2}
                \leq[x,x][y,y]=
                \langle{Sx|x}\rangle\langle{Sy|y}\rangle
            \end{equation}
            But $S\leq{T}$, and therefore:
            \begin{equation}
                \langle{Sx|x}\rangle\langle{Sy|y}\rangle
                \leq\langle{Tx|x}\rangle\langle{Ty|y}\rangle
                \leq\norm{T}^{2}
            \end{equation}
            Therefore $\norm{S}^{2}\leq\norm{T}^{2}$. Taking
            square roots completes the proof.
        \end{proof}
        \begin{theorem}
            If $S$ is a self-adjoint bounded operators on
            $\mathcal{H}$, and if $S\geq{0}$, then
            $S\leq{I}$ if and only if $\norm{S}\leq{1}$.
        \end{theorem}
        \begin{proof}
            For if $0\leq{S}\leq{U}$, then
            $\norm{S}\leq\norm{I}=1$. But if $0\leq{S}$ and
            $\norm{S}\leq{1}$, then:
            \begin{equation}
                \langle{Sx|x}\rangle\leq\norm{x}^{2}=
                \langle{x|x}\rangle
            \end{equation}
            Thus, $S\leq{I}$.
        \end{proof}
        \begin{theorem}
            If $T$ is a self-adjoint bounded operator on
            $\mathcal{H}$, if $\minus{I}\leq{T}\leq{I}$, then
            $\norm{T}\leq{1}$.
        \end{theorem}
        \begin{proof}
            Suppose $T=T^{*}$ and $\minus{I}\leq{T}\leq{I}$.
            Then:
            \begin{equation}
                \langle{T(x+y)|x+y}\rangle
                \leq\norm{x+y}^{2}
            \end{equation}
            And similarly:
            \begin{equation}
                \minus\langle{T(x-y)|x-y}\rangle
                \leq\norm{x-y}^{2}
            \end{equation}
            Summing, we obtain:
            \begin{equation}
                4\Re\big(\langle{Tx|y}\rangle\big)
                \leq\norm{x+y}^{2}+\norm{x-y}^{2}
            \end{equation}
            By the parallelogram law:
            \begin{equation}
                4\Re\big(\langle{Tx|y}\rangle\big)
                \leq{2}\norm{x}^{2}+2\norm{y}^{2}
            \end{equation}
            Therefore:
            \begin{equation}
                4|\langle{Tx|y}\rangle|
                \leq{2}\norm{x}^{2}+2\norm{y}^{2}
            \end{equation}
            But, for $\norm{x}=\norm{y}=1$,
            $\sup|\langle{Tx|y}\rangle|\leq{1}$, and therefore
            $\norm{T}\leq{1}$. On the other hand, if
            $T=T^{*}$, then:
            \begin{equation}
                |\langle{Tx|x}\rangle|\leq\norm{x}^{2}
            \end{equation}
            But $T=T^{*}$, and thus
            $\langle{Tx|x}\rangle$ is a real number. Thus:
            \begin{equation}
                \minus\langle{x|x}\rangle
                \leq\langle{Tx|x}\rangle
                \leq\langle{x|x}\rangle
            \end{equation}
            Thus, $\minus{I}\leq{T}\leq{I}$.
        \end{proof}
        Let $A\in{M}_{n}(\mathbf{F})^{\dagger}$. That is,
        $A=A^{*}$ and $A=\mathcal{U}\mathcal{D}\mathcal{U}^{*}$
        for some unitary $\mathcal{U}$ and a diagonal
        $\mathcal{D}$:
        \begin{equation}
            \mathcal{D}=
                \begin{pmatrix}
                    \lambda_{1}&0&\dots&0\\
                    0&\lambda_{2}&\dots&0\\
                    \vdots&\vdots&\ddots&0\\
                    0&0&\dots&\lambda_{n}
                \end{pmatrix}
        \end{equation}
        Where $\lambda_{k}\geq{0}$.
        \begin{theorem}
            There is a sequence o polynomials with positive
            coefficients such that:
            \begin{equation}
                \sum_{n=1}^{\infty}p_{n}(t)=1-\sqrt{1-t}
            \end{equation}
            Uniformly on $[0,1]$.
        \end{theorem}
        \begin{proof}
            Let $q_{0}=0$ and define:
            \begin{equation}
                q_{n+1}(t)=\frac{1}{2}\big(t+q_{n}(t)\big)^{2}
            \end{equation}
            For all $n\in\mathbb{N}$. By induction we see that
            $q_{n}$ is a sequence of polynomials with positive
            coefficients and such that:
            \begin{equation}
                0\leq{q}_{n}(t)\leq{1}
            \end{equation}
            For all $t\in[0,1]$, and $n\in\mathbb{N}$. Define:
            \begin{equation}
                p_{n}(t)=q_{n}(t)-q_{n-1}(t)
            \end{equation}
            For all $n\in\mathbb{N}$. But then:
            \begin{equation}
                2p_{n+1}(t)
                =q_{n}(t)^{2}-q_{n-1}(t)^{2}
                =\big(q_{n}(t)-q_{n-1}(t)\big)
                    \big(q_{n}(t)+q_{n-1}(t)\big)
                =p_{n}(t)\big(q_{n}(t)+q_{n-1}(t)\big)
            \end{equation}
            Thus, each $p_{n}$ has positive coefficients, and
            therefore:
            \begin{equation}
                q_{n}(t)\leq{q}_{n-1}(t)
            \end{equation}
            But $q_{n}$ is bounded by 1 and monotonic, and thus
            by completeness, there is a limit function. Let:
            \begin{equation}
                Q(t)=\underset{n\rightarrow\infty}{\lim}q_{n}(t)
            \end{equation}
            But then:
            \begin{equation}
                q(t)=\frac{1}{2}\big(t+q(t)^{2}\big)
            \end{equation}
            And therefore:
            \begin{equation}
                q(t)=1-\sqrt{1-t}
            \end{equation}
            Moreover, by Dini's theorem, the convergence is
            uniform. And the $p_{n}$ form a telescoping series,
            and therefore:
            \begin{equation}
                \sum_{n=1}^{\infty}p_{n}(t)=1-\sqrt{1-t}
            \end{equation}
            Therefore, etc.
        \end{proof}
        \begin{lexample}
            Define the following $2\times{2}$ matrices:
            \begin{equation}
                A=\begin{pmatrix}
                    2&1\\
                    1&1
                \end{pmatrix}
                \quad\quad
                B=\begin{pmatrix}
                    4&\minus{1}\\
                    \minus{1}&1
                \end{pmatrix}
            \end{equation}
            Then $A$ and $B$ are positive, but:
            \begin{equation}
                AB=\begin{pmatrix}
                    7&\minus{1}\\
                    2&1
                \end{pmatrix}
            \end{equation}
            And this is not symmetric, and thus not positive.
            The product of positive operators need not be
            positive.
        \end{lexample}
        \begin{theorem}
            If $S\geq{0}$, and for all $n\in\mathbb{N}$,
            $S^{n}\geq{0}$. In particular, if $p$ is a
            polynomial with positive coefficients, then
            $p(S)\geq{0}$.
        \end{theorem}
        \begin{proof}
            We have $(S^{n})^{*}=S^{n}$. Thus, yeah.
        \end{proof}
        \begin{theorem}
            If $T$ is a bounded operator, $T\geq{0}$, then
            there is a unique $A\in\mathscr{L}(\mathcal{H})$ such
            that $A\geq{0}$ and $A^{2}=T$. If $B$ commutes with
            $T$< then $B$ commutes with $A$.
        \end{theorem}
        \begin{proof}
            Let $\alpha>0$. If $A^{2}=\alpha{T}$, then
            $(\alpha^{\minus{1}/2}A)^{2}=T$. Thus we can replace
            $T$ by $\alpha{T}$ such that $\norm{T}\leq{1}$.
            Thus, $0\leq{T}\leq{I}$. Then, if $S=I-T$, then
            $0\leq{S}\leq{I}$. Let $p_{n}$ and $q_{n}$ be
            defined as before. Let:
            \begin{equation}
                S_{n}=p_{n}(S)
            \end{equation}
            Then $S_{n}\geq{0}$. Thus:
            \begin{equation}
                0\leq\sum_{k=m}^{n}p_{k}(t)=
                \sum\alpha_{r}t^{r}\leq\varepsilon
            \end{equation}
            For all $t\in[0,1]$. Note that $\alpha_{k}\geq{0}$.
            Hence, we have:
            \begin{equation}
                \norm{\sum_{k=m}^{n}S_{k}}
                \leq\sum\alpha_{r}\norm{S}^{r}\leq
                \sum\alpha{r}<\varepsilon
            \end{equation}
            Thus, $S_{k}$ forms a Cauchy sequence and therefore
            converges. Moreover:
            \begin{equation}
                R=\sum_{k=1}^{\infty}S_{k}\geq{0}
            \end{equation}
            Moreover, $0\leq{R}\leq{I}$. But:
            \begin{align}
                (I-R)^{2}
                &=\Big(I-\sum_{k=1}^{\infty}S_{k}\Big)^{2}\\
                &=\underset{n\rightarrow\infty}{\lim}
                    \big(I-q_{n}(S)\big)^{2}\\
                &=\underset{n\rightarrow\infty}{\lim}
                    \big(I-2q_{n}(S)+q_{n}(S)^{2}\big)\\
                &=I-S
            \end{align}
            And this is equal to $T$. Thus, $(I-R)^{2}=T$.
            Now, if $BT=TB$, then:
            \begin{equation}
                AB=(I-R)B
                =\underset{n\rightarrow\infty}{\lim}
                    \big(I-q_{n}(I-T)\big)B
                =B\underset{n\rightarrow\infty}{\lim}
                    \big(I-q_{n}(I-T)\big)
                =BA
            \end{equation}
            Lastly, $A$ is unique. For if $B\geq{0}$ and
            $B^{2}=T$, then $B$ commutes with $T$ and hence
            $B$ commutes with $A$. Therefore:
            \begin{equation}
                (A-B)^{2}x=(A-B)(A+B)x=(T-T)x=0
            \end{equation}
            Hence, if $y$ is in the range of $A+B$, then
            $(A-B)y=0$.Let $\mathcal{E}$ be the range of $A+B$.
            If we can show that $(A-B)y=0$ for all
            $y\in\mathcal{E}^{\perp}$, then we are done. But since
            $A$ and $B$ are self-adjoint, $\mathcal{E}^{\perp}$
            is the kernel of $A+B$. Thus:
            \begin{equation}
                \langle{Ay|y}\rangle\leq
                \langle{(A+B)z|z}\rangle=0
            \end{equation}
            Thus, for all $y\in\mathcal{E}^{\perp}$,
            $\langle{Ay|y}\rangle=0$. But $A\geq{0}$, hence there
            is a $C$ such that $A=C^{2}$ and $C\geq{0}$.
            Then:
            \begin{equation}
                \langle{Az|Z}\rangle=\norm{Cz}^{2}=0
            \end{equation}
            Thus $Cz=0$ and hence $Az=C^{2}z=0$. Similarly,
            $Bz=0$. Thus $(A-B)z=0$.
        \end{proof}
        \begin{theorem}
            If $T\geq{0}$ and $S\geq{0}$, and if
            $TS=ST$, then $TS\geq{0}$.
        \end{theorem}
        \begin{proof}
            For:
            \begin{equation}
                TS=T(\sqrt{S})^{2}=\sqrt{S}T\sqrt{S}\geq{0}
            \end{equation}
            Therefore, etc.
        \end{proof}
        Here, $A$ is called the positive square root of the
        operator $T$.
    \clearpage
    \printglossary[style=longpara]
    \chapter{Homeworks}
    \section{Homework I}
        \begin{problem}
            Show that, for $1\leq{p}\leq{q}\leq\infty$, that
            $\norm{\cdot}_{p}$ and $\norm{\cdot}_{q}$ are strongly
            equivalent.
        \end{problem}
        \begin{solution}[1]
            First, if $X$ is a set, $d_{1},d_{2},d_{3}$ are
            metrics on $X$, if $d_{1}$ is strongly equivalent to
            $d_{2}$, and $d_{2}$ is strongly equivalent to $d_{3}$,
            then $d_{1}$ is strongly equivalent to $d_{3}$.
            For if $d_{1}$ is strongly equivalent to $d_{2}$, then
            there are $\alpha,\beta>0$ such that, for all
            $x,y\in{X}$:
            \begin{equation}
                \alpha{d}_{1}(x,y)\leq{d}_{2}(x,y)
                \leq\beta{d}_{1}(x,y)
            \end{equation}
            But we have seen that strongly equivalent is a symmetric
            relation, and therefore if $d_{2}$ and $d_{3}$ are
            strongly equivalent then there are $a,b>0$ such that:
            \begin{equation}
                ad_{3}(x,y)\leq{d}_{2}(x,y)\leq{b}d_{3}(x,y)
            \end{equation}
            Therefore:
            \begin{equation}
                \frac{\alpha}{b}d_{1}(x,y)\leq{d}_{3}(x,y)
                \leq\frac{\beta}{a}d_{1}(x,y)
            \end{equation}
            Thus, strongly equivalent is a transitive relation.
            Let $n\in\mathbb{N}$, $p\in[1,\infty)$, and let
            $\mathbf{x}\in\mathbb{R}^{n}$. Then, since
            $\norm{\mathbf{x}}_{\infty}$ is the supremum norm,
            it is greater than or equal to all of the compononents
            of $\mathbf{x}$. Thus:
            \begin{equation}
                n\norm{\mathbf{x}}_{\infty}^{p}=
                \sum_{k=1}^{n}\norm{\mathbf{x}}_{\infty}^{p}
                \geq\sum_{k=1}^{n}|x_{k}|^{p}
                =\norm{\mathbf{x}}_{p}^{p}
            \end{equation}
            Taking $p^{th}$ roots, we have:
            \begin{equation}
                n^{\frac{1}{p}}\norm{\mathbf{x}}_{\infty}
                \geq\norm{\mathbf{x}}_{p}
            \end{equation}
            Going the other way:
            \begin{equation}
                \norm{\mathbf{x}}_{\infty}^{p}\leq
                \sum_{k=1}^{n}|x_{k}|^{p}
            \end{equation}
            Taking $p^{th}$ roots again, we obtain the following:
            \begin{equation}
                \norm{\mathbf{x}}_{\infty}\leq\norm{\mathbf{x}}_{p}
                \leq{n}^{\frac{1}{p}}\norm{\mathbf{x}}_{\infty}
            \end{equation}
            Thus, for all $p\in[1,\infty)$, $\norm{\cdot}_{p}$
            is strongly equivalent to $\norm{\cdot}_{\infty}$.
            But strongly equivalent is a transitive relation,
            thus for all $p,q\in[1,\infty]$, $\norm{\cdot}_{p}$
            is strongly equivalent to $\norm{\cdot}_{q}$.
        \end{solution}
        \begin{solution}[2]
            Let $p\in[1,\infty]$, and let Let
            $f:S^{n}\rightarrow\mathbb{R}$ be defined by:
            \begin{equation}
                f(\mathbf{x})=\norm{\mathbf{x}}_{p}
            \end{equation}
            But $S^{n}$ is a closed and bounded subset of
            $\mathbb{R}^{n}$, and is therefore, by the Heine-Borel
            theorem, it is compact. But continuous functions
            attain their maximum and minimum on compact sets, by
            the extreme value theorem. Thus, there are
            $\mathbf{x}_{\min}$ and $\mathbf{x}_{\max}$ such
            that, for all $\mathbf{x}\in{S}^{n}$:
            \begin{equation}
                \norm{\mathbf{x}_{\min}}_{p}\leq
                \norm{\mathbf{x}}_{p}\leq
                \norm{\mathbf{x}_{\max}}_{p}
            \end{equation}
            But also $\norm{\mathbf{x}_{\min}}_{p}>0$, for
            $\mathbf{x}_{\min}\in{S}^{n}$, and therefore
            $\mathbf{x}_{\min}\ne\mathbf{0}$. Moreover:
            \begin{equation}
                \norm{e_{i}}_{p}=1
            \end{equation}
            For all $i\in\mathbb{Z}_{n}$. But also, for all
            $\mathbf{x}\in{S}^{n}$, we have:
            \begin{equation}
                \norm{\mathbf{x}}_{2}=1
            \end{equation}
            Therefore, for all $\mathbf{x}\in{S}^{n}$:
            \begin{equation}
                \frac{\norm{\mathbf{x}_{\min}}_{p}}
                     {\norm{\mathbf{x}_{\max}}_{p}}
                \norm{\mathbf{x}}_{p}\leq
                \norm{\mathbf{x}}_{2}\leq
                \frac{\norm{\mathbf{x}_{\max}}_{p}}
                     {\norm{\mathbf{x}_{\min}}_{p}}
                \norm{\mathbf{x}}_{p}
            \end{equation}
            For the general
            $\mathbf{x}\in\mathbb{R}^{n}\setminus\{\mathbf{0}\}$,
            we can scale back to the unit $n$ sphere. Thus,
            $\norm{\cdot}_{p}$ and $\norm{\cdot}_{2}$ are
            strongly equivalent for all $p\in[1,\infty]$. Since
            strongly equivalent is a transitive relation,
            $\norm{\cdot}_{p}$ and $\norm{\cdot}_{q}$ are
            strongly equivalent for all $p,q\in[1,\infty]$.
        \end{solution}
        \begin{problem}
            Give an example of a metric on $\mathbb{R}^{n}$
            that is not strongly equivalent to $\norm{\cdot}_{p}$
            for any $p\in[1,\infty]$.
        \end{problem}
        \begin{solution}
            The discrete metric is not strongly equivalent to
            any of the metrics induced by $\norm{\cdot}_{p}$.
            For suppose not. Then:
            \begin{equation}
                \alpha\norm{\mathbf{x}}_{p}\leq
                d(\mathbf{x},\mathbf{0})\leq{1}
            \end{equation}
            Where $d$ is the discrete metric.
            But $\norm{\mathbf{x}}_{p}$ is unbounded, and thus if
            $\alpha\norm{\mathbf{x}}_{p}\leq{1}$, then $\alpha=0$.
            Thus there is no $\alpha>0$ that satisfies
            the inequality.
        \end{solution}
        \begin{problem}
            Show that, if $a,b\geq{0}$ and $0<\lambda<1$, then:
            \begin{equation}
                a^{\lambda}b^{1-\lambda}
                \leq\lambda{a}+(1-\lambda)b
            \end{equation}
        \end{problem}
        \begin{solution}
            If $a=0$ or $b=0$, then we are done. Suppose not.
            Define $t=ab^{\minus{1}}$.
            Then, if $\lambda\in(0,1)$, and $t\geq{1}$, we have:
            \begin{subequations}
                \begin{align}
                    \lambda(t^{\lambda-1}-1)&\geq0\\
                    \Rightarrow\int_{1}^{t}\lambda
                        \Big(\tau^{\lambda-1}-1\Big)\diff{\tau}
                        &\geq{0}\\
                    \Rightarrow
                    (t^{\lambda}-\lambda{t})-(1-\lambda)&\geq{0}
                \end{align}
            \end{subequations}
            For $t\in(0,1)$, we have:
            \begin{subequations}
                \begin{align}
                    \lambda(t^{\lambda-1}-1)&\leq{0}\\
                    \Rightarrow\int_{t}^{1}\lambda
                        \Big(\tau^{\lambda-1}-1\Big)\diff{\tau}
                        &\leq{0}\\
                    \Rightarrow
                    (1-\lambda)-(t^{\lambda}-\lambda{t})&\leq{0}
                \end{align}
            \end{subequations}
            Combining these two, we have for $t\in(0,\infty)$:
            \begin{equation}
                t^{\lambda}-\lambda{t}\leq{1}-\lambda
            \end{equation}
            But $t=ab^{\minus{1}}$, and thus multiplying through
            by $b$:
            \begin{equation}
                a^{\lambda}b^{\lambda-1}\leq
                \lambda{a}+(1-\lambda)b
            \end{equation}
        \end{solution}
        \begin{problem}
            Prove H\"{o}lder's Inequality: If $p\in(1,\infty)$,
            $p^{\minus{1}}+q^{\minus{1}}=1$, then:
            \begin{equation}
                \norm{fg}_{1}\leq\norm{f}_{p}\norm{g}_{q}
            \end{equation}
        \end{problem}
        \begin{solution}
            For let:
            \par
            \begin{subequations}
                \begin{minipage}[b]{0.49\textwidth}
                    \centering
                    \begin{equation}
                        \tilde{f}=\frac{f}{\norm{f}_{q}}
                    \end{equation}
                \end{minipage}
                \hfill
                \begin{minipage}[b]{0.49\textwidth}
                    \centering
                    \begin{equation}
                        \tilde{g}=\frac{g}{\norm{g}_{q}}
                    \end{equation}
                \end{minipage}
            \end{subequations}
            \par\hfill\par
            Then $\norm{\tilde{f}}_{p}=1$ and
            $\norm{\tilde{g}}_{q}=1$. But if $p\in(1,\infty)$,
            then $p^{\minus{1}}<1$, and thus
            by the previous problem, and since
            $1-p^{\minus{1}}=q^{\minus{1}}$, we have:
            \begin{equation}
                |\tilde{f}(x)\tilde{g}(x)|\leq
                    p^{\minus{1}}|\tilde{f}(x)|^{p}+
                    q^{\minus{1}}|\tilde{g}(x)|^{q}
            \end{equation}
            Integrating, we get:
            \begin{subequations}
                \begin{align}
                    \norm{\tilde{f}\tilde{g}}_{1}
                    &=\int_{\mathbb{R}}
                        |\tilde{f}(x)\tilde{g}(x)|\diff{x}\\
                    &\leq\int_{\mathbb{R}}\Big(
                        p^{\minus{1}}|\tilde{f}(x)|^{p}+
                        q^{\minus{1}}|\tilde{g}(x)|^{q}\Big)
                        \diff{x}\\
                    &=p^{\minus{1}}\norm{\tilde{f}}_{p}^{p}+
                    q^{\minus{1}}\norm{\tilde{g}}_{q}^{q}\\
                    &=p^{\minus{1}}+q^{\minus{1}}
                \end{align}
            \end{subequations}
            But $p^{\minus{1}}+q^{\minus{1}}=1$, and thus
            $\norm{\tilde{f}\tilde{g}}_{1}=1$. But from the
            definition of $\tilde{f}$ and $\tilde{g}$, we can
            multiply through by $\norm{f}_{p}\norm{g}_{q}$ to obtain:
            \begin{equation}
                \norm{fg}_{1}\leq\norm{f}_{p}\norm{g}_{q}
            \end{equation}
        \end{solution}
        \begin{problem}
            Prove Minkowski's Inequality
        \end{problem}
        \begin{problem}
            A limit point of a subspace $(E,d_{E})$ of a metric
            space $(X,d)$ is a point $x\in{X}$ such that there
            exists a sequence $a:\mathbb{N}\rightarrow{E}$ such
            that $a_{n}\rightarrow{x}$. Prove that $E$ is closed
            if and only if it has all of it's limit points.
        \end{problem}
        \begin{solution}
            Suppose $E$ is closed and let $x$ be a limit point
            of $E$. Suppose $x\in{E}^{C}$. But if $E$ is closed,
            then $E^{C}$ is open, and thus there is an $r>0$
            such that:
            \begin{equation}
                B_{r}^{(X,d)}(x)\subseteq{E}^{C}
            \end{equation}
            But if $x$ is a limit point of $E$ then there is a
            sequence $a:\mathbb{N}\rightarrow{E}$ such that
            $a_{n}\rightarrow{x}$. But if $a_{n}\rightarrow{x}$ then
            there is an $N\in\mathbb{N}$ such that, for all
            $n\in\mathbb{N}$ such that $n>N$, it is true that
            $d(x,a_{n})<r/2$. But then, for all $n>N$, we have that:
            \begin{equation}
                a_{n}\in{B}_{r}^{(X,d)}(x)
            \end{equation}
            And thus $a_{n}\in{E}^{C}$. But $a_{n}\in{E}$, a
            contradiction. Thus, $x\in{E}$. Now, suppose $x$ has
            all of it's limit points and suppose $E$
            is not closed. Then $E^{C}$ is not open. But then
            there is an $x\in{E}^{C}$ such that, for all
            $\varepsilon>0$:
            \begin{equation}
                B_{\varepsilon}^{(X,d)}(x)\cap{E}\ne\emptyset
            \end{equation}
            Define the following:
            \begin{equation}
                A_{n}=\Big\{y\in{E}:d(x,y)<\frac{1}{n}\Big\}
            \end{equation}
            Then for all $n\in\mathbb{N}$, $A_{n}$ is non-empty.
            By choice there is a sequence
            $a:\mathbb{N}\rightarrow{E}$ such that, for all
            $n\in\mathbb{N}$, $a_{n}\in{A}_{n}$. But then, for all
            $n\in\mathbb{N}$, $d(a_{n},x)<n^{\minus{1}}$. Thus
            $a_{n}\rightarrow{x}$ and therefore $x$ is a limit point
            of $E$. But $x\in{E}^{C}$, a contradiction as $E$
            contains all of its limit points. Therefore, $E$ is
            closed.
        \end{solution}
        \begin{problem}
            State and prove a result characterizing open sets in
            a metric space in terms of sequences, similar to
            the previous problem.
        \end{problem}
        \begin{solution}
            A subset $\mathcal{U}\subseteq{X}$ of a metric space
            $(X,d)$ is open if and only if for any convergence
            sequence $a:\mathbb{N}\rightarrow{X}$ such that the
            limit of $a$ is in $\mathcal{U}$, there is an
            $N\in\mathbb{N}$ such that, for all $n\in\mathbb{N}$ and
            $n>N$, it is true that $a_{n}\in\mathcal{U}$. For
            suppose $\mathcal{U}$ is open, and let
            $a:\mathbb{N}\rightarrow{X}$ be a convergent sequence
            such that there is an $x\in\mathcal{U}$ such that
            $a_{n}\rightarrow{x}$. But if $\mathcal{U}$ is open
            then there is an $\varepsilon>0$ such that:
            \begin{equation}
                B_{\varepsilon}^{(X,d)}(x)\subseteq\mathcal{U}
            \end{equation}
            But if $a_{n}\rightarrow{x}$ then there is an
            $N\in\mathbb{N}$ such that, for all $n\in\mathbb{N}$
            such that $n>N$, it is true that
            $d(x,a_{n})<\varepsilon$. But then for all $n>N$,
            $n\in\mathbb{N}$, we have
            $a_{n}\in{B}_{\varepsilon}^{(X,d)}(x)$, and thus
            $a_{n}\in\mathcal{U}$. Now suppose for any sequence
            that converges to a point in $\mathcal{U}$, the sequence
            is eventually contained within $\mathcal{U}$. Suppose
            $\mathcal{U}$ is not open. Then there is an
            $x\in\mathcal{U}$ such that, for all $\varepsilon>0$
            there is a $y\in\mathcal{U}^{C}$ such that
            $d(x,y)<\varepsilon$. Define the following:
            \begin{equation}
                A_{n}=
                \Big\{y\in\mathcal{U}^{C}:d(x,y)<\frac{1}{n}\Big\}
            \end{equation}
            Then for all $n\in\mathbb{N}$, $A_{n}$ is non-empty.
            By choice there is a sequence
            $a:\mathbb{N}\rightarrow\mathcal{U}^{C}$ such
            that $a_{n}\in{A}_{n}$. But then $a_{n}\rightarrow{x}$.
            But if $a_{n}\rightarrow{x}$, then there is an
            $N\in\mathbb{N}$ such that for all $n\in\mathbb{N}$ such
            that $n>N$ it is true that $a_{n}\in\mathcal{U}$, a
            contradiction. Therefore, $\mathcal{U}$ is open.
        \end{solution}
        \begin{problem}
            Let $\rho$ and $\sigma$ be metrics on $X$ and show that
            $\rho$ and $\sigma$ are equivalent if and only if
            they have the same convergent sequences.
        \end{problem}
        \begin{solution}
            For a sequence $a:\mathbb{N}\rightarrow{X}$ converges to
            $x\in{X}$ if and only if for all open subsets
            $\mathcal{U}\subseteq{X}$ such that $x\in\mathcal{U}$,
            there is an $N\in\mathbb{N}$ such that, for all
            $n\in\mathbb{N}$ and $n>N$, it is true that
            $a_{n}\in\mathcal{U}$. Going one way, if
            $a_{n}\rightarrow{x}$ then for all $\varepsilon>0$
            there is an $N\in\mathbb{N}$ such that for all
            $n\in\mathbb{N}$ and $n>N$, it is true that
            $d(x,a_{n})<\varepsilon$. Let $\mathcal{U}$ be an open
            subset such that $x\in\mathcal{U}$. But then there is
            an $\varepsilon>0$ such that:
            \begin{equation}
                B_{\varepsilon}^{(X,d)}(x)\subseteq\mathcal{U}
            \end{equation}
            But there is an $N\in\mathbb{N}$ such that, for all
            $n>N$, $n\in\mathbb{N}$, we have
            $a_{n}\in{B}_{\varepsilon}^{(X,d)}(x)$. Therefore
            $a_{n}\in\mathcal{U}$. Going the other way, let
            $a:\mathbb{N}\rightarrow{X}$ be a sequence such that,
            for every open set $\mathcal{U}\subseteq{X}$ such
            that $x\in\mathcal{U}$, there is an $N\in\mathbb{N}$
            such that, for all $n\in\mathbb{N}$ and $n>N$, it is
            true that $a_{n}\in\mathcal{U}$. Let:
            \begin{equation}
                A_{k}=B_{k^{\minus{1}}}^{(X,d)}(x)
            \end{equation}
            Given $\varepsilon>0$ there is a $k\in\mathbb{N}$
            such that $k^{\minus{1}}<\varepsilon$. But $A_{k}$ is
            open and $x\in{A}_{k}$, and thus there is an
            $N\in\mathbb{N}$ such that for all $n>N$ and
            $n\in\mathbb{N}$, we have that $a_{n}\in{A}_{k}$. But
            then $d(x,a_{n})<\varepsilon$, so $a_{n}\rightarrow{x}$.
            This converts the notion of convergence from a metric
            space property to a topological property. If $(X,\rho)$
            and $(X,\sigma)$ are equivalent then
            they have the same open sets, and thus convergence of
            sequences is preserved. For suppose
            $a:\mathbb{N}\rightarrow{X}$ converges to $x$ with
            respect to $\rho$. Then, for all open subsets
            $\mathcal{U}$ of $(X,d)$ such that $x\in\mathcal{U}$,
            there is an $N\in\mathbb{N}$ such that, for all
            $n\in\mathbb{N}$ and $n>N$, it is true that
            $a_{n}\in\mathcal{U}$. But if $\mathcal{U}$ is open in
            $(X,\rho)$, then it is open in $(X,\sigma)$, for the
            two metrics are equivalent. Thus $a_{n}\rightarrow{x}$
            with respect to $\sigma$.
        \end{solution}
        \begin{problem}
            Let $(X,\mathcal{M},\mu)$ be a measure space and define
            $\mathcal{U}\subseteq{L}^{1}(X)$ by:
            \begin{equation}
                \mathcal{U}=
                    \Big\{f\in{L}^{1}(X):
                        \int_{X}\Re(f)\diff{\mu}<1\Big\}
            \end{equation}
            Prove that $\mathcal{U}$ is open with respect to the
            metric induced by $\norm{\cdot}_{1}$.
        \end{problem}
        \begin{solution}
            For let $f\in{L}^{1}(X)$ and let $M=\norm{f}_{1}$. As
            $f\in{L}^{1}(X)$, $M<\infty$. Define:
            \begin{equation}
                \alpha=\int_{X}\Re(f)\diff{\mu}
            \end{equation}
            And let $\varepsilon=\min\{1/2M,1-\alpha\}$.
            Then if $\norm{f-g}_{1}<\varepsilon$ we have:
            \begin{equation}
                \int_{X}\Re(g)\diff{\mu}=
                \int_{X}\Re(g-f+f)\diff{\mu}=
                \int_{X}\Re(g-f)\diff{\mu}+\int_{X}\Re(f)\diff{\mu}
            \end{equation}
            We can simplify this further to get:
            \begin{equation}
                \int_{X}\Re(g)\diff{\mu}
                =\int_{X}\Re(g-f)\diff{\mu}+\alpha
                <\varepsilon+\alpha\leq{1}
            \end{equation}
            Therefore:
            \begin{equation}
                B_{\varepsilon}^{(L^{1}(X),\norm{\cdot}_{1})}(f)
                \subseteq\mathcal{U}
            \end{equation}
            And thus $\mathcal{U}$ is open.
        \end{solution}
        \begin{problem}
            For a metric space $(X,d)$, define
            $\dist:X\times\mathcal{P}(X)\setminus\{\emptyset\}%
             \rightarrow[0,\infty)$ by:
            \begin{equation}
                \dist(x,A)=\inf\{d(x,y):y\in{A}\}
            \end{equation}
            Prove that $\dist(x,A)=0$ if and only if
            $x\in\overline{A}$. Show that, for a fixed non-empty
            $A\subseteq{X}$, $\dist(x,A)$ is continuous. Prove that
            if $A,B\subseteq{X}$ are closed disjoint non-empty
            subsets, then there is a continuous function
            $f:X\rightarrow[0,1]$ such that $f(x)=0$
            if and only if $x\in{A}$ and $f(y)=1$ if
            and only if $x\in{B}$.
        \end{problem}
        \begin{solution}
            If $x\in\overline{A}$, then for all $\varepsilon>0$:
            \begin{equation}
                B_{\varepsilon}^{(X,d)}(x)\cap{A}\ne\emptyset
            \end{equation}
            Thus $\dist(x,A)<\varepsilon$ for all positive
            $\varepsilon$, and therefore $\dist(x,A)=0$. If
            $\dist(x,A)=0$ then for all $\varepsilon>0$ there is a
            $y\in{A}$ such that $d(x,y)<\varepsilon$. Therefore
            $x$ is a limit point of $A$, and thus $x\in\overline{A}$.
            \par\hfill\par
            Let $\varepsilon>0$ and let $x\in{X}$, and let
            $\delta=\varepsilon$. Then:
            \begin{subequations}
                \begin{align}
                    \dist(y,A)&=\inf\{d(y,z):z\in{a}\}\\
                    &\leq\inf\{d(x,y)+d(x,z):z\in{A}\}\\
                    &=d(x,y)+\inf\{d(x,z):z\in{A}\}\\
                    &=d(x,y)+\dist(x,A)
                \end{align}
            \end{subequations}
            Similarly:
            \begin{equation}
                \dist(x,A)\leq{d}(x,y)+\dist(y,A)
            \end{equation}
            And therefore, if $d(x,y)<\varepsilon$:
            \begin{equation}
                \big|\dist(x,A)-\dist(y,A)\big|\leq{d}(x,y)
                <\varepsilon
            \end{equation}
            Finally, let:
            \begin{equation}
                f(x)=\frac{\dist(x,B)}{\dist(x,A)+\dist(x,B)}
            \end{equation}
            As $A$ and $B$ are disjoint and closed, $f(x)$ is well
            defined for all $x\in{X}$. Moreover,
            $0\leq{f(x)}\leq{1}$. If $f(x)=0$, then $\dist(x,B)=0$,
            and thus $x\in\overline{B}$. But $B$ is closed, and
            therefore $x\in{B}$. If $f(x)=1$, then
            $\dist(x,A)=0$, and thus $x\in\overline{A}$. Again, as
            $A$ is closed, $x\in{A}$. But $\dist(x,B)$ is
            continuous, and $\dist(x,A)+\dist(x,B)$ is
            continuous, and the quotient of continuous functions
            is continuous. Thus, $f$ is continuous.
        \end{solution}
        \begin{problem}
            Show that a Cauchy sequence with a convergent
            subsequence is convergent.
        \end{problem}
        \begin{solution}
            For let $a:\mathbb{N}\rightarrow{X}$ be a Cauchy
            sequence and let $k:\mathbb{N}\rightarrow\mathbb{N}$ be
            such that $a\circ{k}$ is a convergent subsequence, and
            let $x$ be the limit. That is, $k$ is a stricly
            monotonically increasing sequence of natural numbers.
            Let $\varepsilon>0$. Then there is an
            $N_{1}\in\mathbb{N}$ such that, for all $k_{n}>N_{1}$,
            $n\in\mathbb{N}$, it is true that
            $d(x,a_{k_{n}})<\varepsilon/2$. But $a$ is
            Cauchy, and thus there is an $N_{2}\in\mathbb{N}$
            such that, for all $n,m\in\mathbb{N}$ such that
            $n,m>N_{2}$, it is true that
            $d(a_{n},a_{m})<\varepsilon/2$. Let
            $N=\max\{N_{1},N_{2}\}$. Then for all $n>N$,
            $n\in\mathbb{N}$, $k_{n}>N$ since $k$ is increasing,
            and thus:
            \begin{equation}
                d(a_{n},x)\leq
                d(a_{n},a_{k_{n}})+d(a_{k_{n}},x)<\varepsilon
            \end{equation}
            Therefore, $a_{n}\rightarrow{x}$.
        \end{solution}
        \begin{problem}
            Let $F:\mathbb{N}\times{X}\rightarrow\mathbb{C}$ be a
            sequence of continuous functions and let
            $f:X\rightarrow\mathbb{C}$ be such that
            $F_{n}(x)\rightarrow{f}$ uniformly. Show that $f$ is
            continuous.
        \end{problem}
        \begin{solution}
            For let $\varepsilon>0$. As $F_{n}\rightarrow{f}$
            uniformly, there is an $N\in\mathbb{N}$ such that for
            all $x\in{X}$, it is true that:
            \begin{equation}
                |F_{N}(x)-f(x)|<\frac{\varepsilon}{3}
            \end{equation}
            But $F_{N}(x)$ is continuous, and thus for $x\in{X}$
            there is a $\delta>0$ such that $d(x,x_{0})<\delta$
            implies that:
            \begin{equation}
                |F_{N}(x)-F_{N}(x_{0})|<\frac{\varepsilon}{3}
            \end{equation}
            But then:
            \begin{subequations}
                \begin{align}
                    |f(x)-f(x_{0})|&\leq
                    |f(x)-F_{N}(x)|+|F_{N}(x)-F_{N}(x_{0})|+
                    |F_{N}(x_{0})-f(x_{0})|\\
                    &<\varepsilon
                \end{align}
            \end{subequations}
            Thus, $f$ is continuous.
        \end{solution}
        \begin{problem}
            Let $X$ be a metric space. Recall that we say
            $f:X\rightarrow\mathbb{C}$ is bounded if
            $\norm{f}_{\infty}<\infty$. A sequence of functions
            $f_{n}:X\rightarrow{D}$ is uniformly bounded if there
            is an $M$ such that $\norm{f_{n}}_{\infty}<M$ for
            all $n$. Also, $f_{n}$ is uniformly Cauchy if for
            all $\varepsilon>0$ there is an $N\in\mathbb{N}$
            such that $n,m>N$ implies
            $|f_{n}(x)-f_{m}(x)|<\varepsilon$ for all $x\in{X}$.
            Show that a uniformly Cauchy sequence $f_{n}$ of
            bounded functions is uniformly bounded.
        \end{problem}
        \begin{solution}
            For let $F:\mathbb{N}\times{X}\rightarrow\mathbb{C}$
            be a sequence of functions such that, for all
            $n\in\mathbb{N}$, there is an $M_{n}\in\mathbb{R}^{+}$
            such that:
            \begin{equation}
                \norm{F_{n}}_{\infty}<M_{n}
            \end{equation}
            And such that $F$ is uniformly Cauchy. Let
            $\varepsilon=1$. Then, as $F$ is uniformly Cauchy,
            there is an $N\in\mathbb{N}$ such that, for all
            $n,m\in\mathbb{N}$ such that $n,m>N$, and for all
            $x\in{X}$, it is true that:
            \begin{equation}
                |F_{n}(x)-F_{m}(x)|<\varepsilon
            \end{equation}
            Then, for all $n>N$ and for all $x\in{X}$:
            \begin{subequations}
                \begin{align}
                    |F_{n}(x)|&=
                    |F_{n}(x)+F_{N+1}(x)-F_{N+1}(x)|\\
                    &\leq|F_{n}(x)-F_{N+1}(x)|+|F_{N+1}(x)|\\
                    &<\varepsilon+M_{N+1}\\
                    &=M_{N+1}+1
                \end{align}
            \end{subequations}
            Let:
            \begin{equation}
                M=\max\Big(
                    \{M_{n}:n\in\mathbb{Z}_{N}\}\cup\{M_{N+1}+1\}
                \Big)
            \end{equation}
            Then for all $n\in\mathbb{N}$, and for all $x\in{X}$:
            \begin{equation}
                |F_{n}(x)|\leq{M}
            \end{equation}
            Therefore, $F$ is uniformly bounded.
        \end{solution}
        \begin{problem}
            We say that $D$ is dense in $X$ if $\overline{D}=X$.
            Show that $D$ is dense if and only if $D$ meets every
            non-empty open subset.
        \end{problem}
        \begin{solution}
            Suppose $D$ is a dense subset of $X$ and let
            $\mathcal{U}\subseteq{X}$ be an open subset. Suppose
            $\mathcal{U}\cap{D}=\emptyset$. Let $x\in\mathcal{U}$. As
            $\mathcal{U}$ is open, there is an $r>0$ such that:
            \begin{equation}
                B_{r}^{(X,d)}(x)\subseteq\mathcal{U}
            \end{equation}
            But if $D$ is dense in $X$, then $x$ is a limit point
            of $D$. Thus there is a sequence
            $a:\mathbb{N}\rightarrow{D}$ such
            that $a_{n}\rightarrow{x}$. But if $a_{n}\rightarrow{x}$,
            then there is an $N\in\mathbb{N}$ such that, for all
            $n\in\mathbb{N}$ and $n>N$, we have:
            \begin{equation}
                a_{n}\in{B}_{r}^{(X,d)}(x)
            \end{equation}
            But then, $a_{n}\in\mathcal{U}$, a contradiction as
            as $a_{n}\in{D}$ and $\mathcal{U}$ and $\mathcal{D}$
            are disjoint. Therefore, etc. Now suppose
            $D$ meets every open set. Suppose
            $x\notin\overline{D}$. Define the following:
            \begin{equation}
                A_{n}=\Big\{y\in{B}_{1/n}^{(X,d)}(x):y\in{D}\Big\}
            \end{equation}
            Then $A_{n}$ is non-empty for all $n\in\mathbb{N}$,
            since $D$ meets every open set. By choice there is
            a sequence $a:\mathbb{N}\rightarrow{D}$ such
            that $a_{n}\in{A}_{n}$ for all $n\in\mathbb{N}$. But
            then $x$ is a limit point of $D$, a contradiction.
            Thus, $\overline{D}=X$.
        \end{solution}
        \begin{problem}
            Let $(x_{n})$ be a sequence in a complete metric
            space $(X,\rho)$. Suppose that
            $\rho(x_{n},x_{n+1})<2^{\minus{n}}$ for all
            $n\in\mathbb{N}$. Conclude that $(x_{n})$ is
            convergent. What if instead we have that
            $\rho(x_{n},x_{n+1})<1/n$?
        \end{problem}
        \begin{solution}
            For let $\varepsilon>0$. Let $N\in\mathbb{N}$ such that
            $2^{1-N}<\varepsilon$. But then for $n,m>N$:
            \begin{equation}
                \rho(x_{n},x_{m})\leq
                \sum_{k=\min(n,m)}^{\max(n,m)}d(x_{k},x_{k+1})
                \leq\sum_{k=N}^{\infty}d(x_{k},x_{k_{n+1}})
                \leq\sum_{k=N}^{\infty}\frac{1}{2^{k}}
            \end{equation}
            But by applying the geometric series, we have:
            \begin{equation}
                \sum_{k=N}^{\infty}\frac{1}{2^{k}}
                =2^{1-N}<\varepsilon
            \end{equation}
            Thus $x_{n}$ is Cauchy, and Cauchy sequences
            converge in a complete metric space. Therefore, etc.
            If we replace $2^{\minus{n}}$ with $n^{\minus{1}}$,
            the result may not hold. For let $X=\mathbb{R}$, which
            is complete with the standard metric, and let
            $a:\mathbb{N}\rightarrow\mathbb{R}$ be defined
            by $a_{n}=\ln(n)$. Applying some calculus, we have:
            \begin{equation}
                d(a_{n+1},a_{n})=\ln(n+1)-\ln(n)
                =\int_{n}^{n+1}\frac{1}{x}\diff{x}<\frac{1}{n}
            \end{equation}
            But $\ln(n)$ is not a convergent sequence.
        \end{solution}
        \begin{problem}
            A metric space is separable if it has a countable dense
            subset. Show that a metric space $X$ is separable if
            and only if there is a countable family $\mathcal{D}$
            of open sets such that every open set in $X$ is the union
            of open sets in $\mathcal{D}$.
        \end{problem}
        \begin{solution}
            For let $(X,d)$ be a separable metric space, and let
            $A$ be a countable dense subset. Let:
            \begin{equation}
                \mathcal{D}=\bigcup_{n\in\mathbb{N}}
                \bigcup_{a\in{A}}B_{n^{\minus{1}}}^{(X,d)}(a)
            \end{equation}
            Then $\mathcal{D}$ is countable, and
            for all $\mathcal{U}\in\mathcal{D}$, $\mathcal{U}$
            is open. Let $\mathcal{O}$ be an open subset of $X$.
            Define:
            \begin{equation}
                \mathcal{V}=\{\mathcal{U}\in\mathcal{D}:
                    \mathcal{U}\subseteq\mathcal{O}\}
            \end{equation}
            Then:
            \begin{equation}
                \bigcup_{\mathcal{U}\in\mathcal{V}}\mathcal{U}
                \subseteq\mathcal{O}
            \end{equation}
            Suppose the converse is false, and let
            $x\in\mathcal{O}$ be such that it is not contained
            in this union. But $\mathcal{O}$ is open, and thus
            there is an $r>0$ such that:
            \begin{equation}
                B_{r}^{(X,d)}(x)\subseteq\mathcal{O}
            \end{equation}
            But by the Archimedean principle, there is an
            $n\in\mathbb{N}$ such that $n^{\minus{1}}<r/2$. But $A$
            is dense in $\mathcal{O}$ and thus there is a $y\in{A}$
            such that $d(x,y)<n^{-1}$. But then:
            \begin{equation}
                x\in{B}_{n^{\minus{1}}}^{(X,d)}(y)
                \subseteq{B}_{r}^{(X,d)}(x)\subseteq\mathcal{O}
            \end{equation}
            And thus $x$ is contained in this union, a contradiction.
            Therefore, etc. Going the other way, suppose $(X,d)$
            is a metric space such that there exists a countable set
            $\mathcal{D}$ of open subsets of $X$ such that, for all
            open sets $\mathcal{O}$, $\mathcal{O}$ is the union
            of elements of $\mathcal{D}$. That is, there is a sequence
            $A:\mathbb{N}\rightarrow\mathcal{P}(X)$ such that:
            \begin{equation}
                \mathcal{D}=\{A_{n}:n\in\mathbb{N}\}
            \end{equation}
            But then by choice there is a sequence:
            \begin{equation}
                a:\mathbb{N}\rightarrow\bigcup_{n\in\mathbb{N}}A_{n}
            \end{equation}
            Such that, for all $n\in\mathbb{N}$, $a_{n}\in{A}_{n}$.
            Let $y\in{X}$ and let $\varepsilon>0$, define:
            \begin{equation}
                \mathcal{V}=B_{\varepsilon}^{(X,d)}(y)
            \end{equation}
            But then $\mathcal{V}$ is an open subset of
            $(X,d)$ and is therefore the union of elements of
            $\mathcal{D}$. That is, there is a sequence
            $k:\mathbb{N}\rightarrow\mathbb{N}$ such that:
            \begin{equation}
                \mathcal{V}=
                \bigcup_{n\in\mathbb{N}}A_{k_{n}}
            \end{equation}
            Where we write $k_{n}$ to denote $k(n)$. But then:
            \begin{equation}
                d(y,a_{k_{n}})<\varepsilon
            \end{equation}
            For all $n\in\mathbb{N}$. Thus the set:
            \begin{equation}
                \mathcal{A}=\{a_{n}:n\in\mathbb{N}\}
            \end{equation}
            Is a a countable dense subset, and $(X,d)$ is separable.
        \end{solution}
    \section{Homework II}
        \begin{problem}
            Show that $X$ is compact if and only if every collection
            of closed sets $\mathcal{F}$ with the finite intersection
            property is such that:
            \begin{equation}
                \bigcap_{F\in\mathcal{F}}F\ne\emptyset
            \end{equation}
        \end{problem}
        \begin{solution}
            For suppose $X$ is compact and suppose there is a
            collection $\mathcal{F}$ of closed sets with the finite
            intersection property such that:
            \begin{equation}
                \bigcap_{F\in\mathcal{F}}F=\emptyset
            \end{equation}
            But, for all $F\in\mathcal{F}$, $F$ is closed, and
            therefore $F^{C}$ is open. But then:
            \begin{equation}
                X=\emptyset^{C}
                =\Big(\bigcap_{F\in\mathcal{F}}F\Big)^{C}
                =\bigcup_{F\in\mathcal{F}}F^{C}
            \end{equation}
            And thus:
            \begin{equation}
                \mathcal{O}=
                    \{F^{C}:F\in\mathcal{F}\}
            \end{equation}
            Is an open cover of $X$. But $X$ is compact, and
            therefore there is a finite subcover
            $\Delta\subseteq\mathcal{O}$. But then:
            \begin{equation}
                \emptyset=
                X^{C}=\Big(\bigcup_{\mathcal{U}\in\Delta}
                    \mathcal{U}\Big)^{C}=
                    \bigcap_{\mathcal{U}\in\Delta}
                    \mathcal{U}^{C}
            \end{equation}
            But $\mathcal{U}^{C}\in\mathcal{F}$ for all
            $\mathcal{U}\in\Delta$. And $\Delta$ is finite. Thus
            there is a finite subset of $\mathcal{F}$ such that
            the intersection is empty, a contradiction as
            $\mathcal{F}$ has the finite intersection property.
            Therefore, etc. Now suppose $X$ is such that every
            collection of closed sets $\mathcal{F}$ with the
            finite intersection property is such that the
            intersection over all elements is non-empty. Suppose
            $X$ is not compact. Then there is an open cover
            $\mathcal{O}$ such that there is no finite subcover.
            Let:
            \begin{equation}
                \mathcal{F}=\{\mathcal{U}^{C}:
                    \mathcal{U}\in\mathcal{O}\}
            \end{equation}
            But then for any finite subset, the intersection is
            non-empty. For if not then $\mathcal{O}$ has a finite
            subcover, which it does not. But then $\mathcal{F}$ is
            a collection of closed sets with
            the finite intersection property, and therefore:
            \begin{equation}
                \bigcap_{F\in\mathcal{F}}F\ne\emptyset
            \end{equation}
            But:
            \begin{equation}
                \emptyset=X^{C}=\Big(
                    \bigcup_{\mathcal{U}\in\mathcal{O}}\mathcal{U}
                \Big)^{C}
                =\bigcap_{F\in\mathcal{F}}F
            \end{equation}
            A contradiction. Therefore, $X$ is compact.
        \end{solution}
        \begin{problem}
            Show that $E\subseteq{X}$ is totally bounded if and
            only if there is a finite $\varepsilon\textrm{-net}$
            for all $\varepsilon>0$.
        \end{problem}
        \begin{solution}
            If $E\subseteq{X}$ is totally bounded, then for all
            $\varepsilon>0$ there are finitely many points
            $x_{k}$, $k\in\mathbb{Z}_{n}$ such that:
            \begin{equation}
                E\subseteq\bigcup_{k=1}^{n}
                    B_{\varepsilon}^{(X,d)}(x_{k})
            \end{equation}
            But then:
            \begin{equation}
                \mathcal{O}=\{B_{\varepsilon}^{(X,d)}(x_{k}):
                    k\in\mathbb{Z}_{n}\}
            \end{equation}
            Is a finite $\varepsilon\textrm{-net}$ of $E$. If,
            for all $\varepsilon>0$, there is a finite
            $\varepsilon\textrm{-net}$ of $E$, then there are
            finitely many points $x_{k}$, $k\in\mathbb{Z}_{n}$
            such that:
            \begin{equation}
                \mathcal{O}=\{B_{\varepsilon}^{(X,d)}(x_{k}):
                    k\in\mathbb{Z}_{n}\}
            \end{equation}
            Is an open cover of $E$. But then for all
            $\varepsilon>0$ there are finitely many open balls that
            cover $E$, and therefore $E$ is totally bounded.
        \end{solution}
        \begin{problem}
            Suppose $(X,d_{X})$ is compact and that
            $f:(X,d_{X})\rightarrow(Y,d_{Y})$ is continuous.
            Show that $f(X)$ is compact.
        \end{problem}
        \begin{solution}
            For let $\mathcal{O}$ be an open cover of $f(X)$.
            But $f$ is continuous, and thus for all
            $\mathcal{U}\in\mathcal{O}$,
            $f^{\minus{1}}(\mathcal{U})$ is an open subset of
            $X$. But then:
            \begin{equation}
                \Delta=\{f^{\minus{1}}(\mathcal{U}):
                    \mathcal{U}\in\mathcal{O}\}
            \end{equation}
            Is an open cover of $X$. But $X$ is compact, and thus
            there is a finite sub-cover $\Lambda$. But then:
            \begin{equation}
                \mathscr{O}=
                \{\mathcal{U}:f^{\minus{1}}(\mathcal{U})\in\Lambda\}
            \end{equation}
            Is is a finite subcover of $f(X)$, and therefore
            $f(X)$ is compact.
        \end{solution}
        \begin{problem}
            Let $X=(0,1)$ and let $\delta_{x}>0$ be such that:
            \begin{equation}
                y\in{B}^{(X,||)}_{\delta_{x}}(x)
                \Longrightarrow
                \Big|\frac{1}{x}-\frac{1}{y}\Big|<1
            \end{equation}
            Show that:
            \begin{equation}
                \mathcal{O}=
                \{B^{(X,||)}_{\delta_{x}}(x):x\in{X}\}
            \end{equation}
            Has no Lebesgue number.
        \end{problem}
        \begin{solution}
            Suppose not, and let $d>0$ be a Lebesgue number. Then
            for all $x\in(0,1)$, there is a
            $\mathcal{U}\in\mathcal{O}$ such that:
            \begin{equation}
                B_{d}^{(X,||)}(x)\subseteq\mathcal{U}
            \end{equation}
            Let $n\in\mathbb{N}$ be such that $n^{\minus{1}}<d$.
            Let $x=n^{\minus{1}}/2$. Then $x\in(0,1)$.
            But $d>n^{\minus{1}}$, and thus:
            \begin{equation}
                B_{d}^{(X,||)}(x)=(0,x+d)
            \end{equation}
            But since $x\in(0,1)$,
            there is a $y\in(0,1)$ such that:
            \begin{equation}
                B_{d}^{(X,||)}(x)\subseteq
                B_{\delta_{y}}^{(X,||)}(y)
            \end{equation}
            But then for all $z\in(0,x+d)$, we have:
            \begin{equation}
                \Big|\frac{1}{z}-\frac{1}{y}\Big|<1
            \end{equation}
            Let $N\in\mathbb{N}$ be such that
            $N>x^{\minus{1}}+y^{\minus{1}}+2$.
            But then $N^{\minus{1}}\in(0,x+d)$, and thus:
            \begin{equation}
                \Big|\frac{1}{N^{\minus{1}}}-\frac{1}{y}\Big|<1
            \end{equation}
            But:
            \begin{equation}
                \Big|\frac{1}{N^{\minus{1}}}-\frac{1}{y}\Big|
                =|N-y^{\minus{1}}|>2
            \end{equation}
            A contradiction. Thus, $d$ is not a Lebesgue number.
        \end{solution}
        \begin{problem}
            Show that a compact metric space has a countable
            dense subset.
        \end{problem}
        \begin{solution}
            For if $(X,d)$ is compact, then it is complete and
            totally bounded. But if it is totally bounded, for all
            $n\in\mathbb{N}$ there exists an $N\in\mathbb{N}$ and
            a sequence $a:\mathbb{Z}_{N}\rightarrow{X}$ such that:
            \begin{equation}
                X=\bigcup_{k=1}^{N}B_{n^{\minus{1}}}^{(X,d)}(a_{k})
            \end{equation}
            Define the following:
            \begin{equation}
                A_{n}=\bigcup_{N\in\mathbb{N}}
                    \Big\{a:\mathbb{Z}_{N}\rightarrow{X}:
                    X=\bigcup_{k=1}^{N}
                    B_{n^{\minus{1}}}^{(X,d)}(a_{k})\Big\}
            \end{equation}
            Then, for all $n\in\mathbb{N}$, $A_{n}$ is non-empty.
            Then by choice there is a sequence:
            \begin{equation}
                f:\mathbb{N}\rightarrow
                \bigcup_{n\in\mathbb{N}}A_{n}
            \end{equation}
            Such that, for all $n$, $f_{n}\in{A}_{n}$. Let:
            \begin{equation}
                \mathcal{D}=\bigcup_{n\in\mathbb{N}}
                    \textrm{Im}(f_{n})
            \end{equation}
            Where $\textrm{Im}$ denotes the image of $f_{n}$. From
            construction, for all $n\in\mathbb{N}$,
            $\textrm{Im}(f_{n})$ is finite, and thus $\mathcal{D}$
            is the countable union of countable sets, and is
            therefore countable. Moreover,
            $\overline{\mathcal{D}}=X$. For let $x\in{X}$ and let
            $\varepsilon>0$. By the
            Archimedean property, there and an $n\in\mathbb{N}$
            such that $n^{\minus{1}}<\varepsilon$. But:
            \begin{equation}
                X=\bigcup_{y\in{f_{n}}}
                    B_{n^{\minus{1}}}^{(X,d)}(y)
            \end{equation}
            And thus there is a $y\in{f}_{n}$ such that
            $d(x,y)<n^{\minus{1}}$. But if $y\in{f}_{n}$, the
            $y\in\mathcal{D}$. Thus, $x\in\overline{\mathcal{D}}$.
            Therefore $\mathcal{D}$ is a countable dense subset.
        \end{solution}
        \begin{problem}
            Show that the family of functions $\mathcal{F}$ defined
            on $[0,1]$ by $f_{n}(x)=x^{n}$,
            is equicontinuous at each $x\in[0,1)$. 
        \end{problem}
        \begin{solution}[1]
            If $F:\mathbb{N}\times{X}\rightarrow{Y}$ is a
            sequence of continuous functions such that
            $F_{n}\rightarrow{f}$ uniformly, then
            $F$ is point-wise equicontinuous. For let $\varepsilon>0$
            and let $x\in{X}$. But $F_{n}\rightarrow{f}$ unifomly,
            and $F_{n}$ is continuous for all $n\in\mathbb{N}$,
            and therefore $f$ is continuous. But then there is
            a $\delta_{1}>0$ such that, for all $x_{0}\in{X}$
            such that $d_{X}(x,x_{0})<\delta_{1}$, we have that:
            \begin{equation}
                d_{Y}\big(f(x),f(x_{0})\big)
                <\frac{\varepsilon}{3}
            \end{equation}
            But $F_{n}\rightarrow{f}$ uniformly, and thus there is
            an $N\in\mathbb{N}$ such that, for all
            $n>N$ and $n\in\mathbb{N}$, it is true that:
            \begin{equation}
                d_{Y}\big(F_{n}(x),F_{n}(x_{0})\big)
                <\frac{\varepsilon}{3}
            \end{equation}
            But then, for all $n>N$, and for all $x_{0}\in{X}$
            such that $d_{X}(x,x_{0})<\delta_{1}$, we have that:
            \begin{equation}
                \begin{split}
                    d_{Y}\big(F_{n}(x),F_{n}(x_{0})\big)
                    \leq{d}_{Y}&\big(F_{n}(x),f(x)\big)+\\
                    &d_{Y}\big(f(x),f(x_{0})\big)+
                    d_{Y}\big(f(x_{0}),F_{n}(x_{0})\big)<\varepsilon
                \end{split}
            \end{equation}
            But $F$ is continuous, and thus for all
            $n\in\mathbb{Z}_{N}$ there is a $\delta_{n}$ such that
            $d_{X}(x,x_{0})<\delta_{n}$ implies that:
            \begin{equation}
                d_{Y}\big(F_{n}(x),F_{n}(x_{0})\big)<\varepsilon
            \end{equation}
            Let:
            \begin{equation}
                \delta=\min\Big(\{\delta_{0}\}\cup
                    \{\delta_{n}:n\in\mathbb{Z}_{N}\}\Big)
            \end{equation}
            Now, for all $x_{0}<1$, $f_{n}(x)=x^{n}$ tends
            to zero uniformly on $[0,x_{0}]$. Therefore, etc.
        \end{solution}
        \begin{solution}[2]
            For let $\varepsilon>0$, and let $x\in[0,1)$. If
            $x=0$, Let
            $\delta=\varepsilon\min\{\varepsilon,\tfrac{1}{2}\}$.
            Then, for $0\leq{x}_{0}<\delta$, and for all
            $n\in\mathbb{N}$:
            \begin{equation}
                \big|x_{0}^{n}\big|<
                \delta^{n}\leq\varepsilon\Big(\frac{1}{2}\Big)^{n}
                <\varepsilon
            \end{equation}
            Otherwise, let $\delta_{1}=\tfrac{1}{2}\min\{x,1-x\}$
            and let $y=\delta_{1}+x$. Then $0<y<1$. By the mean
            value theorem, for all $x_{0}$ there is a
            $c_{x_{0}}$ such that $|x-c_{x_{0}}|<|x-x_{0}|$ and
            such that:
            \begin{equation}
                \big|x^{n}-x_{0}^{n}\big|
                =nc_{x_{0}}^{n-1}|x-x_{0}|
            \end{equation}
            But then:
            \begin{equation}
                \big|x^{n}-x_{0}^{n}\big|<
                ny^{n}\delta
            \end{equation}
            But, since $0<y<1$, $ny^{n}$ is bounded. For let
            $f:[0,\infty)\rightarrow\mathbb{R}$ be defined by:
            \begin{equation}
                f(x)=\frac{x}{y^{1-x}}
            \end{equation}
            Then by L'H\"{o}pital, we have:
            \begin{equation}
                \underset{x\rightarrow\infty}{\lim}f(x)
                =\underset{x\rightarrow\infty}{\lim}
                    \frac{x}{y^{1-x}}
                =\underset{x\rightarrow\infty}{\lim}
                    \frac{\minus{1}}{y^{1-x}\ln(y)}
                =\underset{x\rightarrow\infty}{\lim}
                    \frac{\minus{y}^{x-1}}{\ln(y)}
            \end{equation}
            But $0<y<1$, and therefore $y^{x-1}\rightarrow{0}$
            as $x\rightarrow{0}$. Therefore:
            \begin{equation}
                \underset{x\rightarrow\infty}{\lim}f(x)=0
            \end{equation}
            But then for any sequence
            $a:\mathbb{N}\rightarrow\mathbb{R}$ such that
            $a_{n}\rightarrow\infty$, we have
            $f(a_{n})\rightarrow{0}$. Therefore,
            $ny^{n-1}$ converges to zero. But convergent sequences
            are bounded sequences, and therefore there is an
            $M\in\mathbb{R}^{+}$ such that, for all
            $n\in\mathbb{N}$:
            \begin{equation}
                \big|ny^{n-1}\big|\leq{M}
            \end{equation}
            Let $\delta=\min\{\tfrac{\varepsilon}{M},\delta_{1}\}$.
            Then for all $x_{0}\in(0,1)$ such that
            $|x-x_{0}|<\delta$, we have:
            \begin{equation}
                \big|x^{n}-x_{0}^{n}\big|=
                nc_{x_{0}}^{n-1}|x-x_{0}|<
                ny^{n-1}\delta<\varepsilon
            \end{equation}
        \end{solution}
        \begin{problem}
            Show that an equicontinuous family of functions on a
            compact metric space is uniformly equicontinuous.
        \end{problem}
        \begin{solution}
            For let $(X,d_{X})$ be a compact metric space and let
            $\mathcal{F}$ be a family of equicontinuous functions
            to a metric space $(Y,d_{Y})$.
            Let $\varepsilon>0$. Then, as $\mathcal{F}$ is
            equicontinuous, for all $x\in{X}$ there exists a
            $\delta_{x}>0$ such that, for all $f\in\mathcal{F}$,
            we have:
            \begin{equation}
                x_{0}\in{B}_{\delta_{x}}^{(X,d_{X})}(x)
                \Longrightarrow
                f(x_{0})\in{B}_{\varepsilon/2}^{(Y,d_{Y})}
                \big(f(x)\big)
            \end{equation}
            But then:
            \begin{equation}
                \mathcal{O}=\Big\{
                    B_{\delta_{x}}^{(X,d_{X})}(x):x\in{X}\Big\}
            \end{equation}
            Is an open cover of $X$. But $(X,d)$ is compact, and
            therefore this cover has a Lebesgue number $\delta>0$.
            If $x\in{X}$, then there is a $y\in{X}$ such that:
            \begin{equation}
                B_{\delta}^{(X,d_{X})}(x)\subseteq
                B_{\delta_{y}}^{(X,d_{X})}(y)
            \end{equation}
            But then, if $d_{X}(x,x_{0})<\delta$, then:
            \begin{equation}
                x_{0}\in{B}_{\delta}^{(X,d_{X})}(x)
                \Rightarrow
                x_{0}\in{B}_{\delta_{y}}^{(X,d_{X})}(y)
                \Rightarrow
                f(x_{0})\in
                B_{\varepsilon/2}^{(Y,d_{Y})}\big(f(y)\big)
            \end{equation}
            And therefore:
            \begin{equation}
                d_{Y}\big(f(x),f(x_{0})\big)\leq
                d_{Y}\big(f(x),f(y)\big)+
                d_{Y}\big(f(y),f(x_{0})\big)<\varepsilon
            \end{equation}
            Thus, $\mathcal{F}$ is uniformly equicontinuous.
        \end{solution}
        \begin{problem}
            Show that a subset of a compact metric space is compact
            if and only if it is closed.
        \end{problem}
        \begin{solution}
            For let $(X,d)$ be a compact metric space and let
            $(E,d_{E})$ be a compact subspace. Suppose $E$ is not
            closed. Then $E^{C}$ is not open, and therefore
            there is an $x\in{E}^{C}$ such that, for all
            $\varepsilon>0$:
            \begin{equation}
                B_{\varepsilon}^{(X,d)}(x)\cap
                E\ne\emptyset
            \end{equation}
            Let:
            \begin{equation}
                \mathcal{O}=\Big\{\textrm{Cl}\Big(
                    B_{\varepsilon}^{(X,d)}(x)\Big)^{C}:
                    \varepsilon\in\mathbb{R}^{+}\Big\}
            \end{equation}
            Where $\textrm{Cl}$ denotes the closure of a set.
            Then $\mathcal{O}$ is an open cover of $E$. But
            $(E,d_{E})$ is compact, and thus there is a finite
            subcover $\Delta$. But then there is a least
            $r\in\mathbb{R}^{+}$ such that:
            \begin{equation}
                \textrm{Cl}
                \Big(B_{r}^{(X,d)}(x)\Big)^{C}\in\Delta
            \end{equation}
            But then, for all $0<\varepsilon<r$, we have:
            \begin{equation}
                B_{\varepsilon}^{(X,d)}(x)\cap
                E=\emptyset
            \end{equation}
            A contradiction. Therefore, $E$ is closed. Suppose
            $(X,d)$ is compact and $E\subseteq{X}$ is closed.
            Suppose $(E,d_{E})$ is not compact. Then there is an
            open cover $\mathcal{O}_{E}$ of $E$ with no finite
            subcover. But $E$ is closed, and thus $E^{C}$ is open.
            But then:
            \begin{equation}
                \mathcal{O}_{X}=\mathcal{O}_{E}\cup
                \big\{E^{C}\big\}
            \end{equation}
            Is an open cover of $X$. But $(X,d)$ is compact,
            and therefore there is a finite subcover $\Delta_{X}$.
            But then:
            \begin{equation}
                \Delta_{E}=\Delta_{X}\setminus\big\{E^{C}\big\}
            \end{equation}
            Is a finite subcover of $\mathcal{O}_{E}$, a
            contradiction. Therefore, $(E,d_{E})$ is compact.
        \end{solution}
\end{document}