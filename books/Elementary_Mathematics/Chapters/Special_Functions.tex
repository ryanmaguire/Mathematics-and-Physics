\documentclass[crop=false,class=book,oneside]{standalone}                      %
%----------------------------------Preamble------------------------------------%
\input{preamble.tex}                                                           %
                                                                               %
% Add tikz files to the file path.                                             %
\makeatletter                                                                  %
    \def\input@path{{../../../tikz/}}                                          %
\makeatother                                                                   %
%----------------------------------GLOSSARY------------------------------------%
\makeglossaries                                                                %
\loadglsentries{glossary}                                                      %
\loadglsentries{acronym}                                                       %
%--------------------------------Main Document---------------------------------%
\begin{document}
    \ifx\ifmain\undefined
        \pagenumbering{roman}
        \title{Special Functions}
        \author{Ryan Maguire}
        \date{\vspace{-5ex}}
        \maketitle
        \tableofcontents
        \setcounter{chapter}{6}
        \chapter{Special Functions}
        \pagenumbering{arabic}
    \else
        \chapter{Special Functions}
    \fi
    \section{The Error Function}
        There are algebraic functions, square roots, polynomials,
        logarithms, exponential, and trigonometric functions.
        The latter three are elementary transcendental functions,
        and special functions covers the rest. These are often
        defined by integrals, or as solutions to strange
        differential equations that arise from modelling. For
        example, the following arises when one models heat:
        \begin{align}
            \ddot{y}(x)+2x\dot{y}(x)&=0\\
            x\ddot{y}(x)+\dot{y}(x)&=0
        \end{align}
        The solutions to these are, respectively:
        \begin{align}
            y(x)&=C_{1}+C_{2}\Erf(x)\\
            y(x)&=C_{1}+C_{2}\ln(x)
        \end{align}
        The logarithmic function is usually studied in
        calculus in detail, and thus knowing this gives us a
        good idea as to what the graph of the solution looks like.
        However, much about logarithms is also glossed over.
        We may first see this when studying the power rule for
        integral powers:
        \begin{equation}
            \int{x}^{n}\diff{x}=\frac{1}{n+1}x^{n+1}+C
        \end{equation}
        And this fails for $n=-1$. However, we \textit{know}
        from Calculus that when $n=-1$ we obtain the natural
        logarithm. This is somewhat circular, since we actually
        \textit{define} logarithm as follows:
        \begin{equation}
            \ln(x)=\int_{1}^{x}t^{-1}\diff{t}
        \end{equation}
        This is somewhat cheating, as we really haven't obtained
        any knew information, we just have something to write
        when $n=-1$. Similarly, we can define $\Erf$ as:
        \begin{equation}
            \Erf(x)=\int_{0}^{x}\exp(-t^{2})\diff{t}
        \end{equation}
        In calculus, we are very happy with writing the integral
        of $x^{-1}$ is $\ln(x)$, but we are probably uncomfortable
        with writing $\Erf$ as the integral of $\exp(-x^{2})$,
        since one may very reasonably ask
        \textit{What the hell if Erf?}. But we can ask the same
        question for $ln(x)$. Thus, to know more about this function
        we can apply some of the basics from calculus and use the
        integral definition to learn the properties of $\ln$.
        We can come up with a few properties almost immediately:
        \begin{equation}
            \ln(1)=\int_{1}^{1}t^{-1}\diff{t}=0
        \end{equation}
        Using the fundamental theorem of calculus, we obtain:
        \begin{equation}
            \frac{\diff}{\diff{x}}\big(\ln(x)\big)
            =x^{-1}
        \end{equation}
        We can also study the integral, using integration by parts:
        \begin{equation}
            \int\ln(x)\diff{x}
            =x\ln(x)-\int\diff{x}
            =x\ln(x)-x+C
        \end{equation}
        It may be the case that we are unable to solve for the
        integral in terms of other elementary functions, in which
        case we may have had to define the integral of $\ln(x)$
        as another special function. Fortunately, we did not have
        to do this here. What about the most important properties
        of logarithms, such as the product rule?
        \begin{subequations}
            \begin{align}
                \ln(xy)&=\int_{1}^{xy}t^{-1}\diff{t}\\
                &=\int_{1}^{x}t^{-1}\diff{t}
                +\int_{x}^{xy}t^{-1}\diff{t}\\
                &=\ln(x)+\int_{1}^{y}(xu)^{-1}x\diff{u}\\
                &=\ln(x)+\int_{1}^{y}u^{-1}\diff{u}\\
                &=\ln(x)+\ln(y)
            \end{align}
        \end{subequations}
        This now gives us the power rule:
        \begin{equation}
            \ln(x^{2})=\ln(x\cdot{x})=\ln(x)+\ln(x)=2\ln(x)
        \end{equation}
        By induction:
        \begin{subequations}
            \begin{align}
                \ln(x^{n+1})&=\ln(x^{n}\cdot{x})\\
                &=\ln(x^{n})+\ln(x)\\
                &=n\ln(x^{n})+\ln(x)\\
                &=(n+1)\ln(x)
            \end{align}
        \end{subequations}
        By studying the definition of $\ln(x)$, we know that
        $\ln(2)>0$. We can thus determine the limiting behavior:
        \begin{equation}
            \ln(2^{n})=n\ln(2)\rightarrow{+\infty}
        \end{equation}
        The derivative is $x^{-1}$, which is always positive
        for $x>0$, and thus the function is monotonic. Moreover,
        the second derivative is $-x^{-2}$, which is always
        negative for $x>0$, and thus $\ln(x)$ is concave down.
        We can also determine the behavior as $x$ tends to
        zero from the right. We have the following:
        \begin{equation}
            \ln(1)=\ln(x\cdot{x}^{-1})=\ln(x)+\ln(x^{-1})
        \end{equation}
        But $\ln(1)=0$, and thus we have:
        \begin{equation}
            \ln(x)+\ln(x^{-1})=0
        \end{equation}
        Therefore:
        \begin{equation}
            \ln(2^{-n})=-n\ln(2)\rightarrow{-\infty}
        \end{equation}
        Monotonicity gives us that $\ln(x)\rightarrow{-\infty}$
        as $x\rightarrow{0^{+}}$. Finally, we can compute the
        Taylor series about $x=1$. Equivalently, let's compute
        the MacLaurin series for $\ln(x+1)$:
        \begin{align}
            \ln(1+x)&=\int_{1}^{1+x}t^{-1}\diff{t}\\
            &=\int_{0}^{u}\frac{1}{1+u}\diff{u}\\
            &=\int_{0}^{x}\sum_{n=0}^{\infty}(-u)^{n}\diff{u}\\
            &=\sum_{n=0}^{\infty}(-1)^{n}\int_{0}^{x}u^{n}\diff{u}\\
            &=\sum_{n=0}^{\infty}\frac{(-1)^{n}}{n+1}x^{n+1}
        \end{align}
        The radius of convergence of the integral of a power
        series does not change. The \textit{interval} may.
        Indeed, for the original series evaluating at
        $u=1$ is invalid, however for the integral, the series at
        $x=1$ is well define and evaluates to $\ln(2)$. We can
        use this series expansion for the purpose of computation
        for when $|x|<1$.
        \par\hfill\par
        We now have that the natural logarithm is well defined on
        $(0,\infty)$, the range of the function is
        $\mathbb{R}$, and that $\ln(x)$ is strictly monotonically
        increasing. Therefore there is an inverse. We call the
        inverse the \textit{exponential} function. Let's see
        if we can determine some properties of it:
        \begin{align}
            y&=\exp(x)\\
            x&=\ln(y)\\
            \Rightarrow\frac{\diff{x}}{\diff{x}}
            &=\frac{\diff}{\diff{x}}\ln(y)\\
            \Rightarrow\frac{1}{y}\frac{\diff{y}}{\diff{x}}&=1\\
            \Rightarrow\frac{\diff{y}}{\diff{x}}&=y
        \end{align}
        From this, we obtain almost all of the familiar rules for
        $\exp(x)$. Since $\ln(1)=0$, we have that $\exp(0)=1$.
        Using the derivative obtained, have have that, for all
        $n$, $y^{(n)}(0)=1$, and thus the Taylor series is:
        \begin{equation}
            \exp(x)=\sum_{n=0}^{\infty}\frac{x^{n}}{n!}
        \end{equation}
        Now let's return to the original differential equation,
        for which we wrote the solution as $C_{1}+C_{2}\Erf(x)$.
        \begin{equation}
            \ddot{y}(x)+2x\dot{y}(x)=0
        \end{equation}
        Let $u(x)=\dot{y}(x)$, so $\dot{u}(x)=\ddot{y}(x)$.
        Then we obtain:
        \begin{equation}
            \dot{u}(x)+2xu(x)=0
        \end{equation}
        This is separable, so we can solve for $u$ as follows:
        \begin{align}
            \int{u}^{-1}\diff{u}
            &=-2\int{2x}\diff{x}\\
            \Rightarrow\ln(u)&=-x^{2}+\ln(C_{1})\\
            \Rightarrow{u}&=C_{1}\exp(-x)^{2}
        \end{align}
        But $u(x)=\dot{y}(x)$, so we can solve for $y$ now:
        \begin{equation}
            y(x)=C_{1}\int\exp(-x^{2})\diff{x}+C_{2}
        \end{equation}
        What can we learn from this? Let's define the following:
        \begin{equation}
            \Erf(x)=\int_{0}^{x}\exp(-t^{2})\diff{t}
        \end{equation}
        From this, we obtain $\Erf(0)=0$, and for all
        $x$, $\Erf(-x)=-\Erf(x)$, since $\exp(-t^{2})$ is an
        even function. Looking at the derivative, we see that
        $\Erf'(x)=\exp(-x^{2})$, which is always positive, and
        thus $\Erf(x)$ is strictly monotonically increasing.
        The second derivative is $-2x\exp(-x^{2})$, which is
        negative for $x>0$ and positive for $x<0$. Thus $\Erf(x)$
        is concave up when $x<0$ and concave down when $x>0$.
        What happens as $x\rightarrow\infty$? We can use a
        familiar trick from Gauss to evaluate this. Consider
        the square of $\Erf(z)$:
        \begin{align}
            \Erf(z)^{2}
            &=\Big(\int_{0}^{z}\exp(-x^{2})\diff{x}\Big)
            \Big(\int_{0}^{z}\exp(-y^{-2})\diff{y}\Big)\\
            &=\int_{0}^{z}
                \Big(\int_{0}^{z}\exp(-x^{2})\diff{x}\Big)
                \exp(-y^{2})\diff{y}\\
            &=\int_{0}^{z}\int_{0}^{z}\exp(-(x^{2}+y^{2})
                \diff{x}\diff{y}
        \end{align}
        This is a square $A$ in the plane. From the positivity of
        $\exp(-(x^{2}+y^{2})$, the integral over $A$ is greater
        than or equal to integral over the circle that fits
        inside the square $A$, and less than or equal to the
        integral over the circle that contains the square $A$.
        Using this, we swap to polar coordinates. The radius
        of the inner circle is $z$, and the radius of the
        outer circle is $\sqrt{2}z$. So we have:
        \begin{align}
            \int_{0}^{\pi/2}\int_{0}^{z}r\exp(-r^{2})
                \diff{r}\diff{\theta}
            &\leq\int_{0}^{z}\int_{0}^{z}\exp(-(x^{2}+y^{2}))
                \diff{x}\diff{y}\\
            &\leq\int_{0}^{\pi/2}\int_{0}^{\sqrt{2}z}r\exp(-r^{2})
                \diff{r}\diff{\theta}
        \end{align}
        The left and right integrals can be evaluated, and we
        have:
        \begin{equation}
            \frac{\pi}{4}\Big[1-\exp(-z^{2})\Big]
            \leq\Erf(z)^{2}
            \leq\frac{\pi}{4}\Big[1-\exp(-2z^{2})\Big]
        \end{equation}
        From the squeeze theorem, we obtain:
        \begin{equation}
            \underset{x\rightarrow\infty}{\lim}
            \Erf(x)=\frac{\sqrt{\pi}}{2}
        \end{equation}
        We now redefine $\Erf$, and give it a name.
        \begin{definition}
            The Error Function is the function
            $\Erf:\mathbb{R}\rightarrow\mathbb{R}$ defined by:
            \begin{equation}
                \Erf(x)=\frac{2}{\sqrt{\pi}}
                    \int_{0}^{x}\exp(-t^{2})\diff{t}
            \end{equation}
        \end{definition}
        The coefficient out in front of the integral is to make
        $\Erf(x)$ asymptotic to $\pm{1}$ as $x\rightarrow\pm\infty$.
        The integral can also be evaluated in similar manner to
        how we evaluated the integral of $\ln(x)$:
        \begin{align}
            \int\Erf(x)\diff{x}
            &=x\Erf(x)-\frac{2}{\sqrt{\pi}}
                \int{x}\exp(-x^{2})\diff{x}\\
            &=x\Erf(x)+\frac{1}{\sqrt{\pi}}\exp(-x^{2}+C
        \end{align}
        Talking about the weirdness of Taylor series, look at
        $\exp(-1000)$. Taylor says that:
        \begin{equation}
            \exp(-1000)=1-1000+\frac{1000^{2}}{2!}-\hdots
        \end{equation}
        The first term is 1, second term is 1000, third term
        is 500,000, and this goes on. What the largest term?
        We see that the terms get bigger and bigger up until
        the $1000^{th}$ term. Using Sterling's approximation,
        we can see that this term is monumental in size,
        over 400 digits long. After this the terms get smaller
        and smaller, cancelling, until we have have
        something on the order of $10^{-400}$, meaning over
        800 digits cancelled. Woah. Ignoring that, we can
        continue and obtain the power series for $\Erf$:
        \begin{align}
            \Erf(x)&=\frac{2}{\sqrt{\pi}}
                \int_{0}^{x}\exp(-t^{2})\diff{t}\\
            &=\frac{2}{\sqrt{\pi}}\int_{0}^{\infty}
                \sum_{n=0}^{\infty}\frac{(-t^{2})^{n}}{n!}
                \diff{t}\\
            &=\frac{2}{\sqrt{\pi}}
                \sum_{n=0}^{\infty}\frac{(-1)^{n}}{n!}
                \int_{0}^{x}t^{2n}\diff{t}\\
            &=\frac{2}{\sqrt{\pi}}\sum_{n=0}^{\infty}
                \frac{(-1)^{n}}{n!}\frac{x^{2n+1}}{2n+1}
        \end{align}
        This confirms our suspicion that $\Erf(x)$ is an
        odd function. The alternating series test tells us how
        accurate any partial sum is, since the error in the
        approximation will be less than the magnitude of the
        last term. For example, if we want to know $\Erf(1)$,
        we have:
        \begin{equation}
            \Erf(1)\approx
                \sum_{n=0}^{N}\frac{(-1)^{n}}{n!(2n+1)}
        \end{equation}
        The error $E$ will be:
        \begin{equation}
            E<\Big|\frac{1}{(N+1)!(2N+3)}\Big|
        \end{equation}
        To obtain $\Erf(1)$ to high precision does not require
        using a substantial amount of terms. For large $x$,
        we also may use the inequality we obtained before:
        \begin{equation}
            1-\exp(-x^{2})\leq\Erf(x)\leq1-\exp(-2x^{2})
        \end{equation}
        So $\Erf(x)\approx{1}$, with error $\exp(-x^{2})$.
        More often in probability and in statistics one defines
        the \textit{complementary error function}
        $\Erfc(x)=1-\Erf(x)$. This is used when studying the
        tail of the normal distribution. We can obtain another
        series as well by using integration by parts:
        \begin{align}
            \frac{\sqrt{\pi}}{2}\Erf(x)
            &=\int_{0}^{x}\exp(-t^{2})\diff{t}\\
            &=x\exp(-x^{2})+\int_{0}^{x}2t\exp(-t^{2})\diff{t}\\
            &=\exp(-x^{2})\sum_{n=0}^{N}
                \frac{2^{n}}{(2n+1)!!}x^{2n+1}
            +\frac{2^{N+1}}{(2N+1)!!}
            \int_{0}^{x}t^{2n+2}\exp(-t^{2})\diff{t}
        \end{align}
        The double factorial sign means skip every other point.
        So $5!!=5\cdot{3}\cdot{1}$. For even numbers, we have:
        \begin{equation}
            (2n)!!=(2n)\cdot(2n-2)\cdots{2}
            =2^{n}n!
        \end{equation}
        Thus, for odd numbers, we can write:
        \begin{equation}
            (2n-1)!!=\frac{(2n)!}{(2n)!!}
            =\frac{(2n)!}{2^{n}n!}
        \end{equation}
        And thus the notation is redundant. We can also obtain a
        series expansion for $\Erfc(x)$.
        \begin{subequations}
            \begin{align}
                \Erfc(x)&=1-\Erf(x)\\
                &=1-\frac{2}{\sqrt{\pi}}
                    \int_{0}^{x}\exp(-t^{2})\diff{t}\\
                &=\frac{2}{\sqrt{\pi}}
                    \int_{0}^{\infty}\exp(-t^{2})\diff{t}
                    +\frac{2}{\sqrt{\pi}}\int_{0}^{x}
                    \exp(-t^{2})\diff{t}\\
                &=\frac{2}{\sqrt{\pi}}\int_{x}^{\infty}
                    \exp(-t^{2})\diff{t}
            \end{align}
        \end{subequations}
        Using this, we obtain a series:
        \begin{subequations}
            \begin{align}
                \frac{\sqrt{\pi}}{2}\Erfc(x)&=
                \int_{x}^{\infty}\exp(-t^{2})\diff{t}\\
                &=\frac{1}{2x}\exp(-x^{2})+
                \int_{x}^{\infty}\frac{1}{2t^{2}}\frac{1}{2t}
                \frac{\diff}{\diff{t}}
                    \Big(\exp(-t^{2})\Big)\diff{t}\\
                &=\frac{\exp(-t^{2})}{2}
                \sum_{n=0}^{N}(-1)^{n}\frac{(2n-1)!!}{(2x^{2})^{n}}
                +R_{N}
            \end{align}
            Where $R_{N}$ is the remainder term:
            \begin{equation}
                R_{N}=(-1)^{N}\frac{(2N-1)!!}{2^{N+3}}
                \int_{x}^{\infty}\frac{1}{t^{2N+2}}\exp(-t^{2})
                \diff{t}
            \end{equation}
        \end{subequations}
        $R_{N}$ diverges for any $x$. This series is useful
        for large $x$, however, since the factorial term that
        cause $R_{N}$ to diverge take a while to get large.
        Thus, $R_{N}$ decreasing for a while, and then eventually,
        once the factorial term is larger than the exponential
        terms, $R_{N}$ starts to decrease. Choosing the $N$ that
        minimizes $R_{N}$, we obtain a series that can very
        accurately approximate the nature of $\Erfc(x)$ for
        large $x$. This is similar to $\exp(-1000)$ where the
        terms got bigger and bigger, until the eventually cancel.
        We now have the opposite, where the terms get smaller
        and smaller, until $R_{N}$ starts to diverse of to
        infinity. If terms past the ``good'' $N$ are added,
        the series will fail to accurate represent $\Erfc(x)$.
        \par\hfill\par
        Make graph of $R_{N}$ vs $N$ for $\Erf$ and $\Erfc$.
        \par\hfill\par
        This type of series is an example of an asymptotic series.
        We can't write $f(x)=\sum{a}_{n}x^{n}$ since the
        series diverges, so we write
        $f(x)\sim\sum{a}_{n}(x)$ as $x\rightarrow\infty$.
        Taylor series say that the series approximation works
        well every as $n$ gets large. Asymptoptic series say
        that the approximation works well as the argument of
        $f$ gets large, rather than the number of terms.
        More precisely:
        \begin{equation}
            \underset{x\rightarrow\infty}{\lim}
            \frac{f(x)-\sum_{n=0}^{N}a_{n}(x)}{a_{N}(x)}=0
        \end{equation}
        As such, there are divergent as convergent asymptotic
        series. For example, consider the following:
        \begin{align}
            \frac{1}{1+x^{2}}
            &=\frac{1}{x^{2}}\frac{1}{1+\frac{1}{x^{2}}}\\
            &=\sum_{n=0}^{\infty}\frac{(-1)^{n}}{x^{n+2}}\\
            &=\sum_{n=0}^{\infty}(-1)^{n}x^{2n}
        \end{align}
        The third equation is an asymptotic series and the
        fourth equation is the Taylor series. The Taylor
        series has a radius of convergence of 1, and thus for
        all $x>1$, the Taylor series will fail. For
        $x>1$, however, the asymptotic expansion does a very
        good job
    \section{Lecture 2}
        We can approximate the error in the power series and
        asymptotic series expansions by looking at the largest
        and smallest terms, respectively. For the power series:
        \begin{align}
            \Big|\frac{a_{n+1}}{a_{n}}\Big|
            &=\frac{x^{2}}{n+1}\frac{2n+3}{2n+1}\\
            &\approx\frac{x^{2}}{n+1}
        \end{align}
        And this is largest when $n\sim{x}^{2}$.
        Using Stirling's Formula, the term is then:
        \begin{align}
            \frac{x^{2n+1}}{n!(2n+1)}
            &\approx\frac{x^{2n+1}}{(x^{2})!(2x^{2}+1)}\\
            &\approx\frac{x^{2x^{2}-1}}{2}
            \frac{1}{2\pi{x}^{2}(x^{2})^{(x^{2})}\exp(-x^{2})}\\
            &=\frac{1}{2}\frac{\exp(x^{2})}{x^{2}\sqrt{2\pi}}
        \end{align}
        Similarly, the lowest term in the asymptotic series
        can be obtained.
        \begin{align}
            \Big|\frac{a_{n+1}}{a_{n}}\Big|
            &=\frac{(2n+2)(2n+1)}{4(n+1)x^{2n+1}}\\
            &=\frac{2n+1}{2x^{2n+1}}\\
            &\approx\frac{n}{x^{2}}
        \end{align}
        So again, the smallest term occurs when
        $n\sim{x}^{2}$. The smallest term is approximately:
        \begin{align}
            \frac{(2x^{2})!}{2^{2x^{2}}(x^{2})!x^{2x^{2}+1}}
            &\approx
            \frac{\sqrt{2\pi{x}^{2}}(2x^{2})^{2x^{2}}\exp(-2x^{2}}
                 {2^{2x^{2}}\sqrt{2\pi{x}^{2}}(x^{2})^{x^{2}}
                  \exp(-x^{2})x^{2x^{2}+1}}\\
            &=\frac{\sqrt{2}\exp(-x^{2})}{x}
        \end{align}
        \subsection{Watson's Lemma}
            The Laplace transform of an integrable function $f$
            is defined as:
            \begin{equation}
                \mathscr{L}(f)_{s}=\int_{0}^{\infty}
                    f(t)\exp(-st)\diff{t}
            \end{equation}
            Watson's lemma says to substitute the MacLaurin
            series for $f$ and integrate term by term. For example,
            letting $f(x)=\Erfc(x)$, we have:
            \begin{align}
                \Erfc(x)&=\int_{x}^{\infty}\exp(-t^{2})\diff{t}\\
                &=\int_{0}^{\infty}\exp\big(-(s+x)^{2}\big)\diff{s}\\
                &=\exp(-x^{2})\int_{0}^{\infty}
                    \exp(-2sx)\exp(-s^{2})\diff{s}\\
                &\sim
                \exp(-x^{2})\sum_{n=0}^{\infty}\frac{(-1)^{n}}{n!}
                    \int_{0}^{\infty}s^{2n}\exp(-2sx)\diff{s}\\
                &=\exp(-x^{2})\sum_{n=0}^{\infty}
                    \frac{(-1)^{n}(2n)!}{2^{2n+1}n!x^{2n+1}}
            \end{align}
        \subsection{The Exponential Integral}
            The indefinite integral of the function
            $\exp(-x)/x$ can not be expressed in terms of
            elementary functions that one sees in a calculus course.
            Similar to how $\Erf$ was define, the exponential
            integral is defined as the \textit{solution} to this
            problem.
            \begin{equation}
                E_{1}(x)=\int_{x}^{\infty}\frac{\exp(-t)}{t}\diff{t}
            \end{equation}
            $E_{1}(x)$ is defined by an improper integral, and
            as such we must show that this is well defined for
            various values. That is, we must show the following
            limit is well defined:
            \begin{equation}
                E_{1}(x)=\underset{b\rightarrow\infty}{\lim}
                \int_{x}^{b}\frac{\exp(-t)}{t}\diff{t}
            \end{equation}
            Since $\exp(-t)/t$ is positive for all
            $t>0$, the area under the curve increases as $b$
            increases. That is, we have monotonicity. Also, for
            all $t\geq{x}$, $\exp(-t)/t\leq\exp(-t)/x$, and thus
            we obtain:
            \begin{equation}
                \int_{x}^{b}\frac{\exp(-t)}{t}\diff{t}
                \leq\int_{x}^{b}\frac{\exp(-t)}{x}\diff{t}
                =\frac{1}{x}\Big[\exp(-x)-\exp(-b)\Big]
            \end{equation}
            Taking the limit as $b\rightarrow\infty$, we see
            that the integral converges and thus $E_{1}(x)$ is
            well defined for all positive $x$. This is a combination
            of the comparison test and the fact that, if
            $f(b)$ is a monotonic bounded function, then it
            converges. Taking $f(b)$ to be the integral of
            $\exp(-t)/t$ over the interval $(x,b)$ gives us the
            result. From the integral definition, we can obtain
            many properties of $E_{1}(x)$. As $x$ increases, there
            is less area, and thus $E_{1}(x)$ is decreasing.
            We can also show this by taking the derivative, and
            observing that $-\exp(-x)/x$ is negative for all
            $x>0$. Looking at the second derivative, we get:
            \begin{equation}
                E_{1}''(x)=\exp(-x)\frac{x+1}{x^{2}}
            \end{equation}
            And this is positive for all $x>0$, and thus
            $E_{1}(x)$ is concave up. We can obtain a series for
            $E_{1}$ as follows:
            \begin{align}
                E_{1}'(x)&=-\frac{\exp(-x)}{x}\\
                &=-\frac{1}{x}\sum_{n=0}^{\infty}
                    \frac{(-1)^{n}x^{n}}{n!}\\
                \Rightarrow
                E_{1}(x)&=-\ln(x)-\sum_{n=1}^{\infty}
                    \frac{(-1)^{n}x^{n}}{nn!}+C
            \end{align}
            Where the $C$ comes out as a constant of integration.
            Evaluating $E_{1}(1)$ gives us this constant.
            We can't evaluate at zero since the log function is
            undefined there, and similarly for infinity.
            \begin{equation}
                C=\int_{1}^{\infty}\frac{\exp(-t)}{t}\diff{t}
                +\sum_{n=1}^{\infty}\frac{(-1)^{n}}{nn!}
            \end{equation}
            Using integration by parts, we have:
            \begin{equation}
                \int_{1}^{\infty}\frac{\exp(-t)}{t}\diff{t}
                =\int_{1}^{\infty}\ln(t)\exp(-t)\diff{t}
            \end{equation}
            Note that this does not say that $t^{-1}$ and
            $\ln(t)$ are the same function, but rather the
            area under $\exp(-t)t^{-1}$ and $\ln(t)\exp(-t)$
            are the same on the interval $(1,\infty)$.
            Let's look at the following function on $(0,1]$:
            \begin{equation}
                f(b)=\int_{b}^{1}\ln(x)\exp(-x)\diff{x}
            \end{equation}
            We can show this is well defined when
            $b\rightarrow{0}^{+}$ since:
            \begin{equation}
                \int_{b}^{1}\ln(t)\diff{t}
                \leq\int_{b}^{1}\ln(t)\exp(-t)\diff{t}
                \leq{0}
            \end{equation}
            And thus, we have:
            \begin{equation}
                -1+b(1-\ln(b))\leq{f}(b)\leq{0}
            \end{equation}
            Since $f$ is monotonic, the limit exists as
            $b\rightarrow{0}^{+}$. This again uses the comparison
            test for integrals. Getting back to our computation
            of $C$, we obtain a series for integrand of the function
            used in the definition of $f(b)$:
            \begin{align}
                \int_{0}^{1}\ln(t)\exp(-t)\diff{t}
                &=\int_{0}^{1}\ln(t)\sum_{n=0}^{\infty}
                    \frac{(-1)^{n}}{n!}t^{n}\diff{t}\\
                &=\sum_{n=0}^{\infty}\frac{(-1)^{n}}{n!}
                \int_{0}^{\infty}t^{n}\ln(t)\diff{t}
            \end{align}
            Examining that last integral, we get:
            \begin{equation}
                \int_{0}^{1}\ln(t)t^{n}\diff{t}
                =\int_{0}^{1}\ln(t)\frac{\diff}{\diff{t}}
                    \Big(\frac{t^{n+1}}{n+1}\Big)\diff{t}
            \end{equation}
            Applying integration by parts and L'H\"{o}pital's rule,
            we see that this simplifies to:
            \begin{equation}
                -\int_{0}^{1}\frac{1}{n+1}t^{n}\diff{t}
                =-\frac{1}{(n+1)^{2}}
            \end{equation}
            Thus, this original integral is:
            \begin{equation}
                \int_{0}^{1}\exp(-t)\ln(t)\diff{t}
                =\sum_{n=1}^{\infty}\frac{(-1)^{n}}{nn!}
            \end{equation}
            But from how $C$ was defined, we obtain:
            \begin{align}
                C&=\int_{1}^{\infty}\frac{\exp(-t)}{t}\diff{t}
                +\sum_{n=1}^{\infty}\frac{(-1)^{n}}{nn!}\\
                &=\int_{0}^{1}\ln(t)\exp(-t)\diff{t}
                +\sum_{n=1}^{\infty}\frac{(-1)^{n}}{nn!}\\
                &=\int_{0}^{\infty}\exp(-t)\ln(t)\diff{t}
            \end{align}
            This constant appears in many different areas of
            mathematics, and is given a symbol and a name.
            It's called the
            \textit{Euler-Mascheroni Constant}, and is
            denoted $\gamma$. It's also called the Euler Gamma
            constant. It's value is approximately $0.57721$. It is
            unknown whether this is an irrational number or not
            (As of January, 2019). With this we obtain our series
            for $E_{i}(x)$:
            \begin{equation}
                E_{i}(x)=-\gamma-\ln(x)
                -\sum_{n=1}^{\infty}\frac{(-1)^{n}}{nn!}x^{n}
            \end{equation}
            We can also examine the indefinite integral by using
            integration by parts:
            \begin{equation}
                \int{E}_{1}(x)\diff{x}=
                xE_{1}(x)+\exp(-x)+C
            \end{equation}
            Where $C$ is a (different) constant of integration.
            What happens as $x\rightarrow\infty$? We saw earlier
            that $E_{1}(x)$ is bounded by
            $\exp(-x)/x$, and thus the indefinite integral is well
            behaved in this limit. We could, however, continue
            using integration by parts and obtain an asymptotic
            series.
    \section{Lecture 3}
        \subsection{Fresnel Integrals}
            The Fresnel Integrals are defined as:
            \begin{align}
                S(x)&=\int_{0}^{x}\sin(t^{2})\diff{t}\\
                C(x)&=\int_{0}^{x}\cos(t^{2})\diff{t}
            \end{align}
            Occasionally there is a $\pi/2$ inside the sine and
            cosine functions. These functions appear in optics.
            It's not obvious that these integrals converge as
            $x\rightarrow\infty$, but they do:
            \begin{align}
                \underset{x\rightarrow\infty}{\lim}S(x)
                &=\frac{\sqrt{\pi}}{8}\\
                \underset{x\rightarrow\infty}{\lim}C(x)
                &=\frac{\sqrt{\pi}}{8}\\
            \end{align}
            The problem is that the integrands do not tend to
            zero, but rapidly oscillate between -1 and 1. If this
            was a series, we'd be done since the sequence being
            summed over must converge to zero if the series
            converges. For integrals it's slightly trickier and
            rapid cancellations can occur.
            \begin{align}
                \int_{1}^{\infty}\sin(t^{2})\diff{t}
                &=\underset{b\rightarrow\infty}{\lim}
                \int_{1}^{b}\sin(t^{2})\diff{t}\\
                &=\underset{b\rightarrow\infty}{\lim}
                \int_{1}^{b}-\frac{1}{2t}\frac{\diff}{\diff{t}}
                \big[\cos(t^{2})\big]\diff{t}\\
                &=\underset{b\rightarrow\infty}{\lim}
                \Big(\Big[-\frac{1}{2t}\cos(t^{2})\Big]_{1}^{b}
                -\int_{1}^{b}\frac{1}{2t^{2}}\cos(t^{2})\diff{t}
                \Big)\\
                &=\frac{1}{2}\cos(1)-
                \underset{b\rightarrow\infty}{\lim}
                \int_{1}^{b}\frac{1}{2t^{2}}\cos(t^{2})\diff{t}
            \end{align}
            So we need to show that this final integral converges.
            We can use the comparison test. For $t\geq{1}$, we have:
            \begin{align}
                \Big|\frac{1}{2t^{2}}\cos(t^{2})\Big|&\leq
                \frac{1}{2t^{2}}\\
                \underset{b\rightarrow\infty}{\lim}
                \int_{1}^{b}\frac{1}{2t^{2}}\diff{t}\\
                &=\frac{1}{2}
            \end{align}
            It follows that the limit exists. From this, we obtain
            the convergence of $S(x)$ as well. Similarly, $C(x)$
            converges. We can also evaluate these limits. We will
            need a simple result from complex variables known as
            \textit{Euler's Formula}:
            \begin{equation}
                \exp(ix)=\cos(x)+i\sin(x)
            \end{equation}
            This can be derived by showing that both sides of the
            equation satisfy $\dot{z}(t)=iz(t)$, $z(0)=1$. Since
            solutions to such differential equations are unique,
            we thus obtain Euler's Formula. As a side fact:
            \begin{align}
                \exp(ix)^{2}&=\exp(2ix)\\
                &=\big(\cos(x)+i\sin(x)\big)^{2}\\
                &=\big(\cos^{2}(x)-\sin^{2}(x)\big)+
                i\big(2\sin(x)\cos(x)\big)\\
                &=\cos(2x)+i\sin(2x)
            \end{align}
            All of the trig identities learned simply come from
            the exponent rules, and then applying Euler's formula.
            Now:
            \begin{equation}
                \int_{0}^{\infty}\exp(-t^{2})\diff{t}
                =\frac{\sqrt{\pi}}{2}
            \end{equation}
            et $t=\exp(i\pi/2)u$, and secretly using some complex
            variables, we obtain:
            \begin{align}
                \int_{0}^{\infty}\exp(-t^{2})\diff{t}
                &=\int_{0}^{\infty}
                \exp\Big(-i\frac{\pi}{2}u^{2}\Big)
                \exp\Big(i\frac{\pi}{4}\Big)\diff{u}\\
                &=\exp\Big(-i\frac{\pi}{4}\Big)
                \int_{0}^{\infty}\exp(iu^{2})\Big)\\
                &=\frac{1-i}{\sqrt{2}}\int_{0}^{\infty}
                \big(\cos(u^{2})+i\sin(u^{2})\big)\diff{u}\\
                \nonumber&=\frac{1}{\sqrt{2}}\int_{0}^{\infty}
                    \big(\cos(u^{2})+\sin(u^{2})\big)\diff{u}\\
                &\quad\quad\quad\quad
                    +\frac{i}{\sqrt{2}}\int_{0}^{\infty}
                \big(\sin(u^{2})-\cos(u^{2})\big)\diff{u}\\
                &=\frac{\sqrt{\pi}}{2}
            \end{align}
            Equating real and imaginary parts obtains the limits.
            We can obtain the power series for $S(x)$ by integrating
            the power series for $\sin(x^{2})$ term by term:
            \begin{align}
                S(x)&=\int_{0}^{x}\sin(t^{2})\diff{t}\\
                &=\int_{0}^{x}\sum_{n=0}^{\infty}
                \frac{(-1)^{n}}{(2n+1)!}t^{4n+2}\diff{t}\\
                &=\sum_{n=0}^{\infty}
                \frac{(-1)^{n}}{(2n+1)!}\frac{1}{4n+3}x^{4n+3}
            \end{align}
            For the asymptotic series, we note that:
            \begin{equation}
                S(x)=\frac{\sqrt{\pi}}{8}-
                \int_{x}^{\infty}\sin(t^{2})\diff{t}
            \end{equation}
            We use integration by parts to obtain the following
            series:
            \begin{equation}
                S(x)=
                \frac{\sqrt{\pi}}{8}-\frac{1}{2x}\cos(x^{2})+\cdots
            \end{equation}
        \subsection{Bernouli Polynomials}
            One question that is often avoided in a calculus course
            is the Taylor series for $\tan$. The sine, cosine,
            exponential, etc., functions are usually done, but
            $\tan$ and $\sec$ are often avoided. It turns out to be
            tricky, and we'll develop some methods to evaluate this.
            Recall the following from a calculus course:
            \begin{align}
                \sum_{k=0}^{n}k&=\frac{n(n+1)}{2}\\
                \sum_{k=0}^{n}k^{2}&=
                \frac{n(n+1)(2n+1)}{6}\\
                \sum_{k=0}^{n}k^{3}&=\frac{n^{2}(n+1)^{2}}{4}
            \end{align}
            There are different ways to show this, usually via
            induction. One way for the first equation is to count
            this twice:
            \begin{table}[]
                \centering
                \begin{tabular}{cc|c|c|c|c}
                    &1&2&$\cdots$&$n-1$&$n$\\
                    &$n$&$n-1$&$\cdots$&2&1\\
                    \hline
                    +&$n+1$&$n+1$&$\cdots$&$n+1$&$n+1$
                \end{tabular}
            \end{table}
            Noting that we double counted, we divide by 2 and
            obtain the formula.
            Jacques Beroulli evaluated, apparently in half of a
            quarter of an hour, the following sum:
            \begin{equation}
                N=\sum_{k=1}^{100}k^{10}
            \end{equation}
            First we need to find a polynonial $P_{n}(x)$ such that:
            \begin{equation}
                P_{n}(x+1)-P_{n}(x)=x^{n}
            \end{equation}
            For all $n\in\mathbb{N}$. Let's try this with a
            quadratic. Letting $P_{2}(x)=ax^{2}+bx+c$:
            \begin{align}
                x^{2}&\overset{\textrm{?}}{=}
                \big(a(x+1)^{2}+b(x+1)+c\big)-
                \big(ax^{2}+bx+c\big)\\
                &=2ax+a+b
            \end{align}
            So this is impossible since the quadratic term will
            cancel. So instead we try a cubic term:
            \begin{align}
                x^{2}&=
                \big(a(x+1)^{3}+b(x+1)^{2}+c(x+1)+d)-
                \big(ax^{3}+bx^{2}+cx+d)\\
                &=a(3x^{2}+3x+1)+b(2x+1)+c
            \end{align}
            So we obtain the following:
            \begin{align}
                a&=\frac{1}{3}\\
                b&=-\frac{1}{2}\\
                c&=\frac{1}{6}
            \end{align}
            The reason this property is so nice is because we can
            obtain a telescoping series with it.
            \begin{equation}
                1^{n}+\cdots+N^{n}
                =\big(P_{n}(2)-P_{n}(1)\big)+\cdots+
                \big(P_{n}(N+1)-P_{n}(N)\big)
            \end{equation}
            Combining this, we obtain:
            \begin{equation}
                P_{n}(N+1)-P_{n}(1)=\sum_{k=0}^{N}k^{n}
            \end{equation}
            Other than evaluating directly, we can calculate the
            polynomial $P_{n}$ in a more efficient manner. First,
            we differentiate:
            \begin{align}
                P_{n}'(x+1)-P_{n}'(x)&=nx^{n-1}\\
                &=n\big[P_{n-1}(x+1)-P_{n-1}(x)\big]
            \end{align}
            We then obtain:
            \begin{equation}
                P_{n}'(x+1)-nP_{n-1}(x+1)=P_{n}'(x)-nP_{n-1}(x)
            \end{equation}
            But the derivative of a polynomial is another
            polynomial, and thus we have that the polynomial
            $F(x)=P_{n}'(x)-nP_{n-1}(x)$ is such that
            $F(0)=F(1)=\cdots=F(n)=\cdots$. But the only polynomial
            that is constant on all of the integers is a constant
            polynomial. For any polynomial that is not constant
            must diverge. Thus $F(x)=c$. Another way to see this
            is to note that if two polynomials of degree $n$
            agree on $n+1$ points, then they are the same
            polynomial. This again means that $F(x)$ is a constant.
            So, we have:
            \begin{equation}
                P_{n}'(x)-nP_{n-1}(x)=c
            \end{equation}
            Note that the defining property of $P_{n}$ is that
            $P_{n}(x+1)-P_{n}(x)=x^{n}$. Adding any constant to
            $P_{n}$ will yield the same property. So we can define
            our polynomials to be the ones such that
            $P_{n}'(x)-nP_{n-1}(x)=0$. There is only one polynomial
            of degree $n$ that will do this, so we define this as
            the $n^{\textrm{th}}$ Bernouli polynomial $B_{n}(x)$.
            This is not the only \textit{function} that will do
            this, since $B_{n}(x)+\sin(k\pi{x})$ will also have
            all of this property.
            \begin{ldefinition}{Bernouli Polynomial}
                The $n^{th}$ Bernouli polynomial is the polynomial
                of degree $n$ such that:
                \begin{equation}
                    \int_{x}^{x+1}B_{n}'(t)\diff{t}=x^{n}
                \end{equation}
            \end{ldefinition}
            Several properties arise immediately from this:
            \begin{align}
                \int_{0}^{1}B_{n}(t)\diff{t}&=0\\
                B_{n}'(x)&=nB_{n-1}(x)
            \end{align}
            The first few polynomials are:
            \begin{table}[H]
                \captionsetup{type=table}
                \centering
                \begin{tabular}{l|l}
                    $n$&$B_{n}(x)$\\
                    \hline
                    0&1\\
                    1&$x-\frac{1}{2}$\\
                    2&$x^{2}-x+\frac{1}{6}$\\
                    3&$x^{3}-\frac{3}{2}x^{2}+\frac{1}{2}x$
                \end{tabular}
                \caption{Bernouli Polynomials}
                \label{tab:Special_Functions_BERNOULI_POLY}
            \end{table}
            \begin{theorem}
                The $n^{th}$ Bernouli polynomial
                is a monic polynomial of degree $n$.
            \end{theorem}
            \begin{proof}
                We prove by induction. The base case is true since
                $B_{0}(x)=1$. Suppose it is true for $B_{n}(x)$.
                Then $B_{n}(x)=x^{n}+p_{n-1}(x)$, where
                $p_{n-1}(x)$ is a polynomial of degree $n-1$.
                But then:
                \begin{equation}
                    B_{n+1}'(x)=n\big(x^{n}+p_{n-1}(x)\big)
                \end{equation}
                Integrating, we get that $B_{n+1}(x)$ is a monic
                polynomial of degree $n+1$.
            \end{proof}
            \begin{theorem}
                If $B_{n}(x)$ is the $n^{th}$ Bernouli polynomial,
                then the coefficient of the degree $n-1$ term is
                $-n/2$.
            \end{theorem}
            \begin{proof}
                Again, by induction. The base case is true from
                Tab.~\ref{tab:Special_Functions_BERNOULI_POLY}.
                Suppose it is true for $B_{n}(x)$. Then:
                \begin{align*}
                    B_{n+1}(x)&=
                    \int(n+1)B_{n}(x)\diff{x}\\
                    &=\int(n+1)
                    \big[x^{n}-\frac{n}{2}x^{n-1}+\cdots\big]
                    \diff{x}\\
                    &=x^{n+1}-\frac{n+1}{2}x^{n}+\cdots
                \end{align*}
            \end{proof}
    \section{Lecture 4}
        \subsection{Bernoulli Polynomials}
            So far we know that $B_{0}(x)=1$, 
            $B_{n}'(x)=nB_{n-1}(x)$, for $n\in\mathbb{N}$.
            Sequences like this are called Appell sequences.
            The definining property is
            $B_{n}(x+1)-B_{n}(x)=nx^{n-1}$. From this, we obtain:
            \begin{equation}
                \sum_{k=1}^{n}k^{m}=
                \frac{1}{n+1}\Big[B_{n+1}(m+1)-B_{n}(1)\Big]
            \end{equation}
            For $n\geq{2}$:
            \begin{equation}
                B_{n}(1)-B_{n}(0)=0
            \end{equation}
        \subsection{Bernoulli Numbers}
            The $n^{th}$ Bernoulli number is defined
            as $b_{n}=B_{n}(0)$, where $B_{n}$ is the
            $n^{th}$ Bernoulli polynomial. These appear in
            the expanion of the Bernoulli polynomials as well.
            For let:
            \begin{equation}
                B_{n}(x)=\sum_{k=0}^{n}C_{n}^{k}x^{k}
            \end{equation}
            Taking the derivative, we have:
            \begin{subequations}
                \begin{align}
                    B_{n}'(x)
                    &=\sum_{k=1}^{n}C_{n}^{k}kx^{k-1}\\
                    &=nB_{n-1}(c)\\
                    &=n\sum_{k=0}^{n-1}C_{n-1}^{k}x^{k}\\
                    &=n\sum_{k=1}^{n}C_{n-1}^{k-1}x^{k-1}
                \end{align}
            \end{subequations}
            So we have a recurrance relation for the coefficients:
            \begin{equation}
                kC_{k}^{n}=nC_{k-1}^{n-1}
            \end{equation}
            We can keep applying this to obtain:
            \begin{equation}
                C_{k}^{n}=\frac{n}{k}C_{k-1}^{n-1}
                =\frac{n(n-1)}{k(k-1)}C_{k-2}^{n-2}=\cdots
                =\frac{n(n-1)\cdot(n-(k-1))}{k(k-1)\cdots(k-(k-1))}
                C_{0}^{n-k}
            \end{equation}
            This coefficient is the \textit{binomial coefficient}
            $\binom{n}{k}$, read as $n$ choose $k$. So we have:
            \begin{equation}
                C_{k}^{n}=\binom{n}{k}C_{0}^{n-k}
            \end{equation}
            Therefore:
            \begin{equation}
                B_{n}(x)=\sum_{k=0}^{n}
                \binom{n}{k}C_{0}^{n-k}x^{k}
            \end{equation}
            Evaluating at $x=0$, we get:
            \begin{equation}
                C_{0}^{n}=b_{n}
            \end{equation}
            Where $b_{n}$ is the $n^{th}$ Bernoulli number.
            But from this, we obtain:
            \begin{equation}
                C_{k}^{n}=\binom{n}{k}b_{n-k}
            \end{equation}
            Thus, putting this all together, we have:
            \begin{equation}
                B_{n}(x)=\sum_{k=0}^{n}
                \binom{n}{k}B_{n-k}x^{k}
            \end{equation}
            We can then flip the indexing to obtain the following:
            \begin{align}
                B_{n}(x)&=
                \sum_{k=0}^{n}\binom{n}{k}B_{n-k}x^{k}\\
                &=\sum_{k=0}^{n}\binom{n}{n-k}
                B_{n-(n-k)}x^{n-k}\\
                &=\sum_{k=0}^{n}\binom{n}{n}B_{k}x^{n-k}
            \end{align}
            This then gives us a recurrence relation that can be
            used to define the Bernoulli numbers, without first
            defining the Bernoulli polynomials. Let
            $B_{0}=1$, $B_{1}=-\frac{1}{2}$, and then for
            $n\geq{2}$, define:
            \begin{equation}
                B_{n}=\sum_{k=0}^{n}
                \binom{n}{k}B_{k}
            \end{equation}
            Replacing $n$ with $n+1$, we get:
            \begin{equation}
                B_{n+1}=\sum_{k=0}^{n+1}
                \binom{n}{k}B_{k}
                =\sum_{k=0}^{n}
                \binom{n}{k}B_{k}+B_{n+1}
            \end{equation}
            From this we can conclude:
            \begin{equation}
                \sum_{k=0}^{n}\binom{n+1}{k}B_{k}=0
            \end{equation}
            So, finally:
            \begin{equation}
                B_{n}=-\frac{1}{n+1}\sum_{k=0}^{n-1}
                \binom{n+1}{k}B_{k}
            \end{equation}
        \subsection{Generating Functions}
            A generating function for a sequence is a function
            whose power series has the terms of the sequence
            for its coefficients.
            \begin{equation}
                \frac{1}{1-x}=1+x+x^{2}+\cdots
            \end{equation}
            The coefficients of this power series are all one,
            and so this is a generating function for the sequence
            $a:\mathbb{N}\rightarrow\mathbb{R}$ defined by
            $a_{n}=1$. We can differentiate this:
            \begin{equation}
                \frac{1}{(1-x)^{2}}=1+2x+3x^{2}+\cdots
            \end{equation}
            So this is a generating function for
            $1,2,3,4,\dots$. As another example:
            \begin{equation}
                \exp(2x)=1+2x+\frac{2^{2}}{2!}x^{2}
                +\frac{2^{3}}{3!}x^{3}+\cdots
            \end{equation}
            And so $\exp(2x)$ is a generating function for the
            sequence $a:\mathbb{N}\rightarrow\mathbb{R}$ defined by
            $a_{n}=\frac{2^{n}}{n!}$. Moreover, since the
            coefficients of Taylor series for a function are
            uniquely defined by the function:
            \begin{equation}
                \exp(2x)=\sum_{n=0}^{\infty}a_{n}x^{n}
                \Leftrightarrow{a}_{n}=\frac{2^{n}}{n!}
            \end{equation}
            Using this, we can obtain:
            \begin{align}
                \frac{\diff}{\diff{x}}\Big(\exp(2x)\Big)
                &=2\exp(2x)\\
                &=\sum_{n=1}^{\infty}na_{n}x^{n-1}\\
                &=\sum_{n=0}^{\infty}(n+1)a_{n+1}x^{n}\\
                &=2\sum_{n=0}^{\infty}a_{n}x^{n}
            \end{align}
            Since two power series are equal if and only if there
            coefficients are equal, we obtain the following:
            \begin{equation}
                a_{n+1}=\frac{2}{n+1}a_{n}
            \end{equation}
            Recurrence relations like this are often why it is
            useful to study generating functions. We now study
            the generating functions of the Bernoulli Polynomial.
            \begin{equation}
                F(x,t)=\sum_{n=0}^{\infty}
                \frac{B_{n}(x)}{n!}t^{n}
            \end{equation}
            Let's now derive what $F$ is. Differentiating with
            respect to $x$, we get:
            \begin{align}
                \frac{\partial{F}}{\partial{x}}
                &=\sum_{n=1}^{\infty}
                \frac{B_{n}'(x)}{n!}t^{n}\\
                &=\sum_{n=1}^{\infty}
                    \frac{nB_{n-1}(x)}{n!}t^{n}\\
                &=t\sum_{n=1}^{\infty}
                    \frac{B_{n-1}(x)}{(n-1)!}t^{n-1}
                &=t\sum_{n=0}^{\infty}
                    \frac{B_{n}(x)}{n!}t^{n}\\
                &=tF(x,t)
            \end{align}
            So $F$ satisfies the equation $F_{x}(x,t)=tF(x,t)$.
            Thus there is some function $C(t)$ such that:
            \begin{equation}
                F(x,t)=C(t)\exp(xt)
            \end{equation}
            Where the $C(t)$ is obtained as a \textit{function}
            of integration, rather than a constant of integration,
            since we are taking partial derivatives. Integrating,
            we have:
            \begin{align}
                \int_{0}^{1}F(x,t)\diff{x}
                &=\int_{0}^{1}\sum_{n=0}^{\infty}
                    \frac{B_{n}(x)}{n!}t^{n}\diff{x}\\
                &=\sum_{n=0}^{\infty}\frac{t^{n}}{n!}
                \int_{0}^{1}B_{n}(x)\diff{x}\\
                &=1
            \end{align}
            This is because, for $n\geq{1}$,
            $\int_{0}^{1}B_{n}(x)\diff{x}=1$. Thus, we have:
            \begin{align}
                \int_{0}^{1}C(t)\exp(xt)\diff{x}&=
                C(t)\Big[\frac{1}{t}\exp(xt)\Big]_{x=0}^{1}\\
                &=\frac{C(t)}{t}\Big[\exp(t)-1\Big]\\
                &=1
            \end{align}
            And therefore:
            \begin{equation}
                C(t)=\frac{t}{\exp(t)-1}
            \end{equation}
            Thus, the generating function for the Bernoulli
            polynomials is:
            \begin{equation}
                \sum_{n=0}^{\infty}
                \frac{B_{n}(x)}{n!}t^{n}
                =\frac{t}{\exp(t)-1}\exp(xt)
            \end{equation}
            This can be used to show the following:
            \begin{align}
                \frac{t}{\exp(t)-1}
                \exp\big((1-x)t\big)
                &=\sum_{n=0}^{\infty}
                    \frac{B_{n}(1-x)}{n!}t^{}\\
                &=\frac{t}{\exp(t)-1}\exp(t)\exp(-xt)\\
                &=\frac{t}{1-\exp(-t)}\exp(-xt)\\
                &=\frac{(-t)}{\exp(-t)-1}\exp(x(-t))\\
                &=\sum_{n=0}^{\infty}(-1)^{n}
                \frac{B_{n}(1-x)}{n!}t^{n}
            \end{align}
            Comparing coefficients, we get:
            \begin{equation}
                B_{n}(x)=(-1)^{n}B_{n}(1-x)
            \end{equation}
            From this we see that, for $n\geq{0}$, and for all
            odd $n$, $b_{n}=0$, where $b_{n}$ is the
            $n^{th}$ Bernoulli number. We can use this to simplify
            the recurrence relation for $b_{n}$:
            \begin{align}
                b_{n}&=\frac{-1}{n+1}
                \sum_{k=0}^{n-1}\binom{n+1}{k}b_{k}\\
                b_{2n}&=\frac{-1}{2n+1}
                \sum_{k=0}^{2n-1}\binom{2n+1}{k}b_{k}\\
                &=\frac{-1}{2n+1}\sum_{k=0}^{n-1}
                    \binom{2n+1}{2k}b_{2k}
                    -\frac{1}{2n+1}\binom{2n+1}{1}B_{1}\\
                    &=\frac{1}{2}-\frac{1}{2n+1}
                    \sum_{k=0}^{n-1}\binom{2n+1}{2k}b_{2k}
            \end{align}
            Returning to the generating function, evaluate
            $F(x,t)$ at $x=1$. We get:
            \begin{align}
                \sum_{n=1}^{\infty}\frac{B_{n}(1)}{n!}t^{n}\\
                &=\frac{1}{2}t+\sum_{n=0}^{\infty}
                \frac{B_{2n}}{(2n)!}t^{2n}\\
                &=\frac{t\exp(t)}{\exp(t)-1)}
            \end{align}
            From this, we get:
            \begin{equation}
                \sum_{n=0}^{\infty}\frac{B_{2n}}{(2n)!}t^{2n}
                =-\frac{t}{2}+\frac{t\exp(t)}{\exp(t)-1}
            \end{equation}
            Which is an even function, quite bizarrely.
            We can use this to get:
            \begin{equation}
                \sum_{n=0}^{\infty}
                \frac{2^{2n}b_{2n}}{(2n)!}i^{2n}t^{2n}
                =-it\frac{\exp(it)+\exp(-it)}{\exp(it)-\exp(-it)}
            \end{equation}
            Using Euler's Formula, this last part becomes
            $t\cot(t)$. So, we have:
            \begin{equation}
                t\cot(t)=\sum_{n=0}^{\infty}
                (-1)^{n}\frac{2^{2n}b_{2n}}{(2n)!}t^{2n}
            \end{equation}
            From this, we obtain:
            \begin{equation}
                \cot(t)=\frac{1}{t}+\sum_{n=0}^{\infty}
                (-1)^{n}\frac{2^{2n}b_{2n}}{(2n)!}t^{2n-1}
            \end{equation}
            But noting that $\cot(t)-2\cot(t)=\tan(t)$, we can
            obtain the Taylor series for $\tan$:
            \begin{equation}
                \tan(t)=\sum_{n=0}^{\infty}
                (-1)^{n+1}\frac{2^{2n}b_{2n}}{(2n)!}(2^{n}-1)t^{2n-1}
            \end{equation}
            But the coefficients of the Taylor series of $\tan$
            are the derivatives of $\tan$ evaluated at the origin,
            and divided by $n!$. And this is always positive. From
            this we conclude that $b_{2n}$ oscillates from positive
            to negative as $n$ increasing to cancel out the
            $(-1)^{n+1}$ term.
    \section{Lecture 5}
        \begin{align}
            \int{Ci(x)}\diff{x}&=xCi(x)-\sin(x)+c\\
            \int_{0}^{\infty}Ci(x)\diff{x}
            &=\big[xCi(x)-\sin(x)\big]_{0}^{\infty}\\
            &=\underset{b\rightarrow\infty}{\lim}
                \big(xCi(x)-\sin(x)\big)-
                \underset{a\rightarrow{0}}{\lim}
                \big(xCi(x)-\sin(x)\big)
        \end{align}
        But:
        \begin{align}
            Ci'(x)&=\frac{\cos(x)}{x}\\
                &=\frac{1}{x}\sum_{n=0}^{\infty}
                \frac{(-1)^{n}}{(2n)!}x^{2n}\\
            Ci(x)&=\ln(x)+\sum_{n=1}^{\infty}
            \frac{(-1)^{n}}{2n(2n)!}x^{2n}+\gamma\\
            \underset{a\rightarrow{0}}{\lim}(xCi(x)-\sin(x))&=0\\
        \end{align}
        Using the asymptotic expansion:
        \begin{equation}
            Ci(x)=\frac{1}{x}\sin(x)-\frac{1}{x^{2}}\cos(x)+\cdots
        \end{equation}
        Back to Bernoulli Polynomials.
        \begin{align}
            B_{0}(x)&=1\\
            B_{n}'(x)&=nB_{n-1}(x)\\
            \int_{0}^{1}B_{n}(x)\diff{x}&=0
        \end{align}
        \subsection{Half-Range Fourier Series}
            On the interval $(0, L)$, and if $g$ is piece-wise smooth
            and with bounded derivative, then:
            \begin{align}
                f(x)&=\sum_{n=1}^{\infty}b_{n}\sin(\frac{n\pi}{L}x)\\
                &=\frac{a_{0}}{2}+\sum_{n=1}^{\infty}
                    a_{n}\cos(\frac{n\pi}{L}x)
            \end{align}
            Where the constant's $a_{n}$ and $b_{n}$ are determinded by:
            \begin{align}
                a_{n}&=\frac{2}{L}
                    \int_{0}^{L}f(x)\cos(\frac{n\pi}{L}x)\diff{x}\\
                b_{n}&=
                    \int_{0}^{L}f(x)\sin(\frac{n\pi}{L}x)\diff{x}
            \end{align}
            The Fourier series converges to $f$ everywhere except at the
            jumps, where it converges to the middle of the jumps.
            \begin{lexample}
                Find the Fourier Sine Series for $f(x)=x-1/2$ on $(0,1)$.
                \begin{align}
                    f(x)&=\sum_{n=1}^{\infty}b_{n}\sin(n\pi{x})\\
                    b_{n}&=2\int_{0}^{1}f(x)\sin(n\pi{x})\diff{x}\\
                    &=2\int_{0}^{1}(x-\frac{1}{2})\sin(n\pi{x})\diff{x}\\
                    &=\Big[
                        \minus{2}\frac{x-\frac{1}{2}}{n\pi}\cos(n\pi{x}
                    \Big]_{0}^{1}+
                    \int_{0}^{1}\frac{1}{n\pi}\cos(n\pi{x})\diff{x}\\
                    &=\Big[
                        \frac{1-2x}{n\pi}\cos(n\pi{x})+
                        \frac{1}{n^{2}\pi^{2}}\sin(n\pi{x})
                    ]_{0}^{1}\\
                    &=\minus\frac{1}{n\pi}(1+\cos(n\pi))\\
                    &==\minus\frac{1}{n\pi}(1+(-1)^{n})
                \end{align}
                If we plot this we will see some overshoot at the endpoints.
                This is a result of something called Gibb's Phenomenon.
            \end{lexample}
            Find the Fourier Cosine Series of $f(x)=\cos(ax)$ on
            $(0,\pi)$, where $a\in(0,1)$.
            \begin{align}
                \cos(ax)&=\frac{a_{0}}{2}+
                    \sum_{n=1}^{\infty}a_{n}\cos(nx)\\
                a_{n}&=\frac{2}{\pi}
                    \int_{0}^{\pi}\cos(ax)\cos(n\pi{x})\diff{x}\\
                    &=\frac{2}{\pi}
                        \frac{a(-1)^{n}}{a^{2}-n^{2}}\sin(\pi{a})
            \end{align}
            Dividing off by the $\sin(a\pi)$, we get:
            \begin{align}
                \frac{\cos(ax)}{\sin(a\pi)}
                &=\frac{1}{a\pi}+\frac{2a}{\pi}
                    \sum_{n=1}^{\infty}\frac{(-1)^{n}}{a^{2}-n^{2}}
                    \cos(nx)\\
                \cot(a\pi)&=
                    \frac{1}{a\pi}+\frac{2a}{\pi}
                    \sum_{n=1}^{\infty}\frac{1}{a^{2}-n^{2}}
            \end{align}
            Bringing the first term over to the left and then integrating,
            we get:
            \begin{align}
                \int_{0}^{x}\big(\cos(a\pi)-\frac{1}{a\pi}\big)\diff{a}
                &=\int_{0}^{x}\frac{2a}{\pi}\sum_{n=1}^{\infty}
                    \frac{1}{a^{2}-n^{2}}\\
                &=\frac{2}{\pi}\sum_{n=1}^{\infty}\int_{0}^{x}
                    \frac{a}{a^{2}-n^{2}}\diff{a}\\
                &=\frac{1}{\pi}\sum_{n=1}^{\infty}
                    \ln\Big[n^{2}-a^{2}\Big]_{0}^{x}\\
                &=\frac{1}{\pi}\sum_{n=1}^{\infty}
                    \ln\Big(\frac{n^{2}-x^{2}}{n^{2}}\Big)\\
                \int_{0}^{x}\big(\cos(a\pi)-\frac{1}{a\pi}\big)\diff{a}
                &=\frac{1}{\pi}\Big[
                    \ln\big(\sin(\pi{a})\big)-\ln(a)
                \Big]_{0}^{x}\\
                &=\frac{1}{\pi}\ln\Big(\frac{\sin(\pi{x})}{x}\Big)-
                    \underset{a\rightarrow{0}^{+}}{\lim}
                    \ln\Big(\frac{\sin(\pi{a})}{a}\Big)\\
                &=\frac{1}{\pi}\ln\Big(\frac{\sin(\pi{x})}{x}\Big)-
                    \frac{1}{\pi}\ln(\pi)\\
                &=\frac{1}{\pi}\ln\Big(\frac{\sin(\pi{x})}{\pi{x}}\Big)
            \end{align}
            Thus, we have:
            \begin{align}
                \ln\Big(\frac{\sin(\pi{x})}{\pi{x}}\Big)
                &=\sum_{n=1}^{\infty}
                    \ln\Big(\frac{n^{2}-x^{2}}{n^{2}}\Big)\\
                \frac{\sin(\pi{x})}{\pi{x}}
                &=\prod_{n=1}^{\infty}\Big(1-\frac{x^{2}}{n^{2}}\Big)\\
                &=1-x^{2}\sum_{n=1}^{\infty}\frac{1}{n^{2}}+\cdots
            \end{align}
            We can use the Taylor expansion for $\sin(\pi{x})$ as:
            \begin{align}
                \frac{\sin(\pi{x})}{\pi{x}}&=
                \frac{1}{\pi{x}}\Big(\sum_{n=0}^{\infty}
                    \frac{(-1)^{n}}{(2n+1)!}x^{2n+1}\Big)\\
                &=1-\frac{\pi^{2}}{6}x^{2}+\cdots
            \end{align}
            Subtracting 1 and dividing by $x^{2}$, we get:
            \begin{equation}
                \frac{\pi^{2}}{6}+\cdots
                =\sum_{n=1}^{\infty}\frac{1}{n^{2}}+\cdots
            \end{equation}
            Evaluating at $x=0$, we get:
            \begin{equation}
                \sum_{n=1}^{\infty}\frac{1}{n^{2}}=\frac{\pi^{2}}{6}
            \end{equation}
            Now, to get the Fourier Sine series of the Bernoulli polynomials.
            We have the first one:
            \begin{equation}
                B_{1}(x)=\minus\sum_{n=1}^{\infty}\frac{1}{n\pi}\sin(2n\pi{x})
            \end{equation}
            Using the fact that $B_{n}'(x)=nB_{n-1}(x)$ we can obtain the
            Fourier Sine series for the other polynomials by induction.
            Integrating across, we get:
            \begin{equation}
                B_{2}(x)=\sum_{n=1}^{\infty}\frac{4}{(2\pi{n})^{2}}
                    \cos(2n\pi{x})
            \end{equation}
            This is only true on the interval $x\in(0,1)$. Since 
            $B_{n}$ is a polynomial it must diverge as
            $x\rightarrow\pm\infty$, and thus cannot be periodic. By
            induction, we obtain the rest:
            \begin{align}
                B_{2k}(x)&=
                    2\sum_{n=1}^{\infty}
                        \frac{(-1)^{n}(2k)!}{(2\pi{n})^{2k}}\cos(2n\pi{x})\\
                B_{2k+1}(x)&=
                    2\sum_{n=1}^{\infty}
                        \frac{(-1)^{n}(2k+1)!}{(2\pi{n})^{2k+1}}
                            \sin(2n\pi{x})\\
            \end{align}
            We can use this for the even Bernoulli numbers:
            \begin{equation}
                |b_{2k}|=2(2k)!\sum_{n=1}^{\infty}
                    \frac{1}{(2\pi)^{2k}}\frac{1}{n^{2k}}
            \end{equation}
            From this, we have:
            \begin{equation}
                \sum_{n=1}^{\infty}\frac{1}{n^{2k}}
                =\frac{(2\pi)^{2k}|B_{2k}|}{2(2k)!}
            \end{equation}
            Stuff about the Riemann Zeta function. Critical strip,
            trivial zeros, prime number theorem.
            \begin{align}
                |B_{2k}(x)|&=
                    \Big|2(-1)^{k+1}(2k)!\sum_{n=1}^{\infty}
                    \frac{\cos(2n\pi{x})}{(2n\pi)^{2k}}\Big|\\
                    &\leq2(2k)!\sum_{n=1}^{\infty}\frac{1}{(2\pi{n})^{2k}}\\
                    &=|b_{2k}|
            \end{align}
            So the Bernoulli polynomials attain their maximum or minimum
            at the origin.
    \section{Lecture 6}
        We have obtained the following power series:
        \begin{align}
            \tan(x)&=\sum_{n=1}^{\infty}
                \frac{2^{2n}(2^{2n}-1)|B_{2n}|}{(2n)!}x^{2n-1}\\
            \sec(x)&=\sum_{n=0}^{\infty}
                \frac{|A_{2n}|}{(2n)!}x^{2n}
        \end{align}
        Stuff about Boustrophedon transform of 1,0,0,0,...
        Using this to expand $\tan$ and $\sec$. Zigzag numbers,
        up/down numbers. $\tan+\sec$ is the sum of the
        $n^{th}$ zigzag over $n!$, with $x^{n}$.
        Some stuff about integration by parts:
        \begin{align}
            \int_{0}^{1}f(x)\diff{x}
            &=xf(x)\big|_{0}^{1}-\int_{0}^{1}xf'(x)\diff{x}\\
            &=xf(x)\big|_{0}^{1}-\int_{0}^{1}
                f'(x)\diff\big(\frac{1}{2}x^{2}\big)\\
            &=\Big[xf(x)-\frac{1}{2}x^{2}f'(x)\Big]_{0}^{1}+
                \int_{0}^{1}f''(x)\diff\big(\frac{1}{6}x^{3}\big)\\
            &=\Big[
                xf(x)-\frac{1}{2}x^{2}f'(x)+\frac{1}{6}x^{3}f''(x)
            \Big]_{0}^{1}-\int_{0}^{1}\frac{1}{6}x^{3}f'''(x)\diff{x}
        \end{align}
        And in general, if $f$ is $N$ times differentiable:
        \begin{equation}
            \int_{0}^{1}f(x)\diff{x}=
            \sum_{n=0}^{N}\frac{(\minus{1})^{n}f^{(n)}(x)}{(n+1)!}x^{n+1}
                \Big|_{0}^{1}-\frac{(\minus{1})^{N}}{N!}\int_{0}^{1}
                    f^{(N)}(x)x^{N}\diff{x}
        \end{equation}
        Instead of using a monomial like $x^{n}$ we can try with the
        Bernoulli polynomials $B_{n}(x)$. We obtain:
        \begin{equation}
            \int_{0}^{1}f(x)\diff{x}=
                \sum_{n=0}^{N}\frac{(-1)^{n}f^(n)(x)}{(n+1)!}B_{n+1}(x)
                \Big|_{0}^{1}-\frac{(-1)^{N}}{N!}\int_{0}^{1}
                B_{N}(x)f^{(N)}(x)\diff{x}
        \end{equation}
        But for $n>1$, $B_{2n-1}(0)=0$, and also
        $B_{2n-1}(1)=0$. So most of these terms are zero, and we can
        simplify our sum as follows:
        \begin{equation}
            \int_{0}^{1}f(x)\diff{x}=
            \Big[(x-\frac{1}{2})f(x)\Big]_{0}^{1}-
            \sum_{n=0}^{N/2}\frac{f^{(2n)}(x)}{(2n+2)!}B_{2n+2}(x)
                \Big|_{0}^{1}+\frac{f^{(N)}(x)}{(N)!}B_{N}(x)\diff{x}
        \end{equation}
        But also, for all $n\in\mathbb{N}$, $B_{2n}(0)=B_{2n}(1)$. So we can
        simplify further:
        \begin{equation}
            \begin{split}
                \int_{0}^{1}f(x)\diff{x}=
                    \frac{1}{2}(f(0)+f(1))-\sum_{n=1}^{m}&b_{2n}
                        \frac{f^{(2n-1)}(1)-f^{(2n-1)}(0)}{(2n)!}\\
                    &+\frac{1}{(2m)!}\int_{0}^{1}
                        B_{2m}(x)f^{(2m)}(x)\diff{x}
            \end{split}
        \end{equation}
        Apply this to the function $f(x+1)$. Since this is just a translate,
        the derivatives don't change. Replace $B_{2n}(x)$ with its
        periodic extension $\tilde{B}_{2n}(x)$. Do this for $f(x+N)$ and
        then add the results, obtaining a telescoping series. You obtain:
        \begin{equation}
            \begin{split}
                \int_{1}^{n}f(x)\diff{x}
                    =&\frac{1}{2}f(1)+f(2)+f(3)+
                        \cdots{f}(N-1)+\frac{1}{2}f(N)\\
                        &+\frac{B_{2}}{2!}(f'(N)-f'(1))+
                        \frac{B_{4}}{4!}(f'''(N)-f'''(1))+\\
                        &+\frac{B_{6}}{6!}(f^{(5)}(N)-f^{(5)}(1))+\cdots+
                        \frac{B_{2m}}{(2m)!}(f^{(2m-1)}(N)\\
                        &+f^{(2m-1)}(1))+\frac{1}{(2m)!}\int_{1}^{N}
                        \tilde{B}_{2m}(x)f^{(2m)}(x)\diff{x}
            \end{split}
        \end{equation}
        This is one form of the Euler-Maclaurin summation formula. The sum
        of the first $N$ terms, taking into account the factor of one half
        that appears in the first and last term, is the trapezoidal
        approximation for the integral of $f$ on the interval $[1,n]$. The
        remainder can thus be thought of as the correction terms to this
        approximation.
        \begin{lexample}
            Let $f(x)=\frac{1}{x^{2}}$ and apply the Euler-Maclaurin
            summation formula. We get:
            \begin{equation}
                \begin{split}
                    \frac{\pi^{2}}{6}-\sum_{k=1}^{n}\frac{1}{k^{2}}
                    =\frac{1}{n}+
                        &\frac{1}{2n^{2}}+\frac{B_{2}}{n^{3}}+
                        \frac{B_{4}}{n^{5}}+\cdots+
                        \frac{B_{2m}}{n^{2m+1}}\\
                        &-(2m+1)\int_{n}^{\infty}
                            \tilde{B}_{2m}(x)\frac{1}{x^{2m+2}}\diff{x}
                \end{split}
            \end{equation}
        \end{lexample}
        We know that the Bernoulli polynomials obtain their maxima and
        minima on the interval $[0,1]$ at the endpoints. Using this
        we obtain an upper bound for the error:
        \begin{align}
            \Big|(2m+1)\int_{n}^{\infty}
                \frac{\tilde{B}_{2m}(x)}{x^{2m+2}}\diff{x}\Big|
                &\leq(2m+1)\int_{n}^{\infty}
                    \big|\frac{\tilde{B}_{2m}(x)}{x^{2m+2}}\big|\diff{x}\\
                &\leq(2m+1)|B_{2m}|\int_{n}^{\infty}
                    \frac{1}{x^{2m+2}}\diff{x}\\
                &=\frac{|B_{2m}|}{n^{2m+1}}
        \end{align}
        If one were to add the terms in the series, at least one thousand
        terms would be needed to guarantee accuracy of 3 decimals. Using this
        series we can sum the first 10, and then use the Bernoulli numbers
        expansion (The first three terms) to get 8 decimals of accuracy.
        This asymptotic expansion will eventually get bad. These Bernoulli
        numbers get large, and once they're larger than $x^{2m}$, adding
        more terms makes the approximation worse. Indeed, while adding
        60 terms can get the error down to $10^{-20}$, adding 100 terms yields
        an error of well over one hundred trillion.
        \begin{lexample}
            Let's use the Euler-Maclaurin formula on $\ln(x)$.
            \begin{align}
                \int_{1}^{n}\ln(x)\diff{x}
                &=\big[x\ln(x)-x\big]_{1}^{n}\\
                \nonumber
                &=\frac{1}{2}\ln(1)+\ln(2)+\cdots+\frac{1}{2}\ln(n)
                    +\sum_{k=1}^{n}\frac{B_{2k}}{(2k)!}
                    \Big[
                        \frac{(2k-2)!}{1^{2k-1}}-\frac{(2k-2)!}{n^{2k-1}}
                    \Big]\\
                &\quad\quad
                    -\frac{1}{(2m)!}\int_{1}^{n}\tilde{B}_{2m}(x)
                    \frac{(2m-1)!}{x^{2m}}\diff{x}
            \end{align}
            Now $\ln(1)=0$, and $\frac{1}{2}\ln(n)=\ln(n)-\frac{1}{2}\ln(n)$.
            But $\ln(1)+\cdot+\ln(n)=\ln(n!)$. So we can write:
            \begin{align}
                \nonumber
                n\ln(n)-n&=\ln(n!)-\frac{1}{2}\ln(n)\\
                &\quad
                    +\sum_{k=1}^{n}\frac{B_{2k}}{2k(2k-1)}
                    \Big[1-\frac{1}{n^{2k-1}}\Big]
                    -\frac{1}{2m}\int_{1}^{n}
                    \frac{\tilde{B}_{2m}(x)}{x^{2m}}\diff{x}
            \end{align}
            Using this we get Sterling's Approximation for $n!$.
        \end{lexample}
    \section{The Gamma Function}
        The Gamma Function extends the factorial function to
        non-integer values. We've seen the following formula:
        \begin{equation}
            \sum_{k=0}^{n}k=\frac{n(n+1)}{2}
        \end{equation}
        We can use this to extend the sum to have a meaning for
        non-integer values. Similarly, the Harmonic numbers can be
        shown to obey the following equation:
        \begin{equation}
            H_{n}=\int_{0}^{1}\frac{1-^{n}}{1-t}\diff{t}
        \end{equation}
        However, this equation is well defined for any real number,
        and we can use this to extend the Harmonic numbers to any
        real number. In a similar manner, we can extend the factorial
        function by considering various formulas that it obeys. We have:
        \begin{equation}
            (m+n)!=m!(m+1)(m+2)\cdots(m+n)=n!(n+1)(n+2)\cdots(n+m)
        \end{equation}
        Solving for $m$, we obtain:
        \begin{equation}
            m!=\frac{n!(n+1)\cdots(n+m)}{(m+1)(m+2)\cdots(m+n)}
        \end{equation}
        The denominator of this equation relies on the fact that
        $m$ is an integer. So let's try to rewrite this so that it makes
        sense even if $m$ is a non-integer value. Taking out an $n$ from
        each product, we can rewrite this as:
        \begin{equation}
            m!=n^{m}n!
                \frac{(1+\frac{1}{n})(1+\frac{2}{n})\cdots(1+\frac{m}{n})}
                     {(m+1)(m+2)\cdots(m+n)}
        \end{equation}
        While this looks like it depends on $n$, it doesn't, since it is
        equal to $m!$ for all $n$. Thus, taking the limit as
        $n\rightarrow\infty$, we obtain:
        \begin{equation}
            m!=\underset{n\rightarrow\infty}{\lim}n^{m}n!
                \frac{(1+\frac{1}{n})(1+\frac{2}{n})\cdots(1+\frac{m}{n})}
                     {(m+1)(m+2)\cdots(m+n)}
        \end{equation}
        This makes sense for all real $x$, with the exception of
        $x=\minus{1},\minus{2},\dots$ since we'd get division by zero.
        Thus, we can define the following:
        \begin{equation}
            x!=\underset{n\rightarrow\infty}{\lim}n^{x}n!
                \frac{(1+\frac{1}{n})(1+\frac{2}{n})\cdots(1+\frac{x}{n})}
                     {(x+1)(x+2)\cdots(x+n)}
        \end{equation}
        While the limit exists for all $x\in\mathbb{N}$, we still need to
        verify that it exists for $x\ne\minus{1},\minus{2},\dots$ for this
        to be a good definition. Define the following:
        \begin{equation}
            a_{n}(x)=n^{x}n!
                \frac{(1+\frac{1}{n})(1+\frac{2}{n})\cdots(1+\frac{x}{n})}
                     {(x+1)(x+2)\cdots(x+n)}
        \end{equation}
        Then, for $x\in(-1,0]$, $a_{n}(x)>0$ for all $n\in\mathbb{N}$.
        But also:
        \begin{equation}
            \frac{a_{n+1}(x)}{a_{n}(x)}=
            \frac{(n+1)^{x}}{x^{x}}\frac{n+1}{x+n+1}
        \end{equation}
        And therefore:
        \begin{equation}
            \ln\Big(\frac{a_{n+1}(x)}{a_{n}(x)}\Big)=
            x\ln(n+1)+\ln(n+1)-x\ln(n)-\ln(x+n+1)
        \end{equation}
        Let $f_{n}(x)=\ln(a_{n+1}(x)/a_{n}(x))$. Then:
        \begin{align}
            f_{n}(0)&=0\\
            f_{n}(-1)&=0\\
            f_{n}'(x)&=\ln(n+1)-\ln(n)-\frac{1}{x+n+1}\\
            f_{n}''(x)&=\frac{1}{(x+n+1)^{2}}
        \end{align}
        And thus, for all $n\in\mathbb{N}$, $f_{n}''(x)>0$.
        This implies, by using Rolle's Theorem and the extreme value
        theorem, that $f_{n}(x)$ is negative for all $x\in(-1,0)$.
        Thus:
        \begin{equation}
            \frac{a_{n+1}(x)}{a_{n}(x)}<1
            \quad\quad\quad
            \minus{1}<x<0
        \end{equation}
        Therefore $a_{n}(x)$ is a monotonically decreasing sequence
        that is bounded below, and therefore converges. Now we show for
        positive $x$ that $x!$ is well defined. We start on the interval
        $(0,1)$ by letting $x\in(\minus{1},0)$, and considering
        $a_{n}(x+1)$:
        \begin{align}
            a_{n}(x+1)
            &=\frac{n!n^{x+1}}{(x+2)(x+3)\cdots(x+n+1)}\\
            &=\frac{n!n^{x}}{(x+1)(x+2)\cdots(x+n)}
                \frac{n(x+1)}{x+n+1}\\
            &=(x+1)a_{n}(x)\frac{n}{x+n+1}
        \end{align}
        But in the limit as $n\rightarrow\infty$, the quotient on the right
        tends to one. Thus, we have that $(x+1)!$ is well defined for all
        $x\in(-1,0)$, meaning $x!$ is well defined for all
        $x\in(0,1)$, and we have that:
        \begin{equation}
            \label{eqn:Special_Functions_Factorial_of_x_in_minus_1_0}
            (x+1)!=(x+1)x!
            \quad\quad
            \minus{1}<x<0
        \end{equation}
        By induction we have that $x!$ is well defined for all
        $x\ne\minus{1},\minus{2},\dots$ and that
        Eqn.~\ref{eqn:Special_Functions_Factorial_of_x_in_minus_1_0}
        holds for all such $x$. Moreover, for
        $x\ne0,\minus{1},\minus{2},\dots$ we have:
        \begin{equation}
            (x-1)!=\frac{x!}{x}
        \end{equation}
        Another way to show that $x!$ is well defined is as follows:
        \begin{align}
            \ln\big(a_{n}(x)\big)
            &=\ln\Big(\frac{n!n^{x}}{(x+1)(x+2)\cdots(x+n)}\Big)\\
            &=\ln\Big(
                \frac{n^{x}}{(x+1)(\frac{x}{2}+1)\cdots(\frac{x}{n}+1)}
            \Big)\\
            &=x\ln(x)-\sum_{k=1}^{n}\ln\Big(1+\frac{x}{k}\Big)\\
            &=x\sum_{k=1}^{n}\Big(\ln(k+1)-\ln(k)\Big)
                -x\sum_{k=1}^{n}\ln\Big(1+\frac{x}{k}\Big)
                -x\Big[\ln(n+1)-\ln(n)\Big]\\
            &=\sum_{k=1}^{n}\Big[
                x\ln\Big(1+\frac{1}{k}\Big)-
                \ln\Big(1+\frac{x}{k}\Big)\Big]-x\ln\Big(\frac{n+1}{n}\Big)
        \end{align}
        The final logarithm on the right tends to zero as
        $n\rightarrow\infty$, so we need only be concerned with the sum.
        We can prove the convergence of this sum by invoking the limit
        comparison test.
        \begin{theorem}[Limit Comparison Test]
            If $a_{n}$, $b_{n}$ are positive sequences such that
            the limit of $a_{n}/b_{n}$ exists and and
            $\sum_{n=1}^{\infty}b_{n}$ converges, then
            $\sum_{n=1}^{\infty}a_{n}$ converges.
        \end{theorem}
        Looking at the first two terms of the MacLaurin series for
        $\ln(1+x)$, we choose for the sequence $b_{k}$ the following:
        \begin{align}
            x\ln(1+\frac{1}{k})-\ln(1+\frac{x}{k})
            &\approx
            x(\frac{1}{k}-\frac{1}{2k^{2}})
            -(\frac{x}{k}-\frac{x^{2}}{2k^{2}})\\
            &=\frac{x^{2}-x}{2k^{2}}
        \end{align}
        We choose this for the $b_{k}$'s.
        \begin{align}
            A_{k}&=
            \Big|x\ln\big(1+\frac{1}{k}\big)-\ln\big(1+\frac{x}{k}\big)\Big|\\
            B_{k}&=\frac{|x^{2}-x|}{2k^{2}}
        \end{align}
        To find the limit of $A_{k}/B_{k}$, let $s=1/k$. Then:
        \begin{align}
            \underset{k\rightarrow\infty}{\lim}\frac{A_{k}}{B_{k}}
            &=\underset{s\rightarrow{0}^{+}}{\lim}
            \frac{|x\ln(1+s)-\ln(1+sx)|}{s^{2}}\\
            &\underset{s\rightarrow{0}^{+}}{\lim}
            \frac{\big|\frac{x}{1+s}-\frac{x}{1+sx}\big|}{2s}\\
            &=\frac{|x^{2}-x|}{2}
        \end{align}
        Where we have invoked L'H\"{o}pital's Rule and some algebra.
        But:
        \begin{equation}
            \sum_{k=1}^{\infty}B_{k}=
            \sum_{k=1}^{\infty}\frac{|x^{2}-x|}{2k^{2}}
        \end{equation}
        And this converges. Therefore the limit of $\ln(a_{n}(x))$ exists
        and:
        \begin{align}
            \ln(x!)&=\sum_{k=1}^{\infty}\Big[
                x\ln\big(1+\frac{1}{k}\big)-\ln\big(1+\frac{x}{k}\big)\Big]\\
            &=\sum_{k=1}^{\infty}
                \ln\Big(
                    \frac{\big(1+\frac{1}{k}\big)^{x}}{1+\frac{x}{k}}
                    \Big)
        \end{align}
        But the sum of logarithms is equal to the product of the arguments.
        That is:
        \begin{equation}
            \ln(x!)=
            \ln\Big[
                \prod_{k=1}^{\infty}\frac{(1+\frac{1}{k})^{x}}{1+\frac{x}{k}}
            \Big]
        \end{equation}
        Exponentiating, we obtain the following formula for $x!$:
        \begin{equation}
            x!=\prod_{k=1}^{\infty}\frac{(1+\frac{1}{k})^{x}}{1+\frac{x}{k}}
        \end{equation}
        This is used to define the Gamma Function.
        \begin{ldefinition}{The Gamma Function}
            The Gamma Function is the function
            $\Gamma:\mathbb{R}\setminus\{0,\minus{1},\minus{2},\dots\}\rightarrow\mathbb{R}$
            defined by:
            \begin{equation}
                \Gamma(x)=(x-1)!
            \end{equation}
        \end{ldefinition}
        \begin{theorem}[The Reflection Formula for Gamma]
            \begin{equation}
                \frac{1}{\Gamma(x)\Gamma(1-x)}=\frac{\sin(\pi{x})}{\pi}
            \end{equation}
        \end{theorem}
        Looking at the logarithm of the Gamma function, we can obtain some interesting
        results.
        \begin{align}
            \ln(\Gamma(x))&=
            \underset{n\rightarrow\infty}{\lim}\Big[
                x\ln(n)-\ln(x)-\sum_{k=1}^{n}\ln\big(1+\frac{x}{k}\big)\Big]
        \end{align}
        This sum, by the Maclaurin series, behaves like the Harmonic series, which
        diverges.
        \begin{ldefinition}{Harmonic Numbers}
            The $n^{th}$ Harmonic number is:
            \begin{equation}
                H_{n}=\sum_{k=1}^{n}\frac{1}{k}
            \end{equation}
        \end{ldefinition}
        The sequence of Harmonic numbers $H_{n}$ diverges. To see this, first note that
        for all $x>0$, $\exp(x)>1+x$, by the Maclaurin series for $\exp(x)$. Thus:
        \begin{align}
            H_{n}&=\sum_{k=1}^{n}\frac{1}{k}\\
            \exp(H_{n})&=\exp(\sum_{k=1}^{n}\frac{1}{k})\\
            &=\prod_{k=1}^{n}\exp(\frac{1}{k})\\
            &\geq\prod_{k=1}^{n}(1+\frac{1}{k})\\
            &=\prod_{k=1}^{n}\frac{1+k}{k}\\
            &=n+1
        \end{align}
        Therefore:
        \begin{equation}
            \ln(H_{n})\geq\ln(n+1)
        \end{equation}
        And thus diverges. Rewriting $\ln(\Gamma(x))$, we get:
        \begin{align}
            \ln\big(\Gamma(x)\big)
            &=\underset{n\rightarrow\infty}{\lim}\Big[
                x\ln(n)-\ln(x)-\sum_{k=1}^{n}\ln\big(1+\frac{x}{k}\big)\Big]\\
            &=\underset{n\rightarrow\infty}{\lim}\Big[
                x\ln(n)-\ln(x)-\sum_{k=1}^{n}\ln\big(1+\frac{x}{k}-\frac{x}{k}\big)
                -x\sum_{k=1}^{n}\frac{1}{k}\Big]\
        \end{align}
        Using the limit comparison test again, we see that the following limit exists:
        \begin{equation}
            \underset{n\rightarrow\infty}{\lim}
            \sum_{k=1}^{n}\ln\big(1+\frac{x}{k}-\frac{x}{k}\big)
        \end{equation}
        Because of this, the following limit also exists:
        \begin{equation}
            \underset{n\rightarrow\infty}{\lim}
            \sum_{k=1}^{n}\frac{1}{k}-\ln(n)
        \end{equation}
        This is the Euler-Mascheroni constant. Denote this a $\gamma$. Exponentiating,
        we have:
        \begin{align}
            \Gamma(x)&=\frac{1}{x}\exp(-\gamma{x})\prod_{k=1}^{\infty}
                \Big(1+\frac{x}{k}\Big)^{-1}\exp\Big(\frac{x}{k}\Big)\\
            \frac{1}{\Gamma(x)}
            &=x\exp(\gamma{x})\prod_{k=1}^{\infty}
                \Big(1+\frac{x}{k}\Big)\exp\Big(-\frac{x}{k}\Big)
        \end{align}
        By differentiating we obtain:
        \begin{equation}
            \frac{\Gamma'(x)}{\Gamma(x)}=
            \minus\frac{1}{x}-\gamma-\sum_{k=1}^{\infty}
                \Big[\frac{1}{x+k}-\frac{1}{k}\Big]
        \end{equation}
        This is also known as the digamma function, $\Psi(x)$. There is a popular
        formula used for the Gamma function, known as it's integral form. This is
        often taken as the definition of the Gamma function. By doing repeated integration
        by parts, we get:
        \begin{align}
            \frac{1}{x+n}
            &=\int_{0}^{1}s^{x+n-1}\diff{s}\\
            &=\frac{(-1)^{n}}{n!}\prod_{k=1}^{n}(x+k-1)
                \int_{0}^{1}s^{x-1}(s-1)^{n}\diff{s}
        \end{align}
        Flipping this around, we have:
        \begin{equation}
            \int_{0}^{1}s^{x-1}(1-s)^{n}\diff{s}
            =\frac{n!}{(x(x+1)\cdots(x+n)}
        \end{equation}
        Letting $s=t/n$, we get:
        \begin{equation}
            \int_{0}^{n}t^{x-1}(1-\frac{t}{n})^{n}\diff{t}=
            \frac{n!n^{x}}{x(x+1)\cdots(x+n)}
        \end{equation}
        Taking the limit as $n\rightarrow\infty$, and recalling one of the formula's
        for $\exp(x)$, we get:
        \begin{equation}
            \Gamma(x)=\int_{0}^{\infty}t^{x-1}\exp(\minus{t})\diff{t}
        \end{equation}
        Differentiating, we get:
        \begin{equation}
            \Gamma'(x)=
            \int_{0}^{\infty}t^{x-1}\ln(t)\exp(\minus{t})\diff{t}
        \end{equation}
        Therefore:
        \begin{equation}
            \gamma'(1)=\int_{0}^{\infty}\ln(t)\exp(\minus{t})\diff{t}
        \end{equation}
        This integral we have seen before and is equal to $\minus\gamma$, where
        $\gamma$ is the Euler-Macheroni constant.
    \section{Lecture 7}
        Recall that the Gamma Function is defined as:
        \begin{equation}
            \Gamma(x)=
            \underset{n\rightarrow\infty}{\lim}
                \frac{n!n^{x}}{x(x+1)\cdots(x+n)}
        \end{equation}
        This extends the factorial function to the real numbers.
        \begin{equation}
            \Gamma(n+1)=n!
        \end{equation}
        The domain is all of $\mathbb{R}$, with the exception of
        the negative integers and zero. We can rewrite the
        Gamma function by taking the $n!$ in the numerator and
        sneaking it into the denominator:
        \begin{equation}
            \Gamma(x)=
            \underset{n\rightarrow\infty}{\lim}
            \frac{n^{x}}
                 {x\frac{x+1}{1}\frac{x+2}{x}\cdots\frac{x+n}{n}}
            =\underset{n\rightarrow\infty}{\lim}
            \frac{n^{x}}
                 {x(x+1)(\frac{x}{2}+1)\cdots(\frac{x}{n}+1)}
        \end{equation}
        Writing this in $\Pi$ notation, we have:
        \begin{equation}
            \Gamma(x)=
            \underset{n\rightarrow\infty}{\lim}
            \frac{n^{x}}{x}
            \prod_{k=1}^{n}\Big(1+\frac{x}{k}\Big)^{-1}
        \end{equation}
        Taking the logarithm of both sides, we obtain:
        \begin{align}
            \ln\Big(\Gamma(x)\Big)&=
            \underset{n\rightarrow\infty}{\lim}
            \Big[
                -\ln(x)+x\ln(n)-\sum_{k=1}^{n}\ln(1+\frac{x}{k})
            \Big]\\
            &=\underset{n\rightarrow\infty}{\lim}
            \Big[
                -\ln(x)+x\ln(n)-
                \sum_{k=1}^{n}\Big(
                    \ln(1+\frac{x}{k})-\frac{x}{k}
                \Big)-\sum_{k=1}^{n}\frac{x}{k}
            \Big]\\
            &=\underset{n\rightarrow\infty}{\lim}
            \Big[-\ln(x)-x\Big(\sum_{k=1}^{n}\frac{1}{k}-\ln(n)\Big)
                 -\sum_{k=1}^{n}\Big(
                    \ln(1+\frac{x}{k})-\frac{x}{k}
                \Big)
            \Big]\\
            &=-\ln(x)-\gamma{x}-\sum_{k=1}^{\infty}\Big[]
                    \ln(1+\frac{x}{k})-\frac{x}{k}
                \Big]
        \end{align}
        Where $\gamma$ is the Euler-Mascheroni Constant. Taking
        the exponential of this, we obtain:
        \begin{equation}
            \Gamma(x)=\frac{1}{x}\exp(\minus\gamma{x})
                \prod_{k=1}^{\infty}\Big(1+\frac{x}{k}\Big)^{-1}
                    \exp(x/k)
        \end{equation}
        Taking the reciprocal, we have:
        \begin{equation}
            \frac{1}{\Gamma(x)}=
            x\exp(\gamma{x})
            \prod_{k=1}^{\infty}\Big(1+\frac{x}{k}\Big)
            \exp(\minus{x}/k)
        \end{equation}
        This function has no bad points, whereas $\Gamma(x)$ has
        undefined values at $x=0,\minus{1},\minus{2},\dots$
        The reciprocal of $\Gamma$ is an \textit{entire} function,
        it is a complex differentiable function at every point in
        the complex plane. We have seen via integration by parts
        that:
        \begin{equation}
            \Gamma(x)=\int_{0}^{\infty}t^{x-1}\exp(\minus{t})\diff{t}
        \end{equation}
        For all $x>0$. This integral diverges for $x\leq{0}$.
        We can split the interval into two parts:
        \begin{equation}
            \Gamma(x)=\int_{0}^{1}t^{x-1}\exp(\minus{t})\diff{t}+
                \int_{1}^{\infty}t^{x-1}\exp(\minus{t})\diff{t}
        \end{equation}
        The first integral is improper if $x<1$, since then
        $t^{x-1}$ will be a reciprocal power of $t$ and thus
        diverge at $t=0$, whereas the second integral is always
        improper since it has an infinity in the limit.
        Let's evaluate the second integral:
        \begin{equation}
            \int_{1}^{\infty}t^{x-1}\exp(\minus{t})\diff{t}
            =\underset{b\rightarrow\infty}{\lim}
            \int_{1}^{b}t^{x-1}\exp(\minus{t})\diff{t}
        \end{equation}
        If $t\geq{1}$, and if $n\geq{x}+1$, then:
        \begin{equation}
            0\leq{t}^{}\exp(\minus{t})=
            \frac{t^{x-1}}{1+t+\frac{1}{2!}t^{2}+\cdots}
            \leq\frac{t^{x-1}}{\frac{t^{n}}{n!}}
            =n!t^{x-n-1}
        \end{equation}
        But then we have:
        \begin{equation}
            \int_{1}^{b}t^{x-n-1}\diff{t}=
            n!\Big[\frac{1}{x-n}t^{x-n}\Big]_{1}^{b}
            =\frac{n!}{x-n}\Big[b^{x-n}-1\Big]
        \end{equation}
        And the limit of this exists, so by the comparison test:
        \begin{equation}
            \int_{1}^{\infty}t^{x-1}\exp(\minus{t})\diff{t}=
            \underset{b\rightarrow\infty}{\lim}
            \int_{1}^{b}t^{x-1}\exp(\minus{t})\diff{t}
        \end{equation}
        Exists for all $x\in\mathbb{R}$. Next, let's evaluate the
        first integral:
        \begin{equation}
            \int_{0}^{1}t^{x-1}\exp(\minus{t})\diff{t}=
            \underset{a\rightarrow{0}^{+}}{\lim}
            \int_{a}^{1}t^{x-1}\exp(\minus{t})\diff{t}
        \end{equation}
        But for $0<t\leq{1}$, we have:
        \begin{equation}
            0\leq{t}^{x-1}\exp(\minus{t})\leq{t}^{x-1}
        \end{equation}
        But we can evaluate the integral of this last expression.
        If $x>0$, we have:
        \begin{equation}
            \int_{0}^{1}t^{x-1}=
            \underset{a\rightarrow{0}^{+}}{\lim}
            \Big[\frac{1}{x}t^{x}\Big]_{a}^{1}=\frac{1}{x}
        \end{equation}
        We can use the limit comparison test to show that the
        integral diverges for $x\leq{0}$. For, since:
        \begin{equation}
            t^{x-1}\exp(\minus{t})>0
            \quad\quad
            t>0
        \end{equation}
        And since:
        \begin{equation}
            \underset{t\rightarrow{0}^{+}}{\lim}
            \frac{t^{x-1}\exp(\minus{t})}{t^{x-1}}=1
        \end{equation}
        We have that the integral of $t^{x-1}\exp(\minus{t})$
        and the integral of $t^{x-1}$ converge or diverge together.
        But the integral of $t^{x-1}$ diverges for all $x\leq{0}$,
        and thus the itnegral of $t^{x-1}\exp(\minus{t})$ diverges
        for all $x\leq{0}$.
        \begin{ltheorem}{Comparison Test}
            If $0\leq{f}(x)\leq{g}(x)$ for all $x\in[a,b]$,
            if the integral of $g$ on the interval $[a,b]$ exists,
            and if $f$ is integrable, then the integral of
            $f$ on the interval $[a,b]$ exists.
        \end{ltheorem}
        \begin{ltheorem}{Limit Comparison Test}
            If $f:(a,b)\rightarrow\mathbb{R}$ is a positive
            integrable function that is improper at $a$, and if
            $g$ is an integrable function on $(a,b)$ such that:
            \begin{equation}
                \underset{t\rightarrow{a}^{+}}{\lim}
                \frac{f(t)}{g(t)}\in(0,\infty)
            \end{equation}
            Then the integral of $f$ and the integral of $g$
            converge and diverge together.
        \end{ltheorem}
        So we now have that:
        \begin{equation}
            \Gamma(x)=
            \int_{0}^{\infty}t^{x-1}\exp(\minus{t})\diff{t}
            =\int_{0}^{1}t^{x-1}\exp(\minus{t})\diff{t}+
            \int_{1}^{\infty}t^{x-1}\exp(\minus{t})\diff{t}
        \end{equation}
        The second integral is well defined for all $x\in\mathbb{R}$,
        but the first integral diverges for all $x\leq{0}$.
        We can rewrite $\exp(\minus{t})$ in terms of it's
        Taylor series about the origin and see if it's possible to
        simplify from there:
        \begin{align}
            \int_{0}^{1}t^{x-1}\exp(\minus{t})\diff{t}
            &=\int_{0}^{1}t^{x-1}
            \sum_{k=0}^{\infty}
                \frac{1}{k!}(\minus{t})^{k}\diff{t}\\
            &=\int_{0}^{1}\sum_{k=0}^{\infty}
            \frac{(\minus{1})^{k}}{k!}t^{x+k-1}\diff{t}\\
            &=\sum_{k=0}^{\infty}\frac{(\minus{1})^{k}}{k!}
            \int_{0}^{1}t^{x+k-1}\diff{t}\\
            &=\sum_{k=0}^{\infty}\frac{(\minus{1})^{k}}{k!}
                \Big[\frac{1}{x+k}t^{x+k}\Big]_{0}^{1}\\
            &=\sum_{k=0}^{\infty}\frac{(\minus{t})^{k}}{k!(x+k)}
        \end{align}
        So, for all $x>0$, we have:
        \begin{equation}
            \Gamma(x)=
            \int_{1}^{\infty}t^{x-1}\exp(\minus{t})\diff{t}+
            \sum_{k=0}^{\infty}\frac{(\minus{1})^{k}}{k!(x+k)}
        \end{equation}
        However, this formula is well defined for all
        $x\ne{0},\minus{1},\minus{2},\dots$ so does this formula
        agree with $\Gamma(x)$ for all such values? The answer is
        yes and we can show this by defining the following:
        \begin{equation}
            \Delta(x)=
            \int_{1}^{\infty}t^{x-1}\exp(\minus{t})\diff{t}+
            \sum_{k=0}^{\infty}\frac{(\minus{1})^{k}}{k!(x+k)}
        \end{equation}
        We know that $\Delta(x)=\Gamma(x)$ for all
        $x>0$. If we can show that $\Delta(x+1)=x\Delta(x)$ for
        all $x$, then we will have shown that $\Delta(x)=\Gamma(x)$
        for all $x\ne{0},\minus{1},\minus{2},\dots$
        \begin{subequations}
            \begin{align}
                \Delta(x+1)&=
                \int_{1}^{\infty}t^{x}\exp(\minus{t})\diff{t}+
                \sum_{k=0}^{\infty}\frac{(\minus{1})^{k}}{k!(x+k+1)}\\
                &=\Big[\minus{t}\exp(\minus{t})\Big]_{0}^{\infty}+
                x\int_{1}^{\infty}t^{x-1}\exp(\minus{t})\diff{t}+
                \sum_{k=0}^{\infty}\frac{(\minus{1})^{k}}{k!(x+k+1)}\\
                &=x\int_{1}^{\infty}t^{x-1}\exp(\minus{t})\diff{t}+
                \exp(\minus{1})+\sum_{k=0}^{\infty}
                \frac{(\minus{1})^{k}}{k!(x+k+1)}\\
                &=x\int_{1}^{\infty}t^{x-1}\exp(\minus{t})\diff{t}+
                \exp(\minus{1})+\sum_{k=1}^{\infty}
                \frac{(\minus{1})^{k-1}}{(k-1)!(x+k)}\\
                &=x\int_{1}^{\infty}t^{x-1}\exp(\minus{t})\diff{t}+
                \exp(\minus{1})-\sum_{k=0}^{\infty}
                k\frac{(\minus{1})^{k}}{k!(x+k)}\\
                &=x\int_{1}^{\infty}t^{x-1}\exp(\minus{t})\diff{t}+
                \sum_{k=0}^{\infty}\frac{(\minus{1})^{k}}{k!}-
                \sum_{k=0}^{\infty}k\frac{(\minus{1})^{k}}{k!(x+k)}\\
                &=x\int_{1}^{\infty}t^{x-1}\exp(\minus{t})\diff{t}+
                \sum_{k=0}^{\infty}\frac{(\minus{1})^{k}}{k!}
                \Big(1-\frac{k}{(x+k)}\Big)\\
                &=x\int_{1}^{\infty}t^{x-1}\exp(\minus{t})\diff{t}+
                \sum_{k=0}^{\infty}\frac{(\minus{1})^{k}}{k!}
                \Big(\frac{x+k-k}{x+k}\Big)\\
                &=x\int_{1}^{\infty}t^{x-1}\exp(\minus{t})\diff{t}+
                x\sum_{k=0}^{\infty}\frac{(\minus{1})^{k}}{k!}
                \Big(\frac{1}{x+k}\Big)\\
                &=x\Bigg[\int_{1}^{\infty}t^{x-1}
                \exp(\minus{t})\diff{t}+
                \sum_{k=0}^{\infty}\frac{(\minus{1})^{k}}{k!}
                \Big(\frac{1}{x+k}\Big)\Bigg]\\
                &=x\Delta(x)
            \end{align}
        \end{subequations}
        We can use the integral formula for Gamma to get
        approximations for large $x$.
        \begin{subequations}
            \begin{align}
                \Gamma(x+1)&=\int_{0}^{1}t^{x}\exp(\minus{t})\diff{t}
                \quad\quad\quad{x}>0\\
                &=\int_{0}^{1}
                    (sx)^{x}\exp(\minus{sx})x\diff{s}\\
                &=x^{x+1}\int_{0}^{\infty}
                    s^{x}\exp(\minus{sx})\diff{s}\\
                &=x^{x+1}\int_{0}^{\infty}
                    \exp(x\ln(s)-sx)\diff{s}\\
                &=x^{x+1}\int_{0}^{\infty}
                    \exp\big(x(\ln(s)-s)\big)\diff{s}
            \end{align}
        \end{subequations}
        We can apply the \textit{stationary phase} method to produce
        an approximation for integrals of this form. This is also
        known as Laplace's Method. Consider an integral of the
        form:
        \begin{equation}
            \int_{a}^{b}f(t)\exp\big(xh(t)\big)\diff{t}
        \end{equation}
        We wish to approximate this when $x$ is large. Suppose
        $f$ is continuous and $h$ is a smooth function such that
        there is a global maximum $c\in(a,b)$ and $h''(c)\ne{0}$.
        Shifting $h$ by $h(c)$, let $g(t)=h(t)-h(c)$. Then
        $g$ has a global maximum at $c$ and $h(c)=0$. We have:
        \begin{equation}
            \int_{a}^{b}f(t)\exp\big(xh(t)\big)\diff{t}=
            \exp\big(\minus{x}h(c)\big)
            \int_{a}^{b}f(t)\exp\big(x[h(t)-h(c)]\big)\diff{t}
        \end{equation}
        The function $\exp(x[h(t)-h(x)])$ decreases rapidly as
        $t$ ventures away from $c$. The larger $x$ becomes, the
        more this is exacerbated. Thus the main contribution to
        the integral is in the vicinity of $c$. So:
        \begin{align}
            \int_{a}^{b}f(t)\exp\big(xh(t)\big)\diff{t}
            &=\exp\big(xh(c)\big)\int_{a}^{b}f(t)
                \exp\big(x[h(t)-h(c)]\big)\diff{t}\\
            &\approx\exp\big(xh(c)\big)
                \int_{c-\varepsilon}^{c+\varepsilon}
                f(t)\exp\big(x[h(t)-h(c)]\big)\diff{t}
        \end{align}
        But $h$ is smooth, so we can expand it in terms of it's
        Taylor Series:
        \begin{equation}
            h(t)-h(c)\approx(t-c)h'(c)+\frac{1}{2}(t-c)^{2}h''(c)
        \end{equation}
        But $h'(c)=0$, so:
        \begin{equation}
            h(t)-h(c)\approx\frac{1}{2}(t-c)^{2}h''(c)
        \end{equation}
        From the continuity of $f$, by choosing $x$ large enough we
        can consider a very small region about $t=x$, and thus may
        approximate $f$ as a constant. We get:
        \begin{subequations}
            \begin{align}
                \int_{a}^{b}f(t)\exp\big(xh(t)\big)\diff{t}
                &\approx{f}(c)\exp\big(xh(c)\big)
                    \int_{c-\varepsilon}^{c+\varepsilon}
                    \exp\Big(\frac{x}{2}(t-c)^{2}h''(c)\Big)\diff{t}\\
                &\approx{f}(c)\exp\big(xh(c)\big)
                    \int_{\minus\infty}^{\infty}
                    \exp\Big(\frac{x}{2}(t-c)^{2}h''(c)\Big)\diff{t}\\
                &=f(c)\exp\big(xh(c)\big)\int_{\minus\infty}^{\infty}
                    \exp\Big(\frac{x}{2}t^{2}h''(c)\Big)\diff{t}
            \end{align}
        \end{subequations}
        But we can evaluate this last integral directly. Let
        $u=\sqrt{\minus\frac{xh''(c)}{2}}t$. Since $h''(c)<0$, as
        $h(c)$ is a global maximum and $h''(c)\ne{0}$, the inside of
        the square root is indeed positive. Using substitution, we
        obtain:
        \begin{equation}
            \begin{split}
                f(c)\exp\big(xh(c)\big)\int_{\minus\infty}^{\infty}
                    \exp&\Big(\frac{x}{2}t^{2}h''(c)\Big)\diff{t}\\
                &=f(c)\exp\big(xh(c)\big)
                \sqrt{\minus\frac{2}{xh''(c)}}
                \int_{\minus\infty}^{\infty}\exp(u^{2})\diff{u}
            \end{split}
        \end{equation}
        But this is the Error function, and the value of this integral
        is $\sqrt{\pi}$. This gives us Laplace's Approximation:
        \begin{equation}
            \int_{a}^{b}f(t)\exp\big(xh(t)\big)\diff{t}
            \approx{f}(c)\sqrt{\frac{\minus{2}\pi}{xh''(c)}}
            \exp\big(xh(c)\big)
        \end{equation}
        We can apply this to the Gamma function. Let
        $h(t)=\ln(t)-t$. Then $h'(t)=t^{\minus{1}}-1$, and thus $h$
        has a zero at $t=1$. Moreover, $h''(t)=\minus{t}^{\minus{2}}$,
        and thus $h''(1)<0$, so this is a global maximum.
        Using Laplace's Formula, we have:
        \begin{equation}
            \Gamma(x+1)\sim{x}^{x+1}
            \sqrt{\frac{2\pi}{x}}\exp(\minus{x})
            =x^{x}\sqrt{2\pi{x}}\exp(\minus{x})
        \end{equation}
        This is Stirling's formula for when $x=n\in\mathbb{N}$.
    \section{Lecture Whatever}
        If $0\leq{f}(x)\leq{g}(x)$ for all $x\in(a,b)$, then:
        \begin{equation}
            \int_{a}^{b}f(x)\diff{x}=\infty
            \Rightarrow\int_{a}^{b}g(x)\diff{x}
        \end{equation}
        And also:
        \begin{equation}
            \int_{a}^{b}g(x)\diff{x}<\infty
            \Rightarrow\int_{a}^{b}f(x)\diff{x}<\infty
        \end{equation}
        That is, if the \textit{small} function is too big,
        then then bigger function must also diverge. Similarly,
        if the large function converges, then so must the smaller
        function. Knowing that the small function converges tells
        us nothing about whether or not the large function does.
        Similarly, knowing that the large function diverges tells
        us nothing about the smaller function. To make use of the
        limit comparison test we need to know \textit{a priori} a
        good function to test against our given function.
        \begin{lexample}
            Consider $f(t)=\frac{1}{1+t^{p}}$. It would make sense
            to compare this with $t^{\minus{p}}$, since for all
            $t\in(0,\infty)$:
            \begin{equation}
                t^{p}<1+t^{p}\Rightarrow
                \frac{1}{t^{p}}<\frac{1}{t^{p}}
            \end{equation}
            However, this latter function is improper at the origin
            and so we may not be able to integrate it there. Thus
            we've replaced one problem with another. Instead, split
            the integral into to parts:
            \begin{equation}
                \int_{0}^{\infty}\frac{1}{1+t^{p}}\diff{t}=
                \int_{0}^{1}\frac{1}{1+t^{p}}\diff{t}+
                \int_{1}^{\infty}\frac{1}{1+t^{p}}\diff{t}
            \end{equation}
            The first integral on the right hand side of the equation
            is not a problem, since the integrand is well defined
            across the entirety of the interval $[0,1]$. The
            second integral can be compared with $t^{\minus{p}}$,
            and so we have:
            \begin{equation}
                0\leq\int_{0}^{\infty}\frac{1}{1+t^{p}}\diff{t}
                \leq\int_{0}^{1}\frac{1}{1+t^{p}}\diff{t}+
                \int_{1}^{\infty}\frac{1}{t^{p}}\diff{t}
            \end{equation}
            We can evaluate this integral for when $p\ne{1}$ and
            get:
            \begin{equation}
                0\leq\int_{0}^{\infty}\frac{1}{1+t^{p}}\diff{t}
                \leq\int_{0}^{1}\frac{1}{1+t^{p}}\diff{t}+
                \underset{b\rightarrow\infty}{\lim}
                \Big[\frac{1}{1-p}t^{1-p}\Big]_{1}^{b}
            \end{equation}
            Now when $p>1$, the right hand side is well defined
            and this shows the the original integral converges.
            When $p<1$ we see that the right hand side diverges.
            This tells us nothing about the original integral. All
            we've seen is that the larger function diverges. We don't
            know yet whether the smaller function does. If $p=1$,
            we get:
            \begin{equation}
                0\leq\int_{0}^{\infty}\frac{1}{1+t^{p}}\diff{t}
                \leq\int_{0}^{1}\frac{1}{1+t^{p}}\diff{t}+
                \underset{b\rightarrow\infty}{\lim}
                \Big[\ln(t)\Big]_{1}^{b}
            \end{equation}
            And this diverges. Again, this tells us nothing about
            the original integral. However, we can evaluate this
            directly:
            \begin{equation}
                \int_{0}^{\infty}\frac{1}{1+t}\diff{t}=
                \underset{b\rightarrow\infty}{\lim}
                \Big[\ln(1+t)\Big]_{1}^{b}
            \end{equation}
            And this diverges. So for we have solved to problem
            for $p>1$ and $p=1$. When $p<1$, and for $t>1$, we have:
            \begin{equation}
                t^{p}<t
            \end{equation}
            And this gives us something good to compare with,
            since we've already solved the case of $p=1$. We have:
            \begin{equation}
                0\leq\int_{0}^{1}\frac{1}{1+t^{p}}\diff{t}
                +\int_{1}^{\infty}\frac{1}{1+t}\diff{t}
                \leq\int_{0}^{\infty}\frac{1}{1+t^{p}}\diff{t}
            \end{equation}
            And we have already seen that this diverges.
        \end{lexample}
        The comparison test was able to solve the problem posed in
        the previous example, but it was rather laborious to go
        through all of the steps. The limit comparison test will make
        the problem significantly easier.
        \begin{lexample}
            If $p>0$, then from L'H\^{o}pital's rule, we have:
            \begin{equation}
                \underset{t\rightarrow\infty}{\lim}
                \frac{t^{\minus{p}}}{(1+t^{p})^{\minus{1}}}
                =\underset{t\rightarrow\infty}{\lim}
                    \frac{1+t^{p}}{t^{p}}=1
            \end{equation}
            And therefore:
            \begin{equation}
                \int_{1}^{\infty}\frac{1}{1+t^{p}}\diff{t}<\infty
                \Longleftrightarrow
                \int_{1}^{\infty}\frac{1}{t^{p}}\diff{t}<\infty
            \end{equation}
            Thus, from the fact that:
            \begin{equation}
                \int_{1}^{\infty}\frac{1}{t^{p}}\diff{t}<\infty
                \Longleftarrow
                p>1
            \end{equation}
            We see that the original integral converges if and only
            if $p>1$.
        \end{lexample}
        \begin{lexample}
            Consider the integral:
            \begin{equation}
                \int_{0}^{\infty}\frac{t^{p}}{1+t}\diff{t}
            \end{equation}
            By comparing with $t^{1-p}$, this integral
            converges if and only if $\minus{1}<p<0$.
            Indeed, we have the following:
            \begin{equation}
                \int_{0}^{\infty}\frac{t^{p-1}}{1+t}\diff{t}
                =\frac{\pi}{\sin(\pi{p})}
            \end{equation}
            This looks like the reflection formula for $\Gamma$,
            and comes from the definition of the Beta function.
        \end{lexample}
        The DiGamma function has the following formula:
        \begin{equation}
            \sum_{k=0}^{\infty}\frac{(\minus{1})^{k}}{a+k}
            =\psi(a)-\psi(\frac{a}{2})-\ln(2)
            \quad\quad
            a\ne{0},\minus{1},\minus{2},\dots
        \end{equation}
        We can rewrite this as:
        \begin{subequations}
            \begin{align}
                \sum_{k=0}^{\infty}\frac{(\minus{1})^{k}}{a+k}
                &=\sum_{k=0}^{\infty}\Big[
                    \frac{1}{a+2k}-\frac{1}{a+2k+1}\Big]\\
                &=\frac{1}{a}-\frac{1}{a+1}+\sum_{k=1}^{\infty}\Big[
                    \frac{\frac{1}{2}}{\frac{1}{2}a+k}-
                    \frac{\frac{1}{2}}{k}-
                    \frac{\frac{1}{2}}{\frac{a+1}{2}+k}+
                    \frac{\frac{1}{2}}{k}\Big]\\
                &=\frac{1}{a}-\frac{1}{a+1}+
                    \frac{1}{2}\sum_{k=1}^{\infty}\Big[
                        \frac{1}{\frac{a}{2}+k}-\frac{1}{k}\Big]-
                    \frac{1}{2}\sum_{k=1}^{\infty}\Big[
                        \frac{1}{\frac{a+1}{2}+k}-\frac{1}{k}\Big]
            \end{align}
        \end{subequations}
        Using our formula for DiGamma, we get:
        \begin{subequations}
            \begin{align}
                \sum_{k=0}^{\infty}\frac{(\minus{1})^{k}}{a+k}
                &=\frac{1}{a}-\frac{1}{a+1}-\frac{1}{2}\Big[
                    \gamma+\frac{2}{a}+\psi\big(\frac{a}{2}\big)\Big]+
                    \frac{1}{2}\Big[
                        \gamma+\frac{2}{a+1}+
                        \psi\big(\frac{a+1}{2}\big)
                    \Big]\\
                &=\frac{1}{2}\psi\big(\frac{a+1}{2}\big)-
                    \frac{1}{2}\big(\frac{a}{2}\big)
            \end{align}
        \end{subequations}
        So, we have:
        \begin{equation}
            \frac{1}{2}\psi\big(\frac{a+1}{2}\big)-
            \frac{1}{2}\psi\big(\frac{a}{2}\big)=
            \psi(a)-\psi\big(\frac{a}{2}\big)-\ln(2)
        \end{equation}
        Recall that:
        \begin{equation}
            \frac{\diff}{\diff{a}}\ln\big(\Gamma(a)\big)=
            \psi(a)
        \end{equation}
        Integrating across, we have:
        \begin{equation}
            \ln\Big(\Gamma\big(\frac{a+1}{2}\big)\Big)
            =\ln\big(\Gamma(a)\big)-
            \ln\Big(\gamma\big(\frac{a}{2}\big)\Big)-
            a\ln(2)-\ln(C)
        \end{equation}
        Exponentiating, and letting $a\rightarrow{2x}$, we obtain:
        \begin{equation}
            \Gamma(2x)=\frac{1}{2^{2x-1}\sqrt{\pi}}
            \Gamma(x)\Gamma(x+\frac{1}{2})
        \end{equation}
    \section{Bessel Functions}
        We've defined special functions by various means so far.
        The error function was defined via an integral, the
        Bernouli polynomials are defined by a recursion relation.
        We saw that $\Gamma$ is defined as a limit of a strange
        product, and that $\beta$ is defined by the parameters of
        a certain integral. Now we introduce a set of special
        functions defined by the solutions to a differential equation.
        Bessel's equation is the differential equation:
        \begin{equation}
            x^{2}\ddot{y}+x\dot{y}+(x^{2}-p^{2})y=0
        \end{equation}
        The solutions to this equation model a variety of problems
        in electromagnetism and mechanics. We'll first solve this
        by a power series method. Suppose $y$ is a solution, and let:
        \begin{equation}
            y=\sum_{k=0}^{\infty}a_{k}^{k+r}
        \end{equation}
        This isn't a traditional power series, since the $r$ may not
        be an integer. By Frobenius' Theorem, this method will work.
        If we were to consider the problem:
        \begin{equation}
            x^{3}\ddot{y}+x\dot{y}+(x^{2}-p^{2})y=0
        \end{equation}
        Simply replacing the $x^{2}$ term with an $x^{3}$, this method
        wouldn't work. The $x^{2}$ terms creates a regular
        singularity, and Frobenius can handle that. The cubic
        term $x^{3}$ would create an irregular singularity. The
        $r$ term in the power series allows us to consider problems
        like:
        \begin{equation}
            x^{2}\ddot{y}+\alpha{x}\dot{y}+\beta{y}=0
        \end{equation}
        Try for a solution:
        \begin{equation}
            y=Ax^{r}
        \end{equation}
        Plugging this in, we get:
        \begin{equation}
            Ax^{r}\big(r(r-1)+\alpha{r}+\beta\big)=0
        \end{equation}
        Thus, either $A=0$ and $y$ is the zero solution, or $r$
        satisfies:
        \begin{equation}
            r^{2}+r(\alpha-1)+\beta=0
        \end{equation}
        This may not be an integer. The Frobenius method takes care
        of this. Using this series to evaluate Bessel's equation,
        we get:
        \begin{subequations}
            \begin{align}
                y&=\sum_{k=0}^{\infty}a_{k}^{k+r}\\
                \dot{y}&=\sum_{k=0}^{\infty}(k+r)a_{k}^{k+r-1}\\
                \ddot{y}&=\sum_{k=0}^{\infty}(k+r)(k+r-r)a_{k}^{k+r-2}
            \end{align}
        \end{subequations}
        The original differential equation then says:
        \begin{equation}
            \begin{split}
                \sum_{k=0}^{\infty}(k+r)(k+r-1)&a_{k}x^{k+r}+
                \sum_{k=0}^{\infty}(k+r)a_{k}x^{k+r}+\\
                &\sum_{k=0}^{\infty}a_{k}x^{k+r+2}-
                \sum_{k=0}^{\infty}p^{2}a_{k}x^{k+r}=0
            \end{split}
        \end{equation}
        We can factor out the $x^{r}$ term to get:
        \begin{equation}
            \begin{split}
                \sum_{k=0}^{\infty}(k+r)(k+r-1)&a_{k}x^{k}+
                \sum_{k=0}^{\infty}(k+r)a_{k}x^{k}+\\
                &\sum_{k=0}^{\infty}a_{k}x^{k+2}-
                \sum_{k=0}^{\infty}p^{2}a_{k}x^{k}=0
            \end{split}
        \end{equation}
        Combing the sums, we get:
        \begin{equation}
            \sum_{k=0}^{\infty}\Big[(k+r)(k+r-1)+(k+r)-p^{2}\Big]
                a_{k}x^{k}+
            \sum_{k=0}^{\infty}a_{k}x^{k+2}=0
        \end{equation}
        Simplifying the expression in the left-most sum, we have:
        \begin{equation}
            \sum_{k=0}^{\infty}\Big[(k+r)^{2}-p^{2}\Big]
                a_{k}x^{k}+
            \sum_{k=0}^{\infty}a_{k}x^{k+2}=0
        \end{equation}
        Finally, performing a shift in the index on the second
        sum, we obtain:
        \begin{equation}
            \sum_{k=0}^{\infty}\Big[(k+r)^{2}-p^{2}\Big]
                a_{k}x^{k}+
            \sum_{k=2}^{\infty}a_{k-2}x^{k}=0
        \end{equation}
        We can rewrite this to get it all under one sum:
        \begin{equation}
            (r^{2}-p^{2})a_{0}+[(r+1)^{2}-p^{2}]a_{1}x+
            \sum_{k=2}^{\infty}\Big[
                [(k+r)^{2}-p^{2}]a_{k}+a_{k-2}\Big]x^{k}=0
        \end{equation}
        This is a power series for the zero function, and thus all
        of the coefficients must be zero. We obtain the three
        following equation, called the indicial equations:
        \begin{subequations}
            \begin{align}
                (r^{2}-p^{2})a_{0}&=0\\
                [(r+1)^{2}-p^{2}]&=0\\
                [(k+r)^{2}-p^{2}]a_{k}+a_{k-2}&=0
            \end{align}
        \end{subequations}
        If $a_{n}=0$ for all $n\in\mathbb{N}$, then $y$ is simply
        the zero solution, which is a valid solution. If not then
        there is a least $n\in\mathbb{N}$ such that $a_{n}\ne{0}$.
        We can choose this to be $n=0$ by shifting $r$ by the
        appropriate amount. That is, if $N=20$ it the first non-zero
        term, replace $r$ with $r+20$. From this, we have:
        \begin{equation}
            r^{2}-p^{2}=0\Longleftarrow{r}=\pm{p}
        \end{equation}
        Start with $r=p\geq{0}$. Looking at the second equation, we
        get:
        \begin{subequations}
            \begin{align}
                [(r+1)^{2}-p^{2}]a_{1}&=0\\
                \Rightarrow[(p+1)^{2}-p^{2}]a_{1}&=0\\
                \Rightarrow[2p+1]a_{1}&=0\\
                \Rightarrow{a}_{1}&=0
            \end{align}
        \end{subequations}
        So there is no linear term. For $k\geq{2}$ we have:
        \begin{subequations}
            \begin{align}
                [(k+p)^{2}-p^{2}]a_{k}+a_{k}&=0\\
                k(2p+k)a_{k}+a_{k-2}&=0
            \end{align}
        \end{subequations}
        Now we look for a pattern. The first few terms are:
        \begin{subequations}
            \begin{align}
                a_{2}&=\minus\frac{1}{2(2p+2)}a_{0}\\
                a_{3}&=0\\
                a_{4}&=\frac{\minus{1}}{4(2p+4)}
                    \frac{\minus{1}}{2(2p+2)}a_{0}\\
                &=\frac{(\minus{1})^{2}}{4\cdot{2}(2p+4)(2p+2)}a_{0}\\
                a_{5}&=0
            \end{align}
        \end{subequations}
        A pattern starts to emerge. We see that $a_{2k-1}=0$ for
        all $k\in\mathbb{N}$, and looking at $a_{6}$ perhaps the
        pattern will be clear:
        \begin{subequations}
            \begin{align}
                a_{6}=\frac{\minus{1}}{6(2p+6)}a_{k}&=
                \frac{(\minus{1})^{3}}
                    {6\cdot{4}\cdot{2}(2p+6)(2p+4)(2p+2)}a_{0}\\
                &=\frac{(\minus{1})^{3}}{2^{3}3!(p+1)(p+2)(p+1)}a_{0}
            \end{align}
        \end{subequations}
        So we make the following guess:
        \begin{equation}
            a_{2k}=\frac{(\minus{1})^{k}}{2^{2k}k!(p+1)\cdots{p+k)}}
        \end{equation}
        We now prove that this is true by induction.
        We rewrite this as:
        \begin{equation}
            a_{2k}=
            \frac{(\minus{1})^{k}\Gamma(p+1)}
                {2^{2k}k!\Gamma(p+k+1)}a_{0}
        \end{equation}
        Our solution to Bessel's Equation is:
        \begin{equation}
            y(x)=\sum_{k=0}^{\infty}
                \frac{(\minus{1})^{k}\Gamma(p+1)}
                    {2^{2k}k!\Gamma(p+k+1)}a_{0}x^{2k+p}
        \end{equation}
        Simplifying, we get:
        \begin{equation}
            y(x)=a_{0}2^{p}\Gamma(p+1)\sum_{k=0}^{\infty}
                \frac{(\minus{1})^{k}}{k!\Gamma(p+k+1)}
                \Big(\frac{x}{2}\Big)^{2k+p}
        \end{equation}
        By choosing $a_{0}$ so that the coefficient on the outside
        of the sum is 1, we obtain the Bessel functions:
        \begin{equation}
            J_{\alpha}(x)=\sum_{k=0}^{\infty}
            \frac{(\minus{1})^{k}}{k!\Gamma(\alpha+k+1)}
                \Big(\frac{x}{2}\Big)^{2k+\alpha}
        \end{equation}
        The zeroth Bessel function, $J_{0}$, can be simplified:
        \begin{equation}
            J_{0}(x)=\sum_{k=0}^{\infty}
                \frac{(\minus{1})^{k}}{2^{k}(k!)^{2}}x^{2k}
        \end{equation}
        And we see that this decays extremely fast, and indeed the
        radius of convergence is the entire real line.
    \section{Lecture I-Stopped-Counting}
        Previously we studied Bessel's Equation:
        \begin{equation}
            x^{2}\ddot{y}+x\dot{y}+(x^{2}-p^{2})y=0
        \end{equation}
        Using the method of Frobnius, we found one solution to be:
        \begin{equation}
            J_{p}(x)=\sum_{n=0}^{\infty}
                \frac{(\minus{1})^{n}}{n!\Gamma(n+p+1)}
                \Big(\frac{x}{2}\Big)^{2n+p}
        \end{equation}
        If $p\ne{0}$, then $J_{p}$ and $J_{\minus{p}}$ are both
        solutions. If $p\notin\mathbb{N}$, then $J_{p}$ and
        $J_{\minus{p}}$ are solution. We can see this since
        $J_{p}(x)\rightarrow{0}$ as $x\rightarrow{0}$, whereas
        $J_{\minus{p}}(x)\rightarrow\infty$ as
        $x\rightarrow{0}^{+}$. In the case that $p\notin\mathbb{N}$,
        we can write the general solution as:
        \begin{equation}
            y(x)=C_{1}J_{p}(x)+C_{2}J_{\minus{p}}(x)
        \end{equation}
        If $n\in\mathbb{N}$, then:
        \begin{equation}
            J_{\minus{n}}(x)=\sum_{k=0}^{\infty}
                \frac{(\minus{1})^{k}}{k!\Gamma(k+1-n)}
                \Big(\frac{x}{2}\Big)^{2k-n}
        \end{equation}
        The coefficients are thus zero, due to the Gamma function,
        for when $k+1-n\leq{0}$, and thus we may as well start
        the sum from $k\geq{n}$. Doing this we obtain:
        \begin{subequations}
            \begin{align}
                J_{\minus{n}}(x)&=\sum_{k=n}^{\infty}
                    \frac{(\minus{1})^{k}}{k!\Gamma(k+1-n)}
                    \Big(\frac{x}{2}\Big)^{2k-n}\\
                &=\sum_{k=0}^{n}
                    \frac{(\minus{1})^{k+n}}{k!\Gamma(k+1)}
                    \Big(\frac{x}{2}\Big)^{2(k+n)-n}\\
                &=(\minus{1})^{n}\sum_{n=0}^{\infty}
                    \frac{(\minus{1})^{n}}{k!(n+k)!}
                    \Big(\frac{x}{2}\Big)^{2k+n}\\
                &=(\minus{1})^{n}J_{n}(x)
            \end{align}
        \end{subequations}
        Frobenius thus failed to find two independent solutions for
        when $p\in\mathbb{N}$. However, the Frobenius method does
        then suggest looking for a second solution in the form:
        \begin{equation}
            y(x)=J_{p}(x)\ln(x)+\sum_{k=0}^{\infty}b_{k}x^{k-p}
        \end{equation}
        This is not the standard approach, and instead $J_{p}(x)$
        is taken as the first solution, and we define the
        Bessel Function of the second kind to be:
        \begin{equation}
            Y_{p}(x)=
            \frac{J_{p}(x)\cos(\pi{p})-J_{\minus{p}}(x)}
                {\sin(\pi{p})}
            \quad\quad
            p\notin\mathbb{Z}
        \end{equation}
        In the limit as $p\rightarrow{n}$, for $n\in\mathbb{Z}$,
        we obtain an indeterminate form. This is because:
        \begin{equation}
            J_{n}(x)\cos(\pi{n})-J_{\minus{n}}=
            (\minus{1})^{n}J_{n}(x)-J_{\minus{n}}(x)=0
        \end{equation}
        And $\sin(n\pi)=0$, so we obtain $\tfrac{0}{0}$. To
        evaluate this we need to invoke L'H\"{o}pital's rule.
        Differentiating the denominator is easy, and we get
        $\pi\cos(\pi{p})$. Differentiating the numerator is trickier:
        We obtain:
        \begin{equation}
            \underset{p\rightarrow{n}}{\lim}
            \frac{-\pi{J}_{p}(x)\sin(\pi{p})+\partial_{p}J_{p}(x)-
                  \partial_{p}J_{\minus{p}}(x)}{\pi\cos(\pi{p})}
        \end{equation}
        We can evaluate $\partial_{p}J_{p}(x)$ as follows:
        \begin{subequations}
            \begin{align}
                \partial_{p}J_{p}(x)&=
                \frac{\partial}{\partial{p}}
                \Big[\sum_{k=0}^{\infty}
                    \frac{(\minus{1})^{k}}{k!\Gamma(p+k+1)}
                    \Big(\frac{x}{2}\Big)^{2k+p}\Big]\\
                &=\sum_{k=0}^{\infty}
                    \frac{(\minus{1})^{k}}{k!\Gamma(k+p+1)}
                    \Big(\frac{x}{2}\Big)^{2k+p}
                    \Big[\ln\big(\frac{x}{2}\big)-\Psi(p+k+1)\Big]
            \end{align}
        \end{subequations}
        Where $\Psi$ is the Digamma function. This can be used
        to evaluate the final limit. From this, we obtain the
        general solution to bessel's equation:
        \begin{equation}
            y(x)=C_{1}J_{p}(x)+C_{2}Y_{p}(x)
        \end{equation}
        The most important case of the Bessel functions is when
        $p\in\mathbb{N}$. The second most important case is when
        $p=n+\frac{1}{2}$, for some $n\in\mathbb{N}$.
        \subsection{Generating Function}
            The Bessel functions of the first kind have the
            following generating function:
            \begin{equation}
                \exp\Big(\frac{x}{2}(t-t^{\minus{1}})\Big)
                =\sum_{n=\minus\infty}^{\infty}
                    J_{n}(x)t^{n}
            \end{equation}
            We can obtain this as follows:
            \begin{subequations}
                \begin{align}
                    \exp\Big(\frac{x}{2}(t-t^{\minus{1}})\Big)
                    &=\sum_{k=0}^{\infty}\frac{1}{k!}
                    \Big[\frac{1}{2}x(t-t^{\minus{1}})\Big]^{k}\\
                    &=\sum_{k=0}^{\infty}\frac{1}{k!}
                        \Big(\frac{x}{2}\Big)^{k}
                        \Big(t-t^{\minus{1}}\Big)^{k}
                \end{align}
            \end{subequations}
            Using the binomial expansion on this last sum, we
            obtain:
            \begin{subequations}
                \begin{align}
                    \exp\Big(\frac{x}{2}(t-t^{\minus{1}})\Big)
                    &=\sum_{k=0}^{\infty}\frac{1}{k!}
                        \Big(\frac{x}{2}\Big)^{k}
                        \sum_{j=0}^{k}\binom{K}{j}
                        (\minus{1})^{k-j}t^{k}t^{j-k}\\
                    &=\sum_{k=0}^{\infty}\sum_{j=0}^{k}
                        \frac{1}{k!}\Big(\frac{x}{2}\Big)^{k}
                        \binom{k}{j}(\minus{1})^{k-j}t^{2j-k}
                \end{align}
            \end{subequations}
            Rearranging this sum, we obtain the result. If
            $t=\exp(i\theta)$, we obtain:
            \begin{align}
                \exp\Big(\frac{x}{2}\big(
                    \exp(i\theta)-\exp(\minus{i}\theta)\big)\Big)
                &=\exp\big(i\sin(\theta)\big)\\
                &=\sum_{n=\minus\infty}^{\infty}
                    J_{n}(x)\exp(in\theta)
            \end{align}
            The Euler-Formula for the coefficients of a Fourier
            series is:
            \begin{equation}
                c_{n}=\frac{1}{\pi}\int_{\minus\pi}^{\pi}
                    f(\theta)\exp(\minus{i}n\theta)\diff{\theta}
            \end{equation}
            From this, we obtain:
            \begin{equation}
                J_{n}(x)=\frac{1}{2\pi}
                    \int_{\minus\pi}^{\pi}
                    \exp\big(ix\sin(\theta)\big)
                    \exp(\minus{i}n\theta)\diff{\theta}
            \end{equation}
            Using Euler's Formula for the exponential function,
            we obtain:
            \begin{equation}
                J_{n}(x)=\frac{1}{2\pi}
                \int_{\minus\pi}^{\pi}\Big[
                    \cos\big(x\sin(\theta)-n\theta)-
                    i\sin\big(x\sin(\theta)-n\theta\big)\Big]
                    \diff{\theta}
            \end{equation}
            We can evaluate the imaginary part by one of two ways.
            The first is to note that the composition of odd
            functions is again an odd function, and the integral
            of an odd function on a symmetric interval is zero.
            The second way is to note that $J_{n}(x)$ is a purely
            real function, and thus has no imaginary part. Therefore
            the imaginary integral must be zero. Using this, we
            obtain:
            \begin{align}
                J_{n}(x)&=\frac{1}{2\pi}\int_{\minus\pi}^{\pi}
                    \cos\big(x\sin(\theta)-n\theta\big)\\
                &=\frac{1}{\pi}\int_{0}^{\pi}
                    \cos\big(x\sin(\theta)-n\theta\big)
            \end{align}
        \subsection{Hankel Functions}
            Hankel functions are also called Bessel functions
            of the third kind, and they come in two flavors.
            \begin{align}
                H_{p}^{(1)}(x)
                    &=J_{p}(x)+iY_{p}(x)\\
                H_{p}^{(2)}(x)&=J_{p}(x)-iY_{p}(x)
            \end{align}
            These are useful in various areas of mathematics and
            have nice properties as $x\rightarrow\infty$.
        \subsection{Airy's Equation}
            The Bessel functions satisfy the following
            general differential equation. Given:
            \begin{equation}
                y(x)=x^{a}J_{p}(bx^{x})
            \end{equation}
            We have:
            \begin{equation}
                x^{2}\ddot{y}(x)+(1-2a)x\dot{y}(x)+
                \Big[b^{2}c^{2}x^{2c}+(a^{2}-c^{2}p^{2}\Big]y(x)
                =0
            \end{equation}
            Similarly if we defined $y$ it terms of $Y_{p}(x)$.
            Airy's equation is a second order differential equation
            that comes up in optics and physics. It is defined by:
            \begin{equation}
                \ddot{y}+xy=0
            \end{equation}
            Multiplying through by $x^{2}$, we obtain:
            \begin{equation}
                x^{2}\ddot{y}+x^{3}y=0
            \end{equation}
            So, we need $1-2a=0$ and $x=\frac{3}{2}$. We also need
            $b^{2}c^{2}=1$ and $a^{2}=c^{2}p^{2}$. This constrains
            us to $p=\frac{1}{3}$ and $b=\frac{2}{3}$. We thus
            arrive at the general solution to Airy's equation:
            \begin{equation}
                y(x)=x^{\frac{1}{2}}C_{1}J_{\frac{1}{3}}
                    \Big(\frac{2}{3}x^{\frac{3}{2}}\Big)+
                    x^{\frac{1}{2}}C_{2}Y_{\frac{1}{3}}
                    \Big(\frac{2}{3}x^{\frac{3}{2}}\Big)
            \end{equation}
        \subsection{More Stuff About Bessel}
            Let $a=\frac{1}{2}$, $b=1$, $c=1$,
            and $p=\frac{1}{2}$. Then the general differential
            equation reduces to:
            \begin{equation}
                \ddot{y}+y=0
            \end{equation}
            Then the solution is:
            \begin{equation}
                y(x)=C_{1}\sqrt{x}J_{\frac{1}{2}}(x)+
                    C_{2}\sqrt{x}J_{\minus\frac{1}{2}}(x)
            \end{equation}
            But we can solve this directly from the standard
            methods found in differential equations to obtain:
            \begin{equation}
                y(x)=A\cos(x)+B\sin(x)
            \end{equation}
            From this, we obtain:
            \begin{align}
                J_{\frac{1}{2}(x)}
                    &=\sqrt{\frac{2}{\pi{x}}}\sin(x)\\
                J_{\minus\frac{1}{2}(x)}
                    &=\sqrt{\frac{2}{\pi{x}}}\cos(x)
            \end{align}
            If $p\ne\frac{1}{4}$ then this result is a good
            approximation for large $x$. The validity of the
            approximation comes from the method of stationary
            phase.
        \subsection{Complex Stationary Phase}
            Suppose $h$ is an analytic function and is such
            that $h$ has a single stationary point
            $c\in[a,b]$ and is such that $h''(c)\ne{0}$.
            Then, as $x\rightarrow\infty$, we obtain the following:
            \begin{equation}
                \int_{a}^{b}f(t)\exp\big(ixh(t)\big)\diff{t}
                \sim{f}(x)\exp\Big(ixh(c)\pm{i}\frac{\pi}{4}\Big)
                    \sqrt{\frac{2\pi}{x|h''(c)|}}
            \end{equation}
            Where the $\pm$ is determined by the sign of
            $h''(c)$. We can use this formula and combine it with
            our integral formula for $J_{p}(x)$ to obtain a good
            approximation for the Bessel functions for large $x$.
            We obtain:
            \begin{equation}
                J_{n}(x)\approx
                \sqrt{\frac{2}{\pi{x}}}
                \cos\Big(x-\frac{\pi}{4}-n\frac{\pi}{2}\Big)
            \end{equation}
        \subsection{The Heat Equation}
            The Heat Equation is defined as:
            \begin{equation}
                \frac{\partial{u}}{\partial{t}}=
                k\nabla^{2}u
            \end{equation}
            Where $\nabla^{2}$ is the Laplacian operator. This
            is defined as:
            \begin{equation}
                \nabla^{2}u=\Div\big(\grad(u)\big)
                =\nabla\cdot\big(\nabla(u)\big)=\Delta{u}
            \end{equation}
            Where $\Delta{u}$ is the notation that is often
            found in various texts on partial differential
            equations. In Cartesian coordinates, this is:
            \begin{equation}
                \nabla^{2}u=\sum_{k=1}^{n}
                    \frac{\partial^{2}u}{\partial{x}_{k}^{2}}
            \end{equation}
            Where $x_{k}$ is the $k^{th}$ coordinate. In
            one dimension spherical, we have:
            \begin{equation}
                \nabla^{2}u=\frac{\partial^{2}u}{\partial{r}^{2}}
            \end{equation}
            In two dimensions, we obtain:
            \begin{equation}
                \nabla^{2}u=\frac{\partial^{2}u}{\partial{r}^{2}}+
                    \frac{1}{r}\frac{\partial{u}}{\partial{r}}+
                    \frac{1}{r^{2}}
                    \frac{\partial^{2}u}{\partial\theta^{2}}
            \end{equation}
            We solve this by using separation of variables.
            We do:
            \begin{equation}
                u(r,t)=R(r)T(t)
            \end{equation}
            In two dimensions, we do the same thing:
            \begin{equation}
                u(r,\theta,t)=R(r)\Theta(\theta)T(t)
            \end{equation}
            Plugging this into the equation, we get:
            \begin{equation}
                R\Theta\dot{T}=
                \ddot{R}\Theta{T}+\frac{1}{r}\dot{R}\Theta{T}+
                \frac{1}{r^{2}}R\ddot{\Theta}T
            \end{equation}
            Dividing through by $kR\Theta{T}$, we obtain:
            \begin{equation}
                \frac{1}{k}\frac{\dot{T}}{T}=
                \frac{\ddot{R}}{R}+\frac{1}{r}\frac{\dot{R}}{R}+
                \frac{1}{r^{2}}\frac{\ddot{\Theta}}{\Theta}
            \end{equation}
            We note that the left side is a function of $t$
            alone, and that the right side is a function of $r$
            and $\theta$ alone, and thus this must be a constant.
            We get the following:
            \begin{equation}
                \dot{T}=\minus\lambda{k}T
            \end{equation}
            The right hand side reduces to:
            \begin{equation}
                \frac{\ddot{R}}{R}+\frac{1}{r}\frac{\dot{R}}{R}+
                \frac{1}{r^{2}}\frac{\ddot{\Theta}}{\Theta}
                =\minus\lambda
            \end{equation}
            Multiplying through by $r^{2}$, we obtain:
            \begin{equation}
                r^{2}\frac{\ddot{R}}{R}+r\frac{\ddot{R}}{R}+
                \lambda{r}^{2}=\minus\frac{\ddot{\Theta}}{\Theta}
            \end{equation}
            Again, the left side is dependent only on $r$ and
            the right side is dependent only on $\theta$, so this
            must be a constant, call it $p^{2}$. Simplifying, we
            get:
            \begin{equation}
                r^{2}\ddot{R}+r\dot{R}+(\lambda{r}^{2}-p^{2})R=0
            \end{equation}
            And this is Bessel's equation, with the addition of
            a $\lambda$ stuck inside with the $r^{2}$ term. We
            can solve the differential equation for $T$ by using
            standard results from differential equations, and obtain:
            \begin{equation}
                T(t)=C\exp(\minus{k}\lambda{t})
            \end{equation}
            Similarly, for $\Theta$ we get:
            \begin{equation}
                \ddot{\Theta}+p^{2}\Theta=0
            \end{equation}
            And we can solve this to obtain:
            \begin{equation}
                \Theta(\theta)=C_{1}\cos(p\theta)+C_{2}\sin(p\theta)
            \end{equation}
            Finally, we are left with $R$. The general solution is:
            \begin{equation}
                R(r)=C_{3}J_{p}(\sqrt{\lambda}r)+
                C_{4}Y_{p}(\sqrt{\lambda}r)
            \end{equation}
            But we are modelling heat moving through some medium,
            and thuse a good boundary condition is that
            $u(r,\theta,t)$ is non-zero at the origin. Thus we
            must throw out the $Y_{p}$ term and conclude that
            $C_{4}=0$. Note also that the heat equation is linear,
            and thus any linear combination of solutions is again
            a solution. We obtain:
            \begin{equation}
                u(r,\theta,t)=\sum_{n=0}^{\infty}
                \big[C_{1}\cos(n\theta)+C_{2}\sin(n\theta)\big]
                J_{n}(\sqrt{\lambda_{n}}r)
                \exp(\minus{k}\lambda_{n}{t})
            \end{equation}
            If we impose the boundary condition that, for some
            $a>0$, we have $u(a,\theta,t)=0$, we obtain the
            condition that:
            \begin{equation}
                J_{n}(\sqrt{\lambda_{n}}a)=0
                \quad\quad
                n=0,1,2,\dots
            \end{equation}
            Letting $j_{n,k}$ denote the positive zeros of
            $J_{n}$, we obtain the general solution:
            \begin{equation}
                u(r,\theta,t)=
                \sum_{n=0}^{\infty}\sum_{k=1}^{\infty}
                    \big[A_{n,k}\cos(n\theta)+B_{n,k}\sin(n\theta)
                    \big]J_{n}\Big(\frac{j_{n,k}}{a}r\Big)
                    \exp\Big(\minus{k}\frac{j_{n,k}^{2}}{a^{2}}t\Big)
            \end{equation}
            Suppose we impose the condition:
            \begin{equation}
                u(r,\theta,0)=f(r,\theta)
            \end{equation}
            We obtain:
            \begin{subequations}
                \begin{align}
                    f(r,\theta)&=
                    \sum_{n=0}^{\infty}\sum_{k=1}^{\infty}
                    \big[A_{n,k}\cos(n\theta)+B_{n,k}\sin(n\theta)
                    \big]J_{n}\Big(\frac{j_{n,k}}{a}r\Big)\\
                    &=\sum_{n=0}^{\infty}\Big[\sum_{k=1}^{\infty}
                    A_{n,k}J_{n}\big(\frac{j_{n,k}}{a}r\big)\Big]
                    \cos(n\theta)+
                    \sum_{n=0}^{\infty}\Big[\sum_{k=1}^{\infty}
                    B_{n,k}J_{n}\big(\frac{j_{n,k}}{a}r\big)\Big]
                    \sin(n\theta)
                \end{align}
            \end{subequations}
            Using some tricks from Fourier Analysis, we have:
            \begin{equation}
                \frac{1}{\pi}\int_{0}^{2\pi}
                    f(r,\theta)\cos(n\theta)\diff{\theta}
                =\sum_{k=1}^{\infty}A_{n,k}
                    J_{n}\big(\frac{j_{n,k}}{a}r\big)
            \end{equation}
            Similarly for $\sin$. Define $F_{n}(r)$ as:
            \begin{equation}
                F_{n}(r)=\frac{1}{\pi}\int_{0}^{2\pi}
                    f(r,\theta)\cos(n\theta)\diff{\theta}
            \end{equation}
            Then, we have:
            \begin{equation}
                \sum_{k=1}^{\infty}A_{n,k}J_{n}
                    \big(\frac{j_{n.k}}{a}r\big)
                =F_{n}(r)
            \end{equation}
            How do we obtain the $A_{n,k}$ from this information?
            And similarly, the $B_{n,k}$. It would be nice to use
            the same trick as is done for Fourier series. That is,
            use orthogonality and integrate across. Unfortunately,
            the $J_{n}$ are not orthogonal. But, if we introduce
            a weight function, then we have:
            \begin{equation}
                \int_{0}^{1}J_{n}(j_{n,k}r)J_{m}(j_{m,k}r)r\diff{r}
                =\delta_{nm}
            \end{equation}
            Where $\delta_{nm}$ is the Kronecker Delta. This makes
            some sense since our problem is in spherical
            coordinates, and when we integrate we have
            $r\diff{r}$ rather than just $\diff{x}$. We can see
            by studying Bessel's differential equation. We have:
            \begin{equation}
                r^{2}\ddot{J}_{n}+r\dot{J}_{n}+
                (\lambda^{2}r^{2}-n^{2})J_{n}=0
            \end{equation}
            We can simplify this down to:
            \begin{equation}
                \frac{n^{2}}{r}J_{n}-\frac{\diff}{\diff{r}}\Big(
                    r\dot{J}_{n}\Big)
                =\lambda^{2}rJ_{n}
            \end{equation}
            Thus, we have:
            \begin{subequations}
                \begin{align}
                    \lambda\int_{0}^{1}rJ_{n}J_{m}\diff{r}
                    &=\int_{0}^{1}(\lambda{r}J_{n})J_{m}\diff{r}\\
                    &=\int_{0}^{1}\Big[\frac{n^{2}}{r}J_{n}-
                        \dot{J}_{n}\Big]
                        J_{m}\diff{r}\\
                    &=\int_{0}^{1}\frac{n^{2}}{r}J_{n}J_{m}\diff{r}+
                        \int_{0}^{1}r\dot{J}_{n}\dot{J}_{m}\diff{r}
                \end{align}
            \end{subequations}
            Where this last result was obtained from integration
            by parts. This last integral is symmetric, showing
            it didn't matter if we used integration by parts on
            $J_{n}$ or $J_{m}$. From this we obtain:
            \begin{equation}
                \lambda_{n}^{2}\int_{0}^{1}rJ_{n}J_{m}\diff{r}
                =\lambda_{m}^{2}\int_{0}^{1}rJ_{n}J_{m}\diff{r}
            \end{equation}
            And therefore:
            \begin{equation}
                (\lambda_{n}^{2}-\lambda_{m}^{2})
                \int_{0}^{1}rJ_{n}J_{m}\diff{r}=0
            \end{equation}
            But if $n\ne{m}$, then $\lambda_{n}\ne\lambda_{m}$,
            and thus:
            \begin{equation}
                \int_{0}^{1}rJ_{n}J_{m}\diff{r}=0
            \end{equation}
            The normalization is:
            \begin{equation}
                \int_{0}^{1}J_{n}^{2}(\lambda_{n,k}r)r\diff{r}=
                \frac{1}{2}J_{n+1}(j_{n,k})
            \end{equation}
            In three dimensions, the problem becomes:
            \begin{equation}
                \frac{1}{k}\frac{\partial{u}}{\partial{t}}=
                \frac{\partial^{2}u}{\partial{r}^{2}}+
                \frac{2}{r}\frac{\partial{u}}{\partial{r}}+
                \frac{1}{r^{2}\sin^{2}(\theta)}
                \frac{\partial^{2}u}{\partial\theta^{2}}+
                \frac{1}{r^{2}\sin(\phi)}
                \frac{\partial}{\partial\phi}\Big(
                    \sin(\phi)\frac{\partial{u}}{\partial\phi}\Big)
            \end{equation}
            Separating leads to:
            \begin{equation}
                r^{2}\ddot{R}+2r\dot{R}+(\lambda^{2}r^{2}-m)R=0
            \end{equation}
            And:
            \begin{equation}
                \frac{1}{\sin(\phi)}\frac{\diff}{\diff{\phi}}
                    \Big(\sin(\phi)\frac{\diff\Phi}{\diff{\phi}}\Big)
                =m\Phi
            \end{equation}
            Where we let $m=n(n+1)$. Using the generalized Bessel's
            differential equation, we obtain:
            \begin{equation}
                R(r)=C_{1}\frac{1}{\sqrt{x}}J_{n+\frac{1}{2}}(
                    \lambda{x})+C_{2}\frac{1}{\sqrt{x}}
                    Y_{n+\frac{1}{2}}(\lambda{x})
            \end{equation}
            This is called the Spherical Bessel Function. For the
            $\Phi$ equation, make the substitution $x=\cos(\phi)$.
            Let $y(x)=\Phi(\cos^{\minus{1}}(x))$. Then:
            \begin{equation}
                (1-x^{2})\ddot{y}-2x\dot{y}+n(n+1)y=0
            \end{equation}
            And this is Legendre's equation. To solve this, we try
            a power series:
            \begin{equation}
                y(x)=\sum_{n=0}^{\infty}a_{n}x^{n}
            \end{equation}
            Plugging this into the equation and simplifying,
            we get:
            \begin{equation}
                \sum_{k=0}^{\infty}\Big[
                    (k+2)(k+1)a_{k+2}-\big(k(k+1)-n(n+1)\big)a_{k}
                    \Big]x^{k}=0
            \end{equation}
            Thus, all of the coefficients must be zero. Simplifying
            further, we obtain:
            \begin{equation}
                a_{k+2}=\frac{(k+n+1)(k-n)}{(k+2)(k+1)a}a_{k}
            \end{equation}
            We automatically see that $a_{n+2}=0$ for all $n$, and
            thus this is simply a polynomial.
    \section{Lecture The Last One}
        Consider Legendre's Differential Equation:
        \begin{equation}
            (1-x^{2})\ddot{y}-2x\dot{y}+n(n+1)y=0
        \end{equation}
        We showed that one such solution is the following
        polynomial:
        \begin{equation}
            P_{n}(x)=\sum_{k=0}^{\floor{n/2}}
                \frac{(\minus{1})^{k}(2n-2k)!}{k!(n-k)!(n-2k)!}
                    x^{n-2k}
        \end{equation}
        However, we know that, for $n,p\in\mathbb{N}$, and for
        $n\leq{p}$:
        \begin{equation}
            \frac{\diff^{n}}{\diff{x}^{n}}x^{p}=
            \frac{p!}{(p-n)!}x^{p-n}
        \end{equation}
        Letting $p=2n-2k$, we get:
        \begin{equation}
            P_{n}(x)=\sum_{k=0}^{\floor{n/2}}
                \frac{(\minus{1})^{k}}{k!(n-k)!}
                \frac{\diff^{n}}{\diff{x}^{n}}x^{2n-2k}
        \end{equation}
        Noting that when $p<n$, differentiating $x^{p}$ $n$ times
        results in zero, since differentiating constants results
        in zero. We can re-arrange the sum, obtaining:
        \begin{equation}
            P_{n}(x)=\frac{1}{2^{n}n!}\frac{\diff^{n}}{\diff{x}^{n}}
                \Big[\sum_{k=0}^{n}\frac{n!}{k!(n-k)!}
                    (\minus{1})^{k}(x^{2})^{n-k}\Big]
        \end{equation}
        This is similar to Rodrigues formula for the
        Legendre polynomials.
        Noting that the coefficients can be written as the
        binomial coefficients, and invoking the binomial theorem,
        we obtain:
        \begin{equation}
            P_{n}(x)=\frac{1}{2^{n}n!}\frac{\diff^{n}}{\diff{x}^{n}}
                \big(x^{2}-1\big)^{n}
        \end{equation}
        This is Rodrigues Formula. It gives us a method of
        proving orthogonality. For we have:
        \begin{equation}
            \int_{\minus{1}}^{1}P_{n}(x)P_{m}(x)\diff{x}
            =\int_{\minus{1}}^{1}P_{m}(x)\frac{1}{2^{n}n!}
                \frac{\diff^{n}}{\diff{x}^{n}}
                \big(x^{2}-1\big)^{n}\diff{x}
        \end{equation}
        Integrating by parts, we have:
        \begin{equation}
            \begin{split}
                \int_{\minus{1}}^{1}P_{n}(x)P_{m}(x)\diff{x}&=
                    \Big[\frac{P_{m}(x)}{2^{n}n!}
                    \frac{\diff^{n-1}}{\diff{x}^{n-1}}(x^{2}-1)^{n}
                    \Big]_{\minus{1}}^{1}+\\
                    &\quad\quad\quad
                    \int_{\minus{1}}^{1}P_{m}'(x)\frac{1}{2^{n}n!}
                    \frac{\diff^{n-1}}{\diff{x}^{n-1}}
                    \big(x^{2}-1\big)^{n}\diff{x}
                \end{split}
        \end{equation}
        This first term evaluates to zero at both endpoints,
        so we are left with:
        \begin{equation}
            \int_{\minus{1}}^{1}P_{n}(x)P_{m}(x)\diff{x}=
                \int_{\minus{1}}^{1}P_{m}'(x)\frac{1}{2^{n}n!}
                \frac{\diff^{n-1}}{\diff{x}^{n-1}}
                \big(x^{2}-1\big)^{n}\diff{x}
        \end{equation}
        Continuing, we obtain:
        \begin{equation}
            \int_{\minus{1}}^{1}P_{n}(x)P_{m}(x)\diff{x}=
            (\minus{1})^{n}\int_{\minus{1}}^{1}
                P_{m}^{(n)}\frac{1}{2^{n}n!}(x^{2}-1)^{n}
                    \diff{x}
        \end{equation}
        If $m<n$, then $P_{m}^{(n)}$ is zero, since we've
        differentated a polynomial of degree $m$ more than
        $m$ times. If $n<m$, do the integration by parts the
        other way, again showing that the integral is zero.
        So, we have:
        \begin{equation}
            \int_{\minus{1}}^{1}P_{m}(x)P_{n}(x)\diff{x}=0
            \quad\quad
            n\ne{m}
        \end{equation}
        The next thing to discuss is when $n=m$. We have:
        \begin{equation}
            \int_{\minus{1}}^{1}P_{n}^{2}(x)\diff{x}=
            \frac{1}{2^{n}n!}\int_{\minus{1}}^{1}
                P_{n}^{(n)}(x)(x^{2}-1)^{n}\diff{x}
        \end{equation}
        Letting $a_{n}$ be the coefficient of the $x^{n}$ term,
        we have:
        \begin{equation}
            \int_{\minus{1}}^{1}P_{n}^{2}(x)\diff{x}=
            \frac{1}{2^{n}}\int_{\minus{1}}^{1}a_{n}(x^{2}-1)^{n}
                \diff{x}
            =\frac{1}{2^{n}}\frac{(2n)!}{2^{n}(n!)^{2}}
                \int_{\minus{1}}^{1}(x^{2}-1)^{n}\diff{x}
        \end{equation}
        Finally, further simplifying, we obtain:
        \begin{equation}
            \int_{0}^{1}P_{n}^{2}(x)\diff{x}=
                \frac{2(2n)!(\minus{1})^{n}}{4^{n}(n!)^{2}}
                \int_{\minus{1}}^{1}(1-x^{2})^{n}\diff{x}
        \end{equation}
        Letting $x=\sin(\theta)$, which is valid since
        $x$ ranges between $\minus{1}$ and $1$, we get:
        \begin{equation}
            \int_{0}^{1}P_{n}^{2}(x)\diff{x}=
            \frac{2(2n)!}{4^{n}(n!)^{2}}\int_{0}^{\frac{\pi}{2}}
                \cos^{2(n+1)-1}(\theta)
                \sin^{2(\frac{1}{2})-1}(\theta)\diff{\theta}
        \end{equation}
        And this can be written in terms of the Beta and Gamma
        functions. So:
        \begin{equation}
            \int_{0}^{1}P_{n}^{2}(x)\diff{x}=
            \frac{(2n)!}{4^{n}(n!)^{2}}\beta(n+1,\frac{1}{2})=
            \frac{(2n)!}{4^{n}(n!)^{2}}
            \frac{\Gamma(n+1)\Gamma(\frac{1}{2}}
                {\Gamma(n+\frac{3}{2})}
        \end{equation}
        Further simplifying, we get:
        \begin{equation}
            \int_{0}^{1}P_{n}^{2}(x)\diff{x}=
            \frac{(2n)!}{4^{n}(n!)}
            \frac{\Gamma(\frac{1}{2}}{\Gamma(n+\frac{3}{2})}
            =\frac{2}{2n+1}
        \end{equation}
        \subsection{Three Step Recurrence Relation}
            For any sequence of orthogonal polynomials $P_{n}$,
            where $P_{n}$ is of degree $n$, with weight $w$:
            \begin{equation}
                \int_{a}^{b}P_{n}(x)P_{m}(x)w(x)\diff{x}=
                \begin{cases}
                    0,&m\ne{n}\\
                    \frac{2}{2n+1},&n=m
                \end{cases}
            \end{equation}
            $P_{n}$ is orthogonal to every polynomial of
            degree less than $n$. This is because every
            polynomial of degree $k$ is a linear combination
            of $P_{0},\dots,P_{k}$.
            \begin{equation}
                P_{n+1}(x)=(A_{n}x+B_{n})P_{n}(x)+
                    C_{n}P_{n-1}(x)
            \end{equation}
            For Legendre, we have $B_{n}=0$ by parity, and
            $A_{n}=(2n+1)(n+1)^{\minus{1}}$. Lastly,
            $C_{n}=n(n+1)^{\minus{1}}$. By parity it is meant
            that, since $P_{n+1}$ and $P_{n-1}$ are either both
            odd or both even, and $P_{n}$ is the opposite, for
            this to be true the $B_{n}$ would have to be zero.
            Using this we can obtai the generating function. Let:
            \begin{equation}
                G(t)=\sum_{n=0}^{\infty}P_{n}(x)t^{n}
            \end{equation}
            And use the three step rule to obtain:
            \begin{equation}
                \dot{G}(t)=2xt\dot{G}(t)+xG(t)-t\big[
                    G(t)+t\dot{G}(t)\big]
            \end{equation}
            This simplifies to the following differential
            equation:
            \begin{equation}
                \dot{G}(t)=\frac{x-t}{1-2xt+t+t^{2}}G(t)
            \end{equation}
            Dividing by $G$, we get:
            \begin{equation}
                \frac{\dot{G}(t)}{G(t)}=
                \frac{\minus{1}}{2}\frac{2x-2t}{1-2xt+t^{2}}
            \end{equation}
            Integrating across, we obtain:
            \begin{equation}
                \ln(G)=\frac{\minus{1}}{2}\ln(1-2xt+t^{2})+\ln(C)
            \end{equation}
            Thus, we have:
            \begin{equation}
                G(t)=\frac{C}{2\sqrt{1-2xt+t^{2}}}
            \end{equation}
            But we note that $G(0)=P_{0}=1$, so $C=2$. Thus:
            \begin{equation}
                G(t)=\frac{1}{\sqrt{1-2xt+t^{2}}}
            \end{equation}
            And this is the generating function for the
            Legendre polynomials. Plug in $x=1$, we get:
            \begin{equation}
                \sum_{n=0}^{\infty}P_{n}(1)t^{n}=
                \frac{1}{\sqrt{1-2t+t^{2}}}=
                \frac{1}{\sqrt{(1-t)^{2}}}=
                \frac{1}{|1-t|}
            \end{equation}
            For $0\leq{t}<1$, we get:
            \begin{equation}
                \sum_{n=0}^{\infty}P_{n}(0)t^{n}=
                \frac{1}{1-t}=\sum_{n=0}^{\infty}t^{n}
            \end{equation}
            And thus we have that $P_{n}(0)=1$ for all $n$.
            If we differentiate with respect to $x$, we get:
            \begin{equation}
                \sum_{n=1}^{\infty}\dot{P}_{n}(x)t^{n}=
                \frac{t}{(1-2xt+t^{2})^{3/2}}=
                \frac{t}{1-2xt+t^{2}}\frac{1}{\sqrt{1-2xt+t^{2}}}
                =\frac{t}{1-2xt+t^{2}}G(t)
            \end{equation}
            So, we obtain:
            \begin{equation}
                \sum_{n=1}^{\infty}(1-2xt+t^{2})\dot{P}_{n}(x)t^{n}
                =\sum_{n=0}^{\infty}tP_{n}(x)t^{n}
            \end{equation}
            Rearranging, we get:
            \begin{equation}
                \dot{P}_{n+1}(x)-2x\dot{P}_{n}(x)+\dot{P}_{n-1}(x)
                =P_{n}(x)
            \end{equation}
        \subsection{Roots of Orthogonal Polynomials}
            \begin{theorem}
                If $P:\mathbb{N}\rightarrow{Poly}[a,b]$ is
                sequence of orthogonal polynomials, then for
                all $n\in\mathbb{N}$, $P_{n}$ has $n$ distinct
                zeros in $[a,b]$.
            \end{theorem}
            \begin{proof}
                By the fundamental theorem of algebra, $P_{n}$
                does not have more than $n$ zeros. Suppose it
                has $m$ distinct zeros of odd multiplicity,
                and suppose $m<n$. Then:
                \begin{equation}
                    \int_{a}^{b}P_{n}(x)\prod_{k=1}^{m}
                        (x-x_{k})w(x)\diff{x}\ne{0}
                \end{equation}
                Since the sign of the integrand does not
                change sign across the interval. But $P_{n}$
                is orthogonal to all polynomials of degree less
                than $n$, and thus:
                \begin{equation}
                    \int_{a}^{b}P_{n}(x)\prod_{k=1}^{m}
                        (x-x_{k})w(x)\diff{x}=0
                \end{equation}
                A contradiction. Thus $m=n$.
            \end{proof}
\end{document}