\documentclass{article}
\RequirePackage{etex}
\usepackage{geometry}
\geometry{a4paper, margin = 1.0in}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[dvipsnames]{xcolor}
\usepackage{framed,graphicx}
\usepackage{hyperref}
\hypersetup{colorlinks=true,linkcolor=blue,filecolor=magenta,urlcolor=Cerulean,citecolor=SkyBlue}
\usepackage{listings}
\let\olddiv\div
\usepackage{physics}
\usepackage{amsfonts,amsthm,amsmath,esint,mathrsfs}
\usepackage{paracol}
\usepackage{wrapfig}
\usepackage[nottoc]{tocbibind}
\usepackage{natbib}
\usepackage[font={scriptsize,it}]{caption}
\usepackage{float}
\usepackage{multicol}
\usepackage[font={scriptsize}]{subcaption}
\usepackage{mathtools}
\usepackage{pgfplots,tikz,tkz-euclide}
\usetikzlibrary{calc,patterns,angles,quotes,arrows,arrows.meta,shapes,shapes.geometric,cd,hobby,positioning}
\usetkzobj{all}
\pgfplotsset{compat=1.9}
\usepackage[toc,acronym,nogroupskip]{glossaries}
\usepackage{enumitem}
\usepackage{upgreek}
\graphicspath{{images/}}
%-------------Tikz Presets----------%
\pgfdeclareradialshading{myring}{\pgfpointorigin}{
color(0cm)=(transparent!0);
color(5mm)=(pgftransparent!50);
color(1cm)=(pgftransparent!100)}
\pgfdeclarefading{ringo}{\pgfuseshading{myring}}
%------------Theorem Styles---------%
\newtheoremstyle{mystyle}{\topsep}{\topsep}{}{}{\bfseries}{}{.5em}{}
\theoremstyle{mystyle}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{example}{Example}[section]
\newtheorem{problem}{Problem}[section]
\newtheorem{question}{Question}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{properties}{Properties}[section]
\newtheorem{notation}{Notation}[section]
\newtheorem{axiom}{Axiom}[section]
\newtheorem*{theorem*}{Theorem}
\newtheorem*{definition*}{Definition}
\newtheorem*{properties*}{Properties}
\newtheorem*{remark*}{Remark}
%--------Declared Math Operators----%
\DeclareMathOperator{\Refl}{Refl}
\DeclareMathOperator{\Span}{Span}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\multideg}{mutlideg}
\DeclareMathOperator{\LC}{LC}
\DeclareMathOperator{\LT}{LT}
\DeclareMathOperator{\LM}{LM}
\DeclareMathOperator{\LCM}{LCM}
\DeclareMathOperator{\Mon}{Mon}
\DeclareMathOperator{\Spec}{Spec}
\DeclareMathOperator{\proj}{proj}
\DeclareMathOperator{\comp}{comp}
\DeclareMathOperator{\sinc}{sinc}
%-------------New Commands----------%
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\definecolor{shadecolor}{gray}{0.9}
\renewcommand*{\glstextformat}[1]{\textcolor{RoyalBlue}{#1}}
\renewcommand{\glsnamefont}[1]{\textbf{#1}}
\renewcommand\labelitemii{$\circ$}
\renewcommand\thesubfigure{\arabic{chapter}.\arabic{figure}.\arabic{subfigure}}
%---------------GLOSSARY------------%
\makeglossaries
\loadglsentries{glossary}
\loadglsentries{acronym}

\title{Linear Algebra II}
\author{Ryan Maguire}
\date{\vspace{-5ex}}
\begin{document}
\maketitle
\tableofcontents

\section{Miscellaneous Lecture Notes}
\subsection{Orthogonal Projections}
\begin{definition}
The span of $\{X_{1},\hdots, X_{k}\} \subset \mathbb{R}^n$, denoted $\Span(X_{1},\hdots,X_{k})$, is the set of all linear combinations of $\{X_1,\hdots,X_{k}\}$, $\Span(X_{1},\hdots,X_{k})=\{\sum_{i=1}^{k} a_i X_i: a_i \in \mathbb{R}\}$.
\end{definition}
\begin{remark}
If $X_1,\hdots, X_k$ are linearly independent, then $\Span(X_1,\hdots, X_k)$ is a $k-$dimensional subspace of $\mathbb{R}^n$.
\end{remark}
If we are given a set of $k-$linearly independent vectors $\{X_1,\hdots, X_k\}\subset\mathbb{R}^n$, and some other vector $Y$, we may wish to consider the orthogonal projection of $Y$ onto the $k-$dimensional subspace spanned the vectors $X_1,\hdots, X_k$. That is, we may wish to find a vector $Y'\in$ $\Span(X_1,\hdots, X_k)$ such that $Y-Y'$ is orthogonal to $\Span(X_1,\hdots, X_k)$.
\begin{theorem}
If $\{X_1,\hdots, X_k\}\subset\mathbb{R}^n$ is linearly independent, $W = \Span(X_1,\hdots, X_n)$, and if $Y\in \mathbb{R}^n$ is such that $Y\perp X_i$ for $i=1,2,\hdots, k$, then $\forall_{Z\in W}$, $Y\perp Z$.
\end{theorem}
\begin{proof}
For let $Y\in \mathbb{R}^n$ be such that for $i=1,2,\hdots, k$, $Y\perp X_i$. Let $Z\in W$. Then $Z= \sum_{i=1}^{k} a_i X_i$, where $a_i\in \mathbb{R}$. But then $\langle Y, Z\rangle = \sum_{i=1}^{k} a_i \langle Y, X_i\rangle = \sum_{i=1}^{k} a_i\cdot 0 = 0$. 
\end{proof}
\begin{lemma}
If $P$ is an $n\times k$ matrix whose columns are linearly independent, then $P^TP$ is invertible.
\end{lemma}
\begin{proof}
Suppose $P^TPX = 0$. Then $PX$ is orthogonal to the columns of $P$. But $PX$ is a linear combination of the columns of $P$, and thus $PX$ must be orthogonal to itself. Therefore $PX = 0$. But as the columns of $P$ are linearly independent, if $PX = 0$, then $X=0$. Thus $P^TPX = 0$ if and only if $X=0$. Therefore $P^TP$ is invertible.
\end{proof}
So we need $X_i^T(Y-Y') = 0$. Let $X_i = (x_{1i},x_{2i},\hdots, x_{ni})^{T}$ and let $P = (x_{ij})$. Then we have $P^T(Y-Y') = 0$, or $P^TY = P^T Y'$. But $Y'\in W$, so $Y' = \sum_{i=1}^{k} c_i X_i = P(c_1,\hdots, c_k)^T = PC$. So we have $P^TY = P^TPC$, and thus $C = (P^TP)^{-1}P^TY$. Therefore $Y'=P(P^TP)^{-1}P^T Y$.
\begin{definition}
The projection matrix of $\Span(X_{1},\hdots,X_{k})$ is $P(P^TP)^{-1}P^T$.
\end{definition}
\begin{theorem}
If $Q = P(P^TP)^{-1}P^T$ is a projection matrix for some subspace $W$, then:
\begin{enumerate}
    \begin{multicols}{2}
        \item $Q^T =Q$.
        \item $Q^2 = Q$.
    \end{multicols}
\end{enumerate}
\end{theorem}
\begin{proof}
In order,
\begin{enumerate}
    \item   \begin{align*}
                Q^T &= \big(P(P^TP)^{-1}P^T\big)^T & &= P((P^TP)^T)^{-1}P^T\\
                &= (P^T)^T\big(P(P^TP)^{-1}\big)^T & &= P(P^TP)^{-1}P^T\\
                &= P \big((P^TP)^{-1}\big)^TP^T & &= Q\\
        \end{align*}
    \item   \begin{align*}
                Q^2 &= QQ & &=P(P^TP)^{-1}P^T\\
                &= P(P^TP)^{-1}P^T P(P^TP)^{-1}P^T & &= Q\\
                &= P\big((PT^TP)^{-1}(P^TP)\big)(P^TP)^{-1}P^T
        \end{align*}
\end{enumerate}
\end{proof}
\begin{theorem}
If $Q$ is an $n\times n$ matrix, $Q = Q^{2}$, and $Q=Q^{T}$, then there is a subspace $W\subset \mathbb{R}^{n}$ such that $Q$ is the projection matrix of $Q$.
\end{theorem}
\subsection{Reflections}
Let $W$ be a plane passing through the origin, and suppose we want to reflect a vector $v$ across this plane. Let $u$ be a unit vector along $W^{\perp}$. That is, $u$ is normal to the plane. The projection of $v$ along the line through $u$ is then given by $\hat{v} = Proj_{u}(v) = u(u^Tu)^{-1}u^Tv$. But $u$ is a unit vector, and therefore $u^Tu = 1$, so $\hat{v} = uu^T v$. Let $Q_u = uu^T$. The definition of the reflection of $v$ across $W$ is the vector $\Refl_{W}(v)$ such that has the same magnitude as $v$ lying on the opposite side of $W$. Thus $v-\Refl_{W}(v) = 2Q_u v$, and so we have:
\begin{equation*}
    \Refl_{W}(v) = v-2Q_u v = (I-2Q_u)v =(I-2uu^T)v
\end{equation*}
\begin{definition}
$H_{W} = I-2uu^T$ is called the Reflection (Householder) Matrix for $W$.
\end{definition}
\begin{definition}
An orthogonal matrix is a matrix $P$ such that $P^TP = I$.
\end{definition}
\subsection{Lecture Notes on Orthogonal Matrices}
\begin{definition}
An orthoganal matrix is a $n\times n$ matrix $A$ such that $A^{T}A = I$.
\end{definition}
\begin{theorem}
If $A$ is an orthogonal matrix, then $A^T = A^{-1}$.
\end{theorem}
\begin{proof}
For $A^TA = I$, and inverses are unique. Thus $A^T = A^{-1}$.
\end{proof}
If we let $A_{i} = Ae_{i}$, then $A^TA = (A_{i}^{T}A_{j}) = I$. Therefore $A_i^TA_j = \delta_{ij}$.
\begin{theorem}
If $A$ is an $n\times n$ real-valued matrix and $A_i = Ae_i$, $i=1,2,\hdots, n$, then $A$ is orthogonal if and only if $\{A_1,\hdots, A_n\}$ is an orthonormal set of vectors.
\end{theorem}
\begin{proof}
If $A$ is orthogonal, then $A_{i}^{T}A_{j} = \delta_{ij}$, and from this we have orthonormality. If $\{A_1,\hdots, A_n\}$ is orthonormal, then $A^TA = I$ and is therefore orthogonal.
\end{proof}
\begin{theorem}
The following statements are equivalent:
\begin{enumerate}
    \item $A$ is orthogonal
    \item $\forall_{X\in\mathbb{R}^{n}}$, $\norm{AX} = \norm{X}$
    \item $\forall_{X,Y\in\mathbb{R}^{n}}$, $\langle AX, AY\rangle = \langle X, Y\rangle$
\end{enumerate}
\end{theorem}
\begin{proof}
We show $1\Rightarrow 2 \Rightarrow 3 \Rightarrow 1$.
\begin{enumerate}
\item If $A$ is orthogonal, then $A^TA = I$. But then we have:
        \begin{align*}
            \norm{AX}^{2} &= (AX)^{T}AX & &= X^{T}IX\\ 
            &= (X^{T}A^{T})AX & &= X^{T}X\\ 
            &= X^{T}(A^{T}A)X & &= \norm{X}^{2}
        \end{align*}
    Therefore $\norm{AX} = \norm{X}$.
\item If $A$ is a square matrix such that $\forall_{X\in\mathbb{R}^{n}}$, $\norm{AX} = \norm{X}$, then:
    \begin{align*}
        \norm{X+Y}^{2} &= (X+Y)^T(X+Y)\\
        &= X^TX+X^TY+Y^TX+Y^TY\\
        &= \norm{X}^2+2X^TY+\norm{Y}^2
    \end{align*}
    But:
    \begin{align*}
        \norm{A(X+Y)}^{2} &= \norm{AX+AY}^{2}\\
        &= \norm{AX}^{2}+2(AX)^{T}AY+\norm{AY}^2\\
        &= \norm{X}^{2}+2(AX)^{T}AY+\norm{Y}^2
    \end{align*}
    Therefore $(AX)^TAY = X^TY$. That is, $\langle AX, AY\rangle = \langle X, Y\rangle$.
    \item If $A$ is a square matrix such that $\forall_{X,Y\in \mathbb{R}^n}$, $\langle AX, AY\rangle = \langle X, Y\rangle$, and $A_i =  Ae_i$, then:
        \begin{align*}
            A_{i}^{T}A_{j} &= (Ae_{i})^{T}Ae_{j} & &= \langle e_i, e_j\rangle\\
            &= \langle Ae_{i}, Ae_{j}\rangle & &= \delta_{ij}
        \end{align*}
        Therefore, $A$ is orthogonal.
\end{enumerate}
\end{proof}
\begin{theorem}
If $A$ and $B$ are $n\times n$ orthogonal matrices, then $AB$ is orthogonal.
\end{theorem}
\begin{proof}
For if $A^{T}A = I$ and $B^{T}B = I$, then $(AB)^{T}AB = B^{T}A^{T}AB = B^{T}IB = B^{T}B = I$. Therefore $AB$ is orthogonal.
\end{proof}
\begin{theorem}
\label{theorem:LINEAR_ALGEBRA_orthogonal_matrices_have_determinant_pm_1}
If $A$ is an $n\times n$ orthogonal matrix, then $\det(A) = 1$ or $-1$.
\end{theorem}
\begin{proof}
For $\det(I) = \det(A^TA) = \det(A^T)\det(A) = \det(A)^2$. Thus, $\det(A) = \pm 1$.
\end{proof}
\begin{remark}
The converse of theorem \ref{theorem:LINEAR_ALGEBRA_orthogonal_matrices_have_determinant_pm_1} is false.
\end{remark}
Recall that if $u\in \mathbb{R}^n$ is a unit vector and $W = u^{\perp}$, then $H=2uu^T$ is the reflection matrix for $W$. Reflections preserve distance, and therefore $H$ must be orthogonal.
\begin{theorem}
If $A$ is an $n\times n$ orthogonal matrix, then there exist $k$ $n\times n$ reflection matrices $H_1,\hdots, H_k$, $0\leq k \leq n$, such that $A = \prod_{i=1}^{j}H_i$.
\end{theorem}
\begin{proof}
We prove by induction. The base case is trivial. Suppose it holds for $n-1$. Let $z = Ae_n$, and let $H$ be the reflection matrix that exchanges $z$ and $e_n$. Then $HAe_n = Hz = e_n$, so $HA$ fixes $e_n$. But $HA$ is an orthogonal matrix, and thus preserves distances and angles. Thus $HA$ maps $\mathbb{R}^{n-1}$ onto itself, and thus by induction there are $H_2,\hdots, H_k$ such that $HA = \prod_{i=2}^{k} H_i$. Letting $H_{1}=H$, we have $A = HHA = \prod_{i=1}^{k}H_i$.
\end{proof}
\begin{theorem}
If $H$ is a reflection matrix, then $\det(H) = -1$.
\end{theorem}
\begin{theorem}
If $A$ is an orthogonal matrix and $A=\prod_{i=1}^{k} H_i$, then $\det(A) = (-1)^k$.
\end{theorem}
If $A$ is an orthogonal $2\times 2$ matrix, then we know that columns must be unit vectors that are also orthogonal (Orthonormal). That is, the two columns must lie on the unit circle about the origin. So we may express the first column as $\big(\cos(\theta),\sin(\theta)\big)$ for some angle $\theta$. There are then two options for the seconds column: $\big(-\sin(\theta),\cos(\theta)\big)$ or $\big(\sin(\theta),-\cos(\theta)\big)$. The first is the rotation matrix which rotates $\mathbb{R}^2$ counterclockwise around the origin, and the second is the reflection matrix that makes a reflection across the line that makes an angle $\frac{\theta}{2}$ with the $x-$axis. 
\begin{theorem}
If $A$ is a $3\times 3$ orthogonal matrix, then one of the following is true:
\begin{enumerate}
\item If $\det(A) = 1$, then $A$ is a rotation matrix.
\item If $\det(A) = -1$ and $A=A^T$, then either $A=-I$ or $A$ is a reflection matrix.
\item If $\det(A) = -1$ and $A\ne A^T$, then $A$ is the product of three reflections.
\end{enumerate}
\end{theorem}
\begin{proof}
$A$ must be the product of $0,1,2,$ or $3$ reflection matrices. If $\det(A) = 1$, then $A$ is the product of an even number of reflections, and thus either $A=I$ or $A$ is the product of two reflections, and is thus a rotation. If $\det(A)=-1$, then $A$ is the product of an odd number of reflections, either $1$ or $3$. If $A$ is a single reflection, then $A=H$ for some Householder matrix $H$. Thus $A^T = A$. Conversely, if $A = A^T$ and $\det(A) = -1$, then $\det(-A) = 1$, and $-A^T = -A = -A^{-1}$. Therefore $-A$ is a rotation whose square is the identity. If $A\ne I$, then $A$ must be a rotation of $\pi/2$ around some axis, and thus $A$ is a reflection. If $\det(A) = -1$, and $A\ne A^T$, then $A$ cannot be a rotation or a pure reflection, and thus $A$ is the product of $3$ reflection matrices.
\end{proof}
\begin{theorem}
If $A$ and $B$ are $3\times 3$ rotation matrices, then $AB$ is a rotation matrix.
\end{theorem}
\begin{proof}
For $A$ and $B$ must be orthogonal, and thus $AB$ is orthogonal. But $\det(AB) = \det(A)\det(B) = 1\cdot 1 = 1$, and thus $AB$ is an orthogonal matrix with determinant equal to $1$, and is therefore a rotation matrix.
\end{proof}
\subsection{Rotations}
The $2\times 2$ matrix $A_{\theta} = \begin{bmatrix*}[r] \cos(\theta) & -\sin(\theta) \\ \sin(\theta) & \cos(\theta)\end{bmatrix*}$ rotates the plane $\mathbb{R}^2$ counter-clockwise by the angle $\theta$ around the origin. The question that then arises is, ``Is there a similar way to do this for $\mathbb{R}^3$"? The simple case would be rotating by an angle $\theta$ about the $z-$axis, analogous the rotating the Earth by $\theta$ about the North Pole. This fixes the $z-$axis and acts on the $xy$ plane only. This can be represented by the matrix:
\begin{equation*}
 S_{\theta} = \begin{bmatrix*} \cos(\theta) & -\sin(\theta) & 0 \\ \sin(\theta) & \phantom{-}\cos(\theta) & 0\\ 0 & \phantom{-}0 & 1 \end{bmatrix*}   
\end{equation*}
$S_{\theta}$ is an orthogonal matrix. That is, $S_{\theta} S_{\theta}^T = I$, and therefore $S_{\theta}^T = S_{\theta}^{-1}$. Suppose we want to rotate by an angle $\theta$ about a different axis. Let $\mathbf{u}$ be a unit vector pointing in the direction of the axis of rotation and let $R_{\theta,\mathbf{u}}$ be the new rotation matrix. To compute $R_{\theta,\mathbf{u}}$ we need to choose a unit vector $\mathbf{v}$ that is orthogonal to $\mathbf{u}$. Let $\mathbf{w} = \mathbf{u}\times \mathbf{v}$. Then $\{\mathbf{u},\mathbf{v},\mathbf{w}\}$ is an orthonormal basis of $\mathbb{R}^3$ such that $\mathbf{v}\times \mathbf{w} = \mathbf{u}$. Let:
\begin{equation*}
    P = \begin{bmatrix} v_1 & w_1 & u_1 \\ v_2 & w_2 & u_2 \\ v_3 & w_3 & u_3 \end{bmatrix}
\end{equation*}
The columns of $P$ form an orthonormal set, and therefore $P$ is orthogonal. In particular:
\begin{align*}
    P^{T}\mathbf{v} &= e_{1} & P^{T}\mathbf{w} &= e_{2} & P^{T}\mathbf{u} &= e_{3}
\end{align*}
\begin{theorem}
If $\theta \in [0,2\pi]$ and $\mathbf{u}\in \mathbb{R}^3$ is a unit vector, then $R_{\theta, \mathbf{u}} = PS_{\theta}P^T$.
\end{theorem}
\begin{proof}
For:
\begin{align*}
    PS_{\theta}P^T\mathbf{u} &= PS_{\theta} (0,0,1)^{T} & PS_{\theta}P^{T}\mathbf{v} &= \cos(\theta)\mathbf{v}+\sin(\theta) \mathbf{w}\\
    &= P(0,0,1)^{T} & PS_{\theta}P^{T} \mathbf{w} &= -\sin(\theta) \mathbf{v}+\cos(\theta) \mathbf{w}\\
    &= \mathbf{u}
\end{align*}
Thus, if $X = a\mathbf{v}+b\mathbf{w}+c\mathbf{u}$, then:
\begin{align*}
    PS_{\theta}P^TX &= a(\cos(\theta)\mathbf{v}+\sin(\theta)\mathbf{w})+b(-\sin(\theta) \mathbf{v}+\cos(\theta)\mathbf{w})+c\mathbf{u}\\
    &= R_{\theta,\mathbf{u}}X
\end{align*}
\end{proof}
From the orthogonality of $P$ and $S_{\theta}$ we have that $R_{\theta,\mathbf{u}}$ is also orthogonal.
\begin{theorem}
\label{theorem:LINEAR_ALGEBRA_a_rotation_matrix_is_an_orthoganal_matrix_with_determinant_1}
A rotation matrix $R$ is an orthogonal matrix with determinant $1$.
\end{theorem}
\begin{proof}
For:
\begin{align*}
    R^{T}R &= (PS_{\theta}P^{T})^{T}PS_{\theta}P^{T} & &=PS_{\theta}^{T}S_{\theta}P^{T}\\
    &= (PS_{\theta}^{T}P^{T})PS_{\theta}P^{T} & &= PP^{T}\\
    &= PS_{\theta}^{T}(P^{T}P)S_{\theta}P^{T} & &= I
\end{align*}
And
\begin{align*}
    \det(R) &= \det(PS_{\theta}P^T) & &=\det(P)\det(S_{\theta})\frac{1}{\det(P)}\\
    &= \det(P)\det(S_{\theta})\det(P^T) & &= \det(S_{\theta})\\
    &= \det(P)\det(S_{\theta})\det(P^{-1}) & &= 1
\end{align*}
\end{proof}
\begin{remark}
The converse of theorem \ref{theorem:LINEAR_ALGEBRA_a_rotation_matrix_is_an_orthoganal_matrix_with_determinant_1} is also true.
\end{remark}
We now turn to the question of how to compute the rotation of $\mathbb{R}^3$ represented by a given orthogonal matrix. If $R$ is an orthogonal matrix such that $\det(R) = 1$, how do we compute the angle of rotation? Firstly, recall that the trace of a matrix $\Tr(A)$, is the sum of the diagonal components $\Tr(A) = a_{11}+\hdots + a_{nn}$.
\begin{theorem}
If $A$ and $B$ are $n\times n$ matrices, then $\Tr(AB) = \Tr(BA)$
\end{theorem}
\begin{theorem}
If $R$ is a rotation matrix of angle $\theta$, then $\cos(\theta) = \frac{\Tr(R) - 1}{2}$.
\end{theorem}
\begin{proof}
Since $R = PS_{\theta} P^T = PS_{\theta}P^{-1}$, we have:
\begin{equation*}
    \Tr(R) = \Tr(PS_{\theta}P^{-1}) = \Tr(PP^{-1}S_{\theta}) = \Tr(S_{\theta}) = 1+2\cos(\theta)\Rightarrow \cos(\theta) = \frac{\Tr(R)-1}{2}
\end{equation*}
\end{proof}
This doesn't tell us everything, as we still don't know $\mathbf{u}$, and $\cos(\theta) = \cos(-\theta)$, so we still don't know the sign of $\theta$. Since $R$ is an orthogonal matrix, $R^T = R^{-1}$. So if $\mathbf{u}$ lies on the axis of rotation, then $(R-R^T)\mathbf{u} = R\mathbf{u}-R^{-1}\mathbf{u} = \mathbf{u}-\mathbf{u} = 0$. Thus we can find the axis of rotation by determining the null space of $R-R^T$. 
\begin{equation*}
    R = \begin{bmatrix*}[r] r_{11} & r_{12} & r_{13} \\ r_{21} & r_{22} & r_{23} \\ r_{31} & r_{32} & r_{33} \end{bmatrix*} \Rightarrow R-R^{T} = \begin{bmatrix*}[r] 0 & r_{12} - r_{21} & r_{13} - r_{31} \\ r_{21} - r_{12} & 0 & r_{23}-r_{32} \\ r_{31} - r_{13} & r_{32} - r_{23} & 0 \end{bmatrix*}
\end{equation*}
Let $\alpha = r_{12} - r_{21},\beta = r_{13} - r_{31},$ and $\gamma = r_{23}-r_{32}$. Then:
\begin{equation*}
    R-R^T = \begin{bmatrix*}[r] 0 & \alpha & \beta \\ -\alpha & 0 & \gamma \\ -\beta & -\gamma & \phantom{-}0 \end{bmatrix*}
\end{equation*}
This suggests that $\mathbf{u}$ is parallel to $(-\gamma, \beta, -\alpha)^{T} = (r_{32}-r_{23}, r_{13}-r_{31}, r_{21}-r_{12})^{T}$.
\begin{theorem}
If $R$ is a rotation matrix such that $R\ne R^T$, then the axis of rotation of $R$ is parallel to $\mathbf{q}=(-\gamma, \beta, -\alpha)^{T} = 2\sin(\theta)\mathbf{u}$, where $\mathbf{u}$ is a unit vector about the axis of rotation.
\end{theorem}
\begin{proof}
Let $R = PS_{\theta}P^T$. Then:
\begin{align*}
    R-R^{T} &= PS_{\theta}P^{T} - PS_{\theta}^{T}P^{T} & &= P\begin{bmatrix}0 & -2\sin(\theta) & 0 \\ 2\sin(\theta) & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix}P^{T}\\
    &= P(S_{\theta}-S_{\theta}^{T})P^{T} & &= 2\sin(\theta)(\mathbf{w}\mathbf{v}^{T} - \mathbf{v}\mathbf{w}^{T})
\end{align*} 
Where $\mathbf{v}$ is orthogonal to $\mathbf{u}$ and $\mathbf{w} = \mathbf{u}\times \mathbf{v}$. Therefore:
\begin{equation*}
    \mathbf{q} = (-\gamma, \beta, -\alpha)^{T} = 2\sin(\theta) \mathbf{v}\times \mathbf{w} = 2\sin(\theta) \mathbf{u}
\end{equation*}
\end{proof}
\begin{remark}
What about the case when $R-R^T = 0$? When this happens either $\theta = 0$ or $\theta = \pi$. If $\theta = 0$, then this is the identity rotation and thus $R = I$, and we are done. If $R\ne I$, then $\theta = \pi$. To find out the axis of rotation, we have that:
\begin{equation*}
    R = PS_{\pi}P^T = \begin{bmatrix}-1 & 0 & 0 \\ 0 & -1 & 0 \\ 0 & 0 & 1 \end{bmatrix} = -\mathbf{v}\mathbf{v}^T - \mathbf{w}\mathbf{w}^T +\mathbf{u}\mathbf{u}^T    
\end{equation*}
But $\mathbf{v},\mathbf{w},$ and $\mathbf{u}$ form an orthonormal basis, and therefore $\mathbf{v}\mathbf{v}^T + \mathbf{w}\mathbf{w}^T+\mathbf{u}\mathbf{u}^T = I$. Thus, $R = -I+2\mathbf{u}\mathbf{u}^T$, so $\mathbf{u} \mathbf{u}^T = \frac{1}{2}(R+I)$. But the columns of $\mathbf{u}\mathbf{u}^T$ are parallel to $\mathbf{u}$, and therefore we can determine $\mathbf{u}$ by normalizing one of the columns of $\frac{1}{2}(R+I)$.
\end{remark}
\subsection{The Matrix Exponential}
\begin{definition}
If $A$ is an $n\times n$ matrix, then the exponential of $A$ is $e^{A} =\sum_{k=0}^{\infty} \frac{A^k}{k!}$.
\end{definition}
\begin{remark}
Notationally, we write $A^0 = I$. For any complex-valued matrix $A$ of finite dimension, it can be shown that this sum converges.
\end{remark}
\begin{theorem}
If $A$ and $P$ are complex $n\times n$ matrices and $P$ is an $n\times n$ invertible matrix, then $e^{P^{-1}AP} = P^{-1}e^{A}P$.
\end{theorem}
\begin{proof}
For all $m\in \mathbb{N}$, $(P^{-1}AP)^{m} = P^{-1}A^mP$. Thus:
\begin{equation*}
    e^{P^{-1}AP} = \sum_{k=0}^{\infty} P^{-1}\frac{A^k}{k!}P = P^{-1}\big(\sum_{k=0}^{\infty} \frac{A^k}{k!}\big)P = P^{-1}e^A P
\end{equation*}
\end{proof}
\begin{theorem}
If $0$ is the zero matrix, then $e^0 = I$.
\end{theorem}
\begin{theorem}
If $A$ is an $n\times n$ matrix and $m\in \mathbb{N}$, then $A^{m} e^{A} = e^{A} A^{m}$.
\end{theorem}
\begin{proof}
For $A^{m} e^{A} = A^{m} \sum_{k=0}^{\infty} \frac{A^{k}}{k!} = \sum_{k=0}^{\infty} \frac{A^{k+m}}{k!} = \big(\sum_{k=0}^{\infty} \frac{A^k}{k!}\big)A^{m}$.
\end{proof}
\begin{theorem}
If $A$ is an $n\times n$ matrix, then $e^{A^{T}} = (e^{A})^{T}$.
\end{theorem}
\begin{proof}
For $e^{A^{T}} = \sum_{k=0}^{\infty} \frac{(A^{T})^{k}}{k!} = \sum_{k=0}^{\infty} \frac{(A^{k})^{T}}{k!} = \big(\sum_{k=0}^{\infty} \frac{A^{k}}{k!}\big)^{T} = (e^{A})^{T}$.
\end{proof}
\begin{theorem}
If $A$ and $B$ are $n\times n$ matrices and if $AB = BA$, then $Ae^{B} = e^{B} A$.
\end{theorem}
\begin{proof}
For $Ae^{B} = A\sum_{k=0}^{\infty} \frac{B^{k}}{k!} = \sum_{k=0}^{\infty} A\frac{B^{k}}{k!} = \sum_{k=0}^{\infty} \frac{B^{k}}{k!}A = \big(\sum_{k=0}^{\infty} \frac{B^{k}}{k!}\big)A = e^{B}A$.
\end{proof}
\begin{theorem}
If $A$ and $B$ are $n\times n$ matrices and $AB = BA$, then $e^{A}e^{B} = e^{B}e^{A}$.
\end{theorem}
\begin{proof}
For:
\begin{align*}
    e^A e^B &= e^A\sum_{k=0}^{\infty} \frac{B^k}{k!} & &=\sum_{k=0}^{\infty}\big(\sum_{j=0}^{\infty} \frac{B^k}{k!}\frac{A^j}{j!}\big) \\
    &= \sum_{k=0}^{\infty} e^A\frac{B^k}{k!} & &=\sum_{k=0}^{\infty}\big(\sum_{j=0}^{\infty} \frac{B^k}{k!}\big)\frac{A^j}{j!} \\
    &= \sum_{k=0}^{\infty} \big(\sum_{j=0}^{\infty} \frac{A^j}{j!}\big) \frac{B^k}{k!} & &= \sum_{k=0}^{\infty} \frac{B^k}{k!} \sum_{j=0}^{\infty} \frac{A^j}{j!} \\
    &= \sum_{k=0}^{\infty}\big(\sum_{j=0}^{\infty} \frac{A^j}{j!}\frac{B^k}{k!}\big) & &= e^Be^A
\end{align*}
\end{proof}
If is NOT true that $e^{A+B} = e^Ae^B$, in general. Matrix exponentiation lacks this feature.
\begin{theorem}
If $A$ is a $n\times n$ matrix and $s,t\in \mathbb{C}$, then $e^{A(s+T)} = e^{As}e^{At}$.
\end{theorem}
\begin{proof}
For $e^{As}e^{At} = \sum_{j=0}^{\infty} \sum_{k=0}^{\infty} \frac{A^{j+k}s^jt^k}{j!k!}$. Letting $n = j+k$, so $j = n-k$, we have:
\begin{equation*}
    \sum_{n=0}^{\infty} \sum_{k=0}^{\infty} \frac{A^n}{n!}\frac{n!}{k!(n-k)!}s^{n-k}t^k = \sum_{n=0}^{\infty}\frac{A^n}{n!}\big(\sum_{k=0}^{\infty} \frac{n!}{k!(n-k)!}s^{n-k}t^k\big)    
\end{equation*}
From the binomial theorem, the expression inside the parenthesis is equal to $(s+t)^n$. So we have $e^{As}e^{At}=\sum_{n=0}^{\infty} \frac{A^n(t+s)^n}{n!} = e^{A(s+t)}$.
\end{proof}
\begin{theorem}
If $A$ is an $n\times n$ matrix, then $e^A$ is invertible and $(e^A)^{-1} = e^{-A}$.
\end{theorem}
\begin{proof}
For $I = e^{0} = e^{A(1-1)} = e^Ae^{-A}$. Thus $(e^{A})^{-1} = e^{-A}$.
\end{proof}
\begin{theorem}
If $A$ is an $n\times n$ matrix and $t\in \mathbb{R}$, then $\frac{d}{dt}\big(e^{At}\big) = Ae^{At}$.
\end{theorem}
\begin{proof}
For $\underset{h\rightarrow 0}\lim \frac{e^{A(t+h)}-e^{At}}{h} = e^{At}\underset{h\rightarrow 0}\lim \frac{e^{Ah}-I}{h} = e^{At}\underset{h\rightarrow 0}\lim\big[A+\frac{A^2}{2!}h+\hdots\big] = e^{At}A = Ae^{At}$.
\end{proof}
\begin{theorem}
If $A$ and $B$ are $n\times n$ matrices and $AB=BA$, then $e^{A+B} = e^{A}e^{B}$.
\end{theorem}
\begin{proof}
For let $g(t) = e^{(A+B)t}e^{-Bt}e^{-At}$. Then from commutativity of $A$ and $B$, $g'(t) = 0$. But then $g(t)$ is a constant. From the definition, $g(0) = I$, and thus $g(t) = I$. So $e^{(A+B)t}e^{-Bt}e^{-At} = I$, and therefore $e^{(A+B)t} = e^{At}e^{Bt}$.
\end{proof}
\begin{theorem}
If $A^{2} = 0$, then $e^{A} = I+A$.
\end{theorem}
\begin{proof}
For $e^{A} = I+A+A^{2}\big(\frac{I}{2!}+\frac{A}{3!}+\hdots\big) = I+A+0 = I+A$.
\end{proof}
\subsection{Linear Systems of Ordinary Differential Equations}
Consider the equation $y' = ky$, where $k$ is some constant. We can solve this via calculus using separation of variables:
\begin{equation*}
    \frac{y'}{y} = k\Rightarrow \int \frac{y'}{y}dx = \int kdx \Rightarrow \ln(y) = kx+c \Rightarrow y = e^c e^{kx}    
\end{equation*}
Setting $x=0$, we have $e^c = y_0$. So $y = y_0e^{kx}$. Let us solve this a different way: Let $F(x) = e^{-kx}y$, and let $y'=kx$. Differentiating we have:
\begin{equation*}
    F'(x) = -ke^{kx}y + e^{-kx}y' = -ke^{-kx}+e^{=kx}ky = 0    
\end{equation*}
So $F'(x) = 0$, and therefore $F(x)$ is a constant. Setting $x=0$, we have $F(x) = y_0$. So $y = y_0e^{kx}$. This shows us that $y_0e^{kx}$ is the $only$ solution to this problem. Let:
\begin{equation*}
    Y(t) = \begin{bmatrix} y_1(t) \\ y_2(t)\end{bmatrix}    
\end{equation*}
Where $A$ is an $n\times n$ matrix, and consider the system $Y'(t) = AY(t)$. Let $F(t) = e^{-At}Y(t)$. Then $F'(t) = 0$, and we have that $Y(t) = Y_0 e^{At}$.
\begin{theorem}
If $Y:\mathbb{R}\rightarrow \mathbb{R}^n$ is a differentiable function such that $Y'(t) = AY(t)$, where $A$ is a diagonalizable matrix with eigenvalues $\lambda_1,\hdots, \lambda_n$ and eigenvectors $v_1,\hdots, v_n$, then $Y(t) = \sum_{k=1}^{n} \lambda_k e^{\lambda_k t}v_k$
\end{theorem}
\section{Problem Sets}
\subsection{Problem Set I}
\begin{problem}
Find the point on the line $y=4x$ which is closest to the point $(2,5)$.
\end{problem}
\begin{proof}[Solution 1]
Given a vector $\mathbf{v}$ that is parallel to the line $y$, we know that the vector $\mathbf{w}$ from $(2,5)$ to the point $(x,y)$ that minimizes the distance from $y=4x$ to the point $(2,5)$ will satisfy $\langle \mathbf{v}, \mathbf{w}\rangle = 0$. That is:
\begin{equation*}
    \big\langle (1,4), (2-x,5-y)\big\rangle = 0\Rightarrow 2-x+4(5-y) = 0 \Rightarrow 22 - x - 4 y = 0    
\end{equation*}
But $y = 4x$, and thus $22-17x = 0 \Rightarrow x= \frac{22}{17}$. The point of least distance is $\frac{22}{17}(1,4)$.
\end{proof}
\begin{proof}[Solution 2]
This point is the projection of the vector $(2,5)^T$ onto $(1,4)^T$. That is:
\begin{equation*}
    \mathbf{P} = \frac{\begin{bmatrix}2 & 5 \end{bmatrix} \begin{bmatrix} 1 \\ 4 \end{bmatrix}}{\begin{bmatrix} 1 & 4 \end{bmatrix} \begin{bmatrix} 1 \\ 4 \end{bmatrix}} \begin{bmatrix} 1 \\ 4 \end{bmatrix} = \frac{22}{17} \begin{bmatrix} 1 \\ 4\end{bmatrix}
\end{equation*}
\end{proof}
\begin{problem}
Show that $\mathbf{x}\mathbf{y}^T + \mathbf{y}\mathbf{x}^T$ is symmetric.
\end{problem}
\begin{proof}[Solution]
Recall that a matrix is symmetric if it is equal to its transpose. Thus, we must show $A = A^T$. But for any $n\times n$ matrices $A$ and $B$, $(A+B)^T = A^T + B^T$, and $(AB)^T = B^T A^T$, and $(A^T)^T = A$. Thus, given our matrix $A= \mathbf{x}\mathbf{y}^T + \mathbf{y}\mathbf{x}^T$, we have that $A^T = (\mathbf{x}\mathbf{y}^T + \mathbf{y}\mathbf{x}^T)^T = (\mathbf{x}\mathbf{y}^T)^T + (\mathbf{y}\mathbf{x}^T)^T = (\mathbf{y}^T)^T\mathbf{x}^T + (\mathbf{x}^T)^T\mathbf{y}^T = \mathbf{y}\mathbf{x}^T + \mathbf{x}\mathbf{y}^T = \mathbf{x}\mathbf{y}^T + \mathbf{y}\mathbf{x}^T = A$
\end{proof}
\begin{problem}
Compute the product $\begin{bmatrix} 2 & -1 \\ 3 & 1\end{bmatrix} \begin{bmatrix} -1 & 2 & 3 & 1 \\ 2 & -2 & 1 & -1 \end{bmatrix}$
\end{problem}
\begin{proof}[Solution]
\begin{align*}
    &\begin{bmatrix} 2 & -1 \\ 3 & 1\end{bmatrix} \begin{bmatrix} -1 & 2 & 3 & 1 \\ 2 & -2 & 1 & -1 \end{bmatrix}\\
    =&\begin{bmatrix} 2(-1)+(-1)2 & 2\cdot 2 + (-1)(-2) & 2\cdot 3 + (-1)1 & 2\cdot 1 + (-1)(-1) \\ 3(-1)+1\cdot 2 & 3\cdot 2 + 1(-2) & 3\cdot 3 + 1\cdot 1 & 3\cdot 1 + 1(-1)\end{bmatrix}\\
    =&\begin{bmatrix} -4 & 6 & 5 &3 \\ -1 & 4 & 10 & 2\end{bmatrix}
\end{align*}
\end{proof}
\begin{problem}
Find the equation of the plane that contains $P_{1}(2,2,1),P_{2}(2,3,2)$, and $P_{3}(-1,3,1)$.
\end{problem}
\begin{proof}[Solution]
It suffices to find a vector normal to this plane. We have that:
\begin{align*}
    \overrightarrow{P_1P_2} &= (0,1,1)^T\\
    \overrightarrow{P_1P_3} &= (-3,1,0)^T
\end{align*}
Then both vectors are parallel to the plane, and thus $\overrightarrow{P_1P_2}\times \overrightarrow{P_1P_3}=(-1,3,3)^T$ is perpendicular to the plane. Suppose $Q=(x,y,z)$ is a point in the plane. Then the relative position vector $P_1 Q = (x-2,y-2,z-1)^T$ is orthogonal to $(-1,3,3)^T$. Thus:
\begin{align*}
    (x-2,y-2,z-1)(-1,3,3)^T &= 0\\
    \Rightarrow 2-x+3y-6+3z-3 &= 0\\
    \Rightarrow x-3y-3z +7 &= 0   
\end{align*}
This is the equation of the plane.
\end{proof}
\begin{problem}
Let $S$ be the subspace of $\mathbb{R}^3$ spanned by $\mathbf{x}_1 = (1,-1,2)^T$ and $\mathbf{x}_2 = (-1,2,2)^T$. Find a basis for $S^{\perp}$
\end{problem}
\begin{proof}[Solution]
We seek a vector in $\mathbf{x}_3\in\mathbb{R}^3$ such that $\langle \mathbf{x}_3, \mathbf{x}_{1,2}\rangle = 0$. Or in other words:
\begin{equation*}
    \begin{bmatrix} 1 & -1 & 2 \\ 0 & 1 & 4 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} = 0    
\end{equation*}
This gives the following:
\begin{align*}
x_1 - x_2 + 2x_3 &= 0\\
	x_2 + 4x_3 &= 0
\end{align*}
So $x_2 = -4x_3$, $x_1=-6x_3$. Thus $\mathbf{x}_3 = x_3(-6,-4,1)^T$. $x_3$ is a free variable.
\end{proof}
\begin{problem}
For the matrix $A = \begin{bmatrix} 1 & 2 & 2 \\ -1 & -1 & 0 \end{bmatrix}$, find a basis for the following:
\begin{enumerate}
\begin{multicols}{4}
    \item $R(A^T)$
    \item $N(A)$
    \item $R(A)$
    \item $N(A^T)$
\end{multicols}
\end{enumerate}
\end{problem}
\begin{proof}[Solution] In order,
\begin{enumerate}
    \item Putting $A$ into row-echelon form, we get $\begin{bmatrix} 1 & 1 & 0 \\ 0 & 1 & 2 \end{bmatrix}$. Thus, reading off the rows gives us a basis for $R(A^T)$. That is, $(1,1,0)^T$ and $(0,1,2)^T$.
    \item $N(A) = \{x\in \mathbb{R}^3: Ax = 0\}$. Thus:
    \begin{align*}
        x_1 + 2x_2 + 2x_3 &= 0\\
        x_2 + 2x_3 &= 0
    \end{align*}
    This gives us a basis of $x_3(2,-2,1)^T$, where $x_3$ is a free variable.
    \item Putting $A^T$ into row echelon form gives us $\begin{bmatrix} 1 & 0 \\ 0 & -1 \\ 0 & 0 \end{bmatrix}$. Reading off the non-zero row vectors gives us a basis: $(1,0)^T, (0,-1)^T$.
    \item $N(A^T)= \{x\in \mathbb{R}^2: A^T x = 0\}$. Putting $A^T$ into row echelon form reduces the problem to $\begin{bmatrix} 1 & 0 \\ 0 & -1 \\ 0 & 0 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = 0$. This gives $x_1 = 0$ and $-x_2 = 0$. So, $N(A^T) = \{0\}$.
\end{enumerate}
\end{proof}
\subsection{Problem Set II}
\begin{problem}
Find a point on the line $y=5x$ that is closest to the point $(1,3)$.
\end{problem}
\begin{proof}[Solution]
Pick a point on the line, say $\mathbf{w} = (1,5)^T$. The point $P$ is the projection of $\mathbf{v} = (1,3)^T$ onto the line $y=5x$, and thus:
\begin{equation*}
    P = \frac{v^T w}{w^T w} = \frac{\begin{bmatrix}1 & 5 \end{bmatrix}\begin{bmatrix}1 \\ 5\end{bmatrix}}{\begin{bmatrix}1 & 5 \end{bmatrix}\begin{bmatrix}1 \\ 5\end{bmatrix}}(1,5)^T = \frac{8}{13}(1,5)^T
\end{equation*}
\end{proof}
\begin{problem}
Is $A = xy^T - yx^T$ symmetric? ($x$ and $y$ are $n\times 1$ vectors)
\end{problem}
\begin{proof}[Solution]
In general, no. For if it were, then $A-A^T = 0$. But:
\begin{align*}
    A-A^T &= xy^T - yx^T - (xy^T-yx^T)^T & &= 2xy^T-2yx^T\\
    &= xy^T - yx^T -[(xy^T)^T-(yx^T)^T] & &= 2A\\
    &= xy^T - yx^T - [yx^T - xy^T] & \Rightarrow A+A^{T}&=0 
\end{align*}
But $A-A^{T} = 0$, and therefore $2A = 0$. But then:
\begin{equation*}
    xy^T - yx^T = 0 \Rightarrow xy^T = yx^T    
\end{equation*}
As this is not, in general, true, $A$ is not necessarily symmetric.
\end{proof}
\begin{problem}
Compute the product $\begin{bmatrix} -1 & 3 \\ 4 & 2 \end{bmatrix} \begin{bmatrix} -1 & 1 & 2 & -2 \\ 2 & 3 & 1 & 1 \end{bmatrix}$
\end{problem}
\begin{proof}[Solution]
\begin{align*}
    \begin{bmatrix} -1 & 3 \\ 4 & 2 \end{bmatrix} \begin{bmatrix} -1 & 1 & 2 & -2 \\ 2 & 3 & 1 & 1 \end{bmatrix} &= \begin{bmatrix} 1+6 & -1 + 9 & -2 +3 & 2 + 3 \\ -4 + 4 & 4+6 & 8 + 2 & -8 + 2 \end{bmatrix}\\
    &= \begin{bmatrix} 7 & 8 & 1 & 5 \\ 0 & 10 & 10 & -6 \end{bmatrix}
\end{align*}
\end{proof}
\begin{problem}
Find the equation of the plane that passes through the points $P_1 = (2,2,2),\ P_2 = (2,3,4),\ P_3 = (-1,3,3)$.
\end{problem}
\begin{proof}[Solution]
$\overrightarrow{P_1P_2} = (0,1,2)^{T}$, $\overrightarrow{P_1 P_3} = (-3,1,1)^{T}$. So:
\begin{equation*}
    \overrightarrow{N} = \begin{vmatrix} \hat{\mathbf{i}} & \hat{\mathbf{j}} & \hat{\mathbf{k}} \\ 0 & 1 & 2 \\ -3 & 1 & 1 \end{vmatrix} = \hat{\mathbf{i}}(1-2) + \hat{\mathbf{j}}(0+6) + \hat{\mathbf{k}}(0+3)=\begin{bmatrix}-1 \\ -6 \\ 3\end{bmatrix}   
\end{equation*}
We get the equation of the plane from the fact that for some point $P=(x,y,z)$ in the plane, $\langle \overrightarrow{P_1P}, \overrightarrow{N}\rangle = 0$. Thus, $x + 6y - 3z =0$
\end{proof}
\begin{problem}
Let $S$ be the subspace of $\mathbb{R}^3$ spanned by $\mathbf{x}_1 = (2,1,2)^T$ and $\mathbf{x}_2 = (-2,-1,3)^T$. Find a basis for $S^{\perp}$.
\end{problem}
\begin{proof}[Solution]
Let $A = \begin{bmatrix} 2 & 1 & 2 \\ -2 & -1 & 3\end{bmatrix}$. Then $S^{\perp} = N(A)$. So, it suffices to find a basis for $N(A)$. Putting $A$ into row echelon form, we get $\begin{bmatrix} 2 & 1 & 2 \\ 0 & 0 & 5 \end{bmatrix}$. Solving for $Ax = 0$, we get:
\begin{align*}
     2x_1 + x_2 + 2x_3 &= 0\\ 
     5x_3 &= 0    
\end{align*}
So $x_3 = 0$, and $x_2 = - 2x_1$. $S^{\perp}$ has the basis $x_1(1,-2,0)^T$, where $x_1$ is a free variable.
\end{proof}
\begin{problem}
Let $A = \begin{bmatrix} 2 & 3 & 4 \\ -2 & -2 & 0 \end{bmatrix}$. Find a basis for the following:
\begin{enumerate}
\begin{multicols}{4}
    \item $R(A^T)$
    \item $N(A)$
    \item $R(A)$
    \item $N(A^T)$
\end{multicols}
\end{enumerate}
\end{problem}
\begin{proof}[Solution]
In order,
\begin{enumerate}
    \item Putting $A$ into row echelon form, we get $\begin{bmatrix} 1 & 1 & 0 \\ 0 & 1 & 4 \end{bmatrix}$. Thus, $(1,1,0)^T$ and $(0,1,4)^T$ form a basis for $R(A^T)$.
    \item $N(A) = \{x\in \mathbb{R}^3: Ax = 0\}$. So, we have:
    \begin{align*}
        x_1 + x_2 &= 0\\
        x_2 + 4x_3 &= 0    
    \end{align*}
This leads to $x_3(4,-4,1)^T$, where $x_3$ is a free variable.
\item Putting $A^T$ into row echelon form, $\begin{bmatrix} 1 & 0 \\ 0 & 1 \\ 0 & 0 \end{bmatrix}$. This gives a basis of $(1,0)^T$ and $(0,1)^T$.
\item $N(A^T) = \{x\in \mathbb{R}^2: A^Tx = 0\}$. This gives $\begin{bmatrix} 1 & 0 \\ 0 & 1 \\ 0 & 0 \end{bmatrix} \begin{bmatrix}x_1 \\ x_2 \end{bmatrix} = 0$. So, $x_1 = 0$ and $x_2 = 0$. Thus, $x=0$. $N(A^T) = 0$.
\end{enumerate}
\end{proof}
\subsection{Problem Set III}
\begin{problem}
Let $A,B,C$ be $n\times n$ matrices. Is $A = BC^T + CB^T$ symmetric?
\end{problem}
\begin{proof}[Solution]
A matrix is symmetric if $A = A^T$. If $A = BC^T+CB^T$, then:
\begin{align*}
    A^T &= (BC^T+CB^T)^T & &= (C^T)^TB^T + (B^T)^TC^T & &= BC^T + CB^T\\
    &=  (BC^T)^T + (CB^T)^T & &= CB^T + BC^T & &= A
\end{align*}
$A$ is symmetric.
\end{proof}
\begin{problem}
Compute $\norm{x}_1, \norm{x}_2, \norm{x}_3$ for $x = (2,-3,1)^T$
\end{problem}
\begin{proof}[Solution]
By definition, for $x\in \mathbb{R}^n$, $\norm{x}_p = (\sum_{k=1}^{n}|x_k|^p)^{1/p}$. So we have the following:
\begin{enumerate}
    \item $\norm{x}_1 = |2|+|-3|+|1| = 2+3+1 = 6$
    \item $\norm{x}_2 = (|2|^2+|-3|^2+|1|^2)^{1/2} = (4+9+1)^{1/2} = \sqrt{14}$
    \item $\norm{x}_3 = (|2|^3+|-3|^3+|1|^3)^{1/3} = (8+27+1)^{1/3} = \sqrt[3]{36}$
\end{enumerate}
\end{proof}
\begin{problem}
For the matrix $A = \begin{bmatrix} 2 & -2 & 4 \\ -1 & 1 & -2 \end{bmatrix}$, find a basis for the following:
\begin{enumerate}
\begin{multicols}{4}
    \item $R(A^T)$
    \item $N(A)$
    \item $R(A)$
    \item $N(A^T)$
\end{multicols}
\end{enumerate}
\end{problem}
\begin{proof}[Solution]
In order,
\begin{enumerate}
    \item Putting $A$ into row echelon form, we get $\begin{bmatrix} 1 & -1 & 2 \\ 0 & 0 & 0 \end{bmatrix}$. $(1,-1,2)^T$ is a basis for $R(A^{T})$.
    \item $N(A) = \{x\in \mathbb{R}^3: Ax = 0\}$. Using the row echelon form we get:
    \begin{equation*}
        \begin{bmatrix} 1 & -1 & 2 \\ 0 & 0 & 0 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = 0
    \end{equation*}
    So $x_1 - x_2 + 2x_3 = 0$. This gives us two free variables, and we get $x_2(1,1,0)^T + x_3(-2,0,1)^T$ as a basis.
    \item Putting $A^T$ into row echelon form, we get $\begin{bmatrix} -2 & 1 \\ 0 & 0 \\ 0 & 0 \end{bmatrix}$. $(-2,1)^T$ is a basis for $R(A)$.
    \item $N(A^T) = \{x\in \mathbb{R}^2: A^T x = 0\}$. So:
    \begin{equation*}
        \begin{bmatrix} -2 & 1 \\ 0 & 0 \\ 0 & 0 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = 0    
    \end{equation*}
    So, $-2x_1 + x_2 = -$, or $x_2 = 2x_1$. $(1,2)^T$ is a basis for $N(A^{T})$.
\end{enumerate}
\end{proof}
\begin{problem}
Find the least-squares solution to the following system:
\begin{align*}
    x_1 - x_2 &=2\\
    x_1 + x_2 &= 0\\
    x_1 + 2x_2 &=-1
\end{align*}
\end{problem}
\begin{proof}[Solution]
We want the solution to $A^T A x = A^T b$, where $A = \begin{bmatrix} 1 & -1 \\ 1 & 1 \\ 1 & 2 \end{bmatrix}$, and $b = \begin{bmatrix} 2\\0\\-1\end{bmatrix}$. Computing $A^T A$, we get $\begin{bmatrix} 3 & 1 \\ 1 & 9 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = \begin{bmatrix} 1 \\ -6 \end{bmatrix}$. The solution is $x = \frac{1}{26}(15,-19)^T$
\end{proof}
\begin{problem}
Let $\theta\in\mathbb{R}$ and let $\mathbf{x}_1 = (\cos(\theta), \sin(\theta))^{T}$, $\mathbf{x}_2 = (-\sin(\theta), \cos(\theta))^{T}$. Show that $\{\mathbf{x}_1,\mathbf{x}_2\}$ is an orthonormal basis for $\mathbb{R}^2$. Given the vector $\mathbf{y} = (-2, 3)^{T}$, write it as a linear combination $\mathbf{y} = c_1 \mathbf{x}_1+c_2\mathbf{x}_2$
\end{problem}
\begin{proof}[Solution]
They are orthogonal as $\mathbf{x}_1^T \mathbf{x}_2 = -\cos(\theta)\sin(\theta) + \cos(\theta)\sin(\theta) = 0$. They are orthonormal as $\norm{\mathbf{x}_1} = \sqrt{\cos^2(\theta)+\sin^2(\theta)} = 1$ and $\norm{\mathbf{x}_2} = \sqrt{(-\sin^2(\theta))+\cos^2(\theta)}=1$. Thus they are an orthonormal basis. We want $\mathbf{y} = c_1 \mathbf{x}_1 + c_2 \mathbf{x}_2$, so $\langle \mathbf{y}, \mathbf{x}_{1}\rangle = c_{1}$ and $\langle \mathbf{y},\mathbf{x}_{2}\rangle = c_{2}$. Therefore, $c_1 = -2\cos(\theta)+3\sin(\theta)$ and $c_2 = 2\sin(\theta)+3\cos(\theta)$. Thus, $\mathbf{y} = (-2\cos(\theta)+3\sin(\theta))\mathbf{x}_1 + (2\sin(\theta)+3\cos(\theta)\mathbf{x}_2)$
\end{proof}
\subsection{Problem Set IV}
\begin{problem}
Find the eigenvalues and associated eigenspaces of $A = \begin{bmatrix}4 & 5 \\ 2 & 1 \end{bmatrix}$
\end{problem}
\begin{proof}[Solution]
We need to compute $\det(A-\lambda I)=0$. This gives us:
\begin{equation*}
    \begin{vmatrix} 4-\lambda & 5 \\ 2 & 1-\lambda \end{vmatrix} = (4-\lambda)(1-\lambda)-10 = 0
\end{equation*}
The solutions to this are $\lambda_1 = 6, \lambda_2 = -1$. Solving $Ax = \lambda x$ yields the eigenspaces. We have:
\begin{align*}
    \begin{bmatrix} 4 & 5 \\ 2 & 1 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} &= -\begin{bmatrix} x_1 \\ x_2 \end{bmatrix}\\
    \begin{bmatrix} 4 & 5 \\ 2 & 1 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} &= 6\begin{bmatrix} x_1 \\ x_2 \end{bmatrix}
\end{align*}
These give solutions $x_2(-1,1)^T$ and $x_2 (\frac{5}{2},1)^T$, where $x_2$ is a free variable.
\end{proof}
\begin{problem}
Show that for a $2\times 2$ matrix $A$, $\lambda^2 - \Tr(A)\lambda + \det(A) = 0$, where $\lambda$ is an eigenvalue of $A$.
\end{problem}
\begin{proof}[Solution]
For we have that $\det(A-\lambda I) = 0$. But:
\begin{align*}
    \det(A-\lambda I) &= \begin{vmatrix} a-\lambda & b \\ c & d-\lambda \end{vmatrix} & &= \lambda^2 - (a+d)\lambda + ad - bc\\
    &= (a-\lambda)(d-\lambda) - bc & &= \lambda^{2} - \Tr(A)\lambda + \det(A)
\end{align*}
Thus, $\lambda^2 - \Tr(A) \lambda + \det(A) = 0$.
\end{proof}
\begin{problem}
Find the eigenvalues and associated eigenspaces for $A = \begin{bmatrix} 1 & 1 & 1 \\ 0 & 2 & 1 \\ 0 & 0 & 3\end{bmatrix}$
\end{problem}
\begin{proof}[Solution]
Recall that the determinant expansion can be done along any row. Thus:
\begin{align*}
    \det(A-\lambda I) &= \begin{vmatrix} 1-\lambda & 1 & 1 \\ 0 & 2-\lambda & 1 \\ 0 & 0 & 3-\lambda \end{vmatrix}\\
    &= 0\begin{vmatrix} 1 & 1 \\ 2-\lambda & 1 \end{vmatrix} - 0 \begin{vmatrix} 1-\lambda & 1 \\ 0 & 1 \end{vmatrix} + (3-\lambda)\begin{vmatrix} 1-\lambda & 1 \\ 0 & 2-\lambda\end{vmatrix}\\
    &= (3-\lambda)(1-\lambda)(2-\lambda)    
\end{align*}
The solutions are $\lambda_1 = 1,\ \lambda_2 = 2,\ \lambda_3 = 3$. The eigenspaces correspond to the solutions of the equation $Ax = \lambda x$. Thus we get:
\begin{equation*}
    \begin{bmatrix} 1 & 1 & 1 \\ 0 & 2 & 1 \\ 0 & 0 & 3 \end{bmatrix}\begin{bmatrix} x \\ y \\ z \end{bmatrix} = \lambda \begin{bmatrix}x \\ y \\ z\end{bmatrix}    
\end{equation*}
This gives 3 different equations for each value of $\lambda$.
\begin{enumerate}
    \item $\begin{bmatrix} 1 & 1 & 1 \\ 0 & 2 & 1 \\ 0 & 0 & 3 \end{bmatrix}\begin{bmatrix} x \\ y \\ z \end{bmatrix} = \ \  \begin{bmatrix}x \\ y \\ z\end{bmatrix}\hspace{2.48 cm} \begin{bmatrix} x \\ y \\ z \end{bmatrix} = (1,0,0)^T$
    \item $\begin{bmatrix} 1 & 1 & 1 \\ 0 & 2 & 1 \\ 0 & 0 & 3 \end{bmatrix}\begin{bmatrix} x \\ y \\ z \end{bmatrix} = 2\begin{bmatrix}x \\ y \\ z\end{bmatrix}\hspace{2.54 cm}\begin{bmatrix} x \\ y \\ z \end{bmatrix} = (1,1,0)^T$
    \item $\begin{bmatrix} 1 & 1 & 1 \\ 0 & 2 & 1 \\ 0 & 0 & 3 \end{bmatrix}\begin{bmatrix} x \\ y \\ z \end{bmatrix} = 3\begin{bmatrix}x \\ y \\ z\end{bmatrix}\hspace{2.54 cm} \begin{bmatrix} x \\ y \\ z \end{bmatrix} = (1,1,1)^T$
\end{enumerate}
\end{proof}
\subsection{Problem Set V}
\begin{problem}
Factor the matrix $\begin{bmatrix} 4 & 2 \\ 2 & 1 \end{bmatrix}$ into the form $PDP^T$, where $D$ is a diagonal matrix and $P$ is an orthogonal matrix.
\end{problem}
\begin{proof}[Solution]
The eigenvectors of $A$ are the solutions to $(4-\lambda)(1-\lambda)-4=0$, which gives $\lambda_1 = 0$, and $\lambda_2 = 5$. The eigenvectors are solutions to:
%
\begin{equation*}
    \begin{bmatrix} 4 & 2 \\ 2 & 1 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} = \lambda \begin{bmatrix} x \\ y \end{bmatrix}
\end{equation*}
%
Which gives us $\frac{1}{\sqrt{5}}(2,1)^T$ and $\frac{1}{\sqrt{5}}(-1,2)^T$. Thus:
%
\begin{equation*}
    P = \frac{1}{\sqrt{5}}\begin{bmatrix} -1 & 2 \\ 2 & 1 \end{bmatrix}\quad\quad D = \begin{bmatrix} 0 & 0 \\ 0 & 5 \end{bmatrix}\quad\quad P^{T} = \frac{1}{\sqrt{5}}\begin{bmatrix} -1 & 2 \\ 2 & 1 \end{bmatrix}
\end{equation*}
\end{proof}
\begin{problem}
Solve the differential equation $Y'(t) = \begin{bmatrix} 4 & 2 \\ 2 & 1 \end{bmatrix} Y(t)$ with $Y(0) = \begin{bmatrix} -1 \\ 4 \end{bmatrix}$
\end{problem}
\begin{proof}[Solution]
We know from the previous problem that the eigenvalues and eigenvectors are distinct, and thus $Y(t) = \alpha V_1 e^{\lambda_1 t} + \beta V_2 e^{\lambda_2 t}$ where $\lambda_{i}$ are the distinct eigenvalues, and $V_{i}$ are the distinct eigenvectors. Solving for the initial condition:
%
\begin{align*}
    \frac{1}{\sqrt{5}}\begin{bmatrix} 2 & -1 \\ 1 & 2 \end{bmatrix}  \begin{bmatrix} \alpha \\ \beta \end{bmatrix} &= \begin{bmatrix} -1 \\ 4 \end{bmatrix}\\    
    \Rightarrow \begin{bmatrix} \alpha \\ \beta \end{bmatrix} &= \frac{1}{\sqrt{5}}\begin{bmatrix} -1 & 2 \\ 2 & 1 \end{bmatrix} \begin{bmatrix} -1 \\ 4 \end{bmatrix}\\
    &= \frac{1}{\sqrt{5}} \begin{bmatrix} 9 \\ 2 \end{bmatrix}
\end{align*}
Thus, $Y(t) = \frac{9}{5}(-1,2)^T + \frac{2}{5} (2,1)^T e^{5t}$ 
\end{proof}
\begin{problem}
Solve the following:
\begin{enumerate}
    \item Let $A$ be an $n\times n$ complex Hermitian matrix such that $A^4=I$. What are the possible eigenvalues of $A$?
    \item If $A$ is an $n\times n$ complex matrix and $A^4 = I$, what are the possible eigenvalues?
\end{enumerate}
\end{problem}
\begin{problem}
Using the method of least squares, find the line in $\mathbb{R}^2$ that best fits the data points: $(2,1),\ (3,2),\ (4,2),\ (5,3)$
\end{problem}
\begin{proof}[Solution]
We want a line $y=mx+b$ that best fits the points. Setting up the problem, we get:
\begin{equation*}
    \begin{bmatrix} 1 & 2 \\ 1 & 3 \\ 1 & 4 \\ 1 & 5 \end{bmatrix} \begin{bmatrix} b \\ m \end{bmatrix} = \begin{bmatrix} 1 \\ 2 \\ 2 \\ 3\end{bmatrix}   
\end{equation*}
This has no solution. Denoted $A$ as the left-most matrix, we compute $A^T$:
\begin{equation*}
    A^T = \begin{bmatrix} 1 & 1 & 1 & 1 \\ 2 & 3 & 4 & 5 \end{bmatrix}   
\end{equation*}
So $A^T A = \begin{bmatrix} 4 & 14 \\ 14 & 54 \end{bmatrix}$. We now solve $A^{T}AX$:
\begin{equation*}
    \begin{bmatrix} 4 & 14 \\ 14 & 54 \end{bmatrix} \begin{bmatrix} b \\ m \end{bmatrix} =  A^T \begin{bmatrix} 1 \\ 2 \\ 2 \\ 3 \end{bmatrix} = \begin{bmatrix} 8 \\ 31 \end{bmatrix}   
\end{equation*}
The solution gives us $y = 0.6x-0.1$
\end{proof}
\begin{problem}
Find the projection matrix $P$ that projects $\mathbb{R}^4$ onto the line through the origin spanned by the vector $(2,1,-1,-1)$.
\end{problem}
\begin{problem}
Consider the rotation matrix $R = \begin{bmatrix} \frac{-4}{9} & \frac{-7}{9} & \frac{4}{9} \\ \frac{1}{9} & \frac{4}{9} & \frac{8}{9} \\ \frac{-8}{9} & \frac{4}{9} & \frac{-1}{9} \end{bmatrix}$ Compute the axis vector $\textbf{u}$ and both the sine and cosine of the counterclockwise angle $\theta$ such that $R = R_{\theta,\textbf{u}}$
\end{problem}
\begin{problem}
Find an orthonormal basis for the column space of the matrix:
\begin{equation*}
    A = \begin{bmatrix} 1 & 1 & 1 \\ 0 & 3 & 1 \\ 2 & 2 & 2 \\ 2 & 4 & 3 \\ -1 & 2 & 0 \end{bmatrix}
\end{equation*}
\end{problem}
\begin{proof}[Solution]
We use the Gram-Schmidt procedure to do this. Take $(1,0,2,2,-1)$ and normalize it, giving us:
\begin{equation*}
    e_{1} = \frac{1}{\sqrt{10}}(1,0,2,2,-1)^T    
\end{equation*}
We then compute:
\begin{align*}
    (1,3,2,4,2)^T &- \frac{(1,3,2,4,2)^T(1,0,2,2,-1)}{(1,0,2,2,-1)^T (1,0,2,2,-1)}(1,0,2,2,-1)^T\\
    &= (1,3,2,4,2)^T-\frac{11}{10}(1,0,2,2,-1)\\
    &=(-\frac{1}{10},3,-\frac{2}{10},\frac{18}{10},\frac{33}{10})\\
    &= \frac{1}{10}(-1,30,-2,18,33)
\end{align*}
Thus:
\begin{equation*}
    e_{2} = \frac{ \frac{1}{10}(-1,30,-2,18,33)}{\norm{ \frac{1}{10}(-1,30,-2,18,33)}} = \frac{1}{\sqrt{2318}}(-1,30,-2,18,33)
\end{equation*}
Finishing off, we compute:
\begin{align*}
    \mathbf{v}_{3} &= (1,1,2,3,0)^T - \frac{(1,1,2,3,0)^T(1,0,2,2,-1)}{10}(1,0,2,2,-1)^T\\
    &- \frac{(1,1,2,3,0)^T(1,3,2,4,2)}{34}(1,3,2,4,2,0)^T     
\end{align*}
Finally,
\begin{equation*}
    e_3 = \frac{\textbf{v}_3}{\norm{\textbf{v}_3}}
\end{equation*}
\end{proof}
\begin{problem}
Eliminate crossterms, classify, and sketch the graph of the conic section $6x^2 - 4xy+3y^2 = 1$
\end{problem}
\subsection{Problem Set VI}:
\begin{problem}
Let $\begin{bmatrix*}[r] 1 & 0 & 3 & \vline & 1 \\ 0 & 1 & -2 & \vline & 3 \\ 1 & 2 & 0 & \vline & 0 \end{bmatrix*}$ be an augmented matrix.
\begin{enumerate}
    \item Solve the system using Gaussian elimination.
    \item Express $\begin{bmatrix} 1 \\ 3 \\ 0\end{bmatrix}$ as a linear combination of the column vectors of the coefficient matrix.
    \item Use elementary matrices to find the LU decomposition of the coefficient matrix.
\end{enumerate}
\end{problem}
\begin{proof}[Solution]
In order,
\begin{enumerate}
    \item 
    \begin{align*}
        \begin{bmatrix*}[r] 1 & 0 & 3 & \vline & 1 \\ 0 & \phantom{-}1 & -2 & \vline & 3 \\ 1 & 2 & 0 & \vline & 0 \end{bmatrix*} &\underset{r_{2}\leftrightarrow r_{3}}{\longrightarrow} \begin{bmatrix*}[r] 1 & 0 & 3 & \vline & 1 \\ 1 & 2 & 0 & \vline & 0 \\ 0 & \phantom{-}1 & -2 & \vline & 3 \end{bmatrix*} \underset{r_{2}-r_{1}}{\longrightarrow} \begin{bmatrix*}[r] 1 & 0 & 3 & \vline & 1 \\ 0 & \phantom{-} 2 & -3 & \vline & -1 \\ 0 & 1 & -2 & \vline & 3 \end{bmatrix*}\\
        &\underset{r_{2}\olddiv 2}{\longrightarrow} \begin{bmatrix*}[r] 1 & 0 & 3 & \vline & 1 \\ 0 & 1 & -1.5 & \vline & -0.5 \\ 0 & \phantom{-1.}1 & -2 & \vline & 3\end{bmatrix*} \underset{r_{3}-r_{2}}{\longrightarrow} \begin{bmatrix*}[r] 1 & 0 & 3 & \vline & 1 \\ 0 & 1 & -1.5 & \vline & -0.5 \\ 0 & \phantom{-.}0 & -0.5 & \vline & 3.5 \end{bmatrix*}\\
        &\underset{r_{3}\cdot(-2)}{\longrightarrow} \begin{bmatrix*}[r] 1 & 0 & 3 & \vline & 1 \\ 0 & \phantom{-.5}1 & -1.5 & \vline & -0.5 \\ 0 & 0 & 1 & \vline & -7 \end{bmatrix*} \underset{r_{1}-3r_{3}}{\longrightarrow} \begin{bmatrix*}[r] 1 & 0 & 0 & \vline & 22 \\ 0 & 1 & -1.5 & \vline & -0.5 \\ 0 & \phantom{-}0 & \phantom{-}1 & \vline & -7 \end{bmatrix*}\\
        &\underset{r_{2}+\frac{3}{2}r_{3}}{\longrightarrow} \begin{bmatrix*}[r] 1 & 0 & 0 & \vline & 22 \\ 0 & 1 & 0 & \vline & -11 \\ 0 & 0 & 1 & \vline & -7 \end{bmatrix*}
    \end{align*}
    \item
    \begin{equation*}
        \begin{bmatrix*}[r] 1 \\ 3 \\ 0 \end{bmatrix*} = 22 \begin{bmatrix*}[r] 1 \\ 0 \\ 1 \end{bmatrix*} - 11\begin{bmatrix*}[r] 0 \\ 1 \\ 2 \end{bmatrix*} -7 \begin{bmatrix*}[r] 3 \\ -2 \\ 0 \end{bmatrix*}
    \end{equation*}
    \item
    \begin{equation*}
        A = \begin{bmatrix*}[r] 1 & 0 & 0 \\ 0 & 1 & 0 \\ 1 & 2 & 1 \end{bmatrix*} \begin{bmatrix*}[r] 1 & 0 & 3 \\ 0 & \phantom{-}1 & -2 \\ 0 & 0 & 1 \end{bmatrix*}
    \end{equation*}
\end{enumerate}
\end{proof} 
\begin{problem}
Let $A = \begin{bmatrix*}[r] 1 & 0 & 0 \\ 2 & 1 & 0 \\ 3 & 4 & 1 \end{bmatrix*}$, $B=\begin{bmatrix*}[r]1 & 0 & 0 \\ -2 & 1 & \phantom{-}0 \\ 5 & -4 & 1 \end{bmatrix*}$, and $C = \begin{bmatrix*}[r] 2 & 3 \\ -1 & 0 \\ 1 & 1 \end{bmatrix*}$. 
\begin{enumerate}
\begin{multicols}{3}
    \item Solve $AC+BC$
    \item Solve $AB$
    \item Does $A = B^{-1}$?
\end{multicols}
\end{enumerate}
\end{problem}
\begin{proof}[Solution]
In order,
\begin{enumerate}
    \item $AC+BC = (A+B)C = \begin{bmatrix*}[r] 2 & 0 & 0 \\ 0 & 2 & 0 \\ 8 & 0 & 2 \end{bmatrix*} \begin{bmatrix*}[r] 2 & 3 \\ -1 & 0 \\ 1 & 1 \end{bmatrix*} = \begin{bmatrix*}[r] 4 & 6 \\ -2 & 0 \\ 18 & 26 \end{bmatrix*}$
    \item $AB = \begin{bmatrix*}[r] 1 & 1 & 0 \\ 0 & -15 & \phantom{-}0 \\ 0 & 0 & 1 \end{bmatrix*}$
    \item No, as if $A=B^{-1}$ then $AB=I$.
\end{enumerate}
\end{proof}
\begin{problem}
If $A$ and $B$ are $n\times n$ invertible matrices, what is $(AB)^{-1}$?
\end{problem}
\begin{proof}[Solution]
As $A^{-1}$ and $B^{-1}$ exist, and as $A$ and $B$ are of the same dimension, $B^{-1}A^{-1}$ exists. But $(B^{-1}A^{-1})(AB) = B^{-1}(A^{-1}A)B = B^{-1}IB = B^{-1}B = I$. Thus, as inverses are unique, $(AB)^{-1} = B^{-1}A^{-1}$.
\end{proof}
\begin{problem}
What are the solutions of:
\begin{enumerate}
\begin{multicols}{2}
    \item $\begin{bmatrix*}[r] 1 & 1 & 0 & 0 & \vline & -1 \\ 0 & 1 & 0 & 0 & \vline & 3 \\ 0 & 0 & 1 & 1 & \vline & 2 \\ 0 & 0 & 1 & 1 & \vline & 1 \end{bmatrix*}$
    \item $\begin{bmatrix*}[r] 1 & 1 & 0 & 0 & \vline & -1 \\ 0 & 1 & 0 & 0 & \vline & 3 \\ 0 & 0 & 1 & 1 & \vline & 1 \\ 0 & 0 & 1 & 1 & \vline & 1 \end{bmatrix*}$
\end{multicols}
\end{enumerate}
\end{problem}
\begin{proof}[Solution]
In order,
\begin{enumerate}
    \item No solution as the bottom two rows say $x_3 + x_4 = 2$ and $x_3 + x_4 = 1$. An impossibility.
    \item The entire space $S = \{(-4,3,x,1-x):x\in \mathbb{R}\}$.
\end{enumerate}
\end{proof}
\begin{problem}
If $A$ and $B$ are $n\times n$ matrices, what is $(A+B)^2$?
\end{problem}
\begin{proof}[Solution]
$(A+B)^2 =(A+B)(A+B) = A(A+B)+B(A+B)=A^2+AB+BA+B^2$. Note: It is not true in general that $AB=BA$, and thus we cannot simplify further.
\end{proof}
\begin{problem}
If $A$ and $A^T$ are $n\times n$ invertible matrices, show that $(A^T)^{-1} = (A^{-1})^T$
\end{problem}
\begin{proof}[Solution]
For $A^T(A^{-1})^T = ((A^{-1})^T A^T)^T = (A^{-1}A)^T = I^T = I$. Thus, as inverses are unique, $(A^T)^{-1} = (A^{-1})^T$
\end{proof}
\begin{problem}
If $A,B,$ and $C$ are $n\times n$ invertible matrices, then solve the following equations for $X$:
\begin{enumerate}
    \item $XA+B=C$
    \item $AX+B=X$
    \item $XA+C=X$
\end{enumerate}
\end{problem}
\begin{proof}
In order,
\begin{enumerate}
    \item $XA +B=C\Rightarrow XA = C-B \Rightarrow X = (C-A)A^{-1}$
    \item $AX+B = X\Rightarrow AX-X=-B \Rightarrow (A-I)X=-B \Rightarrow X = -(A-I)^{-1}B$
    \item $X = -C(I-A)^{-1}$
\end{enumerate}
\end{proof}
\subsection{Problem Set VII}
\begin{problem}
Determine the basis of the given vector space over the given field.
\begin{enumerate}
    \item $V=\mathbb{R}$ over $K=\mathbb{R}$
    \item $V=\mathbb{C}$ over $K=\mathbb{C}$
    \item $V=\mathbb{C}$ over $K=\mathbb{R}$
\end{enumerate}
\end{problem}
\begin{proof}[Solution]
In order,
\begin{enumerate}
    \item The set $\{1\}$ is a basis. Let $r \in \mathbb{R}$. Then $r=1\cdot r$.
    \item The set $\{(1,0)\}$ is a basis. Let $z\in \mathbb{Z}$. Then $z\cdot(1,0) = z$
    \item The set $\{(1,0),(0,1)\}$ is a basis. Let $z=a+bi\in \mathbb{Z}$. Then $z = a(1,0)+b(0,1)$.
\end{enumerate}
\end{proof}
\begin{problem}
What is the nullspace of an $n\times n$ matrix $A$ with real entires?
\end{problem}
\begin{proof}[Solution]
The nullspace is the set $N(A) = \{X\in \mathbb{R}^n: AX = 0\}$
\end{proof}
\begin{problem}
Let $A=\begin{bmatrix} 1 & 2 & 3 & 4 \\ -1 & -1 & -4 & -2 \\ 3 & 4 & 11 & 8 \end{bmatrix}$ and suppose it can be row reduced to $\begin{bmatrix} 1 & 0 & 5 & 0 \\ 0 & 1 & -1 & 2 \\ 0 & 0 & 0 & 0 \end{bmatrix}$. What is the rank of $A$?
\end{problem}
\begin{proof}[Solution]
The rank is the dimension of the space spanned by the column vectors of the matrix. Using the row-reduced form, we see that these columns span $\mathbb{R}^2$ and thus the matrix has rank $2$.
\end{proof}
\begin{problem}
What is the rank-nullity theorem?
\end{problem}
\begin{proof}[Solution]
For an $n\times n$ matrix $A$, $Rk(A)+Nul(A) = n$.
\end{proof}
\subsection{Problem Set VIII}
\begin{problem}
Let $T:\mathbb{R}^3\rightarrow \mathbb{R}^2$ be defined by $T\begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} = \begin{bmatrix} x_3 \\ x_1+x_2 \end{bmatrix}$.
\begin{enumerate}
    \item Determine $\ker(T)$.
    \item Determine the dimensions of $\ker(T)$.
    \item Using the Nullity Theorem, determine the dimension of im$(T)$.
\end{enumerate}
\end{problem}
\begin{proof}[Solution]
In order,
\begin{enumerate}
    \item $\ker(T)= \{x\in \mathbb{R}^3: Tx = 0\}$. So, $x = (x_1,x_2,x_3)^T$ such that $Tx = 0$. Thus $x_3 = 0$ and $x_1+x_2 = 0$. $\ker(T) = \{(x,-x,0):x\in \mathbb{R}\}$.
    \item This is a line through the origin, so the dimension is $1$. 
    \item The Nullity Theorem states that $\dim(\ker(T))+\dim(im(T)) = \dim(\mathbb{R}^3) = 3$. Thus $\dim(im(T)) = 2$.
\end{enumerate}
\end{proof}
\begin{problem}
Using the linear transformation of the previous problem, determine the matrix representation of $T$ in the standard basis of $\mathbb{R}^3$.
\end{problem}
\begin{proof}[Solution]
$Te_1 = (0,1)^T$, $T e_2 = (0,1)^T$, and $Te_3 = (1,0)^T$. The matrix representation is $T=\begin{bmatrix} 0 & 0 & 1 \\ 1 & 1 & 0 \end{bmatrix}$
\end{proof}
\begin{problem}
Let $P_n$ be the set of all polynomials with real coefficients of degree less than $n$. The standard basis is $\{1,x,\hdots, \ x^{n-1}\}$. Let $D:P_3 \rightarrow P_2$ be defined by $D(p) = 5\frac{dp}{dx}$. Determine the matrix representation of $D$ with respect to the standard basis.
\end{problem}
\begin{proof}[Solution]
We need only check how $D$ acts on the basis vectors. $D(1) = 0+0x$, $D(x) = 1+0x$, $D(x^2) = 0+2x$. So, we have $D = \begin{bmatrix} 0 & 2 & 0 \\ 1 & 0 & 0 \end{bmatrix}$
\end{proof}
\begin{problem}
Let $V$ be a vector space over $\mathbb{R}$ and let $S$ be a subspace of $V$.
\begin{enumerate}
    \item Define $S^{\perp}$.
    \item If $S = $Span$\{ (1,2,1)^T, (1-1,2)^T\}$, what is $S^{\perp}$?
\end{enumerate}
\end{problem}
\begin{proof}[Solution]
In order,
\begin{enumerate}
    \item $S^{\perp} = \{x\in V: \forall y\in S, x^T y = 0\}$.
    \item $S^{\perp} = \{x\in \mathbb{R}^3: x^T (1,2,1)^T = x^T(1,-1,2)^T = 0\}$. Let $x = (x_1,x_2,x_3)^T \in S^{\perp}$. Then $x_1+2x_2+x_3=0$ and $x_1-x_2+2x_3 = 0$. We may write this as $\begin{bmatrix} 1 & 2 & 1 \\ 1 & -1 & 2 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2\\ x_3 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}$. In row echelon form we have $\begin{bmatrix} 1 & 0 & \frac{5}{3} \\ 0 & 1 & \frac{-1}{3} \end{bmatrix}$. We thus get $x_1 = - \frac{5}{3}x_3$ and $x_2 = \frac{1}{3} x_3$.  $S^{\perp} = \{x(-\frac{5}{3}, \frac{1}{3}, 1)^T: x\in \mathbb{R}\}$.
\end{enumerate}
\end{proof}
\begin{problem}
\begin{enumerate}
    \item Let $V$ be a vector space over $\mathbb{R}$. Define an inner product.
    \item What is the difference between the standard dot product in $\mathbb{R}^n$ and an inner product? Can a vector space have more than one inner product?
    \item If $\langle x,y \rangle = xy$, what is $\norm{x}$?
\end{enumerate}
\end{problem}
\begin{proof}[Solution]
In order,
\begin{enumerate}
    \item An inner product is a function from $\mathbb{R}\times \mathbb{R}\rightarrow \mathbb{R}$ with the following properties:
    \begin{enumerate}
        \item $\langle ax+by,z\rangle = a\langle x,z\rangle+b\langle y,z\rangle$
        \item $\langle x,y\rangle = \langle y,x \rangle$
        \item $\langle x,x\rangle \geq 0$.
    \end{enumerate}
    \item An inner product is a generalization of the standard dot product. The dot product is itself an inner product, but not all inner products are dot products. There are infinitely many inner products for $\mathbb{R}$. Let $n\in \mathbb{N}$ be arbitrary, then $\langle x,y \rangle = nxy$ is an inner product.
    \item $\norm{x} = \sqrt{\langle x,x \rangle } = \sqrt{x^2}= |x|$.
\end{enumerate}
\end{proof}
\begin{problem}
Let $V = C[-1,1]$ and let $\langle f,g\rangle = \int_{-1}^{1} f(x)g(x)dx$.
\begin{enumerate}
    \item Show that $f(x)=1$ and $g(x) = x$ are orthogonal with respect to this inner product.
    \item Determine $\norm{f}$ and $\norm{g}$.
    \item Show that $f$ and $g$ satisfy the Pythagorean Law.
\end{enumerate}
\end{problem}
\begin{proof}[Solution]
In order,
\begin{enumerate}
    \item $\langle 1,x\rangle = \int_{-1}^{1} xdx = \frac{x^2}{2}\big|_{-1}^{1} = \frac{1^2}{2}-\frac{(-1)^2}{2} = \frac{1}{2}-\frac{1}{2} = 0$
    \item $\norm{1} = \sqrt{ \int_{-1}^{1} dx} = \sqrt{2}$. $\norm{x} = \sqrt{\int_{-1}^{1}x^2dx} = \sqrt{\frac{2}{3}}$
    \item $\norm{1+x}^2 = \langle 1+x,1+x\rangle = \langle 1,1\rangle + 2\langle 1,x \rangle + \langle x,x\rangle = \norm{1}^2 + \norm{x}^2$
\end{enumerate}
\end{proof}
\begin{problem}
Let $V$ be any inner product space. State and prove the Pythagorean Theorem for inner product spaces.
\end{problem}
\begin{proof}[Solution]
The Pythagorean Theorem for Inner Product Spaces state that if $V$ is an inner product space with inner product $\langle, \rangle$, and if $\langle x,y\rangle = 0$, then $\norm{x}^2+\norm{y}^2 = \norm{x+y}^2$. For $\norm{x+y}^2 = \langle x+y,x+y\rangle = \langle x,x\rangle + 2\langle x,y\rangle +\langle y,y\rangle$. But as $x$ and $y$ are orthogonal, $\langle x,y \rangle = 0$. Thus $\norm{x+y}^2 = \langle x,x\rangle + \langle y,y\rangle = \norm{x}^2+\norm{y}^2$. $\norm{x+y}^2 =\norm{x}^2+\norm{y}^2$.
\end{proof}
\begin{problem}
Prove that if $V$ is an inner product space and $S$ is a subspace of $V$, then $S^{\perp}$ is a subspace of $V$.
\end{problem}
\begin{proof}[Solution]
We must check that $0\in S^{\perp}$ and that $S^{\perp}$ is closed under addition and scalar multiplication.
\begin{enumerate}
    \item For all $x\in S$, $\langle 0,x \rangle = 0$, and thus $0\in S^{\perp}$.
    \item If $x,y\in S^{\perp}$ and $z\in S$, then $\langle x+y,z\rangle = \langle x,z\rangle + \langle y,z\rangle = 0+0=0$. Thus $x+y\in S^{\perp}$.
    \item If $x\in S^{\perp}$, $y\in S$, and $\alpha$ is a scalar, then $\langle \alpha x,y \rangle = \alpha \langle x,y \rangle = \alpha \cdot 0 = 0$. Thus $\alpha x \in S^{\perp}$. $S^{\perp}$ is a subspace.
\end{enumerate}
\end{proof}
\clearpage
\printglossaries
\end{document}