\documentclass[crop=false,class=article,oneside]{standalone}
%----------------------------Preamble-------------------------------%
\input{../../../../preamble.tex}
\graphicspath{{../../../../images/}}    % Path to Image Folder.
%--------------------------Main Document----------------------------%
\begin{document}
    \ifx\ifcoursesmathgre\undefined
        \section*{Mathematics GRE Subject Test}
        \setcounter{section}{3}
        \renewcommand\thefigure{\arabic{section}.\arabic{figure}}
        \renewcommand\thesubfigure{%
            \arabic{section}.\arabic{figure}.\arabic{subfigure}}
    \else
        \section{Notes from REA Mathematics}
    \fi
    \subsection{Polynomials and Equations}
        \begin{definition}
            A polynomomial in $x$, denoted $P(x)$,
            is a term or
            a sum of terms that are a
            real number of the product
            of a real number and a positive integral power
            of $x$.
        \end{definition}
        \begin{example}
            \
            \begin{enumerate}
                \begin{multicols}{2}
                    \item $5x^{3}+2x^{2}+3$ is a
                        polynomial in $x$.
                    \item $2^{2}+x^{\frac{1}{2}}-1$
                        is not a polynomial in $x$.
                    \item $9x^{3}+3x^{-2}+4$ is
                        not a polynomial in $x$.
                    \item $x^{2}+1$ is a polynomial in $x$.
                \end{multicols}
            \end{enumerate}
        \end{example}
        \begin{definition}
            A monomial is the product of a real number with
            variables raised to integral powers.
        \end{definition}
        \begin{definition}
            The degree of a monomial is the sum of the exponents
            of the variables. A monomial with no variables has
            degree $0$.
        \end{definition}
        \begin{example}
            \
            \begin{enumerate}
                \begin{multicols}{2}
                    \item $5x^{2}$ has degree $2$.
                    \item $3x^{3}y^{2}z$ has degree $6$.
                    \item $9$ has degree $0$.
                    \item $xyz$ has degree $3$.
                \end{multicols}
            \end{enumerate}
        \end{example}
        \begin{definition}
            The degree of a polynomial in $x$ is the largest
            exponent of non-zero terms.
        \end{definition}
        \begin{example}
            \
            \begin{enumerate}
                \begin{multicols}{2}
                    \item $5x^{4}+7x+12$ has degree $4$.
                    \item $x^{2}+1$ has degree $2$.
                    \item $4x+6x^{5}-3x^{2}+1$ has degree $5$.
                    \item $0x^{7}+x^{3}-x+x^{2}$ has degree $3$.
                \end{multicols}
            \end{enumerate}
        \end{example}
        The following operations can be
        performed on polynomials:
        \begin{enumerate}
            \item Addition: Add like terms
                that differ only in coefficients.
            \item Subtraction: Change the sign of the
                coefficients of the term being subtracted,
                and then add.
            \item Multiplication:
                Using the distributive property
                to multiply the terms of one polynomial to the
                term of another and then combine the terms.
        \end{enumerate}
        \begin{example}
            \
            \begin{enumerate}
                \item Addition:
                    \begin{align*}
                        (x^{2}-3x+5)+(4x^{2}+6x-3)
                        &=(x^{2}+4x^{2})+(-3x+6x)+(5+(-3))\\
                        &=5x^{2}+3x+2
                    \end{align*}
                \item Subtraction:
                    \begin{align*}
                        (x^{2}+1)-(3x^{2}-x+2)
                        &=(x^{2}+1)+(-3x^{2}+x-2)\\
                        &=(x^{2}-3x^{2})+(x)+(1-2)\\
                        &=-2x^{2}+x-1
                    \end{align*}
                \item Multiplication:
                    \begin{align*}
                        (x^{4}+3)(2x^{2}-1)
                        &=(x^{4}+3)2x^{2}+(x^{4}+3)(-1)\\
                        &=(x^{4})(2x^{2})+(3)(2x^{2})
                        +(x^{4})(-1)+(3)(-1)\\
                        &=2x^{6}-x^{4}+6x^{2}-3
                    \end{align*}
            \end{enumerate}
        \end{example}
        \begin{definition}
            Factors of a polynomial $P(x)$ are polynomials
            $Q_{k}(x)$ such that $P(x)=\Pi_{k=0}^{n}Q_{k}(x)$
        \end{definition}
        \begin{definition}
            A prime factor of a polynomial is a factor that
            cannot be factored further.
        \end{definition}
        \begin{example}
            $x^{2}+1$ is a prime factor of $x^{4}-1$.
            $x^{2}-1$ is not a prime factor since
            $x^{2}-1=(x-1)(x+1)$.
        \end{example}
        \begin{definition}
            The least common multiple of a set of numbers
            is the smallest number that is divisble by
            every element of the set.
        \end{definition}
        \begin{definition}
            The least common multiple of a set of polynomials
            is the polynomial with lowest degree and smallest
            numerical coefficients for which every element
            of the set is a factor.
        \end{definition}
        In a similar manner, then greatest common factor
        can be defined.
        \begin{theorem*}
            The following are true:
            \begin{enumerate}
                \begin{multicols}{2}
                    \item $a(c+d)=ac+ad$
                    \item $(a+b)(a-b)=a^{2}-b^{2}$
                    \item $(a+b)^{2}=a^{2}+2ab+b^{2}$
                    \item $(a-b)^{2}=a^{2}-2ab+b^{2}$
                    \item $(x+a)(x+b)=x^{2}+(a+b)x+ab$
                    \item $(ax+b)(cx+d)=acx^{2}+(ad+bc)x+bd$
                    \item $(a+b)(c+d)=ac+bc+ad+bd$
                    \item $(a+b)^{3}=%
                           a^{3}+3a^{2}b+3ab^{2}+b^{3}$
                    \item $(a-b)^{3}=%
                           a^{3}-3a^{2}b+3ab^{2}-b^{3}$
                    \item $(a-b)(a^{2}+ab+b^{2})=a^{3}-b^{3}$
                    \item $(a+b)(a^{2}-ab+b^{2})=a^{3}+b^{3}$
                    \item $(a+b+c)^{2}=%
                           a^{2}+b^{2}+c^{2}+2(ab+ac+bc)$
                    \item $(a-b)(a^{3}+a^{2}b+ab^{2}+b^{3})=%
                           a^{4}-b^{4}$
                    \item $(a-b)%
                           (a^{4}+a^{3}b+%
                            a^{2}b^{2}+ab^{3}+b^{4})%
                           =a^{5}-b^{5}`$
                \end{multicols}
            \end{enumerate}
        \end{theorem*}
        \begin{theorem*}
            If ${n}\geq{2}$, then
            $a^{n}-b^{n}=(a-b)\sum_{k=0}^{n-1}a^{n-1-k}b^{k}$
        \end{theorem*}
        \begin{theorem*}
            If $n$ is odd, then
            $a^{n}+b^{n}=%
             (a+b)\sum_{k=0}^{n-1}(-1)^{k}a^{n-1-k}b^{k}$
        \end{theorem*}
        To factor a polynomial completely,
        first compute the greatest common factor,
        if there is any. Then examine the remaining factors.
        Continue factoring until all of the factors are prime.
        \begin{example}
            $4-16x^{2}=4(1-4x^{2})=4(1-2x)(1+2x)$
        \end{example}
        \begin{definition}
            An equation is a statement of equality
            between two separate expressions.
        \end{definition}
        \begin{definition}
            A rational integral equation is an equation
            involving two expressions that have only rational
            coefficients and integral powers.
        \end{definition}
        Equations can be simplified by applying the following:
        \begin{enumerate}
            \item Adding or subtracting an expression to both
                sides of an equation results in an equivalent
                equation.
            \item Multiplying both sides of an equation by a
                non-zero expression results in an equivalent
                equation.
            \item The reciprocal of both sides of a non-zero
                equation are equivalent.
            \item When evaluating equations containing
                absolute values, note that $|P(x)|=Q(x)$ is
                equivalent to $P(x)=Q(x)$ OR $P(x)=-Q(x)$.
        \end{enumerate}
        \begin{example}
            \
            \begin{enumerate}
                \begin{multicols}{2}
                    \item $y+6={10}\Rightarrow{y+6-6}=10-6%
                           \Rightarrow{y}=4$
                    \item $3x={6}\Rightarrow{\frac{3x}{3}}=%
                           \frac{6}{3}\Rightarrow{x=2}$
                    \item $\frac{1}{x}=\frac{1}{3}%
                           \Rightarrow{x}=3$
                    \item $|x|=2\Rightarrow{x}=2$ OR
                        $x=-2$
                \end{multicols}
            \end{enumerate}
        \end{example}
        Both sides of an equation can also be raised to a power,
        but this may not be an equivalent expression.
        For example $x^{2}=1$ has the solution set $\{-1,1\}$,
        however if we take the square root of both sides
        we have $x=1$. You must be careful when using
        powers to make sure that both equations are equivalent.
    \subsection{Inequalities}
        \begin{definition}
            An inequality is a statement that the value of
            one expression is greater or less than that
            of another.
        \end{definition}
        \begin{example}
            $4<5$ means that 4 is less than 5.
            $5>4$ means that 5 is greater than 4.
        \end{example}
        \begin{definition}
            A conditional inequality is an inequality
            whose validity depends on the values
            of the variables in the expressions.
        \end{definition}
        \begin{definition}
            An absolute inequality is an inequality
            that is true of all values.
        \end{definition}
        \begin{definition}
            An inconsistent inequality is an inequality
            that is never true for any values.
        \end{definition}
        \begin{example}
            \
            \begin{enumerate}
                \item $x+3>3-x$ is a conditional inequality.
                    It is true for $x>0$, but false for
                    ${x}\leq{0}$.
                \item $x+2>x$ is an absolute inequality. This
                    is always true, regardless of $x$.
                \item $x>x+10$ is an inconsistent inequality.
            \end{enumerate}
        \end{example}
        \begin{theorem*}
            The following are true:
            \begin{enumerate}
                \begin{multicols}{2}
                    \item If $a<b$ and $b<c$, then $a<c$
                    \item If $a>b$ and $b>c$, then $a>c$
                    \item If $a>b$, then $a+c>b+c$
                    \item If $a>b$, then $a-c>b-c$
                    \item If $a>b$ and $c>0$, then $ac>bc$
                    \item If $a>b$ and $c<0$, then $ac<bc$
                    \item If $a>b>0$ and $n$ is positive,
                        then $a^{n}>b^{n}$
                    \item If $a>b>0$ and $n$ is negative,
                        then $a^{n}<b^{n}$
                    \item If $x>y$ and $p>q$, then
                        $x+p>y+q$
                    \item If $x>y>0$ and $p>q>0$,
                        then $xp>yq$
                    \item $|x|<a$ has solution set
                        $\{x:-a<x<a\}$
                    \item $|x|>a$ has solution set
                        $\{x:{x>a}\textrm{ or }{x<-a}\}$
                \end{multicols}
            \end{enumerate}
        \end{theorem*}
        \begin{definition}
            A linear inequality in two variables is
            an inequality of the form
            $ax+by<c$
        \end{definition}
        \begin{example}
            Solve for $2x-3y>6$. In the limiting case,
            ${2x-3y=6}\Rightarrow{y=\frac{2}{3}x-2}$.
            This is the equation of a line in the $xy$ plane.
            Choosing a point not on this line, say $(1,1)$,
            we have $2(1)-3(1)=2-3=-1<6$. Therefore the point
            $(1,1)$ is not contained in the region
            $2x-3y>6$. But then we also not that
            $(1,-\frac{4}{3})$ is on the boundary. So
            $2x-3y>6$ is the region
            \textit{below} the line $y=\frac{2}{3}x-2$
        \end{example}
    \subsection{Linear Algebra}
        A system of linear equations can be written
        using an equivalent matrix notation.
        \begin{example}
            \begin{align*}
                a_{11}x_{1}+a_{12}x_{2}+a_{13}x_{3}&=b_{1}\\
                a_{21}x_{1}+a_{22}x_{2}+a_{23}x_{3}&=b_{2}\\
                a_{31}x_{1}+a_{32}x_{2}+a_{33}x_{3}&=b_{3}\\
            \end{align*}
            This is equivalent to either of the following:
            \begin{align*}
                \begin{bmatrix}
                    a_{11}&a_{12}&a_{13}\\
                    a_{21}&a_{22}&a_{31}\\
                    a_{31}&a_{32}&a_{33}
                \end{bmatrix}
                \begin{bmatrix}
                    x_{1}\\
                    x_{2}\\
                    x_{3}
                \end{bmatrix}
                &=
                \begin{bmatrix}
                    b_{1}\\
                    b_{2}\\
                    b_{3}
                \end{bmatrix}
                &
                \mathbf{A}\mathbf{x}
                &=\mathbf{b}
            \end{align*}
        \end{example}
        Matrices can also be written as $\mathbf{A}=(a_{ij})$.
        The following rules are used to
        define matrix arithmetic.
        \begin{enumerate}
            \item Addition: To add two matrices, add their
                corresponding elements. That is, if
                $\mathbf{A}=(a_{ij})$ and $\mathbf{B}=(b_{ij})$,
                then $\mathbf{A}+\mathbf{B}=(a_{ij}+b_{ij})$.
                Matrix addition is only defined on matrices of
                the same size.
            \item Scale multiplication: To multiply a
                matrix by a real or complex number $c$,
                multiply this number to every element. That is,
                if $\mathbf{A}=(a_{ij})$, then
                $c\mathbf{A}=({c}\cdot{a_{ij}})$
            \item Matrix Multiplication: The product of
                and ${M}\times{N}$ matrix with an
                ${N}\times{P}$ matrix is defined by
                $\mathbf{C}=\mathbf{A}\mathbf{B}$, where
                $(c_{ij})=(\sum_{k=1}^{N}a_{ik}b_{kj})$. Note
                that it is possible for
                $\mathbf{A}\mathbf{B}\ne\mathbf{B}\mathbf{A}$.
                Indeed, it is possible for
                $\mathbf{A}\mathbf{B}$ to be defined, whereas
                $\mathbf{B}\mathbf{A}$ is undefined.
        \end{enumerate}
        \begin{example}
            Let the following be true:
            \begin{align*}
                A&=
                \begin{bmatrix}
                    1&2\\
                    3&4
                \end{bmatrix}
                &
                B&=
                \begin{bmatrix}
                    5&6\\
                    7&8
                \end{bmatrix}
            \end{align*}
            Then, using the defined rules, we have:
            \begin{align*}
                A+B&=
                \begin{bmatrix}
                    6&8\\
                    10&12
                \end{bmatrix}
                &
                5A&=
                \begin{bmatrix}
                    5&10\\
                    15&20
                \end{bmatrix}
                \\
                AB&=
                \begin{bmatrix}
                    19&22\\
                    43&50
                \end{bmatrix}
                &
                BA&=
                \begin{bmatrix}
                    23&34\\
                    31&46
                \end{bmatrix}
            \end{align*}
        Note that even in this trivial example,
        ${AB}\ne{BA}$.
        \end{example}
        \begin{definition}
            The ${n}\times{n}$ identity matrix is the matrix
            $I_{n}=(I_{ij})$, where
            $I_{ij}=%
            \begin{cases}%
             0,&{i}\ne{j}\\%
             1,&{i}={j}%
            \end{cases}$
        \end{definition}
        \begin{definition}
            An inverse matrix of an ${n}\times{n}$ matrix
            $A$ is a matrix $A^{-1}$ such that
            $AA^{-1}=A^{-1}A=I_{n}$
        \end{definition}
        Not every matrix has an inverse matrix. If one
        does exists, there are many properties it contains.
        \begin{theorem*}
            The following are true:
            \begin{enumerate}
                \item If $\mathbf{A}$ and $\mathbf{B}$
                    are invertible ${n}\times{n}$ matrices,
                    then $\mathbf{A}\mathbf{B}$ is invertible
                    and
                    $\mathbf{A}\mathbf{B}^{-1}%
                     =\mathbf{B}^{-1}\mathbf{A}^{-1}$
                \item If $\mathbf{A}$ is an invertible matrix,
                    then $\mathbf{A}^{-1}$ is an invertible
                    matrix and
                    $(\mathbf{A}^{-1})^{-1}=\mathbf{A}$
            \end{enumerate}
        \end{theorem*}
        \begin{definition}
            The trace of an ${n}\times{n}$ matrix
            $A$ is the sum of
            it's diagonal: $\Tr(A)=\sum_{i=1}^{n}a_{ii} $
        \end{definition}
        \begin{example}
            \begin{equation*}
                \Tr\Bigg(
                \begin{bmatrix}
                    4&5&6\\
                    1&2&3\\
                    8&8&3
                \end{bmatrix}
                \Bigg)
                =4+2+3=9
            \end{equation*}
        \end{example}
        \begin{definition}
            The determinant of a ${2}\times{2}$ matrix
            $A=%
             \begin{bmatrix}%
                a&b\\%
                c&d%
             \end{bmatrix}$
            is $\det(A)=ad-bc$
        \end{definition}
        \begin{definition}
            The minor of the $i^{th}$ row and $j^{th}$
            column of an ${n}\times{n}$ matrix $\mathbf{A}$,
            denoted $M_{ij}$, is the determinant of the
            ${(n-1)}\times{(n-1)}$ matrix formed by
            removing the $i^{th}$ row and $j^{th}$ column
            from $\mathbf{A}$.
        \end{definition}
        \begin{definition}
            The cofactor of the minor $M_{ij}$ of an
            ${n}\times{n}$ matrix $\mathbf{A}$,
            denoted $C_{ij}$, is $(-1)^{i+j}M_{ij}$.
        \end{definition}
        \begin{example}
            \begin{align*}
                A&=
                \begin{bmatrix}
                    7&1&3\\
                    1&3&5\\
                    17&4&20
                \end{bmatrix}
                &
                M_{11}
                &=
                \det\Bigg(\begin{bmatrix}
                         3&5\\
                         4&20
                     \end{bmatrix}
                    \Bigg)
                =40
                &
                C_{11}
                &=(-1)^{1+1}M_{11}=40
            \end{align*}
        \end{example}
        \begin{definition}
            The determinant of an ${n}\times{n}$ matrix
            $\mathbf{A}$ is
            $\det(A)=\sum_{j=1}^{n}a_{1j}C_{1j}$
        \end{definition}
        \begin{theorem*}
            If $\mathbf{A}$ is an ${n}\times{n}$ matrix
            and ${1}\leq{i}\leq{n}$, then
            $\det(A)=\sum_{j=1}^{n}a_{ij}C_{ij}$
        \end{theorem*}
        \begin{definition}
            The transpose of an ${n}\times{m}$ matrix
            $\mathbf{A}$, denoted $\mathbf{A}^{T}$,
            is the ${m}\times{n}$ matrix formed by
            swapping the rows and columns of $\mathbf{A}$
            with each other. That is $(a_{ij})^{T}=(a_{ji})$.
        \end{definition}
        \begin{definition}
            A symmetric matrix is a matrix $\mathbf{A}$
            such that $\mathbf{A}^{T}=\mathbf{A}$
        \end{definition}
        \begin{theorem*}
            If $\mathbf{A}$ is an ${n}\times{m}$ matrix and
            $\mathbf{B}$ is an ${m}\times{p}$ matrix, then
            the following are true:
            \begin{enumerate}
                \begin{multicols}{2}
                    \item $(\mathbf{A}^{T})^{T}=\mathbf{A}$
                    \item $(\mathbf{A}+\mathbf{B})^{T}%
                           =\mathbf{A}^{T}+\mathbf{B}^{T}$
                    \item $(k\mathbf{A})^{T}=k\mathbf{A}^{T}$
                    \item $(\mathbf{A}\mathbf{B})^{T}%
                           =\mathbf{B}^{T}\mathbf{A}^{T}$
                \end{multicols}
            \end{enumerate}
        \end{theorem*}
        \begin{definition}
            The adjoint of an ${n}\times{n}$ matrix
            $\mathbf{A}$, denoted $\adjoint(\mathbf{A})$,
            is the matrix $(C_{ij})^{T}$.
        \end{definition}
        \begin{theorem*}
            If ${\det(\mathbf{A})}\ne{0}$, then $\mathbf{A}$
            is invertible and
            $\mathbf{A}^{-1}=%
             \frac{1}{\det(\mathbf{A})}\adjoint(\mathbf{A})$
        \end{theorem*}
        \begin{theorem*}
            If $\mathbf{A}$ and $\mathbf{B}$ are
            ${n}\times{n}$ matrices, then the following
            are true:
            \begin{enumerate}
                \begin{multicols}{3}
                    \item $\det(\mathbf{A})%
                           =\det(\mathbf{A}^{T})$
                    \item $\det(k\mathbf{A})%
                           =k^{n}\det(\mathbf{A})$
                    \item $\det(\mathbf{A}\mathbf{B})%
                           =\det(\mathbf{A})\det(\mathbf{B})$
                \end{multicols}
            \end{enumerate}
        \end{theorem*}
        \begin{theorem*}
            A matrix $\mathbf{A}$ is invertible if and only
            if ${\det(\mathbf{A})}\ne{0}$
        \end{theorem*}
        \begin{theorem*}
            If $\mathbf{A}$ is invertible, then
            $\det(\mathbf{A}^{-1})=\frac{1}{\det(\mathbf{A})}$
        \end{theorem*}
        The differential equation
        $\sum_{k=0}^{n}a_{k}y^{(k)}(x)$ Can be expression
        in terms of the characteristic polynomial
        $\sum_{k=0}^{n}a_{k}D^{k}$. Factoring this linear
        operator into $\Pi_{k=0}^{n}(D-r_{k})$,
        the general solution is
        $y(x)=\sum_{k=1}^{n}c_{k}e^{r_{k}x}$. If some of the
        $r_{k}$ repeat, we have $c_{k}x^{m_{k}-1}e^{r_{k}x}$,
        where $m_{k}$ is the number of repetitions.
        In general, if we have
        $\Pi_{k=0}^{n}(D-r_{k})^{m_{k}}$, the general
        solution is
        $y(x)=%
         \sum_{k=1}^{n}c_{k}e^{r_{k}x}%
         (\sum_{j=0}^{m_{k}-1}x^{j})$
        \begin{example}
            \
            \begin{enumerate}
                \item $y'''-4y''+4y'=0$ has the characteristic
                    polynomial $D(D-2)^{2}$, so
                    $y(x)=c_{1}+c_{2}e^{2x}+c_{3}xe^{2x}$
            \end{enumerate}
        \end{example}
        In linear algebra, the determinant
        $\det(\mathbf{A}-\lambda{I})$ is the characteristic
        polynomial of the square matrix $\mathbf{A}$.
        \begin{definition}
            A vector space $V$ over a Field (Set of scalars)
            $F$ is a set $V$ with two operations
            $+$ and $\cdot$
            such that the following are true:
            \begin{enumerate}
                \begin{multicols}{3}
                    \item $\forall_{{\mathbf{a},%
                                     \mathbf{b}}\in{V}}$
                          ${\mathbf{a}+\mathbf{b}}\in{V}$
                    \item $\mathbf{a}+\mathbf{b}%
                           =\mathbf{b}+\mathbf{a}$
                    \item $\mathbf{a}+(\mathbf{b}+\mathbf{c})%
                           =(\mathbf{a}+\mathbf{b})+\mathbf{c}$
                    \item $\forall_{\mathbf{a}\in{V}}%
                           \exists_{\mathbf{b}\in{V}}:%
                           \mathbf{a}+\mathbf{b}=\mathbf{0}$
                    \item $\forall_{{k}\in{F},\mathbf{a}\in{V}}$
                          $k\mathbf{a}\in{V}$
                    \item $k(\mathbf{a}+\mathbf{b})%
                           =k\mathbf{a}+k\mathbf{b}$
                    \item $(k_{1}+k_{2})\mathbf{a}%
                           =k_{1}\mathbf{a}+k_{2}\mathbf{a}$
                    \item $1\mathbf{a}=\mathbf{a}$
                    \item $k_{1}(k_{2}\mathbf{a})%
                           =(k_{1}k_{2})\mathbf{a}$
                \end{multicols}
            \end{enumerate}
        \end{definition}
        \begin{theorem*}
            If $V$ is a vector space, then there is a
            $\mathbf{0}\in{V}$ such that for all
            $\mathbf{a}\in{V}$,
            $\mathbf{a}+\mathbf{0}=\mathbf{a}$
        \end{theorem*}
        \begin{definition}
            A linearly dependent subset of a vector space
            $V$ (Over $\mathbb{R}$)
            is a subset ${S}\subset{V}$ such that
            there exists an $N\in\mathbb{N}$, a non-zero
            $a_{n}:\mathbb{Z}_{N}\rightarrow\mathbb{R}$
            and an injective function
            $\mathbf{v}_{n}:\mathbb{Z}_{N}\rightarrow{V}$
            such that
            $\sum_{k=1}^{N}a_{n}\mathbf{v}_{n}=\mathbf{0}$
        \end{definition}
        \begin{definition}
            A linearly independent subset of a vector space
            $V$ is a subset ${S}\subset{V}$ that is not
            linearly dependent.
        \end{definition}
        \begin{theorem*}
            If $V\subset\mathbb{R}^{n}$ has more than
            $n$ vectors, then $V$ is linearly dependent.
        \end{theorem*}
        \begin{definition}
            The rank of a matrix is the number
            of linearly independent columns of
            the matrix.
        \end{definition}
        \begin{example}
            Let
            $\mathbf{A}=[A_{1}\ A_{2}]$
            where $A_{1}=(1,2)^{T}$ and
            $A_{2}=(2,4)^{T}$. So
            $2A_{1}+(-1)A_{2}=(0,0)^{T}=\mathbf{0}$. Therefore
            $\{A_{1},A_{2}\}$ is a linearly independent
            subset. Thus, $\rk(\mathbf{A})=1$.
        \end{example}
        \begin{definition}
            A matrix with full rank is a square
            ${n}\times{n}$ matrix $\mathbf{A}$ such that
            $\rk(\mathbf{A})=n$.
        \end{definition}
        \begin{theorem*}
            If $\mathbf{A}$ is a square matrix and
            $\det(\mathbf{A})\ne{0}$, then $\mathbf{A}$
            has full rank.
        \end{theorem*}
        \begin{theorem*}
            If $\mathbf{A}$ is a square matrix
            with full rank, then it is invertible.
        \end{theorem*}
        \begin{definition}
            A finite basis of a vector space $V$ is a
            linearly independent subset ${S}\subset{V}$
            where
            $S=\{\mathbf{v}_{k}\}_{k=0}^{n}$
            and for all
            $\mathbf{x}\in{V}$ there is an
            $a_{n}:\mathbb{Z}_{n}\rightarrow\mathbb{R}$
            such that
            $\mathbf{x}=\sum_{k=1}^{n}a_{k}\mathbf{v}_{k}$
        \end{definition}
        \begin{theorem*}
            All bases of a vector space $V$ have the
            same number of elements.
        \end{theorem*}
        \begin{definition}
            The dimension of a vector space
            $V$ is the number of elements in any
            basis of $V$.
        \end{definition}
        \begin{definition}
            An inner product on a vector space $V$ is a function
            $\langle|\rangle:{V}\times{V}\rightarrow\mathbb{R}$
            such that:
            \begin{enumerate}
                \begin{multicols}{3}
                    \item $\langle{v_{1},v_{2}}\rangle%
                           =\langle{v_{2},v_{3}}\rangle$
                    \item $\langle{v_{1},v_{1}}\rangle\geq{0}$
                    \item $\langle{v_{1}+v_{2},v_{3}}\rangle%
                           =\langle{v_{1},v_{3}}\rangle%
                           +\langle{v_{2},v_{3}}\rangle$
                \end{multicols}
            \end{enumerate}
        \end{definition}
        \begin{definition}
            The Euclidean inner product on $\mathbb{R}^{n}$
            is defined as:
            $\langle{\mathbf{x},\mathbf{y}}\rangle%
             =\sum_{k=1}^{n}x_{k}y_{k}$
        \end{definition}
        \begin{definition}
            An eigenvector of an ${n}\times{n}$ matrix
            $\mathbf{A}$ is a vector
            $\mathbf{x}\in\mathbb{R}^{n}$
            such that there exists a $\lambda\in\mathbb{R}$
            such that
            $\mathbf{A}\mathbf{x}=\lambda\mathbf{x}$
        \end{definition}
        \begin{definition}
            An eigenvalue of an ${n}\times{n}$ matrix
            $\mathbf{A}$ is a real number
            $\lambda\in\mathbb{R}$ such that there is
            a vector $\mathbf{x}\in\mathbf{R}^{n}$ such
            that $\mathbf{A}\mathbf{x}=\lambda\mathbf{x}$
        \end{definition}
        \begin{definition}
            The characteristic equation, or
            the characteristic polynomial, of an
            ${n}\times{n}$ matrix $\mathbf{A}$
            is $\det(\lambda{I}-\mathbf{A})=0$
        \end{definition}
        \begin{definition}
            A diagonalizable matrix is an
            ${n}\times{n}$ matrix
            $\mathbf{A}$ such that there exists
            an invertible matrix $\mathbf{B}$
            such that
            $\mathbf{A}=\mathbf{B}^{-1}\mathbf{A}\mathbf{B}$
        \end{definition}
        \begin{theorem*}
            The following are true:
            \begin{enumerate}
                \item If $\mathbf{A}$ is an ${n}\times{n}$
                    diagonable matrix, then $\mathbf{A}$
                    has $n$ linearly independent
                    eigenvectors.
                \item If $\mathbf{A}$ is an ${n}\times{n}$
                    matrix with $n$ linearly independent
                    eigenvectors, then $\mathbf{A}$
                    is diagonalizable.
                \item A symmetric matrix has all real
                    eigenvalues.
            \end{enumerate}
        \end{theorem*}
    \subsection{Abstract Algebra}
        \begin{definition}
            A group is a set $G$ and a binary relation $*$
            on $G$, denoted $(G,*)$, such that:
            \begin{enumerate}
                \item For all ${a,b,c}\in{G}$, $(a*b)*c=a*(b*c)$
                \item There is an ${e}\in{G}$ such that for all
                    ${a}\in{G}$, $a*e=e*a=a$.
                \item For all ${a}\in{G}$ there is a ${b}\in{G}$
                    such that $a*b=e$
            \end{enumerate}
        \end{definition}
        \begin{definition}
            An Abelian group is a grou $(G,*)$ such that
            $*$ is commutative.
        \end{definition}
        \begin{example}
            $G=\{1\}$ is an Abelian group
            under multiplication.
            This is the trivial group.
        \end{example}
        \begin{theorem*}
            If $(G,*)$ is a group, then the following are true:
            \begin{enumerate}
                \begin{multicols}{2}
                    \item The identity ${e}\in{G}$ is unique.
                    \item If $a*b=a*c$, then $b=c$.
                    \item If $b*a=c*a$, then $b=c$.
                    \item Inverses $a^{-1}$ are unique.
                    \item $\forall_{{a,b}\in{G}}%
                        \exists_{{x}\in{G}}:a*x=b$
                    \item $(a*b)^{-1}=b^{-1}*a^{-1}$
                \end{multicols}
            \end{enumerate}
        \end{theorem*}
        \begin{definition}
            The order of a group is number of elements in the
            group.
        \end{definition}
        \begin{definition}
            A group of finite order, or a finite group,
            is a group with finitely many elements.
        \end{definition}
        \begin{definition}
            The direct product of two groups $(G,*)$ and
            $(H,\circ)$ is the group  $({G}\times{H},\star)$
            where $\star$ is the binary operation defined by
            $(g_{1},h_{1})\star(g_{2},h_{2})%
             =(g_{1}*g_{2},{h_{1}}\circ{h_{2}})$
        \end{definition}
        \begin{definition}
            A permutation group on $n$ elements is a
            group whose elements are permutations of
            $n$ elements.
        \end{definition}
        \begin{definition}
            The symmetric group on $n$ elements,
            denoted $S_{n}$, is the group formed by
            permuting $n$ elements.
        \end{definition}
        \begin{definition}
            A homomorphism from a group $(G,*)$ to
            a group $(H,\circ)$ is a function
            $h:{G}\rightarrow{H}$ such that for all
            ${a,b}\in{G}$, $h(a*b)={h(a)}\circ{h(b)}$
        \end{definition}
        \begin{definition}
            An epimorphism from a group $(G,*)$ to
            a group $(H,\circ)$ is a homomorphism
            $h:{G}\rightarrow{H}$ such that
            $h$ is surjective.
        \end{definition}
        \begin{definition}
            A monomorphism from a group $(G,*)$ to
            a group $(H,\circ)$ is a homomorphism
            $h:{G}\rightarrow{H}$ such that
            $h$ is injective.
        \end{definition}
        \begin{definition}
            An isomorphism from a group $(G,*)$ to
            a group $(H,\circ)$ is a homomorphism
            $h:{G}\rightarrow{H}$ such that
            $h$ is bijective.
        \end{definition}
        \begin{definition}
            A ring is a set $R$ and two binary operations
            on $R$, denoted $(R,\cdot,+)$, such that:
            \begin{enumerate}
                \begin{multicols}{3}
                \item $(R,+)$ is an Abelian group.
                \item $a\cdot({b}\cdot{c})%
                       =({a}\cdot{b})\cdot{c}$
                \item ${a}\cdot(b+c)%
                       ={a}\cdot{b}+{a}\cdot{c}$
                \end{multicols}
            \end{enumerate}
        \end{definition}
        \begin{definition}
            A ring with identity is a ring $(R,\cdot,+)$
            such that there is a ${1}\in{R}$ such that for
            all ${a}\in{R}$, ${a}\cdot{1}={1}\cdot{a}=a$.
        \end{definition}
        \begin{remark}
            Left and right identities are elements such
            that ${e_{L}}\cdot{a}=a$ and ${e_{R}}\cdot{a}=a$.
            If inverses $a_{L}^{-1}$ and $a_{R}^{-1}$ exist
            for $a$, then $a_{L}^{-1}=a_{R}^{-1}$. That is,
            the inverse is the same for both right and left
            identities.
        \end{remark}
        \begin{definition}
            A commutative ring is a ring $(R,\cdot,+)$ such that
            $\cdot$ is commutative.
        \end{definition}
        \begin{definition}
            A commutative ring with identity is a
            ring with identity such that $\cdot$
            is commutative.
        \end{definition}
        \begin{definition}
            A Field is a commutative ring with identity
            $(F,\cdot,+)$
            such that for all ${a}\in{F}$ such that
            $a$ is not an identity with respect to $+$,
            there is a $b\in{F}$ such that ${a}\cdot{b}=1$.
        \end{definition}
        \begin{definition}
            Equivalent sets are sets $A$ and $B$ such that
            there exists a bijective function
            $f:{A}\rightarrow{B}$
        \end{definition}
        \begin{definition}
            A finite set is a set $A$ such that there
            is an ${n}\in{\mathbb{N}}$ such that $A$
            is equivalent to $\mathbb{Z}_{n}$.
        \end{definition}
        \begin{definition}
            A countable set (Or a denumerable set) is a
            set $A$ that is equivalent to $\mathbb{N}$.
        \end{definition}
        \begin{definition}
            An uncountable set is a set that is neither finite
            nor countable.
        \end{definition}
        \begin{theorem*}
            Set Equivalence is an equivalence relation.
        \end{theorem*}
        This equivalence allows to classify all sets by the
        number of elements they contain or, more generally,
        by their cardinality. We say that two sets $A$ and
        $B$ have the same cardinality, denoted
        $\Card(A)$, if and only if $A$ and $B$ are equivalent.
        \begin{theorem*}
            The following are true:
            \begin{enumerate}
                \begin{multicols}{2}
                    \item $\Card(A)=0$ if and only if
                          $A=\emptyset$.
                    \item If ${A}\sim{\mathbb{Z}_{n}}$, then
                          $\Card(A)=n$.
                \end{multicols}
            \end{enumerate}
        \end{theorem*}
        \begin{definition}
            A finite cardinal number is a cardinal
            number of a finite set.
        \end{definition}
        \begin{definition}
            The standard ordering on the finite cardinal
            number is $0<1<\hdots<n<n+1<\hdots$
        \end{definition}
        Thus, if $A$ and $B$ are finite sets, then we write
        $\Card(A)<\Card(B)$ if $A$ is equivalent to a
        subset of $B$ but not equivalent to $B$.
        We take this notion and generalize to
        all sets. For $A$ and $B$, we write
        $\Card(A)<\Card(B)$ if $A$ is equivalent to a subset
        of $B$ but is not equivalent to $B$. This is the
        same as saying $A$ is equivalent to a subset of $B$,
        but $B$ is not equivalent to a subset of $A$.
        We write that
        $\Card(A)\leq\Card(B)$ is $A$ is equivalent to a
        subset of $B$.
        \begin{theorem*}[Schr\"{o}der-Bernstein Theorem]
            If $A$ and $B$ are sets such that
            $\Card(A)\leq\Card(B)$ and
            $\Card(B)\leq\Card(A)$, then
            $\Card(A)=\Card(B)$.
        \end{theorem*}
        \begin{theorem*}
            The following are true:
            \begin{enumerate}
                \item If $\Card(A)\leq\Card(B)$ and
                      $\Card(B)\leq\Card(A)$, then
                      $\Card(A)\leq\Card(C)$.
                \item If $\Card(A)\leq\Card(B)$, then
                      $\Card(A)+\Card(C)\leq\Card(B)+\Card(C)$
            \end{enumerate}
        \end{theorem*}
        \begin{theorem*}
            If ${A}\subset{B}\subset{C}$, and
            $\Card(A)=\Card(C)$, then $\Card(B)=\Card(C)$
        \end{theorem*}
        \begin{theorem*}
            If $f:{X}\rightarrow{Y}$ is a function,
            then $\Card(f(X))\leq\Card(X)$.
        \end{theorem*}
        \begin{proof}
            Note that $f^{-1}(\{y\})$ creates a set of
            mutually disjoint subsets of $X$. By the
            axiom of choice there is a function
            $F:{f(X)}\rightarrow{X}$
            such that for all ${y}\in{f(X)}$,
            ${F(y)}\in{f^{-1}(\{y\})}$. But since these
            sets are disjoint, $F$ is injective.
            Thus, $f(X)$ is equivalent to a subset of $X$.
            Therefore, $\Card(f(X))\leq\Card(X)$.
        \end{proof}
        The Schr\"{o}der-Bernstein theorem can be restated
        equivalently as ``If $A$ is equivalent to a subset
        of $B$ and $B$ is equivalent to a subset of $A$,
        then $A$ is equivalent to $B$.''
        Addition and multiplication of finite cardinals
        follows directly from the standard arithmetic
        for the natural numbers. For cardinals of infinite
        sets, the arithmetic becomes a little more complicated.
        \begin{definition}
            The sum of two cardinal numbers is the
            cardinality of the union of two disjoint sets $A$
            and $B$. That is, if ${A}\cap{B}=\emptyset$, then
            $\Card(A)+\Card(B)=\Card({A}\cup{B})$.
        \end{definition}
        \begin{theorem*}
            If $a$ and $b$ are distinct cardinal numbers,
            then there exists sets $A$ and $B$ such that
            ${A}\cap{B}=\emptyset$, $\Card(A)=a$, and
            $\Card(B)=b$.
        \end{theorem*}
        \begin{theorem*}
            If $A,B,C,$ and $D$ are sets such that
            $\Card(A)=\Card(C)$, $\Card(B)=\Card(D)$,
            and if ${A}\cap{B}=\emptyset$ and
            ${C}\cap{D}=\emptyset$, then
            $\Card({A}\cup{B})=\Card({C}\cup{D})$.
        \end{theorem*}
        \begin{theorem*}
            If $x,y,$ and $z$ are cardinal numbers, then
            $x+y=y+x$ and $x+(y+z)=(x+y)+z$.
        \end{theorem*}
        \begin{notation}
            The carinality of the set of natural numbers
            is denoted $\aleph_{0}$. That is,
            $\Card(\mathbb{N})=\aleph_{0}$
        \end{notation}
        \begin{example}
            Find the cardinal sum of $2$ and $5$. Let
            $N_{2}=\{1,2\}$ and $N_{5}=\{3,4,5,6,7\}$.
            Then $N_{2}$ and $N_{5}$ are disjoint,
            $\Card(N_{2})=2$ and $\Card(N_{5})=5$.
            Therefore $2+5=\Card(N_{2}\cup{N_{5}})$.
            But ${N_{2}}\cup{N_{5}}$ is just $\mathbb{Z}_{7}$,
            and $\Card(\mathbb{Z}_{7})=7$. Thus, $2+5=7$.
        \end{example}
        \begin{theorem*}
            If $n$ and $m$ are finite cardinalities,
            then the cardinal sum of $n$ and $m$ is the
            integer $n+m$, where $+$ is the usual
            arithmetic addition.
        \end{theorem*}
        \begin{example}
            Compute the cardinal sum
            $\aleph_{0}+\aleph_{0}$. Let
            $\mathbb{N}_{e}$ be the set of even natural
            numbers, and let $\mathbb{N}_{o}$ be the set
            of odd natural numbers. Then
            $\Card(\mathbb{N}_{e})=\aleph_{0}$,
            $\Card(\mathbb{N}_{o})=\aleph_{0}$, and
            ${\mathbb{N}_{o}}\cap{\mathbb{N}_{e}}=\emptyset$.
            Thus
            $\aleph_{0}+\aleph_{0}%
             =\Card({\mathbb{N}_{o}}\cup{\mathbb{N}_{e}})$.
            But
            ${\mathbb{N}_{o}}\cup{\mathbb{N}_{e}}%
             =\mathbb{N}$ and $\Card(\mathbb{N})=\aleph_{0}$.
            Therefore, $\aleph_{0}+\aleph_{0}=\aleph_{0}$.
        \end{example}
        \begin{example}
            Find $n+\aleph_{0}$, where $n\in\mathbb{N}$.
            We have that
            $\Card(\mathbb{Z}_{n}z)=n$ and
            $\Card(\mathbb{N}\setminus\mathbb{Z}_{n})%
             =\aleph_{0}$
            But then
            $n+\aleph_{0}=%
             \Card(\mathbb{Z}_{n}\cup%
             \mathbb{N}\setminus\mathbb{Z}_{n})%
             =\Card(\mathbb{N})=\aleph_{0}$.
            Therefore, $n+\aleph_{0}=\aleph_{0}$.
        \end{example}
        \begin{definition}
            The cardinality of the continuum,
            denoted $\mathfrak{c}$, is the
            cardinality of the set of real numbers.
            That is, $\mathfrak{c}=\Card(\mathbb{R})$.
        \end{definition}
        \begin{theorem*}
            $\Card([0,1])=\mathfrak{c}$.
        \end{theorem*}
        \begin{theorem*}
            $\Card\big((0,1)\big)=\mathfrak{c}$.
        \end{theorem*}
        \begin{theorem*}
            $\mathbb{R}$ is uncountable. That is,
            $\mathfrak{c}>\aleph_{0}$.
        \end{theorem*}
        \begin{theorem*}
            $\mathfrak{c}+\aleph_{0}=\mathfrak{c}$.
        \end{theorem*}
        \begin{proof}
            We have $\Card((0,1))=\mathfrak{c}$ and
            $\Card(\mathbb{N})=\aleph_{0}$. But
            $(0,1)\cap\mathbb{N}=\emptyset$, and thus
            $\aleph_{0}+\mathfrak{c}%
             =\Card((0,1)\cup\mathbb{N})$.
            But $\mathbb{R}\sim(0,1)$ and
            $\mathbb{N}\cup(0,1)\subset\mathbb{R}$.
            By the Schr\"{o}der-Bernstein theorem,
            $\mathbb{N}\cup(0,1)\sim\mathbb{R}$.
            Therefore, etc.
        \end{proof}
        \begin{definition}
            The product of two cardinal numbers $a$ and $b$
            is the cardinality of the cartesian product
            of two set $A$ and $B$ such that
            $\Card(A)=a$ and $\Card(B)=b$. That is,
            ${a}\times{b}=\Card({A}\times{B})$.
        \end{definition}
        \begin{theorem*}
            The following are true of cardinal numbers:
            \begin{enumerate}
                \begin{multicols}{3}
                    \item $xy=yx$
                    \item $x(yz)=(xy)z$
                    \item $x(y+z)=xy+xz$
                \end{multicols}
            \end{enumerate}
        \end{theorem*}
        \begin{proof}[Proof of Part 3]
            Let $A,B,$ and $C$ be disjoint.
            Then
            ${A}\times{({B}\cup{C})}%
             =({A}\times{B})\cup({A}\times{C})$, and thus
            $\Card({A}\times{({B}\cup{C})})%
             =\Card(({A}\times{B})\cup({A}\times{C}))$.
            But ${A}\times{B}$ and ${A}\times{C}$ are disjoint.
            Thus we have
            $\Card(({A}\times{B})\cup({A}\times{C}))%
             =\Card({A}\times{B})+\Card({A}\times{C})$.
            Therefore, etc.
        \end{proof}
        \begin{theorem*}
            If $\Card(T)=x$ and
            $F:{T}\rightarrow{\mathcal{P}(T)}$
            is a set-valued mapping such that for all
            ${t}\in{T}$ we have that
            $\Card(F(t))=y$ and
            for all ${t}\ne{t}$,
            ${F(t)}\cap{F(t')}=\emptyset$, then
            $\Card(\cup_{t=1}^{N}F(t))=xy$
        \end{theorem*}
        \begin{example}
            Let $f:{\mathbb{N}^{2}}\rightarrow{\mathbb{N}}$
            be defined by $f(n,m)=2^{n}3^{m}$.
            Then $f$ is injective, since $2$ and $3$
            are coprime. Therefore,
            $\aleph_{0}\times\aleph_{0}=\aleph_{0}$.
        \end{example}
        \begin{example}
            Show that $\mathbb{R}^{2}\sim\mathbb{R}$.
            Let $f:\mathbb{R}^{2}\rightarrow\mathbb{R}$
            be the rather bizarre function defined by the image
            $f(x_{0}.x_{1}x_{2}\hdots,y_{0}.y_{1}y_{2}\hdots)%
             =x_{0}y_{0}.x_{0}y_{0}x_{1}y_{1}\hdots$ Then
            $f$ is inective. But the mapping
            $g:\mathbb{R}\rightarrow\mathbb{R}^{2}$
            defined by $g(x)=(x,0)$ is also injective.
            By Schr\"{o}der-Bernstein,
            $\mathbb{R}^{2}\sim\mathbb{R}$.
        \end{example}
        \begin{definition}
           Order isomorphic set are two sets $A$ and $B$
           with well orders $<_{A}$ and $<_{B}$ such that
           there exists a bijection $f:{B}A\rightarrow{B}$
           such that for all $a_{1},a_{2}\in{A}$ such that
           $a_{1}<_{A}a_{2}$, $f(a_{1})<_{B}f(a_{2})$.
        \end{definition}
        \begin{theorem*}
           Order-Isomorphism is an equivalence relation.
        \end{theorem*}
        To every well ordered set, an ordinal number is
        assigned, denoted $\Ord(A,<_{A})$. Conversely,
        for every ordinal number there is a set with a
        well order corresponding to it. Two ordinal numbers
        are equal if and only if the well-ordered sets
        corresponding to them are order isomorphic.
        That is,
        $\Ord(A,<_{A})=\Ord(B,<_{B})$ if and only if
        $(A,<_{A})$ and $(B,<_{B})$ are order isomorphic.
        \begin{theorem*}
           If $(A,<_{A})$ and $(B,<_{B})$ are well ordered
           sets, and if $\Card(A)=\Card(B)$, then
           $(A,<_{A})$ and $(B,<_{B})$ are order
           isomorphic.
        \end{theorem*}
        The ordinal number of the empty set is $0$. The
        ordinal number of a finite set of $n$ elements with
        a well ordering is denoted $n\in\mathbb{N}$.
        The ordinal for the natural numbers $\mathbb{N}$
        with their usual well-ordering is denoted $\omega$.
        A given well-ordered set has only one cardinal number,
        but it is possible for it to have two ordinal numbers.
        \begin{definition}
           An ordinal number $\alpha$ is less than or equal
           to an ordinal number $\beta$ if there are
           well-ordered sets $(A,<_{A})$ and $(B,<_{B})$
           such that $\alpha=\Ord((A,<_{A}))$ and
           $\beta=\Ord(B,<_{B})$, and $(A,<_{B})$ is
           order isomorphic to subset of
           $(B,<_{B})$.
        \end{definition}
        \begin{theorem*}
           The only order isomorphism from a well ordered
           set $(A,<_{A})$ to itself is the identity
           isomorphism.
        \end{theorem*}
        \begin{theorem*}
           If $\alpha$ and $\beta$ are ordinal numbers and
           ${\alpha}\leq{\beta}$ and ${\beta}\leq{\alpha}$,
           then $\alpha=\beta$.
        \end{theorem*}
        \begin{theorem*}
           If $\alpha$ and $\beta$ are ordinal numbers,
           either ${\alpha}\leq{\beta}$, or
           ${\beta}\leq{\alpha}$.
        \end{theorem*}
        \begin{theorem*}
           If $\alpha$ and $\beta$ are ordinal numbers,
           either $\alpha<\beta$, $\beta<\alpha$, or
           $\alpha=\beta$.
        \end{theorem*}
        \begin{definition}
           The total ordering relation of a
           well-ordered set $(A,<_{A})$ with respect
           to a well-ordered set $(B,<_{B})$ is the ordering
           on the set ${A}\cup{B}$ defined as: For all
           $a_{1},a_{2}\in{A}$ such that $a_{1}<_{A}a_{2}$,
           $a_{1}<_{*}a_{2}$, for all $b_{1},b_{2}\in{B}$
           such that $b_{1}<_{B}b_{2}$, $b_{1}<_{*}b_{2}$,
           and for all ${a}\in{A}$ and ${b}\in{B}$,
           ${a}<_{*}{b}$.
        \end{definition}
        \begin{theorem*}
           The total ordering relation $<_{*}$ on the set
           ${A}\cup{B}$ is a well-ordering.
        \end{theorem*}
        \begin{definition}
           The ordinal sum of two ordinal numbers
           $\Ord((A,<_{A}))$ and $\Ord((B,<_{B}))$,
           where $A$ and $B$ are disjoint,
           is the ordinal number
           $\Ord(({A}\cup{B},<_{*}))$.
        \end{definition}
        \begin{theorem*}
           The following are true of ordinal numbers:
           \begin{enumerate}
               \begin{multicols}{3}
                   \item $\alpha<\beta\Rightarrow%
                          \alpha+\gamma<\beta+\gamma$
                   \item $(\alpha+\beta)+\gamma%
                          =\alpha+(\beta+\gamma)$
                   \item $\alpha+\beta=\alpha+\gamma%
                          \Rightarrow\beta=\gamma$.
               \end{multicols}
           \end{enumerate}
        \end{theorem*}
        \begin{definition}
           The lexicographic ordering on the cartesian
           product of well ordered set $(A,<_{A})$ and
           $(B,<_{B})$ is the ordering on
           ${A}\times{B}$ defined by: If ${a}<_{A}{x}$,
           then $(a,b)<_{*}(x,y)$ for all $b,y\in{B}$, and
           if $a=x$ and $b<_{B}y$, then $(a,b)<_{*}(x,y)$.
        \end{definition}
        \begin{theorem*}
           If $(A,<_{A})$ and $(B,<_{B})$ are well ordered
           sets, then the lexicographic ordering
           on ${A}\times{B}$ is a well ordering.
        \end{theorem*}
        \begin{definition}
           The ordinal product of two ordinal numbers
           $\Ord((A,<_{A}))$ and $\Ord((B,<_{B}))$,
           is $\Ord(({A}\times{B},<_{*}))$
        \end{definition}
        \begin{theorem*}
           The following are true of ordinal numbers:
           \begin{enumerate}
               \begin{multicols}{2}
                   \item $\alpha(\beta\gamma)%
                          =(\alpha\beta)\gamma$
                   \item $\alpha(\beta+\gamma)%
                          =\alpha\beta+\alpha\gamma$
               \end{multicols}
           \end{enumerate}
        \end{theorem*}
        \begin{definition}
           Relatively prime integers are integers
           $a,b\in\mathbb{N}$ such that $\gcd(a,b)=1$.
        \end{definition}
        \begin{theorem*}
           If $p$ is prime and $a\in\mathbb{N}$ is
           such that $p$ does not divide $a$, then $a$ and $p$
           are relatively prime.
        \end{theorem*}
        \begin{theorem*}
           There are infinitely many prime numbers.
        \end{theorem*}
        \begin{theorem*}
           If $a\in\mathbb{N}$, $a>1$, then either
           $a$ is a prime number, or $a$ is the product
           of finitely many primes.
        \end{theorem*}
        \begin{theorem*}
           If $a\in\mathbb{N}$, $a>1$, and if $a$ is not
           prime, then the prime expansion of $a$ is
           unique.
        \end{theorem*}
        \begin{definition}
           A diophantine equation is an equation whose
           solutions are required to be integers.
        \end{definition}
        \begin{definition}
           A linear diophantine equation in two variables
           $x$ and $y$ is an equation
           $ax+by=c$, where $a,b,c\in\mathbb{Z}$.
        \end{definition}
        \begin{theorem*}
           If $a,b,c\in\mathbb{Z}$ $d=\gcd(a,b)$,
           and if $d$ does not divide $c$,
           then $ax+by=c$ has no integral solutions.
        \end{theorem*}
        \begin{theorem*}
           If $a,b,c\in\mathbb{Z}$ $d=\gcd(a,b)$,
           and if $d$ divides $c$,
           then $ax+by=c$ has infinitely many solutions.
        \end{theorem*}
\end{document}