\documentclass[crop=false,class=book,oneside]{standalone}
%----------------------------Preamble-------------------------------%
\input{../../../preamble.tex}
\graphicspath{{../../../images/}}   % Path to Image Folder.
%--------------------------Main Document----------------------------%
\begin{document}
    \ifx\ifmathcourses\undefined
        \pagenumbering{roman}
        \title{Functional Analysis}
        \author{Ryan Maguire}
        \date{\vspace{-5ex}}
        \maketitle
        \tableofcontents
        \clearpage
        \chapter{Functional Analysis}
        %\addcontentsline{toc}{chapter}{Functional Analysis}
        %\markboth{}{FUNCTIONAL ANALYSIS}
        %\setcounter{chapter}{1}
        \pagenumbering{arabic}
    \else
        \chapter{Functional Analysis}
    \fi
    \section{A Review of Real Analysis}
        \subsection{Sets, Functions, and Countability}
            A quick review of the notations of various sets:
            \begin{table}[H]
                \centering
                \begin{tabular}{|l|l|l|}
                    \hline
                    Notation:&Description:&Importance:\\
                    \hline
                    $\mathbb{N}$&The Natural Numbers
                    &Used a lot.\\
                    \hline
                    $\mathbb{Z}$&The integers.&Never used.\\
                    \hline
                    $\mathbb{Z}_{n}$&Integers from $1$ to $n$.
                    &Mentioned occasionally.\\
                    \hline
                    $\mathbb{Q}$&The Rational Numbers&
                    Good for examples/counterexamples.\\
                    \hline
                    $\mathbb{R}$&The Real Numbers.&
                    Primary set of concern.\\
                    \hline
                    $\mathbb{C}$&The Complex Numbers&
                    Rarely used.\\
                    \hline
                \end{tabular}
            \end{table}
            From set theory, a function $f$ from a set $X$ to a
            set $Y$ is a subset of $X\times{Y}$ such that, for
            all $x\in{X}$, there is a unique $y\in{Y}$
            such that $(x,y)\in{f}$. We often call $X$ the
            domain, $Y$ the range or co-domain, and write
            $f:X\rightarrow{Y}$ to indicate this. The
            \textit{image} of $x\in{X}$ is often written
            as $y=f(x)$. Requiring that $y$ be unique for each
            $x$ is equivalent to the \textit{vertical line test}
            one might find in a calculus course.
            \begin{definition}
                The image of $S\subseteq{X}$
                by a function $f:X\rightarrow{Y}$
                is the set:
                \begin{equation}
                    f(S)=\{f(x)\in{Y}:x\in{X}\}
                \end{equation}
            \end{definition}
            The image of $S\subseteq{X}$ is the set of all
            points in $Y$ that $S$ gets mapped to by $f$.
            \begin{definition}
                The pre-image of $S\subseteq{Y}$ by a function
                $f:X\rightarrow{Y}$ is the set:
                \begin{equation}
                    f^{-1}(S)=\{x\in{X}:f(x)\in{S}\}
                \end{equation}
            \end{definition}
            An injective function is a function such
            that $f(x_{1})=f(x_{2})$ if and only if $x_{1}=x_{2}$.
            A surjective function is a function $f:X\rightarrow{Y}$
            such that $f(X)=Y$. That is, every point $y\in{Y}$
            gets mapped to by at least one point in $X$. Or, in
            more familiar notation, for all $y\in{Y}$ there is
            and $x\in{X}$ such that $y=f(x)$. A bijective
            function is a function that is both injective and
            surjective. Sets $X$ and $Y$ such that there
            exists a bijective function $f:X\rightarrow{Y}$ are
            called \textit{equivalent}. Such sets can be said
            to have the same size. We say that $X$ is strictly
            smaller than $Y$ if there is an injective function
            $f:X\rightarrow{Y}$, but no bijective function.
            \begin{definition}
                A finite set is a set $X$ such that there
                exists an $n\in\mathbb{N}$ such that there is
                a bijective function
                $f:\mathbb{Z}_{n}\rightarrow{S}$
            \end{definition}
            \begin{definition}
                A countable set is a set
                $X$ such that there exists a bijective
                function $f:\mathbb{N}\rightarrow{X}$.
            \end{definition}
            Being countable means you can write
            the elements out in a list, or a
            one-to-one correspondence with all of
            the positive integers. Many sets are countable,
            including the whole numbers, integers, rational
            numbers, and \textit{algebraic} numbers. The
            union of finitely many countable sets is also
            countable, as is the union of countably many
            countable sets.
            \begin{theorem}
                \label{thm:Funct_Countable_Union_of_Countable}
                If $A$ is a countable set such that for all
                $\mathcal{U}\in{A}$, $\mathcal{U}$ is a
                countable set, and if for all $a,b\in{A}$,
                $a\cap{b}=\emptyset$, then
                $\bigcup_{\mathcal{U}\in{A}}\mathcal{U}$
                is countable set.
            \end{theorem}
            \textit{Sketch of Proof.} The proof of
            Thm.~\ref{thm:Funct_Countable_Union_of_Countable}
            follows in the same manner
            as proving that the rationals are countable. Since
            there are countably many sets, write them out in
            a list $\mathcal{U}_{1}$, $\mathcal{U}_{2}$, and
            so on. Then write out the elements in a table as
            follows:
            \begin{table}[H]
                \captionsetup{type=table}
                \centering
                \begin{tabular}{ccccc}
                    $u_{11}$&$u_{12}$&$u_{13}$
                    &$u_{14}$&$\hdots$\\
                    $u_{21}$&$u_{22}$&$u_{23}$
                    &$u_{24}$&$\hdots$\\
                    $u_{31}$&$u_{32}$&$u_{33}$
                    &$u_{34}$&$\hdots$\\
                    $u_{41}$&$u_{42}$&$u_{43}$
                    &$u_{44}$&$\hdots$\\
                    $\vdots$&$\vdots$&$\vdots$
                    &$\vdots$&$\ddots$
                \end{tabular}
                \caption{Construction of a Bijection on the
                         Countable Union of Countably Infinite
                         Sets.}
                \label{table:Func_Countable_Union_of_Countable}
            \end{table}
            Using the \textit{diagonal argument}, we constuct
            the bijection as follows:
            \begin{table}[H]
                \centering
                \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
                    \hline
                    $\mathbb{N}$&1&2&3&4&5&6&7&8&9&$\hdots$\\
                    \hline
                    $\bigcup_{\mathcal{U}\in{A}}\mathcal{U}$&
                    $u_{11}$&$u_{12}$&$u_{21}$&$u_{13}$&
                    $u_{22}$&$u_{31}$&$u_{14}$&$u_{23}$&
                    $u_{32}$&$\hdots$\\
                    \hline
                \end{tabular}
                \label{table:Func_Bijection_on_Countable_Union}
            \end{table}
            \begin{definition}
                An uncountable set is a set that is
                neither finite nor countable.
            \end{definition}
            \begin{theorem}
                $\mathbb{Q}$ is countable.
            \end{theorem}
            \textit{Sketch of Proof.}
            We can show easily that $\mathbb{Q}$ is at least
            greater than or equal to $\mathbb{N}$ since
            $\mathbb{N}\subset\mathbb{Q}$. We now show
            that there is a surjective function
            $f:\mathbb{N}\rightarrow\mathbb{Q}$. The
            Cantor-Schr\"{o}eder-Bernstein Theorem then says
            that there is a bijection between
            $\mathbb{N}$ and $\mathbb{Q}$. Using
            Table~\ref{table:Func_Countable_Union_of_Countable}
            and letting $u_{ij}=i/j$, the list shown in
            Eqn.~\ref{table:Func_Bijection_on_Countable_Union}
            is thus a \textit{surjection} from
            $\mathbb{N}$ to $\mathbb{Q}$. It is not bijective,
            for $u_{nn}=1$ for all $n$, and thus
            1 gets mapped to infinitely many times. The
            Cantor-Schr\"{o}eder-Bernstein Theorem implies
            there is a bijection between $\mathbb{N}$ and
            $\mathbb{Q}$.
            \begin{theorem}
                $\mathbb{R}$ is uncountable.
            \end{theorem}
            \textit{Sketch of Proof.} We'll show that the unit
            interval $(0,1)$ is uncountable. Suppose not.
            Let $r_{ij}$ be the $j^{th}$ decimal of the $i^{th}$
            element in the list. We construct the real number
            $d$ as follows: If $d_{j}$ denotes the $j^{th}$
            decimal in $d$, let $d_{j}=r_{jj}+1$ if
            $r_{jj}\ne{9}$, and $d_{j}=0$ otherwise. Then
            $d\in(0,1)$, but $d$ is not on the list. For it's not
            the $n^{th}$ element, for it differs in the
            $n^{th}$ decimal place. Thus there is no bijection.
            Therefore, $(0,1)$ is uncountable. By extension,
            $\mathbb{R}$ is uncountable.
            \par\hfill\par
            \vspace{-2ex}
            For a set $X$, we often write
            $\mathcal{P}(X)$ to denote the
            \textit{power set} of $X$. This is the
            set of all subsets of $X$.
            For any set $X$ you can show that $X$ is
            strictly smaller than $\mathcal{P}(X)$.
            For example, $\mathcal{P}(\mathbb{N})$
            can be shown to be equivalent to $\mathbb{R}$.
            Since $\mathbb{N}$ is stricly smaller than
            $\mathbb{R}$, one might ask if there exists
            a set $X$ such that $\mathbb{N}$ is strictly
            smaller than $X$, but $X$ is strictly smaller
            than $\mathbb{R}$. Continuing, you can ask the
            same thing about $\mathbb{R}$ and
            $\mathcal{P}(\mathbb{R})$, and so on.
            This is called the continuum hypothesis.
            It turns out to be independent of
            the standard axioms of mathematics.
        \subsection{Completeness}
            One of the fundamental properties of $\mathbb{R}$ is
            that is is \textit{complete}. This property is
            fundamental to many theorems involved in a
            standard calculus or real analysis course. For
            example, the concepts of differentiation and
            convergence rely on completeness, and the
            intermediate value theorem may fail without it.
            On the other hand, $\mathbb{Q}$ is not complete. The
            rationals are, however, \textit{dense} in the reals.
            That is, elements of $\mathbb{R}$ can be
            approximated arbitrarily well by elements of
            $\mathbb{Q}$. $\mathbb{R}$ is also something
            called a \textit{field}. From algebra,
            a field is just a set with two operations
            (Usually called addition and multiplication)
            that are defined in such a way as to give rise
            to the usual notions of addition, subtraction,
            multiplication, and non-zero division,
            and to give them the basic properties of
            associativity, commutativity,
            and the distributive law that is found in
            arithmetic. $\mathbb{Q}$ is also a field.
            Moreover, $\mathbb{R}$ and $\mathbb{Q}$ are
            \textit{ordered fields} with respect to
            their standard ordering. What makes
            $\mathbb{R}$ special is that it is a
            complete ordered field. In fact, $\mathbb{R}$
            is the \textit{only} complete ordered field
            (Up to isomorphism). Completeness in
            $\mathbb{R}$ can be stated by fact that the real
            numbers have the least upper bound property.
            \begin{definition}
                A bounded above subset of $\mathbb{R}$
                is a nonempty subset $S\subseteq{\mathbb{R}}$
                such that there exists an $M\in\mathbb{R}$
                such that for all $x\in{S}$, $x\leq{M}$.
            \end{definition}
            \begin{definition}
                An upper bound of a bounded above
                subset $S\subseteq\mathbb{R}$ is a real
                number $M\in\mathbb{R}$
                such that for all $x\in{S}$, $x\leq{M}$.
            \end{definition}
            If $S\subseteq\mathbb{R}$ is bounded above,
            then there exists infinitely many bounds.
            Completeness says that every bounded above subset
            has a smallest upper bound.
            \begin{theorem}[Least Upper Bound Theorem]
                \label{thm:Func_Least_Upper_Bound_Theorem}
                If $S\subseteq{\mathbb{R}}$ is bounded above,
                then there exists an $s\in\mathbb{R}$,
                called the least upper bound, such that $s$
                and an upper bound and for all upper bounds
                $M$ of $S$, $s\leq{M}$.
            \end{theorem}
            The proof of
            Thm.~\ref{thm:Func_Least_Upper_Bound_Theorem}
            depends on how one defines the real numbers. This is
            often done via Dedekind cuts or equivalence
            classes of Cauchy sequences in $\mathbb{Q}$.
            \begin{theorem}
                There exist $S\subset\mathbb{Q}$
                such that $S$ is bounded above and
                for all upper bounds $M$ there exists
                an $s\in\mathbb{Q}$ such that $s$ is an upper
                bound of $S$ and $s<M$.
            \end{theorem}
            \begin{example}
                As an example, consider the set
                $\{x\in\mathbb{Q}:x^{2}\leq{2}\}$.
                This set has no least upper bound. Loosely
                speaking this is because the
                least upper bound wants to be $\sqrt{2}$,
                but $\sqrt{2}$ is not a rational number. Thus
                there is no rational number to fill the gap.
                The rationals are incomplete.
            \end{example}
            The least upper bound property gives rise
            to many theorems, many of which are equivalent
            to this axiom. Recall that a sequence is a
            function $x:\mathbb{N}\rightarrow{X}$. That is,
            a sequence is a function whose domain is the
            natural numbers, and whose image lies in some
            set $X$. A sequence of real numbers is thus a
            function $x:\mathbb{N}\rightarrow\mathbb{R}$,
            and a sequence of rational numbers is a function
            $x:\mathbb{N}\rightarrow\mathbb{Q}$.
            Often times sequences are denoted $x_{n}$,
            but also the image of $n$ is usually
            denoted $x(n)=x_{n}$ which may be a cause
            for confusion. That is, when we write $x_{n}$
            we often mean $x(n)$, so $x_{0}$, $x_{1}$,
            $x_{2}$ can be written as $x(0)$, $x(1)$,
            $x(2)$ but nobody does this. Similarly,
            we may mean $x_{n}=x$ since nobody writes
            a sequence as $x$. For consistency, we will.
            \begin{definition}
                A sequence in a set $X$ is a function
                $x:\mathbb{N}\rightarrow{X}$.
                We write the image of $n\in\mathbb{N}$
                as $x(n)=x_{n}$.
            \end{definition}
            The notion of \textit{convergence} of a sequence
            in $\mathbb{R}$ is defined as follows.
            \begin{definition}
                A convergent sequence in a subset
                $S\subseteq\mathbb{R}$ is a sequence
                $x$ such that there exists an $a\in\mathbb{R}$
                such that $|a-x_{n}|\rightarrow{0}$ as
                $n\rightarrow\infty$. We write
                $x_{n}\rightarrow{a}$.
            \end{definition}
            \begin{definition}
                A limit of a sequence $x$
                in a subset $S\subseteq\mathbb{R}$ is an
                element $a\in\mathbb{R}$ such that
                $|a-x_{n}|\rightarrow{0}$.
            \end{definition}
            \begin{theorem}
                If $S\subseteq\mathbb{R}$ and $a$ and $b$ are
                limits of $x:\mathbb{N}\rightarrow{S}$,
                then $a=b$.
            \end{theorem}
            \begin{proof}
                Suppose not. Then $|a-b|>0$. But as $a$ is a
                limit of $x$, there is an $N_{1}\in\mathbb{N}$
                such that, for all $n>N_{1}$,
                $|a-x_{n}|<|a-b|/4$. But, as $b$ is a limit
                of $x$, there is an $N_{2}\in\mathbb{N}$
                such that for all $n>N_{2}$,
                $|b-x_{n}|<|a-b|/4$. Let $N=\max\{N_{1},N_{2}\}+1$.
                But from the triangle inequality:
                $|a-b|\leq|a-x_{N}|+|b-x_{N}|<|a-b|/2$, a
                contradiction. Therefore, $a=b$.
            \end{proof}
            The next notion to discuss is that of
            \textit{subsequences}. There are two ways to define
            a subsequence rigorously. A subsequence of a sequence
            $x:\mathbb{N}\rightarrow{X}$ is a sequence 
            $y:\mathbb{N}\rightarrow{X}$ such that there exists
            a strictly increasing sequence
            $k:\mathbb{N}\rightarrow\mathbb{N}$ such that, for all
            $n\in\mathbb{N}$, $y_{n}=(x\circ{k})(n)$. Here,
            $(x\circ{k})$ is the \textit{composition} of
            the two functions $x$ and $k$. This is
            often written $x_{k_{n}}$, but this can occasionally
            be confusing. We can also just define a subsequence
            to be any strictly increasing sequence
            $k:\mathbb{N}\rightarrow\mathbb{N}$. Given a sequence
            $x:\mathbb{N}\rightarrow{X}$, since $k$ is strictly
            increasing the ordering of $x\circ{k}$
            remains the same, and we've simply skipped over
            some points. Recall that
            monotonic sequences are sequences such
            that, for all $n\in\mathbb{N}$, either
            $x_{n+1}\leq{x_{n}}$ (Monotonically decreasing),
            or $x_{n}\leq{x_{n+1}}$ (Monotonically increasing).
            Strictly monotonic means either $x_{n+1}<x_{n}$
            or $x_{n}<x_{n+1}$ for all $n\in\mathbb{N}$.
            \begin{definition}
                A subsequence is a strictly increasing sequence
                $k:\mathbb{N}\rightarrow\mathbb{N}$
            \end{definition}
            \begin{definition}
                A convergent subsequence of a sequence
                $x:\mathbb{N}\rightarrow{S}$ in
                a subset $S\subseteq\mathbb{R}$ is a
                subsequence $k$ such that
                $x\circ{k}$ is a convergent sequence in $S$.
            \end{definition}
            \begin{definition}
                A monotonic subsequence of a sequence
                $x:\mathbb{N}\rightarrow{S}$ in a subset
                $S\subseteq\mathbb{R}$ is a subsequence
                $k:\mathbb{N}\rightarrow\mathbb{N}$ such
                that $x\circ{k}$ is a monotonic sequence.
            \end{definition}
            \begin{example}
                If $x_{n}:\mathbb{N}\rightarrow\mathbb{N}$ is
                the sequence defined by $x_{n}=n$, and if
                $k_{n}=2n$, then $x_{k_{n}}=2n$. This is the
                subsequence of all even numbers.
                If $k_{n}=2n-1$, then $x_{k_{n}}=2n-1$. This
                is the subsequence of all odd numbers. As a
                boring example, let $k_{n}=n$. Then
                $x_{k_{n}}=n$. This is the identity subsequence.
            \end{example}
            There is an important theorem about
            subsequences of bounded sequences called the
            Bolzano-Weierstrass theorem. It states that
            every bounded sequence has a convergent subsequence,
            and is an equivalent definition of the
            completeness of $\mathbb{R}$.
            \begin{theorem}
                \label{th:funct:bounded_monotone_%
                       sequences_converge}
                If $x:\mathbb{N}\rightarrow\mathbb{R}$
                is a bounded monotonic sequence,
                then $x$ converges.
            \end{theorem}
            \begin{proof}
                Let $x$ be a bounded monotonic sequence that
                is increasing in $\mathbb{R}$.
                If $x$ is decreasing, we replace the least
                upper bound with the greatest lower
                bound and the proof is symmetric.
                Then $S=\{x_{n}:n\in\mathbb{N}\}$ is a
                non-empty subset of $\mathbb{R}$. But $x$ is
                a bounded sequence, and therefore $S$ is a
                bounded subset of $\mathbb{R}$. By the least
                upper bound property there exists a least
                upper bound $s\in\mathbb{R}$ of $S$.
                We now show that $x_{n}\rightarrow{s}$.
                Let $\varepsilon>0$ be given. Since $s$ is
                the least upper bound, $s-\varepsilon$
                is not an upper bound of $S$, since
                $s-\varepsilon<s$. Therefore there exists
                an $N\in\mathbb{N}$ such that
                $s-\varepsilon<x_{N}$. But $x$ is
                monotonically increasing, and therefore
                for all $n>N$, $x_{N}\leq{x_{n}}$.
                But, as $s$ is a least upper
                bound of $S$, $x_{n}\leq{s}$. But then,
                for all $n>N$,
                $0\leq{s-x_{n}}\leq{s-x_{N}}<\varepsilon$.
                Therefore, $x_{n}\rightarrow{s}$.
            \end{proof}
            The least upper bound is, in a sense, the
            reason why decimal expansions of
            real numbers work. For example, let $x$ be the
            sequence 3, 3.1, 3.14, 3.141, 3.1415, 3.14159, ...
            This sequence, which is the decimal
            expansion of $\pi$,
            is bounded by $4$. Therefore it has a least
            upper bound. We define the number $\pi$
            to be the least upper bound of this sequence.
            Completeness is a very important property
            but so far it relies on the ordering
            of the real numbers.
            We want to find an equivalent definition
            of completeness that does not rely on ordering
            so that we may speak of complete spaces,
            or sets, which have no notion of
            order. We start with a different definition
            for the completeness of $\mathbb{R}$.
            \begin{definition}
                A Cauchy sequence in a subset
                $S\subseteq\mathbb{R}$ is a
                sequence $x:\mathbb{N}\rightarrow{S}$
                such that for all $\varepsilon>0$ there
                is an $N\in\mathbb{N}$ such that for all
                $n,m>N$, $|x_{n}-x_{m}|<\varepsilon$.
                That is:
                \begin{equation}
                    \label{thm:Func_Def_Cauchy_Sequence}
                    \forall_{\varepsilon>0}
                    \exists_{N\in\mathbb{N}}:
                    n,m>N\Rightarrow
                    |x_{n}-x_{m}|<\varepsilon
                \end{equation}
            \end{definition}
            \begin{theorem}
                \label{FUNCTIONAL_ANALYSIS:CONVERGENT_%
                       SEEQUENCES_ARE_CAUCHY_SEQUENCES}
                If $S\subseteq\mathbb{R}$ and if
                $x:\mathbb{N}\rightarrow{S}$
                is a convergent sequence, then it
                is a Cauchy sequence.
            \end{theorem}
            \begin{proof}
                For let $x$ be a convergent sequence and
                let $a$ be it's limit.
                Let $\varepsilon>0$ be given. Then, as
                $x_{n}\rightarrow{a}$, there is an
                $N\in\mathbb{N}$ such that for all $n>N$,
                $|x_{n}-a|<\varepsilon/2$.
                But by the triangle inequality,
                for all $n,m>N$:
                \begin{equation*}
                    |x_{n}-x_{m}|\leq
                    |x_{n}-a|+|x_{m}-a|<
                    \frac{\varepsilon}{2}+
                    \frac{\varepsilon}{2}
                    =\varepsilon
                \end{equation*}
                Therefore, $x$ is a Cauchy sequence.
            \end{proof}
            The converse of
            Thm.~\ref{FUNCTIONAL_ANALYSIS:CONVERGENT_%
                      SEEQUENCES_ARE_CAUCHY_SEQUENCES}
            turns out to be a more general notion
            of completeness. That is, we can apply
            this to spaces that do not have
            a notion of order, but do have a notion
            of completeness. There are Cauchy sequences
            $x:\mathbb{N}\rightarrow\mathbb{Q}$ that do
            not converge. This is again related to the fact
            that $\mathbb{Q}$ is not complete. For sequences
            $x:\mathbb{N}\rightarrow\mathbb{R}$,
            if $x$ is Cauchy then it must converge.
            \begin{theorem}
                If $S\subseteq\mathbb{R}$ and if
                $x$ is a Cauchy sequence in $S$,
                then $x$ is bounded.
            \end{theorem}
            \begin{proof}
                For as $x$ is a Cauchy sequence there is an
                $N\in\mathbb{N}$ such that, for all $n,m>N$,
                $|x_{n}-x_{m}|<1$. Then, for all $n>N+1$,
                $x_{N+1}-1<x_{n}<x_{N+1}+1$. Let
                $M=\max\{|x_{0}|,|x_{1}|,\hdots,|x_{N+1}|+1\}$.
                Then for all $n\in\mathbb{N}$,
                $|x_{n}|\leq{M}$.
            \end{proof}
            We cannot replace the requirement that,
            for all $n,m>N$, $|x_{n}-x_{m}|<\varepsilon$
            with $n,n+k$ for some fixed $k\in\mathbb{N}$.
            There are sequences such that
            $x_{n+1}-x_{n}\rightarrow{0}$,
            yet $x$ is not Cauchy.
            \begin{example}
                \begin{subequations}
                    There are unbounded sequences $x$ such that
                    $x_{n+1}-x_{n}\rightarrow{0}$. For let
                    $x:\mathbb{N}\rightarrow\mathbb{R}$
                    be the sequence defined
                    by $x_{n}=\sqrt{n}$. Then:
                    \begin{equation}
                        |x_{n+1}-x_{n}|=|\sqrt{n+1}-\sqrt{n}|
                        =\frac{1}{\sqrt{n+1}+\sqrt{n}}
                        <\frac{1}{2\sqrt{n}}
                        \rightarrow{0}
                    \end{equation}
                    But $\sqrt{n}\rightarrow\infty$.
                    Moreover, there are bounded sequences $x$
                    such that $x_{n+1}-x_{n}\rightarrow{0}$,
                    yet $x$ is not Cauchy. For let
                    $x:\mathbb{N}\rightarrow\mathbb{R}$
                    be the sequence defined by
                    $x_{n}=\sin(\sqrt{n})$.
                    Then $x$ is bounded, and:
                    \begin{align}
                        x_{n+1}-x_{n}
                        &=\sin\big(
                            \sqrt{n+1})-\sin(\sqrt{n}\big)\\
                        &=2\sin\Big(
                            \frac{\sqrt{n+1}-\sqrt{n}}{2}\Big)
                        \cos\Big(\frac{\sqrt{n+1}-\sqrt{n}}{2}\Big)
                    \end{align}
                    But we saw from the previous example that
                    $\sqrt{n+1}-\sqrt{n}\rightarrow{0}$, and
                    therefore $x_{n+1}-x_{n}\rightarrow{0}$.
                    $x$ is not Cauchy, however, for let
                    $k:\mathbb{N}\rightarrow\mathbb{N}$ be
                    the subsequence defined by $k_{n}=n^{2}$. Then:
                    \begin{equation}
                        x_{k_{n+1}}-x_{k_{n}}
                        =\sin(n+1)-\sin(n)
                        =2\sin\Big(\frac{1}{2}\Big)
                        \cos\Big(\frac{2n+1}{2}\Big)
                    \end{equation}
                    And this does not converge to zero.
                    Therefore, $x$ is not Cauchy.
                \end{subequations}
            \end{example}
            \begin{theorem}
                \label{th:funct:sequences_have_%
                       monotonic_subsequence}
                Every sequence in $\mathbb{R}$
                has a monotonic subsequence.
            \end{theorem}
            \begin{proof}
                Let $x$ be a sequence in $\mathbb{R}$.
                Call $n$ a ``peak point'' if
                $x_{n}\geq{x_{m}}$ for all
                ${m}\geq{n}$. If there are infinitely many
                of these peak points, then we have obtained
                a decreasing sequence, since the $n^{th}$
                peak point will be greater than or equal to
                the $(n+1)^{th}$ peak point.
                We have thus obtained
                a monotonically decreasing subsequence.
                If there are finitely many,
                there are either zero or there is a last one,
                $x_{n_{0}}$. Then $x_{n_{0}+1}$ is not a
                peak point. But then there is a
                $k\in\mathbb{N}$ such that $k>n_{0}+1$ and
                $x_{k}\geq{x_{n_{0}+1}}$, for otherwise
                $x_{n_{0}+1}$ would be a peak point. But
                $x_{k}$ is also not a peak point, and so
                there is a $k_{1}$ such that $k_{1}>k$ and
                $x_{k_{1}}\geq{x_{k}}$. This pattern
                continues, and we thus have a monotonically
                increasing subsequence. If there are zero
                peak points, repeat the argument above
                argument with $x_{n_{0}}=x_{1}$.
            \end{proof}
            There's probably some axiom of choice stuff
            going on here.
            \begin{theorem}[Bolzano-Weierstrass Theorem]
                    If $x:\mathbb{N}\rightarrow\mathbb{R}$
                    is a bounded sequence, then there is
                    a convergent subsequence
                    $k:\mathbb{N}\rightarrow\mathbb{N}$
                    of $x$.
                \end{theorem}
            \begin{proof}
                By Thm.~\ref{th:funct:sequences_%
                             have_monotonic_subsequence},
                if $x:\mathbb{N}\rightarrow\mathbb{R}$ is a
                sequence, then there is a monotonic subsequence
                $k:\mathbb{N}\rightarrow\mathbb{N}$.
                But by Thm.~\ref{th:funct:bounded_%
                                 monotone_sequences_converge},
                bounded monotonic sequences converge.
                Thus $x\circ{k}$ converges.
                Therefore $k$ is a convergent
                subsequence of $x$.
            \end{proof}
            This notion is so important it has a name.
            A sequentially compact space is a space such that
            every sequence has a convergent subsequence. The
            Bolzano-Weierstrass Theorem is equivalent
            to saying that every closed and bounded subset
            of $\mathbb{R}$ is sequentially
            compact. The general notion of \textit{compactness}
            is a topological one, but as it turns out
            sequential compactness and compactness are
            identical concepts in a \textit{metric space}.
            Metric spaces will be one of the primary
            subjects of study in functional analysis.
            In $\mathbb{R}^{n}$ there is another equivalent,
            and perhaps more intuitive,
            definition of compactness. A subset of
            $\mathbb{R}^{n}$ is compact if and only if it
            is closed and bounded. A set
            $S\subseteq\mathbb{R}$ is closed if for
            all convergent sequences
            $x:\mathbb{N}\rightarrow{S}$,
            the limit also lies in $S$.
            Compactness will be discussed later in the
            context of continuous functions on a compact set.
            \begin{example}
                \begin{subequations}
                    Find a subsequence $k$ of the identity
                    $x:\mathbb{N}\rightarrow\mathbb{R}$
                    defined by $x_{n}=n$ for which
                    both $\sin(x\circ{k})$ and $\cos(x\circ{k})$
                    converge. First note that for any subsequence
                    $k$, $(x\circ{k})(n)=k_{n}$.
                    In degrees this is simple:
                    \begin{equation}
                        k_{n}=360+45n
                        \Rightarrow
                        \sin(k_{n})=\cos(k_{n})=\frac{1}{\sqrt{2}}
                    \end{equation}
                    In radians we need to be a little more careful.
                    Let $y:\mathbb{N}\rightarrow\mathbb{R}$
                    be defined by $y_{n}=\sin(n)$.
                    Then $y$ is bounded and
                    by the Bolzano-Weierstrass theorem,
                    there is a convergent subsequence $k$.
                    Let $z:\mathbb{N}\rightarrow\mathbb{R}$
                    be defined by $z_{n}=\cos(k_{n})$. Then $z$
                    is bounded and by the
                    Bolzano-Weierstrass theorem there is a
                    convergent subsequence $j$. Let $k_{j}$
                    denote the subsequence $k\circ{j}$. But
                    any subsequequence of a convergent sequence
                    converges to the same limit, and therefore
                    $\sin(k_{j})$ is a convergent sequence. Thus,
                    $\sin(k_{j})$ and $\cos(k_{j})$ are
                    convergent sequences. It's also
                    possible to make them converge to the same
                    limit. We need to know that
                    $\{n\mod\alpha:n\in\mathbb{N}\}$ is dense
                    in $(0,\alpha)$ when $\alpha$ is irrational.
                    Thus there is a subsequence such that
                    $k_{n}\mod2\pi\rightarrow\pi/4$.
                    Then $\sin(k_{n})$ and $\cos(k_{n})$
                    both converge to $1/\sqrt{2}$.
                    Let's first try to find a subsequence such that
                    $\cos(k_{n})\rightarrow{1}$. If we can
                    do that, we simply need to modify the
                    argument so that
                    $\cos(k_{n})\rightarrow{1}/\sqrt{2}$.
                    Let $k$ be a sequence of integers
                    such that $0<n-2\pi{k_{n}}<2\pi$.
                    Let $\varepsilon>0$ and let $N\in\mathbb{N}$
                    be such that $N>\frac{2\pi}{\varepsilon}$.
                    Now consider the set:
                    \begin{equation}
                        A_{N}=\{n-2\pi{k_{n}}:n=1,2,\hdots,N+1\}
                    \end{equation}
                    Then $A_{N}$ has $N+1$ elements and by the
                    pidgeon-hole principle there are
                    elements that are within
                    $2\pi/\frac{2\pi}{\varepsilon}$ of each other.
                    Let $n_{1}$ and $n_{2}$ be such numbers.
                    Then:
                    \begin{align}
                        \cos(n_{2}-n_{1})
                        &=\cos(n_{2}-n_{1}-2\pi(k_{2}-k_{1}))\\
                        &=\cos((n_{2}-2\pi{k}_{2})
                               -(n_{1}-2\pi{k_{1}}))\\
                        &=\cos(\xi)
                    \end{align}
                    Where $\xi$ is a number such that
                    $0<|\xi|<\varepsilon$. But then
                    $|1-\cos(\xi)|<\frac{\varepsilon^{2}}{2}$.
                    And $n_{2}-n_{1}$ is a natural number,
                    so we can find a subsequence $k$ such
                    that $\cos(k_{n})\rightarrow{1}$. Modifying
                    this with $\pi/4$
                    and $1/\sqrt{2}$ gives the result.
                \end{subequations}
            \end{example}
            \begin{theorem}
                If $x:\mathbb{N}\rightarrow\mathbb{R}$ is
                a Cauchy sequence, then it converges.
            \end{theorem}
            \begin{proof}
                If $x$ is Cauchy, then it is bounded.
                By the Bolzano-Weiestrass theorem there
                is a convergent subsequence $k$. But then there
                is an $a\in\mathbb{R}$ such that
                $x_{k_{n}}\rightarrow{a}$. We now must show that
                $x_{n}\rightarrow{a}$. Let $\varepsilon>0$
                be given. As $x_{k_{n}}\rightarrow{a}$,
                there is an $N_{1}\in\mathbb{N}$ such
                that for all $n>N_{1}$,
                $|x_{k_{n}}-a|<\frac{\varepsilon}{2}$.
                But as $x$ is a Cauchy sequence, there
                is an $N_{2}$ such that for all $n,m>N_{2}$, 
                $|x_{n}-x_{m}|<\frac{\varepsilon}{2}$. Let
                $N=\max\{N_{1},N_{2}\}$. 
                But $k$ is a subsequence, and thus for all
                $n>N$, $k_{n}>N$. But then if $n>N$,
                $|x_{k_{n}}-x_{n}|<\frac{\varepsilon}{2}$.
                By the triangle inequality,
                    $|a-x_{n}|\leq
                     |a-x_{k_{n}}|+|x_{k_{n}}-x_{n}|\leq
                     \frac{\varepsilon}{2}+
                     \frac{\varepsilon}{2}%
                     =\varepsilon$.
            \end{proof}
            Real numbers can be constructed by considering
            \textit{equivalence classes} of Cauchy sequences of
            rational numbers. Two Cauchy sequences $x_{n}$ and
            $y_{n}$ are equivalent if $x_{n}-y_{n}\rightarrow{0}$.
            By considering the set
            of all such equivalent sequences, we can give a more
            rigorous construction of the real numbers.
        \subsection{Continuity}
            \begin{definition}
                A function $f:S\rightarrow\mathbb{R}$
                on a subset $S\subseteq\mathbb{R}$ continuous
                at a point $x\in{S}$ is a function such that
                for all $\varepsilon>0$ there is a $\delta>0$
                such that for all $x_{0}\in{S}$,
                $|x-x_{0}|<\delta$ implies
                $|f(x)-f(x_{0})|<\varepsilon$. That is:
                \begin{equation}
                    \forall_{\varepsilon>0}\exists_{\delta>0}:
                    x\in{S},|x-x_{0}|<\delta
                    \Rightarrow|f(x)-f(x_{0})|<\varepsilon
                \end{equation}
            \end{definition}
            \begin{theorem}
                If $S\subseteq\mathbb{R}$, $x\in{S}$,
                $f:S\rightarrow\mathbb{R}$
                is continuous at $x$, and if
                $a:\mathbb{N}\rightarrow{S}$
                is a convergent sequence such that
                $a_{n}\rightarrow{x}$, then
                $f(a_{n})\rightarrow{f(x)}$.
            \end{theorem}
            \begin{proof}
                For let $\varepsilon>0$. As $f$ is
                continuous there is a $\delta>0$ such that,
                for all $x_{0}\in{S}$
                such that $|x-x_{0}|<\delta$,
                $|f(x)-f(x_{0})|<\varepsilon$.
                But $a_{n}\rightarrow{x}$, and thus there is an
                $N\in\mathbb{N}$ such that, for all $n>N$,
                $|x-a_{n}|<\delta$. But then, for all $n>N$,
                $|f(x)-f(a_{n})|<\varepsilon$. Therefore,
                $f(a_{n})\rightarrow{f(x)}$.
            \end{proof}
            The converse of this theorem is true, giving us
            an equivalent definition of continuity.
            \begin{theorem}
                If $S\subseteq\mathbb{R}$, $x\in{S}$ and
                $f:S\rightarrow\mathbb{R}$
                is a function such that for all sequences
                $a:\mathbb{N}\rightarrow\mathbb{R}$ such that
                $a_{n}\rightarrow{x}$,
                $f(a_{n})\rightarrow{f(x)}$,
                then $f$ is continuous at $x$.
            \end{theorem}
            \begin{proof}
                For suppose not. Then there is an
                $\varepsilon>0$ such that, for all $\delta>0$,
                there is an $x_{0}\in{S}$ such that
                $|x-x_{0}|<\delta$ and
                $|f(x)-f(x_{0})|\geq\varepsilon$.
                Let $a:\mathbb{N}\rightarrow\mathbb{R}$
                be a sequence such that, for all
                $n\in\mathbb{N}$, $|a_{n}-x|<1/n$, but
                $|f(x)-f(a_{n})|\geq\varepsilon$.
                But then $a_{n}\rightarrow{x}$. But for all
                sequences $a$ such that $a_{n}\rightarrow{x}$,
                $f(a_{n})\rightarrow{f(x)}$. But, for all $n$,
                $|f(x)-f(a_{n})|\geq\varepsilon$,
                a contradiction.
                Therefore, $f$ is continuous at $x$.
            \end{proof}
            Continuity gives some intuitive results. If a function $f$
            is positive in some intervals $(a,x)$, and continuous at $x$,
            then $f(x)\geq{0}$.
            \begin{theorem}
                If $x\in\mathbb{R}$ and
                $a:\mathbb{N}\rightarrow\mathbb{R}$
                is a convergent sequence such that
                $a_{n}\rightarrow{x}$ and for all
                $n\in\mathbb{N}$, $a_{n}\geq{0}$,
                then $x\geq{0}$.
            \end{theorem}
            \begin{proof}
                For suppose not. Suppose $x<0$. Let
                $\varepsilon=|x|/2$. Then, as $\varepsilon>0$, there
                is an $N\in\mathbb{N}$ such that for all $n>N$,
                $|x-a_{n}|<\varepsilon$. But then
                $a_{N+1}<x+\varepsilon=x/2<0$, a contradiction as
                $a_{N+1}\geq{0}$.
            \end{proof}
            \begin{theorem}
                \label{thm:Funct:Continuous_Limit_%
                       of_Pos_Sequ_is_nonneg}
                If $S\subseteq\mathbb{R}$, $x\in{S}$,
                $f:S\rightarrow\mathbb{R}$ is continuous at $x$, and if
                $a:\mathbb{N}\rightarrow\mathbb{R}$ is a sequence such
                that  $a_{n}\rightarrow{x}$ and $f(a_{n})>0$
                for all $n\in\mathbb{N}$, then $f(x)\geq{0}$.
            \end{theorem}
            \begin{proof}
                For suppose not. Let $r=f(x)<0$, and let
                $\varepsilon=|r|/2$. Then $\varepsilon>0$. But
                from continuity, there is a $\delta>0$ such that
                for all $x_{0}\in{S}$ such that $|x-x_{0}|<\delta$,
                $|f(x)-f(x_{0})|<\varepsilon$. But
                $a_{n}\rightarrow{x}$, and thus there is an
                $N\in\mathbb{N}$ such that for all $n>N$,
                $|x-a_{n}|<\delta$. Thus
                $|f(x)-f(a_{N+1})|<\varepsilon$. But then
                $f(a_{n})<f(x)+\varepsilon=f(x)/2<0$,
                a contradiction as $f(a_{N+1})>0$. Therefore, etc.
            \end{proof}
            \begin{theorem}
                If $x\in\mathbb{R}$,
                $f:\mathbb{R}\rightarrow\mathbb{R}$ is
                continuous at $x$, and if $f(x)>0$,
                then there is an open interval
                $\mathcal{U}$ such that $x\in\mathcal{U}$,
                and for all $y\in\mathcal{U}$, $f(y)>0$.
            \end{theorem}
            \begin{proof}
                For let $\varepsilon=f(x)/2$. Then
                $\varepsilon>0$, and thus there is a $\delta>0$
                such that for all $x_{0}\in\mathbb{R}$
                such that $|x-x_{0}|<\delta$,
                $|f(x)-f(x_{0})|<\varepsilon$. Let
                $\mathcal{U}=(x-\delta,x+\delta)$.
                Then $\mathcal{U}$ is an open intervals and if
                $y\in\mathcal{U}$, then $|x-y|<\delta$, and therefore:
                \begin{equation*}
                    |f(y)-f(x)|<\varepsilon
                    \Rightarrow
                    f(y)>f(x)-\varepsilon
                    =\frac{f(x)}{2}>0
                \end{equation*}
                Thus, for all $y\in\mathcal{U}$, $f(y)>0$.
            \end{proof}
            \begin{definition}
                A continuous function on
                $S\subseteq\mathbb{R}$ is a function
                $f:S\rightarrow\mathbb{R}$ such that
                $f$ is continuous at all $x\in{S}$. That is:
                \begin{equation}
                    \forall_{x\in{S}}\forall_{\varepsilon>0}
                    \exists_{\delta>0}:x_{0}\in{S},
                    |x-x_{0}|<\delta
                    \Rightarrow|f(x)-f(x_{0})|<\varepsilon
                \end{equation}
            \end{definition}
            This definition comes from the fact that
            continuity is a point-wise property, and not a
            ``curve'' property. Continuous functions are
            functions that have point-wise continuity at
            every point. The statement ``A continuous function
            is a curve that you can draw,'' which many have
            heard in calculus, is slightly misleading. There
            are functions that are continuous at one point and
            nowhere else. There are functions that are
            continuous on the irrationals and discontinuous
            on the rationals. For example, if $x$ is
            rational write it as $x=p/q$ where $p$ and
            $q$ are integers and relatively prime. Define $f$
            as follows:
            \begin{equation}
                f(x)=
                \begin{cases}
                    \frac{1}{q},&x\in\mathbb{Q}\\
                    0,&x\notin\mathbb{Q}
                \end{cases}
            \end{equation}
            This function, which is known as
            Dirichlet's Function, but also as the Popcorn
            Function, or Thomae' Function, is continuous at every
            irrational number and discontinuous at every
            rational number.
            \begin{figure}[H]
                \captionsetup{type=figure}
                \centering
                \begin{tikzpicture}[scale=8]
                    \draw[-stealth] (-0.1,0) -- (1.1,0)
                        node[above left] {$x$};
                    \draw[-stealth] (0,-0.1) -- (0,0.6)
                        node[right] {$f(x)$};
                    \draw (0.02,1/2) -- (-0.02,1/2)
                        node[left]{$\frac{1}{2}$};
                    \draw (0.02,1/3) -- (-0.02,1/3)
                        node[left]{$\frac{1}{3}$};
                    \draw (0.02,1/4) -- (-0.02,1/4)
                        node[left]{$\frac{1}{4}$};
                    \foreach\X[%
                        evaluate=\X as \Ymax using {int(\X-1)}]
                        in {25,24,...,2}{%
                            \foreach\Y in {1,...,\Ymax}{%
                                \ifnum\X<5
                                    \draw
                                    (\Y/\X,0.02) -- (\Y/\X,-0.02)
                                    node[below,fill=white]
                                        {$\frac{\Y}{\X}$};
                                \else
                                    \draw[ultra thin]
                                    (\Y/\X,0.01) to (\Y/\X,-0.01);
                                \fi
                                \pgfmathtruncatemacro{\TST}
                                    {gcd(\X,\Y)}
                                \ifnum\TST=1
                                    \fill ({\Y/\X},1/\X) 
                                        circle (0.2pt); 
                                \fi
                            }
                    }
                    \foreach\X in {0,1,...,80}
                    {\fill (\X/80,0) circle(0.2pt);}
                \end{tikzpicture}
                \caption{Dirichlet's Function is Continuous on
                         $\mathbb{R}\setminus\mathbb{Q}$ and
                         Discontinuous on $\mathbb{Q}$.}
                \label{fig:Funct:Dirichlet_Thomae_Function}
            \end{figure}
            There is no ``reverse,'' of this
            function. That is, there is no function which is
            continuous on $\mathbb{Q}$ and discontinuous at
            every irrational number. Uniform continuity is a
            property of all points in the domain of a function.
            Point-wise continuity says that given a point $x$
            and a positive number
            $\varepsilon$, one can find a $\delta$ satisfying a
            certain property. The key part is that the point $x$ must
            be specified first. That is, the $\delta$
            may be dependent on $x$.
            Uniform continuity occurs when a $\delta>0$ can be
            chosen regardless of $x$. $\delta$ is
            only dependent on $\varepsilon$.
            \begin{definition}
                A uniformly continuous function on a subset
                $S\subseteq\mathbb{R}$
                is a function $f:S\rightarrow\mathbb{R}$ such that:
                \begin{equation*}
                    \forall_{\varepsilon>0}\exists_{\delta>0}
                    \forall_{x\in{S}}:\forall_{x_{0}\in{S}},
                    |x-x_{0}|<\delta
                    \Rightarrow|f(x)-f(x_{0})|<\varepsilon    
                \end{equation*}
            \end{definition}
            Continuity is a point-wise property. There are
            functions that are continuous at one point
            and nowhere else. Uniform continuity, however,
            is a set property. You can't have uniform
            continuity at a single point,
            but rather on a set of points. Unless, of course,
            your domain $S$ is a single point. But that's rather boring.
            \begin{theorem}
                \label{thm:Funct:equiv_def_of_uni_cont}
                A function $f:S\rightarrow\mathbb{R}$
                is uniformly continuous if and only if
                for all sequences $x,y:\mathbb{N}\rightarrow\mathbb{R}$
                such that $x_{n}-y_{n}\rightarrow{0}$,
                $f(x_{n})-f(y_{n})\rightarrow{0}$.
            \end{theorem}
            \begin{proof}
                Let $\varepsilon>0$. If $f$ is uniformly continuous,
                then there is a $\delta>0$ such that for all
                $x$, $x_{0}\in{S}$ such that $|x-x_{0}|<\delta$,
                we have that $|f(x)-f(x_{0})|<\varepsilon$. But if
                $x_{n}-y_{n}\rightarrow{0}$, then there is an
                $N\in\mathbb{N}$ such that for all $n>N$,
                $|x_{n}-y_{n}|<\delta$. But then, for all $n>N$,
                $|f(x_{n})-f(y_{n})|<\varepsilon$. Therefore,
                $f(x_{n})-f(y_{n})\rightarrow{0}$. Proving the
                converse, suppose not. If $f$ is not uniformly
                continuous, then there exists $\varepsilon>0$
                such that for all $\delta>0$ there exists
                $x$, $x_{0}\in{S}$ such that
                $|x-x_{0}|<\delta$ and yet
                $|f(x)-f(x_{0})|\geq{\varepsilon}$. Let
                $x_{n}$ and $y_{n}$ be points such that
                $|x_{n}-y_{n}|<\frac{1}{n}$ and yet
                $|f(x_{n})-f(y_{n})|\geq\varepsilon$. Then
                $x_{n}-y_{n}\rightarrow{0}$. But if
                $x_{n}-y_{n}\rightarrow{0}$, then
                $f(x_{n})-f(y_{n})\rightarrow{0}$. But for all
                $n$, $|f(x_{n})-f(y_{n})|\geq{\varepsilon}$,
                a contradiction.
            \end{proof}
            The requirement of uniform continuity is crucial.
            Let $f:(0,1)\rightarrow\mathbb{R}$ be defined by
            $f(x)=x^{-1}$. Then $f$ is continuous, but
            not uniformly continuous. Let $x_{n}=n^{-1}$
            and $y_{n}=2n^{-1}$. Then
            $|y_{n}-x_{n}|=n^{-1}\rightarrow{0}$, but
            $|f(y_{n})-f(x_{n})|=n/2$, which diverges.
            Point-wise continuity says
            $f(x_{n})-f(x)\rightarrow{0}$, whereas
            uniform continuity allows the target to
            vary as well. Point-wise continuity can
            not guarantee this.
            \begin{theorem}
                If $f:[a,b]\rightarrow\mathbb{R}$ is continuous,
                then $f$ is uniformly continuous.
            \end{theorem}
            \begin{proof}
                Let $x,y:\mathbb{N}\rightarrow[a,b]$ be sequences such
                that $x_{n}-y_{n}\rightarrow{0}$. By the
                Bolzano-Weierstrass theorem there is a
                convergent subsequence
                $k:\mathbb{N}\rightarrow\mathbb{N}$ of $x$. Let $\alpha$
                be the limit. But for all $n\in\mathbb{N}$:
                \begin{equation*}
                    y_{k_{n}}=x_{k_{n}}-(y_{k_{n}}-x_{k_{n}})
                    \Rightarrow
                    y_{k_{n}}\rightarrow\alpha
                \end{equation*}
                Let $X,Y:\mathbb{N}\rightarrow[a,b]$ be sequences
                defined by $X_{n}=x_{k_{n}}$ and
                $Y_{n}=y_{k_{n}}$, respectively. Then we have:
                \begin{equation*}
                    f(X_{n})-f(Y_{n})
                    =(f(X_{n})-f(\alpha))-(f(Y_{n}-f(\alpha))
                \end{equation*}
                From continuity, $f(X_{n})\rightarrow{f(\alpha)}$
                and $f(Y_{n})\rightarrow{f(\alpha)}$, and thus
                $f(X_{n})-f(Y_{n})\rightarrow{0}$. Then by
                Thm.~\ref{thm:Funct:equiv_def_of_uni_cont}
                $f$ is uniformly continuous. Therefore, etc.
            \end{proof}
            The above theorem relies on the fact that
            $[a,b]$ is closed and bounded. Indeed, this is
            the only thing it relies on, the fact that it's
            an interval (Or connected) is unnecessary. We can
            write a more general result.
            \begin{theorem}
                If $f:S\rightarrow\mathbb{R}$ is continuous
                and $S$ is compact, then $f$ is
                uniformly continuous.
            \end{theorem}
            \begin{theorem}[Intermediate Value Theorem]
                If $f:[a,b]\rightarrow\mathbb{R}$
                is continuous and
                $f(a)<f(b)$, then for all $z\in\mathbb{R}$ such that
                $f(a)<z<f(b)$,
                there is a $c\in(a,b)$ such that $f(c)=z$.
            \end{theorem}
            \begin{proof}
                Let $x_{1}=\frac{a+b}{2}$. By trichotomy,
                which is one of the ordering properties, either
                $f(x_{1})=z$, $f(x_{1})<z$, or $f(x_{1})>z$. If
                $f(x_{1})=z$, we are done. If not, suppose
                $f(x_{1})<z$. The proof is symmetric for
                $f(x_{1})>z$. Let $x_{2}=\frac{x_{1}+b}{2}$.
                We continue checking whether $f(x_{2})=z$,
                and continue dividing the region in two.
                If $f(x_{2})<z$,  we set
                $x_{3}=\frac{x_{1}+x_{2}}{2}$, and if
                $f(x_{2})>z$ we set $x_{3}=\frac{x_{2}+b}{2}$.
                Note that $|x_{n+1}-x_{n}|=\frac{b-a}{2^{n+1}}$.
                Moreover, the sequence $x$ converges for it is
                Cauchy. Suppose it converges to $c$. Then $c\in[a,b]$.
                That is, the limit of
                $x:\mathbb{N}\rightarrow[a,b]$ is
                contained in $[a,b]$. This is related to the
                ``compactness'' of $[a,b]$. Moreover, it is
                related to the ``closedness'' of $[a,b]$.
                If there is an $N\in\mathbb{N}$ such that
                $f(x_{N})=z$, then we are
                done. Suppose not. Let $k_{n}$ be the
                elements such that $f(x_{k_{n}})<z$ and
                $\ell_{n}$ be the elements such that
                $f(x_{\ell_{n}})>z$. Both of these must be
                infinite. For suppose not.
                Suppose there is a final $N$ such that
                $f(x_{N})<z$. Then for all $n>N$, $f(x_{n})>z$.
                But from how we've defined the sequence $x_{n}$,
                we have that $x_{n}\rightarrow{x_{N}}$.
                From the continuity of $f$, there is an open
                interval about $x_{N}$ such that for all
                points $y$ inside that interval, we have
                that $f(y)<z$. A contradiction, since eventually
                some of the $x_{n}$ will be in this interval,
                and thus $f(x_{n})<z$. So, both $k_{n}$
                and $\ell_{n}$ are
                infinite. From continuity, we have
                $\lim_{n\rightarrow\infty}f(x_{k_{n}})\leq{z}$
                and
                $\lim_{n\rightarrow\infty}
                 f(x_{\ell_{n}})\geq{z}$.
                Thus, $f(c)\leq{z}$ and $f(c)\geq{z}$,
                and therefore $f(c)=z$.
            \end{proof}
            This theorem fails in $\mathbb{Q}$, for it relies
            on the completeness of $\mathbb{R}$. For example,
            $f(x)=x^{2}$ defined on $[0,4]$.
            Then $2\in[0,4]$, but there is no
            rational such that $x^{2}=2$.
            Another way to phrase this, in a more topological
            sense, is that the image of $[a,b]$, which is an
            interval, or a connected subset of $\mathbb{R}$,
            is again an interval, or a connected subset of
            $\mathbb{R}$. The proof that continuous
            functions take connected sets (Intervals)
            to connected sets (Again, intervals) is a lot
            easier than the one presented
            here, but relies on notions from topology.
            So we'll skip that.
            Another commonly used theorem in calculus
            (Again, usually not proved) is the extreme
            value theorem.
            The extreme value is used to proved Rolle's theorem,
            which says that if $f$ is differentiable on $(a,b)$
            and if $f(a)=f(b)$, then there is a point
            $c\in(a,b)$ such that $f'(c)=0$. This is used to
            prove the mean value theorem, which says that
            if $f$ is differentiable on $(a,b)$,
            then there is a point $c\in(a,b)$ such that
            $f'(c)=\frac{f(b)-f(a)}{b-a}$. This is in
            turned used to prove the Fundamental Theorem
            of Calculus. First we prove that continuous functions
            on closed and bounded sets (That is,
            compact sets) are bounded. We stick to
            closed intervals for now.
            \begin{theorem}
                If $f:[a,b]\rightarrow\mathbb{R}$ is
                continuous, then it is bounded.
            \end{theorem}
            \begin{proof}
                Suppose not. Then for all $n\in\mathbb{N}$, 
                there is an $\alpha\in[a,b]$ such that
                $f(\alpha)>n$. Invoking choice and using the
                sequence $x:\mathbb{N}\rightarrow[a,b]$ such
                that $f(x_{n})>n$, we have that
                $x$ is a bounded sequence, and thus by
                Bolzano-Weierstrass there is a convergent
                subsequence $k$ of $x$. Let $a$ be the limit of
                $x\circ{k}$.
                But then $f(x_{k_{n}})\rightarrow{f(a)}$.
                But $f(x_{k_{n}})\rightarrow\infty$,
                a contradiction. Therefore, etc.
            \end{proof}
            \begin{theorem}[Exreme Value Theorem]
                If $f:[a,b]\rightarrow\mathbb{R}$ is
                continuous, then there exists $c\in[a,b]$
                such that for all $x\in[a,b]$,
                $f(x)\leq{f(c)}$
            \end{theorem}
            \begin{proof}
                By the previous theorem,
                $\{f(x):x\in[a,b]\}$ is bounded.
                By completeness, there is a least upper
                bound. Let $s$ be such
                a bound. If $s$ is the least upper bound,
                then for all $n\in\mathbb{N}$, $s-\frac{1}{n}$
                is not the least upper bound. Thus, for
                all $n\in\mathbb{N}$ there is an $\alpha\in[a,b]$
                such that
                $s-\frac{1}{n}<f(\alpha)$. Invoking choice and
                choosing a sequence $x:\mathbb{N}\rightarrow[a,b]$ such
                that, for all $n\in\mathbb{N}$, $s-\frac{1}{n}<f(x_{n})$.
                But then $x$
                is a bounded sequence, and bounded
                sequences have a convergent subsequence.
                Let $a$ be the limit of this subsequence.
                From continuity,
                $f(a)=\lim_{n\rightarrow\infty}f(x_{k_{n}})$.
                But $s-\frac{1}{n}\leq{f(x_{k_{n}})}\leq{s}$,
                and therefore $f(x_{k_{n}})\rightarrow{s}$.
                Thus, $f(a)=s$.
            \end{proof}
            Much the way the intermediate value theorem can be
            generalized to say that the continuous image of
            connected sets is connected, the extreme value
            theorem can be generalized to say that the
            continuous image of a compact set is compact.
            The proof is rather easy, but requires
            topology. So, we'll skip that too.
            The requirement of these previous theorems on
            continuity is crucial. Without continuity, functions
            on $[a,b]$ need not be bounded and functions on
            $(a,b)$ can just ``jump,'' right over other points.
            We end with a brief discussion on sequences of
            functions.
        \subsection{Sequences of Functions}
            \begin{definition}
                A sequence of functions from a
                set $X$ to a set $Y$ is a function
                $F:\mathbb{N}\times{X}\rightarrow{Y}$.
                We often write the image of
                $(n,x)\in\mathbb{N}\times{X}$ as
                $F(n,x)=F_{n}(x)$.
            \end{definition}
            \begin{definition}
                A sequence of real-valued functions
                $F$ converges point-wise to a function
                $f$ if:
                \begin{equation*}
                    \forall_{\varepsilon>0}
                    \forall_{x\in\mathbb{R}}
                    \exists{N\in\mathbb{N}}:
                    \forall_{n>N}\Rightarrow
                    |f(x)-F_{n}(x)|<\varepsilon
                \end{equation*}
            \end{definition}
            That is, a sequence $F$ converges point-wise
            to $f$ if, for all $x\in\mathbb{R}$,
            $F_{n}(x)\rightarrow{f(x)}$.
            Uniform continuity requires that all of the
            points of the domain converge to $f(x)$ at
            the same speed. That is, given any $\varepsilon>0$
            there is an $N\in\mathbb{N}$ that works for
            all points. Point-Wise convergence does not
            have this property.
            \begin{definition}
                A sequence of real-valued functions $F$
                converges uniformly to a function $f$ if:
                \begin{equation*}
                    \forall_{\varepsilon>0}
                    \exists_{N\in\mathbb{N}}
                    \forall_{x\in{S}}:\forall_{n>N}
                    \Rightarrow
                    |f(x)-F_{n}(x)|<\varepsilon
                \end{equation*}
            \end{definition}
            That is, $f_{n}\rightarrow{f}$
            point-wise if for all $x$,
            $|f_{n}(x)-f(x)|\rightarrow{0}$ and
            $f_{n}\rightarrow{f}$ uniformly if
            $\sup\{|f_{n}(x)-f(x)|\}\rightarrow{0}$.
            \begin{example}
                Let
                $F:\mathbb{N}\times[0,1]\rightarrow\mathbb{R}$
                be defined by $F_{n}(x)=nx\exp(-nx)$.
                $F_{n}(x)\rightarrow{0}$ for all $x\in[0,1]$,
                and therefore $F$ converges point-wise to zero.
                Note that $F_{n}'(x)=(n-n^{2}x)\exp(-nx)$.
                This has a zero at $x=\frac{1}{n}$, so
                $F_{n}(x)$ has a maximum of $e^{-1}$. But then
                $\sup|F_{n}(x)-f(x)|=\sup|F_{n}(x)|=e^{-1}$.
                So $F_{n}(x)$ does not converge
                \textit{uniformly} to $0$. The
                convergence is only \textit{point-wise}.
            \end{example}
            \begin{example}
                Let $F_{n}(x)=n^{2}x\exp(-nx)$. Then
                $F_{n}(x)\rightarrow{0}$ for all
                $x\geq{0}$. But $F_{n}(x)$ has
                a maximum of $ne^{-1}$ at $x=\frac{1}{n}$.
                Thus $F_{n}(\frac{1}{n})\rightarrow\infty$.
                It is possible for a sequence
                of functions to converge point-wise to zero
                and for there to be a sequence such that
                $F_{n}(x_{n})\rightarrow\infty$. Uniform
                convergence does not allow this.
            \end{example}
            \begin{theorem}
                If $F$ is a sequence of
                real-valued continuous functions and if
                $F_{n}\rightarrow{f}$ uniformly, then
                $f$ is continuous.
            \end{theorem}
            The word ``uniformly,'' is crucial.
            This theorem is not necessarily true of
            point-wise converging functions. Let $F$ be
            defined on $[0,1]$ be
            $F_{n}(x)=x^{n}$. Then $F$ converges to
            $0$ if $x\ne{1}$, and $1$ if $x=1$. Thus,
            the limit function is discontinuous. This is
            possible because the convergence is
            point-wise and not uniform.
            \begin{theorem}
                    \label{thm:funct:Weak_Weierstrass_%
                           Approx_Theorem}
                    If $f:[0,1]\rightarrow\mathbb{R}$
                    is continuous,
                    and if $f(0)=f(1)=0$,
                    then there is a sequence of polynomials
                    $F$ such that $F_{n}\rightarrow{f}$
                    uniformly on $[0,1]$.
                \end{theorem}
            \begin{proof}
                Extend $f$ to be zero outside of $[0,1]$. Let
                $Q_{n}(x)=c_{n}(1-x^{2})^{n}$ on $[-1,1]$,
                and choose $c_{n}$ such that
                $\int_{-1}^{1}Q_{n}(x)dx=1$. So we have:
                \begin{align*}
                    c_{n}\int_{-1}^{1}(1-x^{2})^{n}dx
                    &=2c_{n}\int_{0}^{1}(1-x^{2})^{n}dx
                    &
                    &\geq{2c_{n}}\int_{0}^{1}(1-x)^{n}dx\\
                    &=2c_{n}\int_{0}^{1}(1-x)^{n}(1+x)^{n}dx
                    &
                    &=\frac{2}{n+1}c_{n}
                \end{align*}
                From this we have that $c_{n}\leq{n+1}$. Let
                $f_{n}(x)=\int_{0}^{1}f(t)Q_{n}(x-t)dx$.
                Then $f_{n}(x)$ is a polynomial. Note that
                $f(t)Q_{n}(x-t)$ is roughly zero when $t$ differs
                from $x$ and $n$ is large enough. So we have:
                \begin{equation*}
                    f_{n}(x)=\int_{0}^{1}f(t)Q_{n}(x-t)dt
                    \approx{f(x)}\int_{0}^{1}Q_{n}(x-t)dt=f(x)
                \end{equation*}
                The remainder of the proof is to quantify this.
                Since $f$ is zero outside of $[0,1]$, if
                we let $s=t-x$, then:
                \begin{align*}
                    f_{n}(x)&=\int_{-x}^{1-x}f(s+x)Q_{n}(s)ds
                    =\int_{-1}^{1}f(s+x)Q_{n}(s)ds\\
                    \Rightarrow|f_{n}(x)-f(x)|
                    &=\bigg|\int_{-1}^{1}f(x+t)Q_{n}(t)dt
                    -\int_{-1}^{1}f(x)Q_{n}(t)dt\bigg|\\
                    \Rightarrow|f_{n}(x)-f(x)|
                    &\leq\int_{-1}^{1}|f(x+t)-f(x)|Q_{n}(t)dt
                \end{align*}
                This comes for the fact
                that $\int_{-1}^{1}Q_{n}(t)=1$
                and from the integral version of the triangle
                inequaility.
                Suppose $\varepsilon>0$. Since $f$ is continuous
                on $[0,1]$, it is uniformly continuous. But
                if $f$ is uniformly continuous then there exists
                a $\delta>0$ such that
                $|f(x+t)-f(x)|<\frac{\varepsilon}{2}$ for all
                $t<\delta$. So we have:
                \begin{equation*}
                    \begin{split}
                    |f_{n}(x)-f(x)|
                    \leq
                    \int_{-1}^{-\delta}|f(x+t)-f(x)|
                    Q_{n}(t)\diff{t}
                    &+\int_{-\delta}^{\delta}
                    |f(x+t)-f(x)|Q_{n}(t)\diff{t}\\
                    &+\int_{\delta}^{1}
                    |f(x+t)-f(x)|Q_{n}(t)\diff{t}
                    \end{split}
                \end{equation*}
                But $f$ is continuous on a closed and bounded
                set and therefore $f$ is bounded. Let $M$ be
                such a bound. Then $|(f(x+t)-f(x)|\leq{2M}$.
                We have:
                \begin{equation*}
                    |f_{n}(x)-f(x)|\leq
                    2M\int_{-1}^{-\delta}Q_{n}(t)dt
                    +\frac{\varepsilon}{2}
                    \int_{-\delta}^{\delta}Q_{n}(t)dt
                    +2M\int_{\delta}^{1}Q_{n}(t)dt
                \end{equation*}
                But for all $t\in[-1,-\delta]$,
                $Q_{n}(t)\leq{Q_{n}(-\delta)}$. Similarly for
                $t$ in $[\delta,1]$. Since $Q_{n}(t)$
                is an even function:
                \begin{equation*}
                    |f_{n}(x)-f(x)|\leq
                    4MQ_{n}(\delta)+
                    \frac{\varepsilon}{2}\int_{-1}^{1}Q_{n}(t)dt
                    =4MQ_{n}(\delta)+\frac{\varepsilon}{2}
                \end{equation*}
                But since $\delta>0$, $Q_{n}(\delta)\rightarrow0$.
                Therefore, there is an $N\in\mathbb{N}$ such that
                for all $n>N$,
                $|Q_{n}(\delta)|<\frac{\varepsilon}{8M}$.
                But then $4MQ_{n}(\delta)<\frac{\varepsilon}{2}$.
                Therefore, etc.
            \end{proof}
            Another way to put this is that if $f$ is continuous
            on $[a,b]$ and if $\varepsilon>0$, then there is
            a polynomial $P$ such that for all $x\in[0,1]$,
            $|f(x)-P(x)|<\varepsilon$. There is a generalization
            of this. The set of functions need not be
            polynomials. The set needs to be closed
            under addition, multiplication, and scalar
            multiplication, it must separate points,
            and must not take every point to zero.
            This is the Stone-Weierstrass theorem.
            This shows that continuous functions on compact sets
            can be approximated arbitrarily well by polynomials.
            Furthermore, any continuous function on a compact
            set can be approximated arbitrarily well by
            polynomials with rational coefficients. To see this,
            let $f[0,1]\rightarrow\mathbb{R}$ be continuous,
            and let $\varepsilon>0$. Then there is a
            polynomial $P$ such that
            $\sup|P(x)-f(x)|<\frac{\varepsilon}{2}$.
            Suppose $P$ is of degree $n$.
            As $\mathbb{Q}$ is dense
            in $\mathbb{R}$, for each coefficient
            $c_{k}$ of $P$ there is a $d_{k}\in\mathbb{Q}$
            such that
            $|c_{k}-d_{k}|<\frac{\varepsilon}{2n}$.
            Let $Q(x)=\sum{d_{k}x^{k}}$. Then:
            \begin{equation*}
                    |P(x)-Q(x)|<
                    \sum_{k=0}^{n}|c_{k}-d_{k}||x|^{n}
                    <\frac{\varepsilon}{2}
                \end{equation*}
            Thus, by the triangle inequality:
            $\sup|Q(x)-f(x)|<\varepsilon$. A set is called
            \textit{separable} if there if it
            contains a countable dense subset. $\mathbb{R}$
            is separable since $\mathbb{Q}$ is dense in
            $\mathbb{R}$, and $\mathbb{Q}$ is countable.
            The set of all continuous functions from
            $[0,1]$ to $\mathbb{R}$, which we label as
            $C(I,\mathbb{R})$, is also separable.
            Since any continuous function can be approximated
            arbitrarily well by a polynomial with rational
            coefficients, we can say the set of polynomials
            with rational coefficients is \textit{dense} in
            $C(I,\mathbb{R})$. But the set of polynomials with
            rational coefficients is countable. For all
            $N\in\mathbb{N}$, define $P_{N}$ as:
            \begin{equation*}
                    P_{N}=\Big\{\sum_{k=0}^{N}
                    q_{k}x^{k}:q_{k}\in\mathbb{Q},
                    q_{N}\ne{0}\Big\}
                \end{equation*}
            This is the set of all rational polynomials
            of degree $N$. It is countable since there is
            a one-to-one correspondence with
            the set $\mathbb{Q}^{N}$, and $\mathbb{Q}^{N}$
            is countable for all $N\in\mathbb{N}$. But the
            set of all rational polynomials is simply the
            union over all $P_{N}$. And the countable union
            of countably many disjoint sets is countable.
            Therefore, the set of all polynomials with
            rational coefficients is countable. Thus
            $C(I,\mathbb{R})$ is \textit{separable}. We need
            to be careful when we say \textit{dense} and
            \textit{separable}, for we are implicitly speaking
            of some sort of notion of \textit{closeness} on the
            sets. This all comes from the notion of
            \textit{metrics} and \textit{metric spaces}.
            \begin{theorem}
                    \label{thm:Funct:Weierstrass_%
                           Approx_on_unit_interval}
                    If $f:[0,1]\rightarrow\mathbb{R}$ is
                    continuous, then there is a sequence
                    of polynomials $F$ such that
                    $F_{n}\rightarrow{f}$ uniformly on $[0,1]$.
                \end{theorem}
            \begin{proof}
                    If $f:[0,1]\rightarrow\mathbb{R}$ be
                    continuous, let
                    $g(x)=xf(1)+(1-x)f(0)$. Then
                    $h(x)=f(x)-g(x)$ is a continuous function
                    such that $h(0)=h(1)=0$ and thus by
                    Thm.~\ref{thm:funct:Weak_Weierstrass_%
                              Approx_Theorem}
                    there is a sequence of polynomials
                    $P_{n}(x)$ such that
                    $P_{n}(x)\rightarrow{h(x)}$
                    uniformly on $[a,b]$.
                    But $g(x)$ is a polynomial and
                    $f(x)=h(x)+g(x)$. Therefore
                    $F_{n}(x)=P_{n}(x)+g(x)$ is a sequence
                    of polynomials and
                    $F_{n}(x)\rightarrow{f(x)}$
                    uniformly on $[0,1]$.
                \end{proof}
            \begin{theorem}[Weierstrass Approximation Theorem]
                If $f:[a,b]\rightarrow\mathbb{R}$ is a
                continuous function, then there is a sequence
                of polynomials $P$ such that
                $P_{n}\rightarrow{f}$ uniformly.
            \end{theorem}
            \begin{proof}
                If $f:[a,b]\rightarrow\mathbb{R}$ is
                continuous, define
                $g:[0,1]\rightarrow\mathbb{R}$ by
                $g(x)=f(\frac{x-a}{b-a})$. Then, since
                the composition of continuous functions
                is continuous, $g$ is a continuou function
                on $[0,1]$. But by the Weierstrass
                approximation theorem there is a sequence
                of polynomials $P_{n}(x)$ such that
                $P_{n}(x)\rightarrow{g(x)}$. Let
                $F_{n}(x)=P_{n}(bx+(1-x)a)$. Then
                $F_{n}(x)$ is a sequence of polynomials
                on $[a,b]$, and $F_{n}(x)\rightarrow{f(x)}$.
            \end{proof}
            An application of this is in the uniform
            approximation of continuous periodic functions
            by Cosines.
        \begin{theorem}
            If $f\in{C[0,\pi]}$ and $\varepsilon>0$,
            then there exists
            $a_{0},\hdots,a_{n}\in\mathbb{R}$ such that, for all
            $x\in[0,\pi]$:
            \begin{equation*}
                |f(x)-\sum_{k=0}^{n}a_{k}\cos(kx)|
                <\varepsilon
            \end{equation*}
        \end{theorem}
        \begin{proof}
            $\cos(x)$ is a bijective function when considered on
            the interval $[0,\pi]$. Thus we can consider the
            function $f(\cos^{-1}(x))$. But since $\cos(x)$ is
            continuous on $[0,\pi]$, $\cos^{-1}(x)$ is
            continuous on $[-1,1]$. And the composition of
            continuous functions is continuous. So
            $f(\cos^{-1}(x))$ is continuous. By the
            Weierstrass Approximation Theorem, there is a
            sequence of polynomials $P$ such that
            $P_{n}(x)\rightarrow{f(\cos^{-1}(x))}$. But then
            $P_{n}(\cos(x))\rightarrow{f(x)}$. But as $P_{n}(x)$
            is a polynomial, it is of the form
            $\sum_{k=0}^{n}a_{k}x^{k}$. But then:
            \begin{equation*}
                P_{n}(\cos(x))=\sum_{k=0}^{n}a_{k}\cos^{k}(x)    
            \end{equation*}
            It now suffices to show that
            $\cos^{k}(x)=\sum_{m=0}^{N}c_{m}\cos(mx)$ for
            suitable $c_{m}$. We prove by induction.
            The base case is trivial. Suppose it holds
            for some $k\in\mathbb{N}$.
            Then:
            \begin{equation*}
                \cos^{k+1}(x)=\cos(x)\cos^{k}(x)
                =\cos(x)\sum_{k=0}^{N}c_{k}\cos(kx)
            \end{equation*}
            Note that
            $\cos(x)\cos(kx)%
             =\frac{1}{2}\cos((k-1)x)+\frac{1}{2}\cos((k+1)x)$.
            So we have:
            \begin{equation*}
                \cos^{k+1}(x)
                =\frac{1}{2}\sum_{k=0}^{N}c_{k}
                \bigg(
                    \cos\Big((k+1)x\Big)+\cos\Big((k-1)x\Big)
                \bigg)
            \end{equation*}
            This completes the theorem.
        \end{proof}
        \subsection{Inequalities}
            \begin{theorem}[Young's Inequality]
                If $x$, $y>0$, $p>1$, and if
                $\frac{1}{p}+\frac{1}{q}=1$, then
                $xy\leq{\frac{1}{p}x^{p}+\frac{1}{q}y^{q}}$.
            \end{theorem}
            For $p=q=2$ this is easy, for:
            \begin{equation*}
                0\leq\frac{(x-y)^{2}}{2}=
                \frac{x^{2}+y^{2}}{2}-xy
            \end{equation*}
            A cute proof of the more general theorem comes from
            considering the area under the graph of
            $x^{p-1}$.
            \begin{definition}
                H\"{o}lder Conjugates are non-zero real numbers
                $p$, $q\in\mathbb{R}$ such that
                $p^{-1}+q^{-1}=1$
            \end{definition}
            \begin{theorem}[H\"{o}lder's Inequality]
                If $a:\mathbb{N}\rightarrow\mathbb{R}$ and
                $b:\mathbb{N}\rightarrow\mathbb{R}$
                are nonnegative
                sequences, if $p$ and $q$ are
                H\"{o}lder Conjugates, then:
                \begin{equation*}
                    \sum_{n=1}^{\infty}a_{n}b_{n}
                    \leq
                    \bigg(
                        \sum_{n=1}^{\infty}a_{n}^{p}
                    \bigg)^{1/p}
                    \bigg(
                        \sum_{n=1}^{\infty}b_{n}^{q}
                    \bigg)^{1/q}
                \end{equation*}
            \end{theorem}
            \begin{theorem}[Minkowski's Inequality]
                If $a:\mathbb{N}\rightarrow\mathbb{R}$
                and $b:\mathbb{N}\rightarrow\mathbb{R}$ are
                non-negative sequences, and if $p>1$,
                then:
                \begin{equation*}
                    \bigg(
                        \sum_{n=1}^{\infty}(a_{n}+b_{n})
                    \bigg)^{1/p}
                    \leq
                    \bigg(
                        \sum_{n=1}^{\infty}a_{n}^{p}
                    \bigg)^{1/p}
                    +
                    \bigg(
                        \sum_{n=1}^{\infty}b_{n}^{p}
                    \bigg)^{1/p}
                \end{equation*}
            \end{theorem}
            When $p=q=2$ this is often
            called the Cauchy-Schwartz inequality.
            That is,
            $|\mathbf{a}\cdot\mathbf{b}|%
             \leq\norm{\mathbf{a}}\norm{\mathbf{b}}$
    \section{Metric Spaces}
        \subsection{Basic Definitions}
            Functional analysis is concerned with normed spaces.
            This is a vector space $V$ with a function, called
            a norm, from $V$ to $[0,\infty)$. This is usually
            written $\norm{\mathbf{x}}$ for an element
            $\mathbf{x}\in{V}$. This norm must satisfy the
            folllowing for all $\mathbf{x}$, $\mathbf{y}\in{V}$:
            \begin{enumerate}
                \item $\norm{\mathbf{x}}=0$ if and only
                      if $\mathbf{x}=\mathbf{0}$
                      \hfill[Definiteness]
                \item $\norm{c\mathbf{x}}=|c|\norm{\mathbf{x}}$
                      for all $c\in\mathbb{R}$.
                      \hfill[Positiveness]
                \item $\norm{\mathbf{x}+\mathbf{y}}%
                       \leq\norm{\mathbf{x}}+\norm{\mathbf{y}}$
                      \hfill[Triangle Inequality]
            \end{enumerate}
            \begin{example}
                $\mathbb{R}$ with $\norm{x}=|x|$ and
                $\mathbb{R}^{n}$ with $\norm{\mathbf{x}}_{2}$
                defined by:
                \begin{equation*}
                    \norm{\mathbf{x}}_{2}
                    =\sqrt{\sum_{k=1}^{n}x_{k}^{2}}
                \end{equation*}
                are some of the most commonly used normed spaces.
                $\mathbb{R}^{n}$ can be thought of a the set
                of vectors in $n$ dimensions and the norm
                $\norm{\mathbf{x}}_{2}$ can be thought of as
                the length of $\mathbf{x}$ using the
                Pythagorean Theorem. There are other norms one
                can define on $\mathbb{R}^{n}$. A common
                one is the $p$ norm, defined by:
                \begin{equation*}
                    \norm{\mathbf{x}}_{p}
                    =\Big(\sum_{k=1}^{n}x_{k}^{p}\Big)^{1/p}
                \end{equation*}
                Together, $(\mathbb{R}^{n},\norm{}_{p})$ defines
                a normed space for all $p\geq{1}$. Another type
                of norm on $\mathbb{R}^{n}$ is the
                $p\textrm{-adic}$ norm.
            \end{example}
            A common family of sets,
            which we will deal with frequently, are the
            $\ell^{p}$ spaces.
            \begin{definition}
                $\ell^{p}$ is the set of all sequences
                $x:\mathbb{R}\rightarrow\mathbb{R}$ such that
                the sequence of partial sums
                $S:\mathbb{N}\rightarrow\mathbb{R}$ defined by
                $S_{N}=\sum_{n=1}^{N}|x_{n}|^{p}$ is bounded.
            \end{definition}
            \begin{definition}
                The $p$ norm on $\ell^{q}$ is the function
                $\norm{}_{p}:\ell^{q}\rightarrow\mathbb{R}$
                defined by:
                \begin{equation*}
                    \norm{x}_{p}
                    =\Big(\sum_{k=1}^{\infty}x_{k}^{p}\Big)^{1/p}
                \end{equation*}
            \end{definition}
            \begin{theorem}
                If $p\geq{1}$, then $(\ell^{p},\norm{}_{p})$
                is a normed space.
            \end{theorem}
            \begin{definition}
                The supremum norm on $\mathbb{R}^{n}$,
                $\norm{}_{\infty}:%
                 \mathbb{R}^{n}\rightarrow\mathbb{R}$,
                is the function:
                \begin{equation*}
                    \norm{\mathbf{x}}_{\infty}
                    =\max\{|x_{1}|,\hdots,|x_{n}|\}
                \end{equation*}
            \end{definition}
            \begin{theorem}
                If $n\in\mathbb{N}$, then
                $(\mathbb{R}^{n},\norm{}_{\infty})$ is a
                normed space.
            \end{theorem}
            \begin{definition}
                $\ell^{\infty}$ is the set of sequences
                $x:\mathbb{N}\rightarrow\mathbb{R}$ such that
                $x$ is bounded.
            \end{definition}
            \begin{definition}
                The supremum norm on $\ell^{\infty}$
                is the function
                $\norm{}_{\infty}:%
                 \ell^{\infty}\rightarrow\mathbb{R}$
                defined by:
                \begin{equation*}
                    \norm{x}_{\infty}
                    =\sup\{|x_{n}|:n\in\mathbb{N}\}
                \end{equation*}
            \end{definition}
            \begin{theorem}
                If $\norm{}_{\infty}$ is the
                supremum norm on $\ell^{\infty}$, then 
                $(\ell^{\infty},\norm{}_{\infty})$
                is a normed space.
            \end{theorem}
            From the fact that $\ell^{\infty}$ is a normed space
            we have that the set of convergent sequences,
            again with the $\norm{}_{\infty}$ norm, is also
            a normed space. The set of null sequences, which
            is the set of sequences that converge to zero,
            is also a normed space.
            A stranger normed space
            is the set of all bounded continuous functions
            $f:S\rightarrow\infty$ with norm
            $\norm{f}=\sup\{|f(x)|\}$. Furthermore, the
            set of all integrable functions with
            bounded integrals, with norm
            $(\int_{S}|f|^{p})^{1/p}$. If you allow integral
            to mean Lebesgue Integrable, then this becomes
            a special space denoted $L^{p}(S)$.
            \begin{definition}
                The Sobolev Space, denoted $W^{n,p}([a,b])$
                is the set of functions
                $f:[a,b]\rightarrow\mathbb{R}$ such that:
                \begin{equation*}
                    \int_{a}^{b}\sum_{k=0}^{n}
                    |f^{(k)}(x)|^{p}\diff{x}<\infty
                \end{equation*}
            \end{definition}
            \begin{definition}
                The $p$ norm on the Sobolev space
                $W^{n,q}([a,b])$ is the function
                $\norm{}_{p}:W^{n,q}([a,b])\rightarrow\mathbb{R}$
                defined by:
                \begin{equation*}
                    \Big(\int_{a}^{b}\sum_{k=0}^{n}
                    |f^{(k)}(x)|^{p}\diff{x}\Big)^{1/p}
                \end{equation*}
            \end{definition}
            \begin{theorem}
                If $p\geq{1}$, then
                $(W^{n,p}([a,b]),\norm{}_{p})$ is a
                normed space.
            \end{theorem}
            A lot of the things we wish to
            prove don't rely on the fact that all of these
            spaces are vector spaces. Really, we only care about
            the properties that the norm on the space has.
            What matters is that there's a set and a notion
            of distance on the set. This abstraction is the
            fundamental concept of a metric space.
            \begin{definition}
                A metric space is a set $S$ and a function
                $d:{X}\times{X}\rightarrow[0,\infty)$ such that:
                \begin{enumerate}
                    \item For all $x$, $y\in{X}$, $d(x,y)=0$
                          if and only if $x=y$.
                          \hfill[Definitenes]
                    \item For all $x$, $y\in{X}$,
                          $d(x,y)=d(y,x)$
                          \hfill[Symmetry]
                    \item For all $x$, $y\in{X}$,
                          $d(x,z)\leq{d(x,y)+d(y,z)}$
                          \hfill[Triangile Inequality]
                \end{enumerate}
            \end{definition}
            It turns out
            that we can actually write the following:
            \begin{definition}
                A metric space is a set $X$ and a function
                $d:{X}\times{X}\rightarrow\mathbb{R}$
                such that:
                \begin{enumerate}
                    \item $d(x,y)=0$ if and only if
                          $x=y$.
                    \item $d(x,z)\leq{d(x,y)+d(z,y)}$
                \end{enumerate}
            \end{definition}
            By writing the triangle inequality in this
            way, symmetry comes for free
            (The fact that $d(x,y)=d(y,x)$), as well
            as positivity (The fact that $d(x,y)\geq{0}$).
            Since it's easier to prove two things are
            true, rather than four things, it's nice to
            take this as the definition of a metric space,
            and then prove that the two definitions are
            equivalent.
            In a metric space $(X,d)$, $d$ is often called the
            \textit{distance function} or
            \textit{metric function}. It is meant to be an
            abstract mimicry of the absolute value function
            that is used with real numbers. Definiteness
            says the only point that is zero meters from a
            point $x$ is $x$ itself. Symmetry says the distance
            walking from $x$ to $y$ is the same as the distance
            walking from $y$ to $x$. The last rule stems from
            Euclidean geometry. It says walking from $x$ to $z$
            is shorter than (or equal to) walking from
            $x$ to $y$ and then $y$ to $z$. In Euclidean
            geometry equality is achieved only when
            $y$ lies between $x$ and $z$. In
            abstract metric spaces there may be no such
            thing as a \textit{line} between two points,
            so we need to be careful.
            \begin{example}
                $\mathbb{R}^{n}$ (for $1\leq{p}<\infty$):
                \begin{equation*}
                    d_{p}(\mathbf{x},\mathbf{y})=
                    \big(
                        \sum_{k=1}^{n}|x_{k}-y_{k}|
                    \big)^{1/p}
                    =\norm{\mathbf{x}-\mathbf{y}}_{p}
                \end{equation*}
            \end{example}
            \begin{example}
                In $\ell^{p}$, which are sequences for
                which
                $\sum_{k=1}^{\infty}|x_{k}|^{p}<\infty$,
                $d_{p}(x,y)$ forms a metric, as well
                as
                $d_{\infty}(x,y)=\sup\{|x_{k}-y_{k}|\}$,
                which is called the supremum norm.
            \end{example}
            \begin{example}
                $C(S,\mathbb{R})$, which is the
                set of continuous functions from
                $S$ to $\mathbb{R}$, letting
                $L^{p}(S)$ be the set of of functions
                such that:
                \begin{equation*}
                    \int_{S}|x(t)|^{p}<\infty
                \end{equation*}
                Then the following is a metric:
                \begin{equation*}
                    d_{p}(x,y)=
                    \bigg(
                        \int_{S}|x(t)-y(t)|^{p}dt
                    \bigg)^{1/p}
                \end{equation*}
                Also,
                $d_{\infty}(x,y)=\sup\{|x(t)-y(t)|\}$,
                which is called the supremum norm.
            \end{example}
            \begin{example}
                Let $C$ be the set of sequences such that
                $x_{n}\rightarrow{0}$. Then, with
                $d_{p}$, this forms a metric space.
                If $C_{0}$ is set of sequences with
                only finitely many non-zero terms,
                then
                $C_{0}\subset{C}\subset{\ell^{\infty}}$.
                Is there a sequence $x\in{C}$ such
                that, for all $1\leq{p}<\infty$,
                $x\notin{\ell^{p}}$.
            \end{example}
            Since the image of the metric function
            lies in $\mathbb{R}$,
            we may speak of \textit{convergence}
            in metric spaces.
            \begin{definition}
                A convergent sequence in a metric space
                $(X,d)$ is a sequence
                $x:\mathbb{N}\rightarrow{X}$ such that there
                is an $a\in{X}$ such that
                $d(a,x_{n})\rightarrow{0}$.
            \end{definition}
            \begin{definition}
                A limit of a sequence
                $x$ in a metric space $(X,d)$ is an
                $a\in{X}$ such that
                $d(x_{n},a)\rightarrow{0}$.
            \end{definition}
            Much like convergence in real numbers, limits
            in metric spaces are unique.
            \begin{theorem}
                \label{thm:Funct:Limit_of_Metric_Sequence_Unique}
                If $(X,d)$ is a metric space,
                $x:\mathbb{N}\rightarrow{X}$
                is a convergence sequence in $X$,
                and if $a$ and $b$ are limits of $x$,
                then $a=b$.
            \end{theorem}
            \begin{proof}
                For suppose not. As
                $(X,d)$ is a metric space, $d(a,b)>0$.
                Let $\varepsilon=\frac{d(a,b)}{4}$.
                Then, as $d(a,x_{n})\rightarrow{0}$
                and $\varepsilon>0$, there is an
                $N_{1}\in\mathbb{N}$ such that, for all
                $n>N_{1}$, $d(a,x_{n})<\varepsilon$. But,
                as $d(b,x_{n})\rightarrow{0}$ and
                $\varepsilon>0$, there is an $N_{2}$ such
                that, for all $n>N_{2}$,
                $d(b,x_{n})<\varepsilon$.
                Let $n=\max\{N_{1},N_{2}\}+1$.
                But then:
                \begin{equation*}
                    d(a,b)\leq{d(a,x_{n})+d(b,x_{n})}
                    <2\varepsilon=\frac{d(a,b)}{2}
                \end{equation*}
                A contradiction. Therefore, $a$ is unique.
            \end{proof}
            \begin{theorem}
                If $(X,d)$ be a metric space and if
                $x$, $y$, $z\in{X}$, then
                $|d(x,z)-d(y,z)|\leq{d(x,y)}$
            \end{theorem}
            \begin{proof}
                Suppose $d(x,z)\geq{d(y,z)}$.
                If $d(x,z)<d(y,z)$, the proof is
                symmetric. Thus we have:
                \begin{equation*}
                    |d(x,z)-d(y,z)|
                    =d(x,z)-d(y,z)
                    \leq{(d(x,y)+d(y,z))-d(y,z)}
                    =d(x,y)
                \end{equation*}
                Therefore,
                $|d(x,z)-d(y,z)|\leq{d(x,y)}$.
            \end{proof}
            \begin{theorem}
                If $(X,d)$ is a metric space
                and $x_{n}\rightarrow{a}$, then
                for all $b\in{X}$,
                $d(x_{n},b)\rightarrow{d(a,b)}$.
            \end{theorem}
            \begin{proof}
                For
                $|d(x_{n},b)-d(a,b)|\leq{d(x_{n},a)}%
                 \rightarrow{0}$.
            \end{proof}
            \begin{theorem}
                If $(V,\norm{})$ is a normed space
                and if $d:{V}\times{V}\rightarrow[0,\infty)$
                is defined by
                $d(\mathbf{x},\mathbf{y})%
                 =\norm{\mathbf{x}-\mathbf{y}}$,
                then $(V,d)$ is a metric space.
            \end{theorem}
            \begin{proof}
                In order:
                \begin{enumerate}
                    \item If $\norm{\mathbf{x}-\mathbf{y}}=0$,
                          then $\mathbf{x}=\mathbf{y}$.
                          Similarly,
                          $\norm{\mathbf{x}-\mathbf{x}}%
                           =\norm{\mathbf{0}}=0$.
                    \item $d(\mathbf{x},\mathbf{y})%
                           =\norm{\mathbf{x}-\mathbf{y}}%
                           =\norm{(-1)(\mathbf{y}-\mathbf{x})}%
                           =|-1|\norm{\mathbf{y}-\mathbf{x}}%
                           =\norm{\mathbf{y}-\mathbf{y}}%
                           =d(y,x)$
                    \item The triangle inequality follows
                          from the triangle inequality that
                          norms have.
                \end{enumerate}
            \end{proof}
            There are metric spaces that have nothing to do
            with vector spaces or norms. Metric spaces are
            a more abstract object. Every normed space
            has an associated metric space since there
            is the ``induced'' metric.
            \begin{example}
                Let $X$ be a set and let
                $d(x,y)=\begin{cases}%
                            0,&x=y\\%
                            1,&{x}\ne{y}%
                        \end{cases}$
                This is the discrete metric on $X$.
            \end{example}
            \begin{example}
                Let $X=\{a,b,c\}$, and
                $d(a,b)=1$, $d(b,c)=2$. What value
                must $d(a,c)$ have if $d$ is a metric on $X$?
                Consider the following table:
                \begin{table}[H]
                    \captionsetup{type=table}
                    \centering
                    \begin{tabular}{|c|c|c|c|}
                        \hline
                        $X$&a&b&c\\
                        \hline
                        a&0&1&?\\
                        \hline
                        b&1&0&2\\
                        \hline
                        c&?&2&0\\
                        \hline
                    \end{tabular}
                \end{table}
                This obeys everything except the triangle
                inequality. We must pick $d(a,c)$
                such that this is upheld.
                So we need the following:
                \begin{align*}
                    d(a,b)&\leq{d(a,c)+d(c,b)}&
                    d(a,c)&\leq{d(a,b)+d(b,c)}&
                    d(b,c)&\leq{d(b,a)+d(a,c)}\\
                    \Rightarrow{1}&\leq{2+d(a,c)}&
                    \Rightarrow{d(a,c)}&\leq{3}&
                    \Rightarrow{2}&\leq{1+d(a,c)}
                \end{align*}
                So we need $1\leq{d(a,c)}\leq{3}$.
                Pick $d(a,c)=2$.
                This makes $(X,d)$ a metric space.
            \end{example}
            \begin{example}
                Let $X=\mathbb{R}$ and $d(x,y)=|x-y|$.
                Then $(X,d)$ is a metric space.
            \end{example}
            \begin{example}
                $\mathbb{R}$ with
                $d(x,y)=|f(x)-f(y)|$, where
                $f:\mathbb{R}\rightarrow\mathbb{R}$
                is injective, is a metric space.
                Let $f$ be a real-valued function. Then
                from the triangle inequality
                \begin{equation*}
                    |f(x)-f(y)|\leq|f(x)-f(z)|+|f(z)-f(y)|
                \end{equation*}
                Therefore $d$ obeys the triangle inequality.
                It also obeys symmetry, for:
                \begin{equation*}
                    |f(x)-f(y)|=|(-1)(f(y)-f(x))|=|f(y)-f(x)|
                \end{equation*}
                The absolute value function is doing
                most of the work.
                But finally we require that
                $|f(x)-f(y)|=0$ if and only if
                $x=y$. But $|f(x)-f(y)|=0$ if and only
                if $f(x)=f(y)$. So we require that $f$
                is injective. If $f$ is not injective,
                then there exists $x_{1}$, $x_{2}$
                such that
                $x_{1}\ne{x_{2}}$ and yet
                $f(x_{1})=f(x_{2})$. But then
                $|f(x_{1})-f(x_{2})|=0$, contradicting the
                fact that this is a metric. If $f$ is
                injective, then this is a metric. Note
                injective functions need not be
                continuous, and can be very crazy.
            \end{example}
            \begin{example}
                $\mathbb{R}$ with
                $d(x,y)=|\tan^{-1}(x)-\tan^{-1}(y)|$ is a
                metric. Moreover, $d(x,y)<\pi$ for all
                $x,y\in\mathbb{R}$. Thus, we have found
                a metric that makes $\mathbb{R}$ a bounded
                set. As a fun fact, $x_{n}=n$ is a Cauchy
                sequence in this metric space, but
                this sequence does not converge to anything.
                Thus we've found a metric on
                $\mathbb{R}$ such that
                $(\mathbb{R},d)$ is not complete.
            \end{example}
            \begin{example}
            Can $d(x,y)=f(x-y)$ be a metric on $\mathbb{R}$
            if $f$ is differentiable? Not everywhere.
            $f$ can not be differentiable at the origin for
            $d(x,y)=f(x-y)$ to be a metric function, however
            $f$ can be differentiable everywhere else. Use
            $f(x)=|x|$ as an example.
            If $f(x-y)$ is a metric, $f$
            must be an even function. But
            then $f'(0)=0$. But $f(x-y)$ also must obey
            the triangle inequality. Therefore:
            \begin{equation*}
                f(2x)\leq{f(x)+f(x)}=2f(x)    
            \end{equation*}
            Define $h(x)$ by:
            \begin{equation*}
                h(x)=
                \left\{
                    \begin{array}{cr}
                    \frac{f(x)}{x},&x\ne{0}\\
                    0,&x=0
                    \end{array}\right.
            \end{equation*}
            Then, from
            the previous statement, $h(2x)\leq{h(x)}$.
            But then:
            \begin{equation*}
                h\Big(\frac{1}{2^{n}}\Big)\leq
                h\Big(\frac{1}{2^{n+1}}\Big)
            \end{equation*}
            But from L'H\^{o}pital's Rule,
            $h(x)\rightarrow{f'(0)}$ as $x\rightarrow{0}$.
            Therefore $h(1)\leq{f'(0)}$. But $h(1)>0$ since
            $f(x-y)$ is a metric, a contradiction.
            Therefore, $f$ can not be differentiable at
            the origin.
            \end{example}
        \subsection{Topology}
            \begin{definition}
                The open ball of radius $r>0$
                about a point $x$ in a metric space
                $(X,d)$ is the set
                $B_{r}(x)=\{y\in{X}:d(x,y)<r\}$
            \end{definition}
            The picture for this is a ``circle'' around the
            point $x$ or radius $r$. However, this circle
            can look very strange for weird metrics.
            \begin{example}
                If $X$ is a set and $d$ is the discrete metric,
                then $B_{r}(x)$ is either the point $x$
                (If $r\leq{1}$), or it is the entire set $X$.
            \end{example}
            \begin{example}
                With $X=\mathbb{R}$ and $d$ the standard metric
                $d(x,y)=|x-y|$, we have $B_{r}(x)$ is simply
                the open interval $(x-r,x+r)$.
            \end{example}
            \begin{example}
                \label{EXAMPLE:FUNCTIONAL:UNIT_BALLS_EXAMPLE}
                Let $X=\mathbb{R}^{2}$ and define
                $d_{p}(x,y)%
                 =(|x_{1}-y_{1}|^{p}+|x_{2}-y_{2}|^{p})^{1/p}$.
                For $p=2$, an open ball is a circle around
                the point $(x,y)$ of radius $r$. For $p=1$,
                we have ``diamonds'' around the point $x$.
                And for $p=\infty$ we have a square
                around $x$.
                Let $X=\mathbb{R}^{2}$ and let $d$ be the metric
                such that you can only travel parallel to the
                $y$ axis, or along the $x$ axis.
                Consider the unit balls in $(X,d)$
                about the following points:
                \begin{enumerate}
                    \begin{multicols}{4}
                        \item $(0,0)$
                        \item $(0,1)$
                        \item $(0, 0.5)$
                        \item $(0.5,0.5)$
                    \end{multicols}
                \end{enumerate}
                If $\mathbf{x}_{1}=(x_{1},y_{1})$ and
                $\mathbf{x}_{2}=(x_{2},y_{2})$, then we have:
                \begin{equation*}
                    d(\mathbf{x}_{1},\mathbf{x}_{2})=
                    \begin{cases}
                        |y_{2}-y_{1}|,&x_{1}=x_{2}\\
                        |x_{2}-x_{1}|+|y_{1}|+|y_{2}|,
                        &x_{1}\ne{x_{2}}
                    \end{cases}
                \end{equation*}
            About the point $(0,0)$, the unit ball
            is simply points
            $(x,y)$ such that $|x|+|y|<1$. This is a ``diamond.''
            About $(0,1)$, first note that to get to any point
            whose $x$ coordinate is not $0$, you first must travel
            the entirety of the $y$ axis. Since this length is
            already $1$, you can't go left or right
            on the $x$ axis.
            The unit ball is the line segment on the $y$ axis
            between $(0,0)$ and $(0,2)$. For the third one, if
            the $x$ coordinate changes, we have
            $0.5+|y|+|x|<1$, which implies
            $|y|+|x|<0.5$. This is again a diamond, but a
            smaller one. If the $x$ coordinate does not
            change, we have $|y-0.5|<1$. This is another
            line segment. Repeat the same arguments for the
            fourth coordinate. The diagrams are show in
            Fig.~\ref{FUNCTIONAL:HOMEWORK:2:PROBLEM:4:FIGURES}.
        \begin{figure}[H]
            \centering
            \captionsetup{type=figure}
            \subimport{../../../tikz/}
                      {Functional_Analysis_Fall_2018_%
                       HW_2_Problem_4}
            \caption{Figures for Example
                     \ref{EXAMPLE:FUNCTIONAL:UNIT_BALLS_EXAMPLE}.}
            \label{FUNCTIONAL:HOMEWORK:2:PROBLEM:4:FIGURES}
        \end{figure}
            \end{example}
            If you have a vector space and a norm on it,
            then the open balls about a point will have the
            property of convexity. Convexity is a vector space
            property, given two points the ``line'' between the
            two remains in the set. Metric spaces have no such
            notion. Since the balls of $\norm{}_{p}$ are not
            convex with $p<1$, we have that $\norm{}_{p}$ is
            a metric on $\mathbb{R}^{n}$
            if and only if $p\geq{1}$.
            \begin{definition}
                An open subset of a metric space
                $(X,d)$ is a set $S\subset{X}$ such that,
                for all $x\in{S}$, there is an
                $r>0$ such that
                $B_{r}(x)\subset{S}$.
            \end{definition}
            \begin{example}
                If $(X,d)$ is a metric space, then
                $X$ is open and $\emptyset$ is open
                (Vacuously true).
            \end{example}
            \begin{theorem}
                If $(X,d)$ is a metric space, $x\in{X}$,
                and $r>0$, then $B_{r}(x)$ is an open
                subset of $X$.
            \end{theorem}
            \begin{proof}
                If $z\in{B_{r}(x)}$, let $t=d(x,z)$.
                Then $0\leq{t}<r$. Let $r'=r-t$.
                But if $y\in{B_{r'}(z)}$, then
                $d(x,y)\leq{d(x,z)+d(y,z)}<t+r'=t+r-t=r$.
                Therefore $B_{r'}(z)\subset{B_{r}(x)}$.
            \end{proof}
            \begin{theorem}
                A finite intersection of open sets is open.
            \end{theorem}
            \begin{proof}
                If $\mathcal{U}_{1},\hdots,\mathcal{U}_{n}$
                are open and if
                $x\in\cap_{k=1}^{n}\mathcal{U}_{k}$, then there
                exists $r_{1},\hdots,r_{n}$ such that
                $B_{r_{i}}(x)\subset\mathcal{U}_{i}$. Let
                $r=\min\{r_{1},\hdots,r_{n}\}$. Then
                $B_{r}(x)\subset\cap_{k=1}^{n}\mathcal{U}_{i}$
            \end{proof}
            \begin{theorem}
                Arbitrary unions of open sets are open.
            \end{theorem}
            Infinite intersections need not be open.
            The proof above would fail since the
            $r_{i}$ can form a sequence tending to zero.
            But indeed, let $X=\mathbb{R}$ and let
            $d(x,y)=|x-y|$, and take
            $\mathcal{U}_{n}=(-\frac{1}{n},\frac{1}{n})$.
            Then all of the $\mathcal{U}_{n}$ are open,
            yet the intersection, which is the set $\{0\}$,
            is not open. All of this mumbo-jumbo creates
            the more general notion of a topological space.
            \begin{definition}
                A topological space is a set $X$ and a
                subset $\tau\subset\mathcal{P}(X)$ such that:
                \begin{enumerate}
                    \item $\emptyset,X\in\tau$
                    \item Finite intersections of sets in $\tau$
                          are also sets in $\tau$.
                    \item Arbitrary unions of sets in $\tau$
                          are also sets in $\tau$.
                \end{enumerate}
            \end{definition}
            Here, $\mathcal{P}(X)$ denotes the \textit{power set}
            of $X$. This is the set of all subsets of $X$.
            The notion of a topological space generalizes the
            notion of a metric space. There is no notion of
            distance in such spaces, and things can be weird.
            There are topological spaces that have no metric
            associated with them.
            \begin{definition}
                An open subset of a topological space
                $(X,\tau)$ is a set $\mathcal{U}\in\tau$.
            \end{definition}
            \begin{definition}
                An interior point of a subset $S$ of a topological
                space $(X,\tau)$ is a point $x\in{S}$ such that
                there is an open subset $\mathcal{U}\subseteq{S}$
                such that $x\in\mathcal{U}$.
            \end{definition}
            \begin{definition}
                The interior of a subset $S$ of a topological
                space $(X,\tau)$, denoted $\Int(S)$, is the set
                of all interior points of $S$.
            \end{definition}
            \begin{theorem}
                If $S$ is an open subset of
                $(X,\tau)$, then $\Int(S)=S$.
            \end{theorem}
            \begin{definition}
                A function from a metric space
                $(X,d_{X})$ to a metric space $(Y,d_{Y})$
                continuous at $x\in{X}$ is a function
                $f:X\rightarrow{Y}$ such that
                for all $\varepsilon>0$ there is
                a $\delta>0$ such that for all
                $x_{0}\in{X}$ such that
                $d_{X}(x,x_{0})<\delta$, we have
                $d_{Y}(f(x),f(x_{0})<\varepsilon$
            \end{definition}
            \begin{theorem}
                If $(X,\tau)$ is a topological space and
                $S\subseteq{X}$, and if $\mathcal{O}$ is the set
                of all open sets $\mathcal{U}$ such that
                $\mathcal{U}\subseteq{S}$, then
                $\Int(S)=%
                 \bigcup_{\mathcal{U}\in\mathcal{O}}\mathcal{U}$.
            \end{theorem}
            \begin{definition}
                A nowhere dense subset of a topological space
                $(X,\tau)$ is a subset $S\subseteq{X}$ such that
                $\Int(S)=\emptyset$.
            \end{definition}
            \begin{theorem}
                If $(X,d)$ is a metric space,
                $y\in{X}$, then
                $f:X\rightarrow\mathbb{R}$ defined by
                $f(x)=d(x,y)$ is uniformly continuous.
            \end{theorem}
            A surprising theorem, and the entire
            basis of the study of topology, goes as
            follows:
            \begin{theorem}
                If $(X,d_{x})$ and $(Y,d_{Y})$
                are metric spaces, then
                $f:X\rightarrow{Y}$ is continuous
                at $x\in{X}$ if and only if
                for all open subsets
                of $S\subset{Y}$ such that
                $f(x)\in{S}$, $f^{-1}(S)$ is an
                open subset of $X$.
            \end{theorem}
            This allows us to talk about continuous
            functions without a notion of metric.
            Thus, for topological spaces, this is
            the \textit{definition} of continuity.
            When the space we're discussing is a
            metric space, this theorem shows that the
            definition from topology and the defintition
            from real analysis are in fact equivalent.
            \begin{theorem}
                A function $f:X\rightarrow{Y}$ between
                metric spaces is continuous at a point
                $x\in{X}$ if and only if for all
                sequences $x_{n}$ such that
                $d_{X}(x,x_{n})\rightarrow{0}$, we have
                $d_{Y}(f(x),f(x_{n})\rightarrow{0}$.
            \end{theorem}
            We now have three different ways to talk
            about continuity. Topological spaces can be
            nastier, however. We saw in
            Thm.~\ref{thm:Funct:Limit_of_Metric_Sequence_Unique}
            that the limit of a convergent sequence in a
            metric space is unique.
            This is not true in a topological space and there
            are topological spaces with sequences
            which converge to every point in the
            space simultaneously. Indeed, it may be impossible
            to distinguish two points in a topological
            space. The ability to
            ``Separate,'' points is special.
            Hausdorff spaces can, but
            we'll save that for topology.
        \subsubsection{Closed Sets}
            \begin{definition}
                A limit point of a subset
                $S\subset{X}$ of a metric space
                $(X,d)$ is a point $a\in{X}$ such
                that there is a sequence
                $x:\mathbb{N}\rightarrow{S}$ such that
                $d(a,x_{n})\rightarrow{0}$.
            \end{definition}
            \begin{definition}
                A closed subset of a metric space $(X,d)$
                is a set $S$ such that for all $x\in{X}$ such
                that $x$ is a limit point of $S$, $x\in{S}$.
            \end{definition}
            This says that if $S$ is closed, and
            $x$ is a sequence in $S$ such
            that $x_{n}\rightarrow{a}$, then
            $a\in{S}$.
            \begin{example}
                In $\mathbb{R}$, with the standard
                metric, $(a,b)$ is open,
                $\mathbb{R}$ is open (and closed),
                $[a,b]$ is closed,
                $[a,\infty)$ is closed,
                $[a,b)$ is neither closed nor open.
            \end{example}
            \begin{example}
                If $X=(0,1)$, and
                $d(x,y)=|x-y|$, then
                $(0,1)$ is closed. This is because
                there is no sequence that converges
                to a point in the space whose limit
                is not in the space. There are no sequences
                in $X$ which converge to zero or one since,
                as far as $X$ is concerned,
                neither or these points exist.
            \end{example}
            \begin{theorem}
                If $(X,d)$ is a metric space,
                then a subset $S\subset{X}$ is open
                if and only if $X\setminus{S}$ is closed.
            \end{theorem}
            \begin{proof}
                Suppose $S$ is open, and let
                $x_{n}$ be a sequence in $S^{c}$.
                Suppose $x_{n}\rightarrow{x}$ and
                $x\in{S}$. But $S$ is open, and thus
                there is an $\varepsilon>0$ such that
                $B_{\varepsilon}(x)\subset{S}$.
                But $x_{n}\rightarrow{x}$, and thus
                this is an $N\in\mathbb{N}$ such that
                for all $n>N$, $d(x,x_{n})<\varepsilon$.
                But then for all $n>N$,
                $x_{n}\in{B_{\varepsilon}(x)}$. But
                $x_{n}\in{S^{c}}$, a contradiction.
                Therefore, $S^{c}$ is closed. On the
                other hand, if $S^{c}$ is closed
                and there is an $x\in{S}$ such that
                for all $r>0$,
                $B_{r}(x)\cap{S}\ne\emptyset$, then
                for all $n\in\mathbb{N}$ there is
                an $x_{n}\in{S^{c}}$ such that
                $d(x,x_{n})<\frac{1}{n}$. But then
                $x_{n}\rightarrow{x}$, and therefore
                $x\in{S^{c}}$. But $x\in{S}$,
                a contradiction. Thus, $S$ is open.
            \end{proof}
            In topology we take the definition of
            closed sets to be the compliment of open
            sets. This theorem shows that the
            topological definition is equivalent when we
            consider metric spaces.
            \begin{definition}
                The closure of a subset
                $S$ of a metric space
                $(X,d)$, denoted $\overline{S}$,
                is the set of all
                limit points of $S$.
            \end{definition}
            \begin{theorem}
                If $(X,d)$ is a metric space, if
                $S\subset{X}$, and if
                $\Delta$ is the set of all closed subsets
                $\mathcal{C}\subset{X}$ such that
                $S\subset\mathcal{C}$, then:
                $\overline{S}=
                 \bigcap_{\mathcal{C}\in\Delta}
                 \mathcal{C}$
            \end{theorem}
            Thus we may loosely say that
            the closure of a set $S$ is the
            ``Smallest,'' closed set that contains $S$.
            \begin{definition}
                The closed ball of radius $r>0$ about
                a point $x$ in a metric space
                $(X,d)$ is the set:
                \begin{equation*}
                    \overline{B}_{r}(x)=
                    \{y\in{X}:d(x,y)\leq{r}\}
                \end{equation*}
            \end{definition}
            There exists metric spaces $(X,d)$
            such that
            $\overline{B}_{r}(x)\ne\overline{B_{r}(x)}$.
            For take the discrete metric, $r=1$.
            Then the closure of $B_{1}(x)$ is simply
            the point $x$. However, the closed ball
            $\overline{B}_{1}(x)$ is the entire space.
            Metric spaces can be very weird like this.
            They have a property, that given a nested
            sequence of closed balls whose radius
            tends to zero, there is precisely one
            point that lies in the intersection. However,
            if the radius does not tend to zero it is
            possible that the intersection is empty.
            This is very counter-intuitive.
            \begin{definition}
                A dense subset of a metric space $(X,d)$
                is a set $S\subset{X}$ such that
                $\overline{S}=X$.
            \end{definition}
            A subset $S$ is dense in $X$ if every point
            in $X$ can be approximated arbitrarily well
            by points in $S$. For any point $a\in{X}$
            there is a sequence $x\in{S}$
            such that $x_{n}\rightarrow{a}$. The
            classic example is $\mathbb{Q}$ and
            $\mathbb{R}$. Every real number can be
            approximated arbitrary well by a rational
            number. To see this, just take the continued
            fraction of a real number and stop once
            the approximation is less than
            $\varepsilon$. When we say $\mathbb{Q}$ is
            dense in $\mathbb{R}$, we of course mean with
            respect to the standard metric on $\mathbb{R}$.
            $\mathbb{Q}$ is \textbf{not} dense in
            $\mathbb{R}$ with respect to the discrete metric.
            Indeed, if $d$ is the discrete metric on $X$,
            then $S\subset{X}$ is dense in $X$ if and only if
            $S=X$.
            \begin{example}
                $\mathbb{Q}$ is dense in $\mathbb{R}$
                with respect to $d_{p}$ for all
                $p\geq{1}$. This includes
                $d(x,y)=|x-y|$.
            \end{example}
            \begin{example}
                The set of polynomials on the interval
                $[a,b]$ are dense in the set of
                continuous functions on $[a,b]$ with
                respect to the $d_{\infty}$ metric.
                This comes from Weierstrass's Theorem.
            \end{example}
            \begin{example}
                The set of polynomials on $[a,b]$
                is dense in the set of continuous
                functions on $[a,b]$ with respect to
                the $d_{p}$ metric, for $p\geq{1}$. This
                is because:
                \begin{align*}
                    d_{p}(P,x)&=
                    \Big(
                        \int_{a}^{b}|P(t)-x(t)|^{p}\diff{t}
                    \Big)^{1/p}
                    &
                    &=\Big(
                        d_{\infty}(P,x)^{p}\int_{a}^{b}\diff{t}
                    \Big)^{1/p}\\
                    &\leq\Big(\int_{a}^{b}
                        |\max\{P(t)-x(t)\}|^{p}\diff{t}
                    \Big)^{1/p}
                    &
                    &=(b-a)^{1/p}d_{\infty}(P,x)
                \end{align*}
            \end{example}
            \begin{example}
                The continuous functions are not dense
                in the set of integrable functions,
                with respect to the supremum metric
                $d_{\infty}$. This is more or less
                because integrable functions can
                be discontinuous, or have jumps. This
                means, with respect to $d_{\infty}$,
                that no continuous functions could
                approximate such a discontinuous function
                arbitrary well.
            \end{example}
            \begin{definition}
                A separable metric space
                is a metric space $(X,d)$ with
                a countable dense subset $S$.
            \end{definition}
            \begin{example}
                $\mathbb{R}$ is separable, with
                the standard metric, since
                $\mathbb{Q}$ is countable and also
                dense in $\mathbb{R}$.
            \end{example}
            \begin{example}
                The set of continuous functions on
                $[a,b]$ is separable. For
                take the set of polynomials with
                rational coefficients. This can
                be seen as a countable union of
                countably many elements. For let
                $P_{N}$ be the set of polynomials
                of degree $N$ with rational
                coefficients. This is countable,
                and the set of all polynomials with
                rational coefficients is simply the
                union of $P_{N}$ over all $N$. This
                is dense in the set of polynomials,
                and the set of polynomials is dense
                in $C[a,b]$, and thus
                the set of polynomials with rational
                coefficients is dense in $C[a,b]$. Thus
                $C[a,b]$ is separable.
            \end{example}
            \begin{example}
                $\ell^{p}$ is separable with the
                $d_{p}$ metric, simply use elements
                with rational entries. That is,
                sequences of rational numbers.
            \end{example}
            \begin{example}
                $\ell^{p}$ with the $d_{\infty}$ metric
                is NOT separable. Consider the real
                numbers in $(0,1)$.
            \end{example}
        \subsection{Completeness}
            \begin{definition}
                A complete metric space is a metric
                space $(X,d)$ such that every
                Cauchy sequence $x_{n}$
                in $X$ converges to a point in $X$
                with respect to $d$.
            \end{definition}
            Recall that a sequence $x_{n}$ is Cauchy if
            $\forall_{\varepsilon>0}\exists_{N\in\mathbb{N}}:%
             \forall_{n,m>N},d(x_{n},x_{m})<\varepsilon$.
            Convergence with respect to $d$ means that
            $d(x,x_{n})\rightarrow{0}$.
            \begin{example}
                $\mathbb{R}$ with the standard metric
                $d(x,y)=|x-y|$ is complete.
            \end{example}
            \begin{example}
                $(\mathbb{R}^{n},d_{p})$ is also complete
                for all $n\in\mathbb{N}$.
            \end{example}
            Completeness is both a property of the set
            and the metric itself. It is not a topological
            property.
            \begin{example}
                $(\mathbb{R},d)$, where
                $d(x,y)=|\tan^{-1}(x)-\tan^{-1}(y)|$
                is \textit{not} complete. For let
                $x_{n}=n$. This is a Cauchy sequence,
                as one can see from the graph
                of $\tan^{-1}(x)$. That is, because
                $\tan^{-1}(x)\rightarrow{\pi/2}$,
                $x_{n}=n$ is a Cauchy sequence in this
                metric. Being even more rigorous, let
                $\varepsilon>0$ and
                $N=\ceil{\tan(\pi/2-\varepsilon)}$.
                Then, for all $n,m>N$,
                $d(x_{n},x_{m})%
                 =|\tan^{-1}(n)-\tan^{-1}(m)|%
                 <|\pi/2-\tan^{-1}(\min\{n,m\})|%
                 <|\pi/2-(\pi/2-\varepsilon)|%
                 =\varepsilon$. But $x_{n}$ does not
                converge. For suppose not.,
                Suppose $x_{n}=n\rightarrow{x}$.
                Then for $n>x+1$,
                $d(x_{n},x)=|\tan^{-1}(n)-\tan^{-1}(x)|%
                 <|\tan^{-1}(x+1)-\tan^{-1}(x)|$,
                so $d(x_{n},x)\not\rightarrow{0}$.
                The sequence does not converge.
            \end{example}
            Let $X=\mathbb{R}\cup\{-\infty,\infty\}$.
            Let $d:X\times{X}\rightarrow\mathbb{R}$
            be defined by
            \begin{align*}
                d(x,y)
                &=|\tan^{-1}(x)-\tan^{-1}(y)|
                &
                d(x,\infty)
                &=\frac{\pi}{2}-\tan^{-1}(x)\\
                d(-\infty,x)
                &=\frac{\pi}{2}+\tan^{-1}(x)
                &
                d(-\infty,\infty)&=\pi
            \end{align*}
            Then $d$ is a metric on $X$, and moreover
            $(X,d)$ is complete. The counterexample
            we found for $(\mathbb{R},d)$ has been
            ``filled,'' in a sense. The hole is
            no longer there. The sequence $x_{n}=n$
            now converges to $\infty$. Somewhat
            unsurpringly, $\mathbb{R}$ is
            dense in $X$, with respect to
            $d$. Every element in $X$ is the limit of
            a sequence of elements in $\mathbb{R}$.
            \begin{definition}
                A completion of a metric space
                $(X,d)$ is a complete metric space
                $(\tilde{X},\tilde{d})$ such that
                $X\subset{\tilde{X}}$ and
                the restriction of
                $\tilde{d}$ onto $X$ is equal to $d$.
            \end{definition}
            \begin{theorem}
                Every metric space has a completion.
            \end{theorem}
            \begin{definition}
                An isometry between
                metric spaces
                $(X,d_{X})$ and
                $(Y,d_{Y})$ is a function
                $f:X\rightarrow{Y}$ such that
                $d_{X}(x,y)=d_{Y}(f(x),f(y))$
                for all $x,y\in{X}$.
            \end{definition}
            \begin{definition}
                Isometric metric spaces are metric spaces
                with an isometry between them.
            \end{definition}
            \begin{theorem}
                If $(X,d)$ is a metric space
                and $(\tilde{X}_{1},\tilde{d}_{1})$
                and $(\tilde{X}_{2},\tilde{d}_{2})$
                are completions of $(X,d)$, then
                $(\tilde{X}_{1},\tilde{d}_{1})$
                and $(\tilde{X}_{2},\tilde{d}_{2})$
                are isometric.
            \end{theorem}
            This says the completion of a metric space is
            unique up to isometry.
            The Lebesgue space $L^{p}(S)$
            can be defined to be the completion of
            $C(S)$ with respect to the $d_{p}$ metric.
            \begin{theorem}
                $(C(S),d_{\infty})$ is complete.
            \end{theorem}
            \begin{proof}
                Suppose $x_{n}$ is a Cauchy sequence
                and let $\varepsilon>0$. As $x_{n}$ is
                Cauchy, there exists $N\in\mathbb{N}$
                such that for all $n,m>N$,
                $\sup|x_{m}(t)-x_{n}(t)|<\frac{\varepsilon}{3}$.
                But then for all $t\in{S}$,
                $|x_{m}(t)-x_{n}(t)|<\frac{\varepsilon}{3}$,
                for all
                $n,m>N$. That is, if $x_{n}$ is
                a Cauchy sequence in $(C(S),d_{\infty})$,
                then it is a Cauchy sequence in
                $(\mathbb{R},d_{1})$. But
                $(\mathbb{R},d_{1})$ is complete, and
                therefore, for all $t\in{S}$, there is
                an $x(t)$ such that
                $x_{n}(t)\rightarrow{x(t)}$ with respect
                to the $d_{1}$ metric on $\mathbb{R}$. We
                now need to show that $x(t)$ is a continuous
                function. That is, that
                $x(t)\in{C(S)}$. Finally we need to show that
                $x_{n}\rightarrow{d}$ with respect to
                $d_{\infty}$. We need to show that
                for all $\varepsilon>0$ and all $t\in{S}$
                there is a $\delta>0$
                such that for all $|t-t_{0}|<\delta$,
                $|x(t)-x(t_{0})|<\varepsilon$. But for
                all $n,m>N$,
                $\sup\{x_{n}(t)-x_{m}(t)\}<\frac{\varepsilon}{3}$.
                Taking the limit on $m$, we have
                $|x(t)-x_{n}(t)|<\frac{\varepsilon}{2}$.
                But $x_{n}(t)$ is continuous, and thus
                there exists $\delta>0$ such that
                for all $|t-t_{0}|<\delta$,
                $|x_{n}(t)-x_{n}(t_{0})|<\frac{\varepsilon}{3}$.
                But
                $|x(t)-x(t_{0})|\leq%
                  |x(t)-x_{n}(t)|%
                 +|x_{n}(t)-x_{m}(t)|%
                 +|x(t_{0})-x_{n}(t_{0})$
                But
                $|x(t_{0})-x_{n}(t_{0})|<%
                 \sup\{|x(t)-x_{n}(t)|\}<\frac{\varepsilon}{3}$,
                and therefore
                $|x(t)-x(t_{0})|<\varepsilon$.
                So $x(t)$ is continuous.
            \end{proof}
            The Weierstrass Approximation Theorem says that,
            for closed finite intervals $S$,
            $(C(S),d_{\infty})$ is the completion
            of the set of polynomials with respect to
            the $d_{\infty}$ metric. On the other hand,
            $(C[0,1],d_{p})$ is not complete when
            $1\leq{p}<\infty$. For define the following:
            \begin{equation*}
                H(x)=
                \begin{cases}
                    0,&0\leq{x}\leq{\frac{1}{2}}\\
                    1,&\frac{1}{2}<x\leq{1}
                \end{cases}
            \end{equation*}
            This is discontinuous and cannot be
            approximated arbitrarily well
            by any continuous function. However, the
            \textit{area} underneath $H$ can be approximated
            arbitrarily well be continuous functions. For define:
            \begin{equation*}
                x_{n}(t)=
                \begin{cases}
                    0,&0\leq{x}\leq{\frac{1}{2}-\frac{1}{n}}\\
                    n(x-\frac{1}{2}+\frac{1}{n}),
                    &\frac{1}{2}-\frac{1}{n}\leq{x}
                     \leq{\frac{1}{2}}\\
                    1,&\frac{1}{2}<{x}\leq{1}
                \end{cases}
            \end{equation*}
            Then the area under $x_{n}(t)$
            is $\frac{1}{2}+\frac{1}{2n}$, and thus
            $d_{1}(x_{n}(t),x_{m}(t))%
             =|\frac{1}{2m}-\frac{1}{2n}|$,
            and therefore $x_{n}(t)$ is a Cauchy sequence.
            But $x_{n}(t)$ does not converge in
            $(C[0,1],d_{1})$. For suppose not, suppose
            $x_{n}(t)\rightarrow{x(t)}$, and
            $x(t)\in{C[0,1]}$.
            If $x(1/2)\geq{1/2}$, then, as $x(t)$ is
            continuous, there is a $\delta>0$ such that
            for all $|t-1/2|<\delta$,
            $x(t)>1/4$. But then
            $d(x_{n},x)=\int_{0}^{1}|x(t)-x_{n}(t)|dt%
            \geq\int_{1/2-\delta/2}^{1/2}|x(t)-x_{n}(t)|dt$.
            But $|x|=|(x-y)+y|\leq{|x-y|+|y|}$,
            and thus
            $|x|-|y|\leq{|x-y|}$. From this we have
            $d(x_{n}(t),x(t))\geq%
             \int_{1/2-\delta/2}^{1/2}(x(t)-x_{n}(t))dt%
             >\int_{1/2-\delta/2}^{1/2}\frac{1}{4}dt%
             -\int_{0}^{1/2}x_{n}(t)dt%
             =\frac{1}{4}\delta-\frac{1}{2n}%
             \rightarrow{\frac{1}{4}}\delta$.
            But then $d(x_{n}(t),x(t))\not\rightarrow{0}$.
            Therefore $x_{n}(t)$ does not converge.
            \begin{theorem}
                If $1\leq{p}<\infty$, then
                $(\ell^{p},d_{p})$ is complete.
            \end{theorem}
            \begin{proof}
                Let $x_{n}$ be a Cauchy sequence
                in $(e\ell^{p},d_{p})$,
                $x_{n}=x_{n}(1),x_{n}(2),\hdots,x_{n}(k),\hdots$
                Then, for $n,m\in\mathbb{N}$,
                $d_{p}(x_{n},x_{m})%
                 =(%
                    \sum_{k=0}^{\infty}|x_{n}(k)-x_{m}(k)|^{p}%
                  )^{1/p}$
                As $x_{n}$ is Cauchy, for all 
                $\varepsilon>0$ there is an $N\in\mathbb{N}$
                such that for all $n,m>N$,
                $d_{p}(x_{n},x_{m})<\varepsilon$.
                But then, for all $n,m>N$ and all
                $k\in\mathbb{N}$,
                $|x_{n}(k)-x_{m}(k)|^{p}<d_{p}(x_{n},x_{m})^{P}%
                 <\varepsilon^{p}$.
                But then
                $|x_{n}(k)-x_{m}(k)|<\varepsilon$. Therefore
                $x_{n}(k)$ is a Cauchy sequence in
                $(\mathbb{R},d)$, and this metric space is
                complete. Therefore, for all $k\in\mathbb{N}$,
                there is a $z_{k}$ such that
                $x_{n}(k)\rightarrow{z_{k}}$. We now need to
                show that $z_{k}$ is an element of
                $\ell^{p}$ and that
                $x_{n}\rightarrow{z_{k}}$ with respect to
                the $d_{p}$ metric. For let $N\in\mathbb{N}$.
                Then
                $\sum_{k=0}^{N}|x_{n}(k)-x_{m}(k)|^{p}%
                 \leq{\sum_{k=0}^{\infty}|x_{n}(k)-x_{m}(k)|^{p}}%
                 <\varepsilon^{p}$. Taking the limit on $m$,
                we have
                $\sum_{k=0}^{N}|z_{k}-x_{n}(k)|<\varepsilon^{p}$.
                The reason we have written a finite sum is to
                avoid getting into trouble with limits. An
                infinite sum is itself a limit, and taking
                limits of limits can get very messy very easily.
                For example,
                $f(n,m)=\frac{m}{n+m}$. Taking the limit on
                $m$ first results in $1$, whereas taking the
                limit on $n$ first gives you $0$.
                That is,
                $\lim_{n}\lim_{m}f(n,m)%
                 \ne\lim_{m}\lim_{n}f(n,m)$.
                You have to
                be careful when considering limits of limits.
                With this we have shown that
                $z_{k}-x_{n}(k)\in\ell^{p}$ for all
                $n\in\mathbb{N}$. But $x_{n}\in\ell^{p}$,
                and $\ell^{p}$ is closed under addition.
                Therefore $z_{k}\in\ell^{p}$. But also,
                for $n>N$, we have
                $d_{p}(x_{n},z)<\varepsilon$. Thus,
                $x_{n}$ converges.
            \end{proof}
            \begin{theorem}
                If $(X,d)$ is complete and $S$ is a closed
                subset of $X$, then $(S,d_{S})$ is complete,
                where $d_{S}$ is the restriction of
                $d$ onto $S$.
            \end{theorem}
            \begin{proof}
                Let $x_{n}$ be a Cauchy sequence in $S$. Then
                $x_{n}\rightarrow{x}$, $x\in{X}$,
                since $x_{n}$ is Cauchy in $X$
                and $X$ is complete. Since $S$ is closed,
                $x\in{S}$. Therefore, etc.
            \end{proof}
            \begin{theorem}
                If $(X,d)$ is complete and
                $S\subset{X}$ is not closed,
                then $(S,d_{S})$ is not complete.
            \end{theorem}
            \begin{proof}
                If $S$ is not closed then there
                is a convergent sequence $x_{n}\in{S}$
                whose limit it not in $S$. But
                then $x_{n}$ is a Cauchy sequence in
                $X$, and therefore is also a
                Cauchy sequence in $S$, but
                $x_{n}$ does not converge in $S$.
                Therefore $(S,d_{S})$ is not complete.
            \end{proof}
            Recall that $c_{0}$ is the set of sequences which
            tend to zero. That is, it is the set of
            null sequences.
            \begin{theorem}
                $c_{0}$ is a closed subset of
                $(\ell^{\infty},d_{\infty})$
            \end{theorem}
            \begin{proof}[proof 1]
                Let $x_{n}$ be a sequence in $c_{0}$
                that converges to $z\in\ell^{\infty}$
                with respect to $d_{\infty}$.
                Then
                $\sup\{|x_{n}(k)-z_{k}|\}\rightarrow{0}$.
                We need to show that $z\in{c_{0}}$.
                Let $\varepsilon>0$. Let $N_{1}\in\mathbb{N}$
                be such that
                $n>N$ implies
                $\sup\{|x_{n}(k)-z_{k}\}<\frac{\varepsilon}{2}$.
                But $x_{n}\in{c_{0}}$ for all $n$, and thus
                $x_{n}(k)\rightarrow{0}$ as $k\rightarrow\infty$.
                Thus, there is an $N_{2}\in\mathbb{N}$
                such that $n>N_{2}$ implies
                $|x_{n}(k)<\varepsilon$.
                But then for $n>\max\{N_{1},N_{2}\}$,
                $|z_{k}|\leq|z_{k}-x_{n}(k)|+|x_{n}(k)|%
                 <\varepsilon$.
            \end{proof}
            \begin{proof}[Proof 2]
                We can also show that
                $c_{0}^{C}$ is open.
                Let $x\in{c_{0}^{C}}$. Then there is
                an $r>0$ and a subsequence
                $x_{k_{n}}$ of $x$ such that
                $x_{k_{n}}>r$ for all $n$.
                But then $B_{r/2}(x)$ is
                an open ball contained in $c_{0}^{C}$.
                For if $y\in{B_{r/2}(x)}$, then
                $d_{\infty}(x,y)%
                 =\sup\{|x_{n}-y_{n}|\}<r<2$,
                and thus
                $|y_{k_{n}}-x_{k_{n}}|<r/2$,
                and there for $|y_{k_{n}}|>r/2$.
                Thus, $y$ is not a null sequence and
                $c_{0}^{C}$ is open. So
                $c_{0}$ is closed.
            \end{proof}
            Let $X$ be the set of sequences with only
            finitely many nonzero terms.
            Then $(X,d_{\infty}$ is not complete.
            Let $x_{1}=(1,0,0,\hdots)$,
            $x_{2}=(1,1/2,0,0,\hdots)$,
            $x_{n}=(1,1/2,\hdots,1/n,0,0,\hdots)$.
            Then
            $d_{\infty}(x_{n},x_{m})=1/\max\{n,m\}\rightarrow{0}$.
            But clearly
            $x_{n}\rightarrow(1,1/2,\hdots,1/n,\hdots)$, which
            is an element of $c_{0}$, but not an element
            of $X$. Thus $X$ is not closed, and therefore is
            not complete. Returning to $C[0,1]$, when we had
            that sequence of continuous functions that clearly
            converged to a discontinuous functions, we still
            needed to show that there is no continuous function
            that the $x_{n}(t)$ converged to. Here we've embedded
            $X$ into a bigger space, shown that the
            sequence converges to something outside of $X$,
            in our case an element of
            $c_{0}\setminus{X}$, and then used the uniqueness
            of limits to show that the limit does
            not converge in $X$.
        \subsection{Banach's Fixed Point Theorem}
            If $(X,d)$ is a complete metric space,
            and if $T:X\rightarrow{X}$ satisfies
            the property that, for all $x$ and $y$
            in $X$, $d(T(x),T(y))<kd(x,y)$ for
            some $k<1$, then $T$ has a unique
            point $x$, called a fixed point,
            such that $T(x)=x$.
            \begin{definition}
                A contraction of a metric
                space $(X,d)$ is a function
                $T:{X}\rightarrow{X}$ such that there
                exists a $k\in(0,1)$ such that
                for all $x,y\in{X}$,
                $d(T(x),T(y))<kd(x,y)$.
            \end{definition}
            \begin{definition}
                A fixed point of a function
                $f:X\rightarrow{X}$ is a point
                $x\in{X}$ such that
                $f(x)=x$.
            \end{definition}
            \begin{theorem}[%
                Banach's Fixed Point Theorem%
            ]
                If $(X,d)$ is a complete
                metric space and $T:X\rightarrow{X}$
                is a contraction, then there is
                a unique fixed point $x\in{X}$
                with respect to $T$.
            \end{theorem}
            \begin{definition}
                A Lipschitz continuous function is a
                function $f:[a,b]\rightarrow\mathbb{R}$
                such that there is an $L\in\mathbb{R}$
                such that
                $|f(x)-f(y)|<L|x-y|$ for all
                $x,y\in[a,b]$.
            \end{definition}
            This says that the slopes of the
            secant lines of the
            function are bounded. The square root
            function $y=\sqrt{x}$ is an example
            of a function that is not Lipschitz. The
            slopes of secant lines go to infinity
            as the points tend towards the origin.
            \begin{theorem}[Picard's Theorem]
                If $f:[a,b]\times\mathbb{R}%
                    \rightarrow\mathbb{R}$
                is Lipschitz continuous,
                Then there is a unique function
                $x:[a,b]\rightarrow\mathbb{R}$
                such that
                $\frac{dx}{dt}=f(t,x(t))$ and $x(a)=a$.
            \end{theorem}
            \begin{proof}
                We prove Picard by using the
                Banach Fixed Point Theorem. First
                we write the problem as an integral
                equation.
                If $\dot{x}=f(t,x(t))$, then:
                \begin{equation*}
                    x(t)
                    =\int_{a}^{t}\frac{dx}{dt}dt
                    =x_{0}+\int_{a}^{t}f(t,x(t))dt
                \end{equation*}
                Let $(X,d)$ be $C[a,b]$ with the
                supremum norm $d_{\infty}$. Then
                $(x,d)$ is a complete metric space.
                Let $T:{X}\rightarrow{X}$ be defined
                by:
                \begin{equation*}
                    Tx=x_{0}+\int_{a}^{t}f(t,x(t))dt
                \end{equation*}
                All we need to do is show that $T$ is
                a contraction. Applying the
                Banach Fixed Point theorem then
                shows that there is a unique
                fixed point of $T$, thus showing
                that there is a unique solution
                to our original initial value problem.
                If $x,y\in{X}$, then:
                \begin{align*}
                    d(Tx,Ty)
                    &=\sup\{|Tx(t)-Ty(t)|\}\\
                    &=\sup\{
                        (x_{0}+
                         \int_{a}^{t}f(t,x(t))dt)
                       -(x_{0}+
                         \int_{a}^{t}f(t,y(t))dt)
                    \}\\
                    &=\sup\{
                        \int_{a}^{t}f(t,x(t))dt)-
                        \int_{a}^{t}f(t,y(t))dt)
                    \}\\
                    &\leq\int_{a}^{t}|
                        f(t,x(t))-f(t,y(t))|dt
                \end{align*}
                But from the Lipschitz continuity
                of $f$, we have:
                \begin{align*}
                    d(Tx,Ty)&\leq
                    L\int_{a}^{t}|x(t)-y(t)|dt\\
                    &\leq{L}(t-a)d(x,y)\\
                    &\leq{L}(b-a)d(x,y)
                \end{align*}
                So $T$ is a contraction for
                $L(b-a)<1$. Usually we can
                extend this solution by taking
                $b$ as the initial condition and
                stepping forward one interval
                at a time. We'll take a different
                approach. We have that
                $d(Tx,Ty)\leq{L}(b-a)d(x,y)$. From
                this, we obtain:
                \begin{align*}
                    d(T^{2}x,T^{2}y)
                    &\leq{L}\int_{a}^{b}d(Tx,Ty)dt\\
                    &\leq{L}\int_{a}^{t}
                        L(t-a)d(x,t)dt\\
                    &=\frac{L^{2}}{2}(t-a)^{2}d(x,y)\\
                    &\leq
                    \frac{L^{2}}{2}(b-a)^{2}d(x,y)
                \end{align*}
                Applying induction, we have:
                \begin{equation*}
                    d(T^{n}x,T^{n}y)
                    \leq\frac{L^{n}}{n!}(b-a)^{n}
                \end{equation*}
                But this tends to zero, and thus
                there is an $N$ such that,
                for all $n>N$, $T^{n}$ is a
                contraction. But then, by the
                Banach Fixed Point Theorem, there
                is a unique point $x$ such that
                $T^{n}x=x$. But then
                $Tx=T^{n}(Tx)$, and thus
                $Tx$ is a fixed point of
                $T^{n}$. But the fixed point of
                $T^{n}$ is unique, and $x$ is a
                fixed point. Therefore
                $Tx=x$. Therefore, etc.
            \end{proof}
            Without Lipschitz continuous you may
            lose uniqueness, but you still have
            existence. This is Peano's theorem.
            An example is $\dot{x}=\sqrt{x}$
            with $x(0)=0$.
            This has solutions $x(t)=0$ and
            $(t)=t^{2}/4$. Now back to compactness.
            \subsubsection{Compactness}
                \begin{definition}
                    A metric space $(X,d)$ is
                    sequentially compact if every
                    sequence in $X$ has a convergent
                    subsequence.
                \end{definition}
                In topology there is a difference
                between sequential compactness
                and regular compactness, but in
                metric spaces they turn out
                to be the same.
                A subset of $S$ of $X$ is
                compact if every sequence in
                $S$ has a subsequence which converges.
                That is, $(S,d)$ is compact.
                \begin{theorem}
                    A subset $S$ of a compact
                    metric space $(X,d)$ is compact
                    if and only if $S$ is closed.
                \end{theorem}
                \begin{proof}
                    For let $x_{n}$ be a sequence
                    in $S$. Then $x_{n}$ is a
                    sequence in $X$ and thus there
                    is a convergent subsequence
                    $x_{k_{n}}$ with a limit $x$.
                    But $x_{k_{n}}$ is in $S$ and
                    $S$ is closed, and therefore
                    $x$ is in $S$. Thus, $S$
                    is compact. Conversely, if
                    $S$ is compact, suppose it is
                    not closed. Then there is a point
                    $y\in{X}$ such that $y$ is a
                    limit point of $S$ but not
                    contained in $S$. Let
                    $x_{n}$ be a sequence that
                    converges to $y$. Then, as
                    $S$ is compact, there is
                    a convergent subsequence. But
                    the limit of this subsequence
                    is $y$, a contradiction as
                    $y\notin{S}$. Therefore $S$
                    is closed.
                \end{proof}
                \begin{theorem}
                    If $(X,d)$ is a compact metric
                    space, then
                    $(X,d)$ is complete.
                \end{theorem}
                \begin{proof}
                    If $x_{n}$ is Cauchy in $X$,
                    then there is a convergent
                    subsequence $x_{k_{n}}$
                    in $X$. But if $x_{k_{n}}$
                    converges to $x$, then
                    $x_{n}$ converges to $x$ as
                    well, as $x_{n}$ is Cauchy.
                    Therefore, $(X,d)$ is complete.
                \end{proof}
                \begin{theorem}[Heine-Borel Theorem]
                    A subset of
                    $\mathbb{R}^{n}$ is
                    compact if and only if
                    it is closed and bounded.
                \end{theorem}
                \begin{example}
                    The closed unit ball
                    of $\ell^{p}$ is not compact,
                    if $1\leq{p}\leq{\infty}$.
                    Let $x_{n}(m)$ be the sequence
                    (of sequences) such that
                    $x_{n}(m)=1$ if $n=m$, and
                    zero otherwise. Then
                    $d_{p}(x_{n},x_{m})=2^{1/p}$,
                    so $x_{n}$ has no subsequence
                    which is Cauchy. But then there
                    is no convergent subsequence
                    either, and therefore
                    $\ell^{p}$ is not compact.
                \end{example}
                \begin{example}
                    The closed unit ball in
                    $(C[0,1],d_{\infty})$ is
                    not compact. For let
                    $x_{n}(t)=t^{2^{n}}$. Then
                    (Do some calculus) the maximum of
                    $d(x_{n},x_{n+1})$ is always
                    $1/4$. So this has no subsequence
                    which is Cauchy, and thus no
                    convergent subsequence exists.
                \end{example}
                \begin{definition}
                    A metric space $X$ is totally
                    bounded if for all
                    $\varepsilon>0$ there is a finite
                    number of points $x_{n}$ such
                    that $B_{\varepsilon}(x_{n})$
                    covers the entirety of $X$.
                \end{definition}
                \begin{theorem}
                    A compact metric space is
                    totally bounded.
                \end{theorem}
                \begin{proof}
                    Suppose not. Then there is an
                    $\varepsilon>0$ such that
                    no finite collection
                    $B_{\varepsilon}(x_{n})$
                    is a covering of $X$. Let
                    $x_{1}\in{X}$. Then
                    $B_{\varepsilon}(x_{1})$ is not
                    $X$. Thus there is an $x_{2}$
                    such that
                    $x_{2}\notin%
                     B_{\varepsilon}(x_{1})$.
                    But also
                    $B_{\varepsilon}(x_{1})\cup%
                     B_{\varepsilon}(x_{2})$ is
                    not the entirety of $X$.
                    Continuing we have that there
                    is a sequence $x_{n}$ such that,
                    for all $n\ne{m}$,
                    $d(x_{n},x_{m})\geq{\varepsilon}$.
                    So there is no convergent
                    subsequence. But $X$ is
                    compact, a contradiction.
                    Therefore, etc.
                \end{proof}
                There are metric spaces that are
                bounded but not totally bounded.
                For let
                $X=\mathbb{R}$ and $d$ be the
                discrete metric. Then, for
                $\varepsilon=1/2$, the is no
                finite covering. Every point needs
                it's own ball, so the covering is
                uncountable.
                \begin{theorem}
                    If $(X,d)$ is complete and
                    totally bounded, then it
                    is compact.
                \end{theorem}
                \begin{proof}
                    Let $x_{n}$ be a sequence
                    in $X$. Let $\varepsilon=1$. Then
                    there are finitely many points
                    $y_{k}$ such that
                    $B_{\varepsilon}(y_{k})$ covers
                    $X$. Then one of these
                    balls has infinitely many of
                    the $x_{n}$. Similarly, for
                    $\varepsilon=\frac{1}{n}$, there
                    is a finite number of points
                    $y_{k}$ such that
                    $B_{\frac{1}{n}}(y_{k})$ covers
                    $X$. Thus there is a point with
                    infinitely many of the $x_{n}$
                    in it. So, we can find a
                    subsequence such that, for
                    $n,m>N$,
                    $d(x_{k_{n}},x_{k_{m}})<%
                     \frac{1}{N}$. But $(X,d)$ is
                    complete, and therefore
                    $x_{k_{n}}$ converges. Therefore
                    $x_{n}$ has a convergent
                    subsequence. Thus, $(X,d)$ is
                    compact.
                \end{proof}
                \begin{theorem}
                    Compact spaces are separable.
                \end{theorem}
                \begin{proof}
                    If $X$ is compact, then
                    it is totally bounded. But
                    then, for $\varepsilon=1/n$
                    there is a finite covering of
                    $X$ with balls of radius
                    $\varepsilon$. Then,
                    taking all of the
                    centers of all of the points
                    for all $n$ (Countable union
                    of finite points is countable),
                    we obtain a countable dense
                    subset.
                \end{proof}
                \begin{example}
                    There are ``infinite dimension''
                    sets that are also compact. Two
                    in particular worth mentioning.
                    The first is the hilbert Cube.
                    It's a subset of $\ell^{2}$
                    whose elements are such that
                    $|x_{n}|<1/n$. That is, elements
                    are sequences whose $n^{th}$
                    elements are less than
                    $1/n$. This is compact.
                    Arzela-Ascoli. Peano.
                \end{example}
    \section{Normed Spaces and Inner Product Spaces}
        \subsection{Basic Definitions}
            We're finally going to put some structure on these
            sets, and talk about vector spaces. In a metric
            space, the only thing you can really talk about
            is the distance between points. In a vector space
            we have a lot more structure. We will start off
            with vector spaces over the reals $\mathbb{R}$.
            The main properties are that there is a
            $\mathbf{0}$ element, addition is well defined
            and is both associative and commutative,
            there is a notion of scalar multiplication that
            is associative, and the distributive law holds.
            \begin{example}
                $\mathbb{R}^{n}$, with it's usual notion
                of addition, and with scalar multiplication
                defined over $\mathbb{R}$, is a vector space.
            \end{example}
            \begin{definition}
                A norm on a vector space $X$ over $\mathbb{R}$
                is a function $\norm{}:X\rightarrow\mathbb{R}$
                such that:
                \begin{enumerate}
                    \item For all $\mathbf{x}\in{X}$,
                          $\norm{\mathbf{x}}\geq{0}$ and
                          $\norm{\mathbf{x}}=0$ if and only
                          if $\mathbf{x}=\mathbf{0}$.
                          \hfill[Positive Definiteness]
                    \item For all $\mathbf{x}\in{X}$ and
                          $c\in\mathbb{R}$,
                          $\norm{c\mathbf{x}}%
                           =|c|\norm{\mathbf{x}}$
                          \hfill[Homogeneity]
                    \item For all $\mathbf{x},\mathbf{y}\in{X}$,
                          $\norm{\mathbf{x}+\mathbf{y}}%
                           \leq\norm{\mathbf{x}}%
                           +\norm{\mathbf{y}}$
                          \hfill[Triangle Inequality]
                \end{enumerate}
            \end{definition}
            We have seen before that
            $d(\mathbf{x},\mathbf{y})%
             =\norm{\mathbf{x}-\mathbf{y}}$
            defines a metric, and thus $(X,d)$ is a metric space.
            Thus, for every vector space there is an associated
            metric space, the metric $d$ called the
            \textit{induced} metric.
            \begin{definition}
                A normed vector space is a vector space
                $X$ over $\mathbb{R}$ with a norm
                $\norm{}$ on $X$.
            \end{definition}
            \begin{example}
                $\mathbb{R}^{n}$ with
                $\norm{\mathbf{x}}_{p}$, for $p\geq{1}$,
                is a normed vector space.
            \end{example}
            \begin{example}
                $\ell^{p}$ with $\norm{x}_{p}$ is
                also a normed vector space.
            \end{example}
            \begin{example}
                $C[a,b]$ equipped with the supremum norm,
                $\norm{x(t)}_{\infty}$,
                is a normed vector space.
            \end{example}
        \subsubsection{Inner Product Spaces}
            \begin{definition}
                An inner product on a vector space
                $X$ over $\mathbb{R}$ is a function
                $\langle\rangle:X\rightarrow\mathbb{R}$
                such that:
                \begin{enumerate}
                    \item For all $x\in{X}$,
                          $\langle{\mathbf{x},\mathbf{x}}%
                           \rangle\geq{0}$
                          and
                          $\langle\mathbf{x},\mathbf{x}\rangle=0$
                          if and only
                          if $\mathbf{x}=\mathbf{0}$.
                          \hfill[Positive Definiteness]
                    \item For all $\mathbf{x},\mathbf{y}\in{X}$,
                          $\langle\mathbf{x},\mathbf{y}\rangle%
                           =\langle\mathbf{y},\mathbf{x}\rangle$
                          \hfill[Symmetry]
                    \item For all
                          $\mathbf{x},\mathbf{y},\mathbf{z}%
                           \in{X}$
                          and all $\alpha,\beta\in\mathbb{R}$,
                          $\langle\alpha\mathbf{x}%
                           +\beta\mathbf{y},\mathbf{z}\rangle%
                           =\alpha\langle\mathbf{x},\mathbf{z}%
                           \rangle+\beta\langle\mathbf{y},%
                           \mathbf{z}\rangle$
                          \hfill[Linearity]
                \end{enumerate}
            \end{definition}
            \begin{example}
                $\mathbb{R}^{2}$ with
                $\langle(x_{1},x_{2}),(y_{1},y_{2})\rangle%
                 =x_{1}y_{1}+x_{2}y_{2}$ is an inner product.
                Replacing this with $\mathbb{R}^{n}$ and doing
                $\sum_{k=1}^{n}x_{k}y_{k}$ is also an inner
                product. This is the usual dot product that one
                sees in a vector calculus course. In $\ell^{2}$,
                $\sum_{k=1}^{\infty}x_{k}y_{k}$ is an inner
                product as well. Note also that
                $\sum|x_{i}y_{i}|$ converges since
                $|x_{i}y_{i}|\leq\frac{1}{2}|x_{i}^{2}|%
                 +\frac{1}{2}|y_{i}|^{2}$.
            \end{example}
            \begin{example}
                In $C[a,b]$, let
                $\langle{x(t),y(t)}\rangle%
                 =\int_{a}^{b}x(t)y(t)dt$. This defines an
                inner product.
            \end{example}
            \begin{definition}
                An inner product space is a vector space
                $X$ over $\mathbb{R}$ with an inner product
                $\langle\rangle$.
            \end{definition}
            \begin{theorem}[Cauchy-Schwarz Inequality]
                If $X$ is an inner product space
                and $x,y\in{X}$, then
                $|\langle{x,y}\rangle<\norm{x}\norm{y}$
            \end{theorem}
            \begin{proof}
                For all $y\in\mathbb{R}$,
                $\langle{x+ty,x+ty}\rangle%
                 =\langle{x,x}\rangle%
                 +2t\langle{x,y}\rangle%
                 +t^{2}\langle{y,y}\rangle%
                 =\norm{x}^{2}+2t\langle{x,y}\rangle%
                 +t^{2}\norm{y}^{2}$. Thus we have a
                quadratic in $t$. But this is always positive,
                and thus the discriminant must be non-positive. Therefore
                $(2\langle{x,y})^{2}-4\norm{x}^{2}\norm{y}^{2}%
                 \leq{0}$
                and thus
                $|\langle{x,y})|\leq\norm{x}\norm{y}$.
            \end{proof}
            \begin{theorem}
                If $X$ is a vector space over $\mathbb{R}$
                and $\langle\rangle$ is an inner product,
                then
                $\norm{\mathbf{x}}%
                 =\sqrt{\langle\mathbf{x},\mathbf{y}\rangle}$
                is a norm on $X$.
            \end{theorem}
            \begin{proof}
                Positivity, homogeneity, and definiteness are
                pretty easy. The only tricky thing to check is
                the triangle inequality. We have that
                $\norm{x+y}=\langle{x+y,x+y}\rangle$,
                and this simplify to
                $\norm{x}^{2}+2\langle{x,y}\rangle+\norm{y}^{2}$.
                But from the Cauchy-Schwartz inequality, we
                have $\langle{x,y}\rangle\leq\norm{x}\norm{y}$.
                Thus
                $\norm{x+y}^{2}\leq\norm{x}^{2}%
                 +2\norm{x}\norm{y}+\norm{y}^{2}%
                 =(\norm{x}+\norm{y})^{2}$. Taking square roots
                 completes the theorem.
            \end{proof}
            In $\mathbb{R}^{n}$, the Cauchy-Schwartz inequality
            says that the dot product of two vectors is less
            than or equal to the product of the magnitude
            of the two vectors.
            This is obvious from the fact that the dot product
            of two vector is the product of the magnitudes and
            the \textit{cosine} of the angle between them.
            Since the cosine of a number is less than or equal
            to one, this would complete the theorem.
            In $\ell^{p}$ and $L^{p}$ spaces, this is the
            special case of the H\"{o}lder inequality for
            when $p=q=2$.
        \subsubsection{Convergence in Normed Spaces}
            In a metric space, convergence meant that
            $d(x_{n},x)\rightarrow{0}$. In a normed space
            we have the induced metric, and thus we may define
            convergence as $\norm{x_{n}-x}\rightarrow{0}$.
            \begin{definition}
                A convergent sequence in a normed space $X$
                is a sequence $x_{n}$ such that there is an
                $x\in{X}$ such that
                $\norm{x_{n}-x}\rightarrow{0}$.
            \end{definition}
            Since
            $\norm{y}=\norm{(y-x)+x}\leq\norm{y-x}+\norm{x}$,
            it follows that
            $|\norm{x}-\norm{y}|\leq\norm{x-y}$.
            But then if $x_{n}\rightarrow{x}$, then
            $|\norm{x_{n}}-\norm{x}|\leq\norm{x_{n}-x}$,
            and $\norm{x_{n}-x}\rightarrow{0}$. Therefore
            $\norm{x_{n}}\rightarrow\norm{x}$. That is,
            the norm function is a continuous function.
            Similarly, if $x_{n}\rightarrow{x}$, then
            $\langle{x_{n},y}\rangle\rightarrow%
             \langle{x,y}\rangle$.
            In fact, if $x_{n}\rightarrow{x}$ and
            $y_{n}\rightarrow{y}$, then
            $\langle{x_{n},y_{n}}\rangle%
             \rightarrow\langle{x,y}\rangle$. To see this, we
            have
            $\langle{x_{n},y_{n}}\rangle-\langle{x,y}\rangle%
             =\langle{x_{n}-x,y}\rangle+\langle{x,y-y_{n}}\rangle$
            and therefore
            $|\langle{x_{n},y_{n}}\rangle-\langle{x,y}\rangle%
             \leq\norm{x_{n}-x}\norm{y_{n}}%
             +\norm{x}\norm{y-y_{n}}$. But $\norm{x-x_{n}}\rightarrow{0}$
            and $\norm{y-y_{n}}\rightarrow{0}$. But also
            $\norm{y_{n}}=\norm{(y_{n}-y)+y}\leq\norm{y_{n}-y}+\norm{y}$,
            which is bounded. Therefore
            $\langle{x_{n},y_{n}}-\langle{x,y}\rangle\rightarrow{0}$.
            So inner product spaces and normed spaces are metric spaces
            and we can define everything we did for metric spaces and all
            of the previous results remain true. That is, the notions and
            theorems pertaining to convergence, completeness, compactness,
            the notion of open and closed. All of these still make sense in
            these new spaces.
        \subsubsection{Banach Spaces and Hilbert Spaces}
            \begin{definition}
                A Banach Space is a normed vector space $X$ that is
                complete with respect to the induced metric.
            \end{definition}
            \begin{definition}
                A Hilbert Space is an inner product space $X$ that is
                complete with respect to the induced metric.
            \end{definition}
        \subsubsection{Linear Operators}
            Let $X$ and $Y$ be normed spaces. A mapping
            $T:X\rightarrow{Y}$ is called a linear operator if, for
            all $x,y\in{X}$, and for all $\alpha,\beta\in\mathbb{R}$,
            $T(\alpha{x}+\beta{y})=\alpha{T(x)}+\beta{T(y)}$. Usually, with
            operators, we simply write $Tx$ and $Ty$. Similar to how
            we write matric multiplication over vectors. In $\mathbb{R}^{n}$,
            every $n\times{n}$ matrix defines a linear operator.
            \begin{definition}
                A linear operator from a normed vector space $X$ to
                a normed vector space $Y$ is a function
                $T:X\rightarrow{Y}$ such that, for all $x,y\in{X}$
                and for all $\alpha,\beta\in\mathbb{R}$,
                $T(\alpha{x}+\beta{y})=\alpha{Tx}+\beta{Ty}$.
            \end{definition}
            \begin{definition}
                A bounded linear operator from a normed vector space
                $X$ to a normed vector space $Y$ is a linear operator
                $T:X\rightarrow{Y}$ such that there is a $K\in\mathbb{R}$
                such that for all $x\in{X}$, $\norm{Tx}\leq{K}\norm{x}$
            \end{definition}
            In a just world, ``bounded'' would mean
            $\norm{Tx}\leq{K}$. However, the only linear mapping that does
            this is the zero mapping. For if $\norm{Tx}=1$,
            then $\norm{T(2x)}=2$, and so on, and thus no linear mapping
            is bounded (With the exception of the zero mapping).
            Boundedness of a norm $T:X\rightarrow{Y}$ depends on
            the norms of the space.
            \begin{theorem}
                Bounded linear operators are continuous.
            \end{theorem}
            \begin{proof}
                If $x_{n}\rightarrow{x}$, then
                $\norm{Tx_{n}-Tx}=\norm{T(x_{n}-x)}$. But
                $T$ is bounded, and thus there is a $K$ such that
                $\norm{T(x_{n}-x)}\leq{K}\norm{x_{n}-x}$. But
                $\norm{x_{n}-x}\rightarrow{0}$. Therefore, etc.
            \end{proof}
            The converse is also true.
            \begin{theorem}
                If $T$ is a continuous linear operator,
                than there exists a $\delta>0$ such that for
                all $x\in{B}_{\delta}(0)$,
                $\norm{Tx-T0}<1$. But from linearity,
                $T0=0$, and thus $\norm{Tx}<1$. Then for any
                $z\in{Z}$, we have
                $\norm{\frac{\delta}{2}\frac{z}{\norm{z}}}=\frac{\delta}{2}$,
                and thus $\norm{T(\frac{\delta}{2}\frac{z}{\norm{z}})}<1$.
                Letting $K=\delta$, we have
                $\norm{Tx}<K\norm{x}$. Thus, $T$ is bounded.
            \end{theorem}
            Continuity at 0 implies uniform continuity since
            if $x_{n}-y_{n}\rightarrow{0}$, then
            $\norm{Tx_{n}-Ty_{n}}=\norm{T(x_{n}-y_{n})}%
             \leq{K}\norm{x_{n}-y_{n}}\rightarrow{0}$.
            The set of bounded linear operators form a vector space,
            where addition is $(S+t)(x)=(Sx)+(Tx)$, and scalar multiplication
            is defined by $(\alpha{T})(x)=\alpha(Tx)$. We must show that
            when you add two bounded linear operators, the result is a
            bounded linear operator.
            \begin{theorem}
                If $T_{1}:X\rightarrow{Y}$ and $T_{2}:X\rightarrow{Y}$
                are bounded linear operators, then $T_{1}+T_{2}$ is a
                bounded linear operator.
            \end{theorem}
            \begin{proof}
                For let $T_{1}$ and $T_{2}$ be bounded. Then there are
                $K_{1},K_{2}$ such that, for all $x\in{X}$,
                $\norm{T_{1}x}\leq{K_{1}}\norm{x}$ and
                $\norm{T_{2}x}\leq{K_{2}}\norm{x}$. But then
                $\norm{(T_{1}+T_{2})x}=\norm{T_{1}x+T_{2}x}%
                 \leq\norm{T_{1}x}+\norm{T_{2}x}%
                 \leq{K_{1}}\norm{x}+K_{2}\norm{x}$. Let $K=K_{1}+K_{2}$.
            \end{proof}
            \begin{theorem}
                If $T:X\rightarrow{Y}$ is a bounded linear operator, and
                $\alpha\in\mathbb{R}$, then $\alpha{T}$ is a bounded
                linear operator.
            \end{theorem}
            \begin{proof}
                For
                $\norm{\alpha{Tx}}=|\alpha|\norm{Tx}%
                 \leq|\alpha|K\norm{x}=K\norm{\alpha{x}}$.
            \end{proof}
            We write $B(X,Y)$ to denote the set of bounded linear
            operators from $X$ to $Y$. That is, linear operators
            $T:X\rightarrow{Y}$.
            We can define a norm on $B(X,Y)$ as follows:
            $\norm{T}_{B}%
             =\sup_{x\in{X},x\ne{0}}\{\frac{\norm{Tx}}{\norm{x}}\}$.
            This is the ``Smallest $K$,'' used as a bounded for the linear
            operator $T$. This shows that
            $\norm{Tx}_{Y}\leq\norm{T}_{B}\norm{x}_{X}$.
        \subsection{Lecture 7: October 22, 2018}
            \subsubsection{Bounded Linear Operators}
                A bounded linear operator is a function
                $T:X\rightarrow{Y}$ between normed spaces
                $X$ and $Y$ such that $T$ is linear, and
                there exists a $K\in\mathbb{R}$ such that,
                for all $x\in{X}$,
                $\norm{Tx}_{Y}\leq{K}\norm{x}_{X}$. The
                norm of $T$, $\norm{T}$, is then defined
                as the smallest such $K$. Equivalently:
                \begin{equation*}
                    \norm{T}=
                    \sup\Big\{\frac{\norm{Tx}_{Y}}{\norm{x}_{X}}:
                              x\in{X},x\ne{0}\Big\}
                    =\sup\{\norm{Tx}_{Y}:\norm{x}_{X}=1\}
                \end{equation*}
                The set of all bounded linear operators
                from a normed space $X$ to a normed space
                $Y$ is denoted $B(X,Y)$. This is a vector
                space with addition defined as
                $(T+S)x=(Tx)+(Sx)$ and $(aT)x=a(Tx)$.
                \begin{theorem}
                    $\norm{T}$ defines a norm on
                    $B(X,Y)$.
                \end{theorem}
                \begin{proof}
                    For $\norm{T}\geq{0}$ and
                    $\norm{Tx}=0$ if and only if
                    $Tx=0$ for all $x\in{X}$, and thus
                    $T$ is the zero operator. If
                    $\alpha\in\mathbb{R}$, then:
                    \begin{align*}
                        \norm{\alpha{T}}
                        &=\sup\Big\{
                            \frac{\norm{\alpha{T}x}_{Y}}{\norm{x}_{X}}:
                            x\in{X},x\ne{0}\Big\}\\
                        &=|\alpha|\sup\Big\{
                            \frac{\norm{Tx}_{Y}}{\norm{x}_{X}}:
                            x\in{X},x\ne{0}\Big\}\\
                        &=|\alpha|\norm{T}
                    \end{align*}
                    Finally, if $S,T\in{B}(X,Y)$, then:
                    \begin{align*}
                        \norm{S+T}&=\sup\Big\{
                            \frac{\norm{(S+t)x}_{Y}}{\norm{x}_{X}}:
                            x\in{X},x\ne{0}\}\\
                        &=\sup\Big\{
                            \frac{\norm{Sx+Tx}_{Y}}{\norm{x}_{X}}:
                            x\in{X},x\ne{0}\Big\}\\
                        &\leq\sup\Big\{
                            \frac{\norm{Sx}_{y}+\norm{Tx}_{Y}}
                                 {\norm{x}_{X}}:
                            x\in{X},x\ne{0}\Big\}\\
                        &\leq\norm{T}+\norm{S}
                    \end{align*}
                \end{proof}
                \begin{theorem}
                    If $Y$ is a Banach space, and 
                    if $X$ is a normed space, then
                    $B(X,Y)$ is a Banach space.
                \end{theorem}
                \begin{proof}
                    For let $T_{n}$ be a Cauchy sequence
                    in $B(X,Y)$ and let $\varepsilon>0$.
                    Then there exists $N_{0}\in\mathbb{N}$
                    such that for all $n,m>N_{0}$,
                    $\norm{T_{n}-T_{m}}<\varepsilon$. That is,
                    for all $n,m>N_{0}$:
                    \begin{align*}
                        \sup\Big\{
                            \frac{\norm{T_{n}x-T_{m}x}_{Y}}
                                 {\norm{x}_{X}}:
                            x\in{X},x\ne{0}\Big\}
                        &\leq\varepsilon\\
                        \Rightarrow
                        \frac{\norm{T_{n}x-T_{m}y}_{Y}}
                             {\norm{x}_{X}}
                        &\leq\varepsilon
                    \end{align*}
                    That is, $T_{n}x$ is a Cauchy sequence
                    in $Y$ for any fixed value $x\in{X}$.
                    But $Y$ is a Banach space, and is therefore
                    complete. But then if $T_{n}x$ is a Cauchy
                    sequence in $Y$ it has a limit $y\in{Y}$.
                    Let $Tx=\lim_{n\rightarrow\infty}T_{n}x$
                    for all $x\in{X}$.
                    Then $T\in{B(X,Y)}$. For:
                    \begin{equation*}
                        T(x+y)
                        =\lim_{n\rightarrow\infty}T_{n}(x+y)
                        =\lim_{n\rightarrow\infty}(T_{n}x+T_{n}y)
                        =Tx+Ty
                    \end{equation*}
                    And similarly $(\alpha{T})x=\alpha{T}x$.
                    Lastly, $T$ is bounded. For all $n,m>N$ we have
                    $\norm{T_{n}x-T_{m}x}_{Y}/\norm{x}_{X}<\varepsilon$.
                    Taking the limit on $m$, we have
                    $\norm{Tx-T_{n}x}_{Y}/\norm{x}_{X}\leq\varepsilon$
                    for all $n>N_{0}$. Thus,
                    $\norm{T_{n}x-Tx}_{X}\leq\varepsilon\norm{x}_{X}$.
                    But
                    $\norm{Tx-T_{n}x}_{Y}=\norm{T_{n}x-(T_{n}-Tx)}_{Y}$,
                    and therefore
                    $\norm{Tx}\leq\varepsilon\norm{x}_{X}+\norm{T_{n}x}$,
                    and $\norm{T_{n}x}\leq\norm{T_{n}}$, and therefore
                    $\norm{Tx}\leq\varepsilon\norm{X}_{X}+%
                     \norm{T}\norm{x}_{X}$. But then
                    $\norm{Tx}_{Y}\leq%
                     (\varepsilon+\norm{T_{n}})\norm{x}_{X}$.
                    But $T_{n}$ is bounded, and therefore
                    $T$ is bounded. Finally, we must show that
                    $T_{n}\rightarrow{T}$ in $B(X,Y)$ with respect
                    to the norm $\norm{T_{n}-T}$. That is, we must
                    show that $\norm{T-T_{n}}\rightarrow{0}$. This
                    follows since
                    $\norm{Tx-T_{n}x}_{Y}/\norm{x}_{X}<\varepsilon$
                    for $n>N_{0}$, and therefore
                    $\norm{T-T_{n}}<\varepsilon$. Therefore, etc.
                \end{proof}
            \subsubsection{Dual Spaces}
                So if $Y$ is a Banach space, and $X$ is any normed
                space, then $B(X,Y)$ is a Banach space. One of the
                most important cases is $Y=(\mathbb{R},||)$, where
                $||$ is the normal absolute value ``norm.''
                $B(X,\mathbb{R})$ is a Banach space, and it is
                called the continuous dual space of $X$, written
                $X'$. Elements of $X'$ are called bounded linear
                functionals. These are bounded linear operators
                whose range of the operator is the real numbers.
                The characterization, or the representation, or
                realization, of these dual spaces is a major
                topic in functional analysis. A lot of these
                theorems are do to a mathematician by the name
                of Riesz.
                \begin{example}
                    A functional takes an element of a normed
                    space $X$ and spits out a real number. For
                    example, if $X$ is the space of continuous
                    functions, then the following are
                    functionals:
                    \begin{align*}
                        f_{1}(x)&=\int_{0}^{1}x(t)t^{2}\diff{t}
                        &
                        f_{2}(x)&=x(0.5)
                        &
                        f_{3}(x)&=0
                    \end{align*}
                \end{example}
                Let $X=(\mathbb{R}^{2},\ell^{1}$. What does
                $X'$ look like? That is, what is the dual
                space of $X$? let $f:X\rightarrow\mathbb{R}$
                be defined by
                $f(x_{1},x_{2})=2x_{1}-5x_{2}$. Then
                $f\in{X'}$ and $\norm{f}=5$. More generally,
                every element of $\mathbb{R}^{2}$ defines
                and element of $X'$. Given
                $(a,b)\in\mathbb{R}^{2}$, we define
                $f(x_{1},x_{2})=ax_{1}+bx_{2}$. $f$ is then linear,
                and:
                \begin{align*}
                    |f(x_{1},x_{2})|
                    &=|ax_{1}+bx_{2}|\\
                    &\leq|a||x_{1}|+|b||x_{2}|\\
                    &\leq\max\{|a|,|b|\}(|x_{1}|+|x_{2}|)\\
                    &=\norm{(a,b)}_{\infty}
                    \norm{(x_{1},x_{2})}_{\ell^{1}}
                \end{align*}
                And therefore $f$ is bounded, as $\norm{(a,b)}_{\infty}$
                is a bound. That is, $\norm{f}\leq\norm{(a,b)}_{\infty}$.
                By choosing $x=(x_{1},x_{2})$, where $x_{1}=1$ and
                $x_{2}=0$ if $|b|\leq|a|$, and $x_{1}=0$ and
                $x_{2}=1$ otherwise, we ge
                $|f|=\max\{(a,b)\}=\norm{(a,b)}_{\infty}$.
                Therefore $\norm{f}=\norm{(a,b)}_{\infty}$. On
                the other hand, if $f\in{X'}$, let
                $a=f(1,0)$ and $b=f(0,1)$. Then, for all
                $(x_{1},x_{2})\in\mathbb{R}^{2}$:
                \begin{equation*}
                    f(x_{1},x_{2})
                    =f(x_{1}(1,0)+x_{2}(0,1))
                    =x_{1}f(1,0)+x_{2}f(0,1)
                    =ax_{1}+bx_{2}
                \end{equation*}
                So the dual of $(\mathbb{R}^{2},\ell^{1})$
                looks very much like $(\mathbb{R}^{2},\ell^{\infty})$.
                In fact, $(\mathbb{R}^{2},\ell^{1})'$ and
                $(\mathbb{R}^{2},\ell^{\infty})$ are isometric
                and isomorphic. That is, we really can't tell them
                apart and we can consider them as the same thing.
                More generally,
                $(\mathbb{R}^{n},\ell^{n})'=(\mathbb{R}^{n},\ell^{\infty})$.
                Even more general, if $p$ and $q$ are exponential
                conjugates of each other (That is,
                $\frac{1}{q}+\frac{1}{p}=1$), then
                $(\mathbb{R}^{n},\ell^{p})'=(\mathbb{R}^{n},\ell^{p})$
                for all $1\leq{p}\leq\infty$. Saying $p=\infty$ is
                equivalent to saying $q=1$. Setting $p=q=2$, we have
                $(\mathbb{R}^{n},\ell^{2})'=(\mathbb{R}^{n},\ell^{2})$.
                This is true of any Hilbert space: The dual of any
                Hilbert Space $\mathcal{H}$ is itself. That is,
                $\mathcal{H}'=\mathcal{H}$. This is one of the
                Riesz Representation Theorems. In infinite dimensions,
                $(\ell^{p})'=\ell^{q}$, where $p$ and $q$ are such that
                $\frac{1}{p}+\frac{1}{q}=1$, and $1\leq{p}<\infty$.
                Now, we cannot allow $p=\infty$. For
                $(\ell^{\infty})'$ is not equal to $\ell^{1}$.
                \begin{theorem}
                    If $1\leq{p}<\infty$ and
                    $\frac{1}{p}+\frac{1}{q}=1$, then
                    $(\ell^{p})'=\ell^{q}$.
                \end{theorem}
                \begin{proof}
                    If $(f_{1},f_{2},\hdots)\in\ell^{q}$, then let
                    $f:\ell^{p}\rightarrow\mathbb{R}$ be defined by
                    $f(x_{1},x_{2},\hdots)=\sum_{k=1}^{\infty}x_{k}f_{k}$.
                    This converges from H\"{o}lder's inequality:
                    \begin{equation*}
                        \sum_{k=1}^{\infty}|x_{k}f_{k}|
                        \leq
                        \Big(\sum_{k=1}^{\infty}f_{k}^{q}\Big)^{1/q}
                        \Big(\sum_{k=1}^{\infty}x_{i}^{p}\Big)^{1/p}
                    \end{equation*}
                    And therefore
                    $|fx|=\norm{(f_{1},f_{2},\hdots)}_{q}%
                     \norm{(x_{1},x_{2},\hdots)}_{p}$. That is,
                    Moreover $f$ is linear. Therefore
                    $f\in(\ell^{p})'$ and
                    $\norm{f}\leq\norm{(f_{1},f_{2},\hdots)}_{q}$.
                    On the other hand, let
                    $x_{i}=|f_{i}|^{q/p}\sgn(f_{i}$. Then
                    \begin{equation*}
                        fx=\sum_{k=1}^{\infty}f_{k}x_{k}
                        =\sum_{k=1}^{\infty}|f_{k}|^{q/p+1}
                    \end{equation*}
                    But $\frac{1}{p}+\frac{1}{q}=1$, and thus
                    $\frac{q}{p}+1=q$. Thus:
                    \begin{align*}
                        |fx|
                        &=\sum_{k=1}^{\infty}|f_{k}|^{q}\\
                        &=\norm{(f_{1},f_{2},\hdots)}_{q}^{q}\\
                        &=\norm{(f_{1},f_{2},\hdots)}_{q}
                            \norm{(f_{1},f_{2},\hdots)}_{q}^{q-1}\\
                        &=\norm{(f_{1},f_{2},\hdots)}_{q}
                            \Big(\sum_{k=1}^{\infty}|f_{k}^{q}
                            \Big)^{\frac{q-1}{q}}
                        &=\norm{(f_{1},f_{2},\hdots)}_{q}
                            \Big(\sum_{k=1}^{\infty}|x_{k}|^{p}
                            \Big)^{1/p}\\
                        &=\norm{(f_{1},f_{2},\hdots)}_{q}\norm{x}_{p}
                    \end{align*}
                    Therefore, $\norm{f}=\norm{(f_{1},f_{2},\hdots)}_{q}$.
                    Thus, for all $y\in\ell^{q}$ there is a bounded
                    linear operator $f\in\ell^{p}$ such that
                    $\norm{y}_{\ell^{q}}=\norm{f}_{(\ell^{p})'}$. That
                    is, every $(f_{i})\in\ell^{q}$ defines an element
                    of $(\ell^{p})'$ by
                    $fx=\sum_{k=1}^{\infty}f_{k}x_{k}$, for any
                    $(x_{i})\in\ell^{p}$. So $\ell^{q}$ can
                    be \textit{embedded} into to $(\ell^{p})'$.
                    Now we need to show that this embedding is
                    the entirety of $(\ell^{p})'$. If
                    $f\in(\ell^{p})'$, let
                    $f_{i}=f(e_{i})$, where $e_{i}$ is the
                    sequence $(0,0,\hdots,1,0,0,\hdots)$, where
                    the 1 occurs in the $i^{th}$ spot. We need
                    to show that $(f_{i})\in\ell^{q}$ and
                    $fx=\sum_{k=1}^{\infty}f_{k}x_{k}$
                    for all $x\in\ell^{p}$. If $x\in\ell^{p}$, then:
                    \begin{align*}
                        x=\sum_{k=1}^{\infty}x_{k}e_{k}
                        \Rightarrow
                        fx=f\Big(\sum_{k=1}^{\infty}x_{k}e_{k}\Big)
                        =\sum_{k=1}^{\infty}f(x_{k}e_{k})
                        =\sum_{k=1}^{\infty}x_{k}f(x_{k})
                        =\sum_{k=1}^{\infty}x_{k}f_{k}
                    \end{align*}
                    Choosing $x_{k}=|f_{k}|^{q/p}\sgn(f_{k})$ and apply
                    H\"{o}lder.
                \end{proof}
        \subsection{Lecture 8: October 29, 2018}
            \subsubsection{Review}
                If $X$ is a vector space, an inner product on
                $X$ is a mapping
                $\langle,\rangle:X\times{X}\rightarrow\mathbb{R}$
                such that:
                \begin{enumerate}
                    \item $\langle{x,x}\rangle\geq{0}$ and
                          $\langle{x,x}\rangle=0$ if and only if
                          $x=0$
                    \item $\langle{x,y}\rangle=\langle{y,x}\rangle$
                    \item $\langle{ax+by,z}\rangle%
                           =a\langle{x,z}\rangle+b\langle{y,z}\rangle$
                \end{enumerate}
                Think of the dot product for vectors. This is a
                generalization of this concept. Every inner product
                on a vector space $V$ induce a norm on $V$:
                \begin{equation*}
                    \norm{x}=\sqrt{\norm{x,x}}
                \end{equation*}
                An inner product that is complete with respect to
                the induced norm is called a Hilbert Space. A mapping
                $f:X\rightarrow\mathbb{R}$ is bounded if there is a
                $K\in\mathbb{R}$ such that, for all $x\in{X}$,
                $|f(x)|\leq{L\norm{x}}$. $f$ is linear if
                $f(ax+by)=af(x)+bf(y)$, for all $x,y\in{X}$ and all
                $a,b\in\mathbb{R}$. The smallest such $K$ that works
                is called the norm of $f$, denoted $\norm{f}$. For
                all $x\in{X}$, $|f(x)|\leq\norm{f}\norm{x}$. The
                vector space of all bounded linear functionals on
                $X$ is the dual space $X'$. This is also a Banach
                space with the functional norm $\norm{f}$. One
                question that arises is, how do we know that there
                are bounded linear functionals on a space $X$? In
                the case that $X$ is a Hilbert space, this is rather
                easy, but for a more general Banach space this is not
                that trivial. For any normed space $X$ we can at least
                one bounded linear functional because the zero mapping
                $f(x)=0$ is such a functional. The question is then
                does every normed space have a bounded linear functional
                on it? The answer is yes, and this is related to
                the Hahn-Banach Theorem. As said before, in the
                Hilbert case this is rather easy.
                \begin{theorem}
                    If $X$ is a Hilbert space, then there is a
                    non-trivial bounded linear functional
                    $f:X\rightarrow\mathbb{R}$.
                \end{theorem}
                \begin{proof}
                    If $X$ is an inner product and
                    $z\in{X}$, let $f(x)=\langle{x,z}\rangle$ for
                    all $x\in{X}$. Then $f$ is linear since the
                    inner product is linear. But moreover, from
                    Cauchy-Schwarz we have:
                    \begin{equation*}
                        |f(x)|=|\langle{x,z}\rangle|
                        \leq\norm{x}\norm{z}
                    \end{equation*}
                    And thus $\norm{f}\leq\norm{z}$
                    But $|f(z)|=\norm{z}$, so
                    $\norm{f}=\norm{z}$. $f$ is a bounded
                    linear functional.
                \end{proof}
                Riesz's Representation theorem says that this is it.
                All bounded linear functionals look like this. Thus,
                if $H$ is a Hilbert space, it's dual $H'$ is the space
                of all functions that look like
                $f(x)=\langle{x,y}\rangle$ for some $y\in{X}$. More
                precisely, if $H$ is a Hilbert space and $f\in{H'}$,
                then there is a $y\in{H}$ such that, for all
                $x\in{X}$, $f(x)=\langle{x,y\rangle}$.
            \subsubsection{Riesz's Representation Theorem}
                \begin{theorem}
                    If $H$ is a Hilbert space, and
                    $f:H\rightarrow\mathbb{R}$ is a bounded
                    linear functional, then there is a unique
                    $y\in{H}$ such that, for all $x\in{X}$,
                    $f(x)=\langle{x,y}\rangle$. Moreover,
                    $\norm{f}=\norm{y}$.
                \end{theorem}
                \begin{proof}
                    Let $f\in{H'}$ and let $N=\nul(f)$. That is,
                    $N$ is the null space of the functional $f$
                    which is the set of all points
                    $x\in{X}$ such that $f(x)=0$. The Null space
                    actually defines a closed vector space, which
                    is a subspace of $H$. If $N=H$, then
                    $f(x)=0$, and thus let $y=0$. Otherwise, let $z$
                    be a non-zero elements such that,
                    for all $x\in{N}$, $\langle{x,y}\rangle=0$.
                    For all $x,z$, $f(x)z-f(z)x\in{N}$, for:
                    \begin{equation*}
                        f(f(x)z-f(z)x)
                        =f(f(x)z)-f(f(z)z)
                        =f(x)f(z)-f(z)f(z)=0
                    \end{equation*}
                    Therefore:
                    \begin{align*}
                        \langle{f(x)z-f(z)x,z}\rangle&=0\\
                        \Rightarrow
                        |f(x)|\norm{z}^{2}-|fz|\langle{x,y}\rangle&=0\\
                        \Rightarrow
                        f(x)
                        &=\langle{x,\frac{f(z)z}{\norm{z}^{2}}}\rangle
                    \end{align*}
                    Therefore, let $y=\frac{f(z)}{\norm{z}^{2}}z$.
                    This is unique since if for all $x\in{H}$,
                    $\langle{x,y_{1}}\rangle=\langle{x,y_{2}}\rangle$,
                    then $y_{1}=y_{2}$. Finally:
                    \begin{equation*}
                        \norm{y}
                        =\frac{|f(z)|}{\norm{z}^{2}}\norm{z}
                        =\frac{|f(z)|}{\norm{z}}
                        \leq\norm{f}
                    \end{equation*}
                    But also:
                    \begin{equation*}
                        |f(x)|=|\langle{x,z}\rangle|
                        \leq\norm{x}\norm{z}
                    \end{equation*}
                    Thus, $\norm{f}\leq\norm{z}$. But
                    $\norm{z}\leq\norm{f}$. Therefore,
                    $\norm{f}=\norm{z}$.
                \end{proof}
                Much like in $\mathbb{R}^{n}$, there is a notion of
                orthogonality in a general inner product space.
                \begin{definition}
                    Orthognal elements in an inner product space $X$
                    are elements $x,y\in{X}$ such that
                    $\langle{x,y}\rangle=0$.
                \end{definition}
                There's also a notion of convexity for a general
                vector space.
                \begin{definition}
                    A convex subset of a vector space $V$ space is a
                    subset $S\subset{V}$ such that, for all
                    $x,y\in{V}$ and for all $\lambda\in\mathbb{R}$,
                    $\lambda{x}+(1-\lambda)y\in{S}$.
                \end{definition}
                \begin{theorem}
                    If $S$ is a subset of $V$, then $S$ is convex.
                \end{theorem}
                Recall that for a general metric space $X$,
                if $S\subset{X}$, we defined
                $dist(x,S)=\inf\{d(x,s):s\in{S}\}$. We proved that,
                if $S$ is compact, then there is an $s\in{S}$ such
                that $dist(x,S)=d(x,s)$. We showed that, without
                compactness, this may not be true. Indeed, even
                complete spaces may lack this property. If
                $X$ is a Hilbert space, however, this property is
                guaranteed.
                \begin{theorem}
                    If $H$ is a Hilbert space and if $S\subset{H}$
                    is a closed convex subset of $H$, then there is
                    a unique $s\in{S}$ such that
                    $dist(x,S)=\norm{x-s}$.
                \end{theorem}
                \begin{proof}
                    As $dist(x,S)=\inf\{d(x,s):s\in{S}\}$, there is
                    a sequence $x_{n}\in{S}$ such that
                    $\norm{x-x_{n}}\rightarrow{dist(x,S)}$. Then, by
                    Appolonius:
                    \begin{align*}
                        \norm{x-x_{n}}^{2}+\norm{x-x_{m}}^{2}
                        &=\frac{1}{2}\norm{x_{n}-x_{m}}^{2}
                        +\frac{1}{2}
                        \norm{\frac{1}{2}(x_{n}+x_{m})-x}^{2}\\
                        &\geq\frac{1}{2}\norm{x_{n}+x_{m}}^{2}
                        +2dist(x,S)
                    \end{align*}
                    But $\norm{x-x_{n}}\rightarrow{dist(x,S)}$ and
                    $\norm{x-x_{m}}\rightarrow{dist(x,S)}$, so:
                    \begin{equation*}
                        \frac{1}{2}\norm{x_{n}-x_{m}}^{2}
                        \leq\norm{x-x-{m}}^{2}
                        +\norm{x-x_{m}}^{2}-2dist(x,S)
                    \end{equation*}
                    Which can be made arbitrarily small. Therefore,
                    $x_{n}$ is Cauchy. But $H$ is a Hilbert space, and
                    is therefore complete, and thus $x_{n}$ converges.
                    Let $s$ be the limit. As $S$ is closed, $s\in{S}$.
                    Moreover, from construction,
                    $\norm{x-s}=dist(x,S)$. If there is another point
                    $v$, then $\norm{x-s}=\norm{x-v}$. From
                    Appolonius:
                    \begin{equation*}
                        \norm{x-s}^{2}+\norm{x-v}^{2}
                        \geq\frac{1}{2}\norm{s-v}+2dist(x,S)^{2}
                    \end{equation*}
                    But $\norm{x-s}=\norm{x-v}=dist(x,S)$, and
                    thus $\norm{s-v}=0$. Therefore, $s=v$.
                \end{proof}
                Is $S$ is a closed subspace of $H$, then it's
                automatically convex. In this case, $x-s\perp{S}$,
                where $z\perp{S}$ means that, for all
                $s\in{S}$, $\langle{s,z}\rangle=0$. For if
                $z\in{S}$, then $s+tz\in{S}$ for all $t$. Thus:
                \begin{equation*}
                    \norm{s+tz-x}\geq{dist(x,S)}=\norm{s-x}^{2}
                \end{equation*}
                And therefore:
                \begin{equation*}
                    \rangle{s-x,s-x}\rangle+2t\langle{s-x,z}
                    +t^{2}\langle{z,z}\geq{s-x}^{2}
                    \Rightarrow{t^{2}\norm{z}^{2}+2t\langle{s-x,z}}
                    \geq{0}
                \end{equation*}
                Looking at the discriminant of this polynomial, we
                have:
                \begin{equation*}
                    \langle{s-x,z}\rangle=0
                \end{equation*}
                Therefore, $s-x\perp{S}$. You obtain $s\in{S}$
                by ``dropping the perpendicular of $x$,'' onto
                $S$. That is, $s$ is the orthogonal projection
                of $x$ onto $S$. $s=P_{S}x$ where
                $P_{S}:H\rightarrow{H}$ is the orthogonal
                projection. This has a few nice properties:
                \begin{enumerate}
                    \item It is idempotent: $P_{S}^{2}=P_{S}$.
                    \item Self adjoint:
                          $\langle{P_{S}x,y}\rangle%
                           =\langle{x,P_{S}y}\rangle=$
                    \item Linear.
                    \item Bounded and $\norm{P_{S}}=1$.
                \end{enumerate}
                If $S$ is a subset of an inner product space $X$,
                we write $S^{\perp}=\{x\in{X}:\langle{x,s}\rangle=0\}$.
                This is often read aloud as ``$S$ perp'' or
                ``$S$ perpendicular.''
                \begin{theorem}
                    If $S\subset{X}$, then $S^{\perp}$ is a
                    closed subspace.
                \end{theorem}
                \begin{theorem}
                    $S\subset(S^{\perp})^{\perp}$
                \end{theorem}
                The direct sum of two subsets of a Hilbert space
                $H$ is
                $S_{1}\oplus{S_{2}}=\{ax+by:x\in{S_{1}},y\in{S_{2}}\}$.
                \begin{theorem}
                    If $H$ is a Hilbert space and $S$ is a closed
                    subspace of $H$, then $H=S\oplus{S^{\perp}}$
                \end{theorem}
                \begin{proof}
                    For $x=P_{S}(x)+(x-P_{S}(x))$, and thus there is
                    an element in $S$ and an element in $S^{\perp}$
                    such that $x$ is the sum of those two elements.
                    This is the only representation. For if
                    $x=s_{1}+s_{1}^{\perp}$ and
                    $x=s_{2}+s_{2}^{\perp}$, then stuff.
                \end{proof}
                If $X$ and $Y$ are normed spaces, and if
                $f\in{B(X,Y)}$, then
                $\{x\in{X}:f(x)=0\in{Y}\}$ is called the null
                space of $f$.
                \begin{theorem}
                    If $X$ and $Y$ are normed spaces, and if
                    $f\in{B(X,Y)}$, then $\nul(f)$ is a closed
                    linear subspace of $X$.
                \end{theorem}
                \begin{proof}
                    Obvious since $f$ is linear and continuous.
                \end{proof}
                In a Hilbert space $H$, then
                $H=\nul(f)\oplus\nul(f)^{\perp}$. Thus, if
                $\nul(f)\ne{H}$, then $\nul(f)^{\perp}\ne\{0\}$.
                That is, there exists a $z\in\nul(f)^{\perp}$ that
                is non-zero. This is the $z$ we used to prove the
                Riesz representation theorem. Riesz's
                Theorem thus says that every Hilbert space is its own dual.
        \subsection{Lecture 9: November 5, 2018}
            \subsubsection{Adjoint}
                If $H$ is a Hilbert space and
                $f\in{H'}$, then there is a $z\in{H}$
                such that $f(x)=\langle{x,z}\rangle$ for
                all $x\in{H}$. Moreover, $\norm{f}=\norm{z}$.
                The adjoint of $T\in{B(H,H)}$ is an
                operator $T^{*}:H\rightarrow{H}$ such that
                $\langle{Tx,y}\rangle=\langle{x,T^{*}}\rangle$.
                There is always such an operator for any
                $T\in{B(H,H)}$. $T^{*}$ is also bounded and
                linear. By Riesz there is a $z=T^{*}y$ such
                that $f(x)=\langle{x,T^{*}y}\rangle$. Then
                $\norm{T^{*}y}=\norm{z}=\norm{f}\leq\norm{T}\norm{y}$.
                Thus, $\norm{T^{*}}\leq\norm{T}$. Therefore
                $T^{*}\in{B(H,H)}$. $T^{*}$ is called the
                ajdoint of $T$.
                \begin{example}
                    Consider $\mathbb{R}^{n}$ with the usual
                    inner product. Let $T$ be the matrix
                    $(T_{ij})$. Then:
                    \begin{equation*}
                        (Tx)_{i}=\sum_{j=1}^{n}T_{ij}x_{j}    
                    \end{equation*}
                    and:
                    \begin{equation*}
                        \langle{Tx,y}\rangle
                        =\sum_{i=1}^{n}(Tx)_{i}y_{i}
                        =\sum_{i=1}^{n}\sum_{j=1}^{n}
                        T_{ij}x_{j}y_{i}
                        =\sum_{j=1}^{n}\sum_{i=1}^{n}
                        T_{ij}y_{i}x_{j}
                    \end{equation*}
                    If $T^{*}$ is the adjoint, then:
                    \begin{equation*}
                        \langle{x,T^{*}y}\rangle
                        =\sum_{j=1}^{n}
                        \Big(\sum_{i=1}^{n}
                             T^{*}_{ji}y_{i}\Big)x_{j}
                    \end{equation*}
                    And thus $T^{*}_{ji}=T_{ij}$.
                    That is, the adjoint
                    of $T$ is the transpose of $T$. If we were in
                    $\mathbb{C}^{n}$ we would use the complex
                    conjugate of the transpose of $T$.
                    In general, if $T=T^{*}$
                    we say that $T$ is \textit{self-adjoint}.
                    This is also called symmetric or Hermitian.
                \end{example}
                \begin{example}
                    As another example, consider
                    $H=\ell^{2}$ and let
                    $T(x_{1},x_{2},\hdots)=(x_{2},x_{3},\hdots)$.
                    This is linear, and:
                    \begin{equation*}
                        \norm{T(x_{1},x_{2},\hdots)}
                        =\norm{(x_{2},x_{3},\hdots)}
                        =\sqrt{\sum_{n=2}^{\infty}x_{n}^{2}}
                        \leq
                        \sqrt{\sum_{n=1}^{\infty}x_{n}^{2}}
                        =\norm{(x_{1},x_{2},\hdots)}
                    \end{equation*}
                    Therefore $T$ is bounded and
                    $\norm{T}\leq{1}$. But
                    $T(0,1,0,0,\hdots)=(1,0,0,\hdots)$
                    showing that $\norm{T}\geq{1}$.
                    Thus, $\norm{T}=1$. Then, from
                    the definition of $T$:
                    \begin{align*}
                        \langle{Tx,y}\rangle
                        &=\langle{(x_{2},x_{3},\hdots),
                                  (y_{1},y_{2},\hdots)}\rangle\\
                        &=x_{2}y_{1}+x_{3}y_{2}+\hdots\\
                        &={x_{1}}\cdot{0}+x_{2}y_{1}+
                        x_{3}y_{2}+\hdots\\
                        &=\langle{(x_{1},x_{2},\hdots),
                                  (0,y_{1},y_{2},\hdots)}\rangle
                    \end{align*}
                    And therefore
                    $T^{*}(y_{1},y_{2},\hdots)=(0,y_{1},y_{2},\hdots)$.
                    Also $\norm{T^{*}}=1$. In general,
                    if $T\in{B(H,H)}$
                    then $\norm{T^{*}}=\norm{T}$.
                \end{example}
                \begin{theorem}
                    If $T\in{B(H,H)}$, then
                    $T^{**}=T$.
                \end{theorem}
                \begin{theorem}
                    $\norm{T}=\norm{T^{*}}$
                \end{theorem}
                \begin{proof}
                    For $\norm{T}\leq\norm{T^{*}}$ and
                    $\norm{T^{*}}\leq\norm{T^{**}}$, but
                    $T=T^{**}$, and therefore
                    $\norm{T}=\norm{T^{*}}$.
                \end{proof}
                \begin{example}
                    Let $x=C[0,1]$ and let
                    $\langle{x,y}\rangle=\int_{0}^{1}x(t)y(t)\diff{t}$.
                    Let $K:X\times{X}\rightarrow\mathbb{R}$
                    be continuous and define $T$ by:
                    \begin{equation*}
                        Tx(t)=\int_{0}^{1}K(t,s)x(s)\diff{s}
                    \end{equation*}
                    Then for all $x\in{X}$, $Tx\in{X}$ as well,
                    since $K$ is continuous.
                    Moreover, from Cauchy-Schwarz:
                    \begin{equation*}
                        \norm{Tx}^{2}
                        =\int_{0}^{1}
                        \Big[\int_{0}^{1}
                             K(t,s)x(d)\diff{s}\Big]^{2}\diff{t}
                        \leq\int_{0}^{1}
                        \Big[\int_{0}^{1}K(t,s)^{2}\diff{s}
                        \int_{0}^{1}x(s)^{2}\diff{s}\Big]\diff{t}
                    \end{equation*}
                    But
                    $\int_{0}^{1}x(s)^{2}\diff{s}=\norm{x}^{2}$.
                    So:
                    \begin{equation*}
                        \norm{Tx}^{2}\leq
                        \norm{x}^{2}\int_{0}^{1}\int_{0}^{1}
                        K(t,s)\diff{s}\diff{t}
                    \end{equation*}
                    Therefore $T$ is bounded and:
                    \begin{equation*}
                        \norm{T}\leq
                        \sqrt{\int_{0}^{1}\int_{0}^{1}
                              K(t,s)\diff{s}\diff{t}}
                    \end{equation*}
                    Computing the adjoint:
                    \begin{align*}
                        \langle{Tx,y}\rangle
                        &=\int_{0}^{1}Tx(t)y(t)\diff{t}\\
                        &=\int_{0}^{1}\Big(
                        \int_{0}^{1}K(t,s)x(s)\diff{s}\Big)
                        y(t)\diff{t}\\
                        &=\int_{0}^{1}\Big(
                        \int_{0}^{1}K(t,s)y(t)\diff{t}\Big)
                        x(s)\diff{s}\\
                        &=\int_{0}^{1}\Big(
                        \int_{0}^{1}K(s,t)y(s)\diff{s}\Big)
                        x(t)\diff{t}\\
                        &=\int_{0}^{1}Ty(s)x(s)\diff{s}\\
                        &=\langle{Ty,x}\rangle
                    \end{align*}
                    We may swap the order of integration since
                    $K$ is continuous on a compact set.
                \end{example}
                \begin{theorem}
                    $\norm{T^{*}T}=\norm{T}^{2}$
                \end{theorem}
                \begin{proof}
                    For:
                    \begin{equation*}
                        \norm{T^{*}Tx}\leq\norm{T^{*}}
                        \norm{Tx}\leq
                        \norm{T^{*}}\norm{T}\norm{x}
                    \end{equation*}
                    And therefore $\norm{T^{*}T}\leq\norm{T}^{2}$.
                    On the other hand:
                \end{proof}
                \begin{theorem}
                    If $T$ is self-adjoint, then
                    $\norm{T}=\sup\{|\langle{Tx,x}\rangle|:\norm{x}=1\}$.
                \end{theorem}
                \begin{proof}
                    \label{thm:Funct:Norm_of_Self_%
                           Adjoint_Operator}
                    Let
                    $\alpha=\sup\{sup\{|\langle{Tx,x}\rangle|:\norm{x}=1\}$.
                    Then:
                    \begin{equation*}
                        |\langle{Tx,x}\rangle|
                        \leq\norm{Tx}\norm{x}\leq
                        \norm{T}\norm{x}^{2}
                    \end{equation*}
                    Taking the supremum over $\norm{x}=1$,
                    we have $\alpha\leq\norm{T}$.
                    But if $\norm{x}=\norm{y}=1$, then:
                    \begin{align*}
                        |\langle{Tx,y}\rangle|
                        &=|\frac{1}{4}\langle{T(x+y),x+y}\rangle
                        -\frac{1}{4}\langle{T(x-y),x-y}\rangle\\
                        &=|\frac{1}{4}\norm{x+y}^{2}
                        \langle{T}\frac{x+y}{\norm{x+y}},
                               \frac{x+y}{\norm{x+y}}\rangle
                        -\frac{1}{4}\norm{x-y}
                        \langle{T}\frac{x-y}{\norm{x-y}},
                               \frac{x-y}{\norm{x-y}}\rangle
                    \end{align*}
                    Since
                    $\langle{Tx,y}\rangle=\langle{x,Ty}\rangle$,
                    as $T$ is self-adjoint.
                    And from the definition of $\alpha$:
                    \begin{equation*}
                        |\langle{Tx,y}\rangle|
                        \leq\frac{\alpha}{4}
                        \big(\norm{x+y}^{2}+\norm{x-y}^{2}\big)
                        \leq\frac{\alpha}{4}
                        \big(2\norm{x}^{2}+2\norm{y}^{2}\big)
                        =\alpha
                    \end{equation*}
                    Let $y=Tx/\norm{Tx}$, we get:
                    \begin{equation*}
                        \langle{Tx,\frac{Tx}{\norm{Tx}}}\rangle
                        \leq\alpha
                    \end{equation*}
                    And therefore $\norm{T}\leq\alpha$. But also
                    $\alpha\leq\norm{T}$. Thus, $\norm{T}=\alpha$.
                \end{proof}
                Thm.~\ref{thm:Funct:Norm_of_Self_Adjoint_Operator}
                can fail if $T$ is not self-adjoint.
                In $\mathbb{R}^{2}$, let
                $T(x_{1},x_{2})=(0,x_{1})$. Then:
                \begin{equation*}
                    \norm{Tx}^{2}=x_{1}^{2}\leq
                    x_{1}^{2}+x_{2}^{2}
                    =\norm{x}^{2}
                \end{equation*}
                And therefore $\norm{T}\leq{1}$.
                But $T(1,0)=(0,1)$, and
                thus $\norm{T}=1$. But if $(x_{1},x_{2})$
                lies on the unit circle, then
                $|x_{1}x_{2}|\leq0.5$. Thus:
                \begin{equation*}
                    |\langle{Tx,x}\rangle|
                    =|\langle(x_{1},x_{2}),(0,x_{1})\rangle
                    =|x_{1}x_{2}|\leq\frac{1}{2}
                \end{equation*}
                Therefore
                $|\langle{Tx,x}\rangle|\leq{0.5}<\norm{T}$ for
                all $x\in\mathbb{R}^{2}$ such that $\norm{x}=1$.
            \subsubsection{Compact Operators}
                Compact operators can be defined in a more
                general spaces
                than that of Hilbert or Banach spaces.
                They can be defined
                on Topological spaces, but we won't go that far.
                For now we will simply define them on a
                general metric space.
                \begin{definition}
                    A compact mapping from a metric space $X$
                    to a metric
                    space $Y$ is a function $T:X\rightarrow{Y}$
                    such that
                    for all bounded subsets $S$ of $X$, the image
                    $T(S)$ is pre-compact in $Y$. That is,
                    $\overline{T(S)}$ is compact
                    (The closure of $T(S)$ is compact).
                \end{definition}
                \begin{theorem}
                    If $T:X\rightarrow{Y}$ is a linear compact
                    operator between normed spaces $X$ and $Y$,
                    then $T$ is continuous.
                \end{theorem}
                \begin{proof}
                    For let $S=\overline{B_{1}(\mathbf{0})}$.
                    This is bounded, so
                    $\overline{T(S)}$ is compact, and therefore bounded.
                    Let $M$ be such a bound.
                    Thus, for all $s\in\overline{S}$
                    such that $\norm{s}=1$,
                    $\norm{Ts}\leq{M}$, and therefore $\norm{T}\leq{M}$.
                    Thus $T$ is bounded and linear, and is therefore
                    continuous.
                \end{proof}
                \begin{example}
                    Every linear mapping
                    $T:\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}$
                    is compact.
                    As a another example, let
                    $X=C[0,1]$ and equip this with the supremum norm.
                    Define $T$ as:
                    \begin{equation*}
                        Tx(t)=\int_{0}^{1}K(t,s)x(s)\diff{s}
                    \end{equation*}
                    Where $K:[0,1]\times[0,1]\rightarrow\mathbb{R}$
                    is continuous. This is a compact operator. For if
                    $S$ is a bounded subset then there exists
                    an $M$ such
                    that for all $x\in{S}$, $\norm{x}\leq{M}$. Thus:
                    \begin{align*}
                        \norm{Tx}=\sup|Tx(t)|
                        &=\sup\Big|\int_{0}^{1}
                        K(t,s)x(s)\diff{s}\Big|\\
                        &\leq\sup\int_{0}^{1}
                        |K(t,s)||x(s)|\diff{s}\\
                        &\leq\kappa\int_{0}^{1}|x(s)|\diff{s}\\
                        &\leq\kappa\norm{x}
                    \end{align*}
                    Where $\kappa=\sup|K(t,s)|$. $\kappa$ exists
                    since $K(t,s)$ is continuous on a compact
                    set and is therefore
                    bounded. So $T(S)$ is uniformly bounded. To apply
                    Arzela-Ascoli we need to show that
                    $T(S)$ is equicontinuous. That is, for all
                    $\varepsilon>0$ there is a $\delta>0$ such that,
                    for all $x\in{S}$, if $|t_{2}-t_{1}|<\delta$
                    then $|Tx(t_{2})-Tx(t_{1})|<\varepsilon$. If we
                    can show that $T$ satisfies this, then
                    $\overline{T(S)}$ is compact,
                    and thus $T$ is compact.
                    Let's show this. If $x\in{S}$, then:
                    \begin{align*}
                        |Tx(t_{2})-Tx(t_{1})|
                        &=\Big|\int_{0}^{1}K(t_{1},s)x(s)\diff{s}
                        -\int_{0}^{1}K(t_{2},s)x(s)\diff{s}\Big|\\
                        &=\Big|\int_{0}^{1}(K(t_{2},s)-
                        K(t_{1},s))x(s)\diff{s}\Big|\\
                        &\leq
                        \int_{0}^{1}|K(t_{2},s)-K(t_{1},s)||x(s)|
                        \diff{s}\\
                        &\leq{M}\int_{0}^{1}|K(t_{2},s)-K(t_{1},s)|
                        \diff{s}
                    \end{align*}
                    But as $K$ is uniformly continuous,
                    there is a $\delta>0$
                    such that, for all $s\in[0,1]$,
                    $|t_{2}-t_{1}|<\delta$ implies
                    $|K(t_{2},s)-K(t_{1},s)|<\varepsilon/M$.
                    Thus, $T(S)$ is equicontinuous. We can replace the
                    supremum norm with $L^{2}$ and $T$ is still compact.
                    Indeed, it is true for $L^{p}$ if we replace the
                    use of Cauchy-Schwarz with the more general
                    H\"{o}lder's Inequality. From this we have that
                    $T$ is a compact self-adjoint operator.
                \end{example}
        \subsection{Lecture 10: November 19, 2018}
            \subsubsection{Compact Linear Operators}
                A linear operator $T:X\rightarrow{Y}$
                is compact if $\overline{T(S)}$ is compact
                for all bounded $S\subset{X}$. Example,
                $Tx(t)=\int_{0}^{1}K(t,s)x(s)\diff{s}$,
                where $K$ is continuous on
                $[0,1]^{2}$, is a compact operator
                $T:C[0,1]\rightarrow{C[0,1]}$. If
                $K(x,t)=K(t,x)$ for all
                $(x,t)\in[0,1]^{2}$, then
                $T$ is a self-adjoint operator on
                $L^{2}[0,1]$. $L^{2}[0,1]$ can be seen
                as the completion of $C[0,1]$ with respect
                to the $L^{2}$ norm.
                \begin{theorem}
                    A linear operator
                    $T:X\rightarrow{Y}$ is compact if and only
                    if for all bounded sequences
                    $x:\mathbb{N}\rightarrow{X}$,
                    $Tx$ has a congergent subsequence in $Y$ 
                \end{theorem}
                We're interested mainly in the case of $Y=X$,
                and when $T:X\rightarrow{X}$ is bounded in
                linear. This is the set of all operators
                $B(X,X)$. We rewrite this as $B(X)$. That is,
                $B(X)$ is the set of all bounded linear operators
                from $X$ to itself. Recall that if
                $Y$ is a Banach space, and if $X$ is a normed
                space, then $B(X,Y)$ is a Banach space. Thus,
                if $X$ is a Banach space, then $B(X)$ is a
                Banach space. But we can also multiply elements
                in $B(X)$ by using function composition.
                If $S,T\in{B(X)}$, then $ST$ is defined by
                $(ST)(x)=S(Tx)$. But then:
                \begin{equation*}
                    \norm{(ST)x}=\norm{S(Tx)}
                    \leq\norm{S}\norm{Tx}
                    \rightarrow
                    \norm{ST}\leq\norm{S}\norm{T}
                \end{equation*}
                A Banach space with such a multiplication
                property is called a Banach Algebra. The set of
                compact linear operators on $X$ is often denoted
                $C(X)$. Thus, $C(X)\subset{B(X)}$. It's
                a two-sided closed ideal in $B(X)$. That is,
                if $S,T\in{C(X)}$, and if $a$ and $b$ are scalars,
                then $aS+bT\in{C(X)}$, $ST\in{C(X)}$,
                and $TS\in{C(X)}$. Finally, if
                $F:\mathbb{N}\rightarrow{C(X)}$ is a sequence of
                compact operators, and if $F_{n}\rightarrow{T}$,
                then $T\in{C(X)}$.
                \begin{definition}
                    Orthogonal Elements in an inner product
                    space $(X,\langle\rangle)$ are elements
                    $x,y\in{X}$ such that
                    $\langle{x,y}\rangle=0$.
                \end{definition}
                \begin{definition}
                    An orthonormal subset of an inner
                    product space $(X,\langle\rangle)$
                    is a subset $S\subset{X}$ such that for
                    all $x,y\in{S}$ such that $x\ne{y}$,
                    $\langle{x,y}\rangle=0$ and for all
                    $x\in{S}$, $\norm{x}=1$.
                \end{definition}
                \begin{theorem}
                    If $x\in{X}$ and
                    $\varphi:\mathbb{N}\rightarrow{X}$
                    is a sequence such that
                    $A=\{\varphi_{n}:n\in\mathbb{N}\}$ is an
                    orthonormal subset of $X$, then for all
                    $x\in{X}$:
                    \begin{equation*}
                        \norm{x}=
                        \sum_{n=1}^{N}\langle{x,\varphi_{n}}
                        \rangle^{2}
                        +\norm{x-\sum_{n=1}^{N}
                               \langle{x,\varphi_{n}}\rangle
                               \varphi_{n}}^{2}
                    \end{equation*}
                \end{theorem}
                \begin{proof}
                    If $m\in\mathbb{Z}_{N}$, then:
                    \begin{equation*}
                        \langle{x-\sum_{n=1}^{N}
                                \langle{x,\varphi_{n}}\rangle
                                \varphi_{n},\varphi_{m}}\rangle
                        =\langle{x,\varphi_{m}}\rangle
                        -\sum_{n=1}^{N}\langle{x,\varphi_{n}}\rangle
                        \langle{\varphi_{n},\varphi_{m}}\rangle
                    \end{equation*}
                    But $A$ is an orthonormal subset of $X$,
                    and thus if $n\ne{m}$ then
                    $\langle{\varphi_{n},\varphi_{m}}\rangle=0$
                    ad if $n=m$ then
                    $\langle{\varphi_{n},\varphi_{m}}\rangle%
                     =\norm{\varphi_{n}}=1$. In terms of the
                    Kronecker-Delta function,
                    $\langle{\varphi_{n},\varphi_{m}}\rangle%
                     =\delta_{nm}$. So we have:
                    \begin{equation*}
                        \langle{x-\sum_{n=1}^{N}
                                \langle{x,\varphi_{n}}\rangle
                                \varphi_{n},\varphi_{m}}\rangle
                        =0
                    \end{equation*}
                    But for all $N\in\mathbb{N}$,
                    $x=(x-\sum_{n=1}^{N}%
                     \langle{x,\varphi_{n}}\rangle\varphi_{n})+%
                     \sum_{n=1}^{N}\langle{x,\varphi_{n}}\rangle%
                     \varphi_{n}$,
                    and these two are orthogonal.
                    Therefore, from Pythagoras:
                    \begin{align*}
                        \norm{x}^{2}&=
                        \norm{x-\sum_{n=1}^{N}
                              \langle{x,\varphi_{n}}\varphi_{n}
                              \rangle}^{2}+
                        \norm{\sum_{n=1}^{N}
                              \langle{x,\varphi_{n}}\varphi_{n}
                              \rangle}^{2}\\
                        &=\sum_{n=1}^{N}\langle{x,\varphi_{n}}
                        \norm{\varphi_{n}}^{2}\rangle
                        +\norm{\sum_{n=1}^{N}
                        \langle{x,\varphi_{n}}\varphi_{n}
                        \rangle}^{2}
                    \end{align*}
                    But $\norm{\varphi_{n}}^{2}=1$ for all $n$.
                    Therefore, etc.
                \end{proof}
                \begin{theorem}[Bessel's Inequality]
                    $\sum_{n=1}^{N}\langle{x,\varphi_{n}}\rangle%
                     \leq\norm{x}^{2}$
                \end{theorem}
                \begin{example}
                    $A=\{e_{n}:n\in\mathbb{N}\}$ is an orthonormal
                    subset of $\ell^{2}$.
                    $A=\{\sin(nt)/\sqrt{\pi}:n\in\mathbb{N}\}$
                    is an orthonormal subset of
                    $C[0,1]$ with the $L^{2}$ inner product.
                \end{example}
                \begin{definition}
                    A basis of an innert product space $X$
                    is an orthonormal subset $A\subset{X}$
                    such that there is no orthonormal subset
                    $B\subset{X}$ such that $A\subset{B}$.
                \end{definition}
                \begin{theorem}
                    If $(X,\langle\rangle)$ is an inner product
                    space, then there is an $A\subset{X}$
                    such that $A$ is an orthonormal subset of $X$.
                \end{theorem}
                \begin{theorem}
                    If $X$ is an inner product space, and if
                    $\varphi:\mathbb{N}\rightarrow{X}$ is a sequence
                    such that $S=\{\varphi_{n}:n\in\mathbb{N}\}$
                    is a basis of $X$, then for all $x\in{X}$
                    there is a sequence
                    $a:\mathbb{N}\rightarrow\mathbb{R}$
                    such that
                    $x=\sum_{n=1}^{\infty}a_{n}\varphi_{n}$
                \end{theorem}
            \subsubsection{Summability}
                What does $\sum_{\alpha\in{A}}z_{\alpha}$
                if $z_{\alpha}\in\mathbb{R}$ for all
                $\alpha\in{A}$?
                \begin{definition}
                    We say
                    $\sum_{\alpha\in{A}}z_{\alpha}$ is summable
                    to $z\in\mathbb{R}$ if for all
                    $\varepsilon>0$ there is a subset
                    $B\subset{A}$ such that, for all
                    $C\subset{A}$ such that
                    $B\subset{C}$,
                    $|\sum_{\alpha\in{C}}z_{\alpha}-z|<\varepsilon$.
                \end{definition}
                It turns out that, if
                $\sum_{\alpha\in{A}}z_{\alpha}$ is summable, then
                only countably many $z_{\alpha}$ are non-zero.
                For all $n$, the set
                $\{z_{\alpha}:z_{\alpha}>1/n\}$ must be finite.
                The set of all non-zero elements is the
                union over all of these $n$, which is the
                countable union of finite sets, which is
                thus countable.
                \begin{theorem}
                    If $(X,\langle\rangle)$ is an inner
                    product space, if
                    $\varphi:\mathbb{N}\rightarrow{X}$ is a
                    sequence such that
                    $S=\{\varphi_{n}:n\in\mathbb{N}\}$ is a
                    basis of $X$, then:
                    \begin{equation*}
                        x=\sum_{n=1}^{\infty}
                        \langle{x,\varphi_{n}}\rangle
                        \varphi_{n}
                    \end{equation*}
                \end{theorem}
                \begin{example}
                    If $L^{2}[0,\pi]$, let
                    $\varphi_{n}(t)=\sin(nt)\sqrt{2/\pi}$. Then
                    $A=\{\varphi_{n}(t):n\in\mathbb{N}\}$.
                \end{example}
        \subsection{Lecture 11: November 26, 2018}
            Let $T$ be a linear operator on a vector space
            $T$. We say $\lambda\in\mathbb{C}$ is an
            eigenvalue of $T$ if there exists $x\ne{0}$ in
            $X$ such that $Tx=\lambda{x}$. The corresponding
            $x$ is called the eigenvector or eigenfunction.
            We're interested in the case of compact
            self-adjoint operators $T$ on a Hilbert space
            $\mathscr{H}$.
            \begin{theorem}
                There is a sequence of real eigenvalues
                $\lambda_{n}$ of $T$, finite or infinite,
                such that $0$ is the only possible
                accumulation point of $\lambda_{n}$, and
                corresponding basis of
                orthogonormal eigenvectors $x_{n}$.
            \end{theorem}
            \begin{proof}
                We'll prove this in steps. First, either
                $\norm{T}$ or $-\norm{T}$ is an eigenvalue. This
                is because:
                \begin{equation*}
                    \norm{T}=
                    \underset{\norm{x}=1}{\sup}
                    \{|\langle{Tx,x}\rangle|\}
                \end{equation*}
                This comes from the fact that
                $T=T^{*}$ for self-adjoint operators. Thus,
                either
                $\norm{T}=\langle{Tx,x}\rangle$ or
                $-\norm{T}=\langle{Tx,x}\rangle$. Choose a
                sequence $x_{n}$ such that
                $\norm{x_{n}}=1$ and
                $\langle{Tx_{n},x_{n}}\rangle%
                 \rightarrow\pm\norm{T}$. By choosing a
                subsequence, we may assume that
                $Tx_{n}$ converges. We can not assume that
                $x_{n}$ converges, however. Thus,
                $Tx_{n}\rightarrow{y}$. Then:
                \begin{align*}
                    \norm{Tx_{n}-\lambda{x}_{n}}^{2}
                    &=\langle{Tx_{n}-\lambda{x}_{n},
                              Tx_{n}-\lambda{x}_{n}}\rangle\\
                    &=\norm{Tx_{n}}^{2}
                    -2\lambda\langle{Tx_{n},x}\rangle
                    +\lambda^{2}\norm{x}^{2}\\
                    &\leq
                    \norm{T}^{2}\norm{x}^{2}
                    -2\lambda\langle{Tx_{n},x}\rangle
                    +\lambda^{2}\norm{x}^{2}
                \end{align*}
                And this converges to zero as $n$ tends to
                infinity. Thus,
                $Tx_{n}-\lambda{x}_{n}\rightarrow{0}$. It
                follows that
                $\lambda{x}_{n}=Tx_{n}-(Tx_{n}-\lambda{x}_{n})$,
                and this converges to $y$. Therefore
                $x_{n}\rightarrow{y}/\lambda$. Note that
                $\lambda$ is only equal to zero if
                $T$ is the zero operator. In this case the
                problem is trivial. Thus we may assume
                $\lambda\ne{0}$. Therefore
                $Tx_{n}\rightarrow{Ty}/\lambda$, but
                $Tx_{n}\rightarrow{y}$ as well. Thus
                $y=Ty/\lambda$. Let $\lambda_{1}=\lambda$
                and $\varphi_{1}=y/\norm{y}$. Then
                $\norm{\phi_{1}}=1$ and
                $T\varphi_{1}=\lambda_{1}\phi_{1}$. Moreover,
                let $T_{1}=T$ and let $H_{1}=\mathscr{H}$.
                Let $H_{2}=\{x\in{H}_{1}:x\perp\varphi_{1}\}$.
                Define $T_{2}:H_{2}\rightarrow{H}_{1}$ by
                $T_{2}x=T_{1}x=Tx$. This is the restriction
                of $T$ to $H_{2}$. Then
                $T_{2}H_{2}\subseteq{T}_{2}H_{2}$. For if
                $x\in{H}_{2}$, then
                $\langle{T}_{2}x,\varphi_{1}\rangle%
                 \langle{T}_{1}x,\varphi_{1}\rangle$. But
                $T$ is self-adjoint, and
                $T_{1}=T$, and therefore
                $T_{1}$ is self-adjoint. But then:
                \begin{equation*}
                    \langle{T}_{2}x,\varphi_{1}\rangle
                    =\langle{T}_{1}x,\varphi_{1}\rangle
                    =\langle{x},T_{1}\varphi_{1}\rangle
                    =\lambda_{1}\langle{x}_{1},
                        \varphi_{1}\rangle
                \end{equation*}
                Therefore $T_{2}x\in{H}_{2}$, and therefore
                $T_{2}:H_{2}\rightarrow{H}_{2}$. $T_{2}$
                is self-adjoint, for:
                \begin{align*}
                    \langle{T}_{2}x,y\rangle
                    =\langle{T}_{1}x,y\rangle
                    =\langle{x},T_{1}y\rangle
                    =\langle{x},T_{2}y\rangle
                \end{align*}
                $T_{2}$ is compact since if $x_{n}$ is
                bounded in $H_{2}$, then it's bounded in
                $H_{1}$, and thus
                $T_{2}x_{n}=T_{1}x_{n}$, which has a convergent
                subsequence,
                and therefore $T_{2}$ is compact. $H_{2}$ is
                a subspace of $\mathscr{H}$ and is closed since
                $x_{n}\in{H}_{2}$ and
                $x_{n}\rightarrow{x}$ in $\mathscr{H}$ then:
                \begin{equation*}
                    \langle{x},\varphi_{1}\rangle=
                    \langle\lim{x}_{n},\varphi_{1}\rangle=
                    \lim\langle{x}_{n},\varphi_{1}\rangle=
                    0
                \end{equation*}
                Thus $H_{2}$ is closed and is therefore complete,
                and thus $H_{2}$ is a Hilbert space. As before
                there is a $\varphi_{2}$ and a $\lambda_{2}$
                such that $\varphi_{2}\in{H}_{2}$,
                $\norm{\varphi_{2}}=1$, and:
                \begin{equation*}
                    T_{2}\varphi_{2}=\lambda_{2}\varphi_{2}
                \end{equation*}
                But then $T\varphi_{2}=\lambda_{2}\varphi_{2}$,
                where $\lambda_{2}=\pm\norm{T_{2}}$. Moreover,
                $|\lambda_{2}|<|\lambda_{1}|$. Continuing in
                this manner, let
                $H_{n}=\{x\in\mathscr{H}:%
                 x\perp\varphi_{1},\hdots,\varphi_{n-1}\}$
                and $T_{n}:H_{n}\rightarrow{H}_{n}$ be the
                restriction of $T$ onto $H_{n}$. We obtain a
                $\varphi_{n}$ such that
                $\norm{\varphi_{n}}$ and
                $T\varphi_{n}=\lambda\varphi_{n}$. Moreover
                $|\lambda_{n}|\leq|\lambda_{n-1}|$. Thus,
                $|\lambda_{n}|$ forms a monotonically
                decreasing sequence and either there is an
                $N\in\mathbb{N}$ such that
                $\lambda_{N}=0$, in which case for all $n>N$,
                $\lambda_{n}=0$ as well, or for all
                $n\in\mathbb{N}$, $|\lambda_{n}|>0$. In the first
                case it is clear that $\lambda_{n}\rightarrow{0}$.
                In the second case we have
                $\lambda_{n}\varphi_{n}=T\varphi_{n}$ has
                a convergent subsequence for $T$ is compact.
                But $|\lambda_{n}|$ is a monotonically
                decreasing sequence bounder below by zero,
                and therefore converges. Let $c$ be the limit.
                Then $\lambda_{n}x_{n}\rightarrow{cx}$.
                Moreover $\norm{\varphi_{n}-\varphi_{m}}^{2}=2$
                since $\varphi_{n}$ and $\varphi_{m}$ are
                orthogonal when $n\ne{m}$. Therefore
                $\varphi_{n}$ is not a Cauchy sequence. Thus,
                for $\lambda_{n}\varphi_{n}$ to converge,
                $c=0$.
            \end{proof}
            \begin{theorem}[Hilbert-Schmidt Theorem]
                If $x\in\mathscr{H}$, then:
                \begin{equation*}
                    Tx=\sum_{n=1}^{\infty}
                        \lambda_{n}\langle{x},\varphi_{n}\rangle
                        \varphi_{n}
                \end{equation*}
            \end{theorem}
            \begin{proof}
                For define $y_{m}$ as:
                \begin{equation*}
                    y_{m}=x-\sum_{n=1}^{m-1}
                    \langle{x},\varphi_{n}\rangle
                    \varphi_{n}
                \end{equation*}
                Then:
                \begin{equation*}
                    \langle{y}_{m},\varphi_{k}\rangle
                    =\langle
                    x-\sum_{n=1}^{m-1}\rangle{x},\varphi_{n}
                    \rangle\varphi_{n},\varphi_{k}\rangle
                    =\langle{x},\varphi_{k}\rangle-
                    \sum_{n=1}^{m-1}\langle{x},\varphi_{n}
                    \rangle\langle\varphi_{n},\varphi_{k}\rangle
                    =0
                \end{equation*}
                If $\lambda_{N}=0$, then $Ty=T_{n}y=0$ by
                setting $m=N$. Thus:
                \begin{equation*}
                    0=Ty
                    =Tx-\sum_{n=1}^{N-1}
                    \langle{x},\varphi_{n}\rangle{T}\varphi_{n}
                    =Tx-\sum_{n=1}^{N-1}
                    \lambda_{n}\langle{x},\varphi_{n}\rangle
                    \varphi_{n}
                \end{equation*}
                If $\lambda_{n}\ne{0}$ for all $n\in\mathbb{N}$,
                then $y_{m}\in{H}_{m}$ and therefore:
                \begin{equation*}
                    x=(x-y_{m})+y_{m}
                \end{equation*}
                But $x-y_{m}$ is orthogonal to $y_{m}$, and
                therefore by Pythagoras:
                \begin{equation*}
                    \norm{x}^{2}=
                    \norm{x-y_{m}}^{2}+\norm{y_{m}}^{2}
                \end{equation*}
                Therefore $\norm{y_{m}}\leq\norm{x}$. Also:
                \begin{equation*}
                    \norm{Ty_{m}}=\norm{T_{m}y_{m}}
                    \leq\norm{T_{m}}\norm{y_{m}}
                    =|\lambda_{m}|\norm{y_{m}}
                \end{equation*}
                And therefore:
                \begin{equation*}
                    \norm{Tx-\sum_{n=1}^{m-1}
                    \lambda_{n}\rangle{x},\varphi_{n}\rangle
                    \varphi_{n}}\leq|\lambda_{m}|\norm{x}
                    \rightarrow{0}
                \end{equation*}
            \end{proof}
            \begin{theorem}
                If $T$ is a compact self-adjoint operator
                on a Hilbert Space $\mathscr{H}$, then there is
                an orthogonal basis for $\mathscr{H}$
                consisting of eigenvector of $T$.
            \end{theorem}
            \begin{proof}
                For any $x\in\mathscr{H}$:
                \begin{equation*}
                    Tx=\sum_{n=1}^{\infty}\lambda_{n}
                    \rangle{x},\varphi_{n}\rangle\varphi_{n}
                \end{equation*}
                This sum may be infinite.
                Let $\{\psi_{\alpha}\}_{\alpha\in{A}}$ be
                and orthogonal basis of $\nul(T)$. Then
                $T\psi_{\alpha}=0$ for all $\alpha\in{A}$,
                and thus $0$ is an eigenvalue for all
                $\psi_{\alpha}$. But also:
                \begin{equation*}
                    \lambda_{n}\langle\varphi_{n},
                    \psi_{\alpha}\rangle
                    =\langle\lambda_{n}\varphi_{n},
                    \psi_{\alpha}\rangle
                    =\langle{T}\varphi_{n},\psi_{\alpha}\rangle
                    =\langle\varphi_{n},T\psi_{\alpha}\rangle
                    =0
                \end{equation*}
                Thus, for all $\alpha\in{A}$ and all
                $n\in\mathbb{N}$,
                $\varphi_{n}\perp\psi_{\alpha}$. Then by
                Hilbert-Schmidt, for every $x\in\mathscr{H}$:
                \begin{equation*}
                    T\big(x-\sum_{n=1}^{\infty}
                    \langle{x},\varphi_{n}\rangle\varphi_{n}\big)
                    =Tx-\sum_{n=1}^{\infty}\lambda_{n}
                    \langle{x},\varphi_{n}\rangle\varphi_{n}=0
                \end{equation*}
                Thus:
                \begin{equation*}
                    x=\sum_{n=1}^{\infty}\lambda_{n}
                    \langle{x},\varphi_{n}\rangle\varphi_{n}+
                    \sum_{\alpha\in{A}}
                    \langle{x},\psi_{\alpha}\rangle\psi_{\alpha}
                \end{equation*}
            \end{proof}
    \section{More Stuffs}
        \subsection{Lecture 12: December 3, 2018}
            Cauchy-Schwarz says that:
            \begin{equation}
                \Big(\int_{a}^{b}x(s)y(s)\diff{s}\Big)^{2}
                \leq\Big(\int_{a}^{b}x^{2}(s)\diff{s}\Big)
                \Big(\int_{a}^{b}y^{2}(s)\diff{s}\Big)
            \end{equation}
            So:
            \begin{align}
                |Tx(t)|^{2}&=
                \Big(\int_{0}^{t}x(s)\diff{s}\Big)^{2}\\
                &\leq\Big(\int_{0}^{t}1^{2}\diff{s}\Big)
                \Big(\int_{0}^{t}x(s)^{2}\diff{s}\Big)\\
                &=t\int_{0}^{t}x(s)^{2}\diff{s}\\
                &\leq{t}\int_{0}^{1}x(s)^{2}\diff{s}\\
                &=t\norm{x}_{2}^{2}
            \end{align}
            So then:
            \begin{equation}
                \int_{0}^{1}Tx(t)^{2}\diff{t}
                \leq\int_{0}^{1}t\norm{x}_{2}^{2}\diff{t}
            \end{equation}
            This then implies:
            \begin{align}
                \norm{Tx}^{2}&\leq
                \frac{1}{2}\norm{x}_{2}^{2}\\
                \Rightarrow
                \norm{T}&\leq\frac{1}{\sqrt{2}}
            \end{align}
            Letting $x(t)=1$, we have $Tx=t$. Thus:
            \begin{equation}
                \norm{Tx}^{2}=\int_{0}^{1}t^{2}\diff{t}
                =\frac{1}{3}
            \end{equation}
            And thus $\norm{Tx}=1/\sqrt{3}$. So
            $1/\sqrt{3}\leq\norm{T}\leq{1}/\sqrt{2}$.
            If $x(t)=1-t$, then $Tx(t)=t-t^{2}/2$. So
            $\norm{Tx}=\sqrt{2/15}$. We're getting closer to
            the answer. Letting $x(t)=\cos(\pi{t}/2)$ gives
            us the norm. We now have to show this. Write:
            \begin{equation*}
                x(t)=\sum_{n=1}^{\infty}b_{n}\sin(n\pi{t})
            \end{equation*}
            Then:
            \begin{equation*}
                Tx(t)=\sum_{n=1}^{\infty}
                \frac{b_{n}}{n\pi}\big[1-\cos(n\pi{t})\big]
                =\Big(\sum_{n=1}^{\infty}\frac{b_{n}}{n\pi}\Big)
                -\Big(\sum_{n=1}^{\infty}\frac{b_{n}}{n\pi}
                \cos(n\pi{t})\Big)
            \end{equation*}
            But:
            \begin{align*}
                \norm{x}^{2}&=\sum_{n=1}^{\infty}\frac{b_{n}^{2}}{2}
                &
                \norm{Tx}^{2}&=
                \Big(\sum_{n=1}^{\infty}\frac{b_{n}}{n\pi}\Big)^{2}
                +\sum_{n=1}^{\infty}\frac{b_{n}^{2}}{2n^{2}\pi^{2}}
            \end{align*}
            Let's maximize $\norm{Tx}^{2}$ subject to
            $\norm{x}^{2}$. Using Lagrange multipliers we get:
            \begin{equation*}
                \frac{2}{n\pi}\Big(\sum_{k=1}^{\infty}
                \frac{b_{k}}{k\pi}\Big)
                +\frac{b_{n}}{n^{2}\pi^{2}}
                =\lambda^{2}b_{n}
            \end{equation*}
            Letting $A=\sum_{k=1}^{\infty}b_{k}/k\pi$, we obtain:
            \begin{equation*}
                b_{n}=\frac{2n\pi}{\lambda^{2}n^{2}\pi^{2}-1}A
            \end{equation*}
            So then:
            \begin{equation*}
                A=\sum_{n=1}^{\infty}
                \frac{2}{\lambda^{2}n^{2}\pi^{2}-1}A
            \end{equation*}
            And thus:
            \begin{equation*}
                \sum_{n=1}^{\infty}
                \frac{2}{\lambda^{2}n^{2}\pi^{2}-1}=1
            \end{equation*}
            And this is the expansion of cotangent:
            \begin{equation*}
                1-\frac{1}{\lambda}\cot\big(\frac{1}{\lambda}\big)
                =1
            \end{equation*}
            And therefore:
            \begin{equation*}
                \lambda=\frac{\pi}{2},\frac{3\pi}{2},\frac{5\pi}{2},
                \hdots
            \end{equation*}
            Finally:
            \begin{equation*}
                x(t)=\sum_{n=1}^{\infty}
                b_{n}\sin(n\pi{t})=
                \sum_{n=1}^{\infty}
                \frac{2n\pi}{4n^{2}-1}A\sin(n\pi{t})
                =\sqrt{2}\cos\big(\frac{\pi}{2}t\big)
            \end{equation*}
            But before we can do all of this we need to show
            that there is such a maximum. That is, the step that
            involves Lagrange multipliers is valid. We want
            an $x$ such that $\norm{x}=1$ and
            $\norm{Tx}=\norm{T}$. Certainly there is a sequence
            such that $\norm{x_{n}}=1$ and
            $\norm{Tx_{n}}\rightarrow\norm{T}$. Since $T$ is
            compact we may assume, taking subsequences as
            necessary, that
            $Tx_{n}\rightarrow{y}$. If $T$ is self adjoint
            and $x_{n}\rightarrow{x}$, then
            $Tx=y$. It would be nice if Bolzano-Weiestrass worked
            and we could say
            $\norm{x_{n}}$ bounded implies that
            $x_{n}\rightarrow{x}$, but this is not always
            true in infinite dimensions. We'll need to
            weaken our notion of convergence for this. We
            could simply say that everything converges to
            everything, which is the chaotic topology, but
            this is not very useful for it loses uniqueness.
            \begin{definition}
                A weakly convergent sequence in an inner product
                space $X$ is a sequence
                $x:\mathbb{N}\rightarrow{X}$ such that there
                is an $a\in{X}$ such that
                for all $z\in{X}$,
                $\langle{x_{n},z}\rangle\rightarrow%
                 \langle{a,z}\rangle$. We write
                 $x_{n}\overset{w}{\rightarrow}{a}$
            \end{definition}
            For example, if $X$ is a Hilbert space and
            $e_{n}$ is an orthonormal basis, then
            $e_{n}\rightarrow{0}$ since, for all $z$:
            \begin{equation*}
                \norm{z}^{2}=\sum_{n=1}^{\infty}
                \langle{e_{n},z}\rangle^{2}
            \end{equation*}
            And thus $\langle{e_{n},z}\rangle\rightarrow{0}$.
            But $\langle{0,z}\rangle=0$, so
            $e_{n}\overset{w}{\rightarrow}{0}$. Normal convergence
            is also called strong convergence. Strong convergence
            implies weak convergence. For if
            $x_{n}\rightarrow{a}$, then
            $\langle{a-x_{n},z}\rangle\rightarrow{0}$ for all
            $z$, and thus $x_{n}\overset{w}{\rightarrow}{a}$.
            Moreover, weak limits are unique. If
            $T$ is a bounded linear operator on a Hilbert space
            $H$, and if $x_{n}$ converges weakly to $a$,
            then $Tx_{n}\overset{w}{\rightarrow}{Ta}$.
            If $x$ converges weakly to $a$ and if $T$ is a
            compact linear operator on $H$, then
            $Tx_{n}$ converges strongly to $Tx$.
            \begin{theorem}
                If $H$ is a Hilbert space,
                $T$ is a compact operator on $H$,
                and if $x:\mathbb{N}\rightarrow{H}$ is a
                weakly convergent sequence such that
                $x_{n}\overset{w}{\rightarrow}{a}$,
                then the sequence
                $y:\mathbb{N}\rightarrow{H}$ defined by
                $y_{n}=Tx_{n}$ is such that 
                $y_{n}\rightarrow{Tx}$. That is,
                $Tx_{n}$ converges strongly to $Ta$.
            \end{theorem}
            \begin{proof}
                For suppose not. As $T$ is compact and
                $x_{n}$ is bounded, there is
                a convergent subsequence. Let $a_{1}$ be the
                limit. If the limit is not unique then there
                is another convergent subsequence with
                a different limit $a_{2}$. But
                $Tx_{n}$ converges weakly to
                $a$, and strong convergence implies weak
                convergence. Therefore $a_{1}=a_{2}=a$, a
                contradiction. Therefore, $Tx_{n}$ converges
                strongly to $Ta$.
            \end{proof}
            \begin{theorem}
                If $x_{n}\overset{w}{\rightarrow}{a}$
                and $\norm{x_{n}}\rightarrow{C}$,
                then $\norm{a}\leq{C}$.
            \end{theorem}
            \begin{proof}
                For:
                \begin{align*}
                    \norm{a}^{2}&=|\langle{a,a}\rangle|\\
                    &=\underset{n\rightarrow\infty}{\lim}
                    |\langle{x_{n},a}\rangle|\\
                    &\leq\underset{n\rightarrow\infty}{\lim}
                    \norm{x_{n}}\norm{a}\\
                    &=C\norm{a}
                \end{align*}
                Dividing by $\norm{a}$ gives the result.
            \end{proof}
            We now prove that there exists $x$ such that
            $\norm{x}=1$ and $\norm{Tx}=\norm{T}$. This works
            for any compact linear operator on a Hilbert space.
            We can always find a sequence, by definition, such
            that $\norm{x_{n}}=1$ and
            $\norm{Tx_{n}}\rightarrow\norm{T}$. But since
            $x_{n}$ is bounded by 1 there is a weakly
            convergent subsequence (Still to be proved).
            This is ``Bolzano-Weierstrass,'' of infinite
            dimensions. Then $x_{n}$ converges weakly to
            $x$, and thus $Tx_{n}$ converges strongly to
            $Tx$. Then $\norm{Tx}=\norm{T}$. Finally,
            $\norm{x}\leq\lim\norm{x_{n}}=1$, and
            $\norm{T}=\norm{Tx}\leq\norm{T}\norm{x}$,
            so $\norm{x}\geq{1}$, and therefore
            $\norm{x}=1$. Let's show $T:L^{2}\rightarrow{L}^{2}$
            defined by $Tx(t)=\int_{0}^{t}x(s)\diff{s}$ is
            compact. Suppose $x_{n}$ is bounded in $L^{2}$ with
            bound $M$. That is, $\norm{x_{n}}\leq{M}$.
            Then:
            \begin{align*}
                |Tx_{n}(t)|^{2}&=
                \Big|\int_{0}^{t}x_{n}(s)^{2}\diff{s}\Big|^{2}\\
                &\leq
                \Big(\int_{0}^{1}|x_{n}(s)|\diff{s}\Big)^{2}\\
                &\leq\Big(\int_{0}^{1}\diff{s}\Big)
                \Big(\int_{0}^{1}|x_{n}(s)|^{2}\diff{s}\Big)\\
                &=\norm{x_{n}}^{2}\\
                &\leq{M}^{2}
            \end{align*}
            Taking the supremum over $t\in[0,1]$ gives:
            \begin{equation*}
                \norm{Tx_{n}}_{\infty}\leq{M}
            \end{equation*}
            So $Tx_{n}$ is bounded in $C[0,1]$. Also, if
            $0\leq{t}_{1}$ and $t_{2}\leq{1}$, then:
            \begin{align*}
                |Tx_{n}(t_{2})-Tx_{n}(t_{1})|^{2}
                &\leq\Big(\int_{t_{1}}^{t_{2}}|x_{n}(s)|
                \diff{s}\Big)^{2}\\
                &\leq\Big(\int_{t_{1}}^{t_{2}}\diff{s}\Big)
                \Big(\int_{t_{1}}^{t_{2}}|x_{n}(s)|^{2}
                \diff{s}\Big)\\
                &=(t_{2}-t_{1})\int_{t_{1}}^{t_{2}}
                |x_{n}(s)|^{2}\diff{s}\\
                &\leq(t_{2}-t_{1})\norm{x_{n}}^{2}\\
                \leq{M}^{2}(t_{2}-t_{1})
            \end{align*}
            So $|Tx_{n}(t_{2})-Tx_{n}(t_{1})|$ can be made
            arbitrarily small for $t_{2}$ and $t_{1}$ close enough,
            independent on the $n$. That is, a
            $\delta$ may be chosen independent of $n$. This is
            the criterion for equicontinuity.
            The compactness of $[0,1]$ then gives uniform
            equicontinuity. Arzela-Ascoli then says there is a
            subsequence $Tx_{n}\rightarrow{y}$,
            with $y\in{c}[0,1]$. That is,
            $\norm{Tx-y}_{\infty}\rightarrow{0}$. But then:
            \begin{align*}
                \norm{Tx_{n}-y}^{2}
                &=\int_{0}^{1}|Tx_{n}(t)-y(t)|^{2}\diff{t}\\
                &\leq\int_{0}^{1}
                \norm{Tx_{n}-y}_{\infty}^{2}\diff{t}\\
                =\norm{Tx_{n}-y}_{\infty}^{2}
            \end{align*}
            And this converges to zero. Thus,
            $Tx_{n}\rightarrow{y}$.
            \begin{theorem}[Baire Category Theorem]
                If $(X,d)$ is a complete metric and
                $C_{n}$ is a sequence of closed sets such
                that $X=\cup_{n=1}^{\infty}C_{n}$, then
                there is an $N\in\mathbb{N}$ such that
                $C_{N}$ contains an open subset.
            \end{theorem}
            \begin{proof}
                Let $r_{1}\in(0,1)$, $x_{1}\in{X}$.
                If $B_{r_{1}}(X_{1})\subset{C_{1}}$ then we're
                done. Otherwise $B_{r_{1}}(x_{1})\setminus{C_{1}}$
                is a non-empty set, so there exists
                $x_{n}\in{B}_{r_{2}}(x_{1})$, where
                $r_{2}\in(0,1/2)$. By induction, choose
                $r_{n}\in(0,1/n)$ and $x_{n}$ such that
                $x_{n}\in{B}_{r_{n-1}}(x_{n-1})$ and
                $\overline{B_{r_{n}}(x_{n})}\subset%
                 B_{r_{n-1}}(x_{n-1})$. For $n<m$,
                $x_{m}\in\overline{B_{r_{n}}(x_{n})}$,
                so $d(x_{n},x_{m})<1/n$. Then $x_{n}$ is
                Cauchy, but $X$ is complete so there is a limit
                $x$. Since $\overline{B_{r_{n}}(x_{n})}$ is
                closed, $x\in\overline{B_{r_{n}}(x_{n})}$. But
                $X=\cup_{n=1}^{\infty}C_{n}$ and thus there is
                an $N$ such that $x\in{N}$. But then
                $B_{r_{n}}(x)\subset{C}_{N})$, so $C_{N}$ contains
                an open subset.
            \end{proof}
            The Baire Category Theorem is used to prove the
            Uniform Boundedness Theorem, which is also called
            the Banach-Steinhau theorem.
            \begin{theorem}[Uniform Boundedness Theorem]
                If $H$ is a Hilbert space, and if
                $x_{n}\overset{w}{\rightarrow}{a}$, then
                $\norm{x_{n}}$ is bounded.
            \end{theorem}
            \begin{proof}
                Let
                $C_{k}=\{y\in{H}:|\langle{x_{n},y}\rangle\leq{k}\}$.
                Then $C_{k}$ is closed since $y_{j}\in{C_{k}}$
                and $y_{j}\rightarrow{y}$ implies that:
                \begin{align*}
                    |\langle{x_{n},y_{j}}\rangle|&\leq{k}\\
                    \Rightarrow
                    \underset{j\rightarrow\infty}{\lim}
                    |\langle{x_{n},y_{j}}\rangle|&\leq{k}\\
                    \Rightarrow
                    |\langle{x_{n},y}\rangle|&\leq{k}
                \end{align*}
                Moreover $H=\cup_{k=1}^{\infty}C_{k}$. By
                the Baire Category Theorem there is a
                $k\in\mathbb{N}$ such that
                $C_{k}$ contains an open subset. Let $z_{0}\in{H}$
                and $r\in\mathbb{R}$ be such that
                $B_{r}(z_{0})\subset{C_{k}}$. Let
                $y\in{H}$, $y\ne{0}$, and set $z=z_{0}+\alpha{y}$.
                where:
                \begin{equation*}
                    \alpha=\frac{r}{2\norm{y}}
                \end{equation*}
                Then $\norm{z-z_{0}}<r$, so
                $z\in{C}_{k}$. That is,
                $|\langle{x_{n},z}\rangle|\leq{k}$ for all
                $n$. Thus we have:
                \begin{align*}
                    |\langle{x_{n},y}\rangle|&=
                    |\langle{x_{n},\frac{z-z_{0}}{\alpha}}\rangle|\\
                    &\leq
                    \frac{1}{\alpha}\Big(
                    |\langle{x_{n},z\rangle}|+
                    |\langle{x_{n},z_{0}}\rangle|\\
                    &\leq
                    \frac{1}{\alpha}(k+k)\\
                    &=\frac{4k}{r}\norm{y}
                \end{align*}
                This is true of any $y\in{H}$. Choosing $y=x_{n}$,
                we get:
                \begin{equation*}
                    \norm{x_{n}}^{2}\leq\frac{4k}{r}\norm{x_{n}}
                \end{equation*}
                Dividing by $\norm{x_{n}}$ shows boundedness.
            \end{proof}
        \subsection{Lecture 13: December 10, 2018}
            \begin{equation}
                Tx(t)=\int_{0}^{1-t}x(s)\diff{s}
            \end{equation}
            If $\norm{x}_{2}\leq{M}$, then:
            \begin{equation}
                |Tx(t)|^{2}=
                \Big|\int_{0}^{1-t}x(s)\diff{s}\Big|^{2}
                \leq\Big(\int_{0}^{1-t}\diff{s}\Big)
                \Big(\int_{0}^{1-t}x(s)^{2}\diff{s}\Big)
                =(1-t)\norm{x}_{2}^{2}
            \end{equation}
            So we have:
            \begin{equation}
                \norm{Tx}^{2}=\int_{0}^{1}Tx(t)^{2}\diff{t}
                \leq\int_{0}^{1}(1-t)\norm{x}_{2}^{2}\diff{t}
                =\frac{1}{2}\norm{x}_{2}^{2}
                \leq\frac{1}{2}M^{2}
            \end{equation}
            So:
            \begin{align}
                |Tx(t_{1})-Tx(t_{2})|^{2}
                &=\Big|\int_{0}^{1-t_{1}}x(s)\diff{s}-
                \int_{0}^{1-t_{2}}x(s)\diff{s}\Big|^{2}\\
                &=\Big|\int_{1-t_{2}}^{1-t_{1}}x(s)
                \diff{s}\Big|^{2}\\
                &\leq\int_{1-t_{2}}^{1-t_{1}}\diff{s}
                \int_{1-t_{2}}^{1-t_{1}}x(s)^{2}\diff{s}\\
                &=|t_{1}-t_{2}|\norm{x}_{2}^{2}\\
                &\leq{M}^{2}|t_{1}-t_{2}|
            \end{align}
            This shows equicontinuity, and thus Arzela-Ascoli
            shows that $T$ is compact.
            \begin{theorem}[Banach-Alaoglu-Hilbert Theorem]
                If $H$ is a Hilbert space and
                $x:\mathbb{N}\rightarrow{H}$ is a bounded sequence,
                then there is a weakly convergent subsequence.
            \end{theorem}
            The next question would be ``What about Banach Space?''
            If $X$ is a normed space, we say $x_{n}$ converges
            weakly to $x$, denoted
            $x_{n}\overset{w}{\rightarrow}{x}$ if for all
            bounded linear functional $f\in{X'}$,
            $f(x_{n})\rightarrow{f(x)}$.
            For example let $X=\ell^{1}$. We have proven that the
            dual of $\ell^{1}$ is $\ell^{\infty}$ and elements
            of the dual take the form:
            \begin{equation}
                f(x)=\sum_{n=1}^{\infty}z_{i}x_{i}
            \end{equation}
            Where $z_{i}\in\ell^{\infty}$. Thus, $z_{i}$ is
            bounded and $x_{i}$ is absolutely convergent,
            since $x_{i}\in\ell^{1}$, and thus the product
            is absolutely convergent. That is,
            $x_{i}z_{i}\in\ell^{1}$. The problem with this is
            that we don't know that $X'\ne\{0\}$ for a given
            Banach space. There are plenty these, enough to
            separate points, thanks to the Hahn-Banach theorem.
            This says that if $f$ is a bounded linear functional
            on a subspace $M$ of $X$, then there exists
            $F\in{X'}$ such that
            $\no{F}_{X'}=\norm{f}_{M'}$ and
            $F(x)=f(x)$ for all $x\in{M}$. So given $m\in{M}$,
            then $\alpha{m}\in{M}$ for al $\alpha\in\mathbb{R}$.
            Define $f(m)=k$, for some $k\in\mathbb{R}$. Then
            $f(\alpha{m})=\alpha{k}$ and
            $|f(\alpha{m})|=|\alpha||f(m)|%
             =|k|\norm{\alpha{m}}/\norm{m}$. And
             $|f(\alpha{m})|/\norm{\alpha{m}}=|k|/\norm{m}$.
            Thus $f\in{M'}$. Thus, Hahn-Banach can be exteneded
            to all of $X$. So we can for all $x\in{X}$ and for
            all $r\in\mathbb{R}$, there is a bounded linear
            function $f\in{X'}$ such that
            $f(x)=r$. If $m_{1}$ and $m_{2}$ are independent
            (That is, $m_{1}\ne\alpha{m}_{2}$ for any real
            number $\alpha$), then let
            $M=\{am_{1}+bm_{2}:a,b\in\mathbb{R}\}$. Define
            $f(am_{1}+bm_{2})=a\norm{m_{1}}$.
            Then $f\in{M'}$ so this
            can be extended to all of $X$ by the Hahn-Banach
            theorem. But $f(m_{1})=1$ and
            $f(m_{2})=0$, so $f$ separates points. Thus,
            if $x_{n}$ converges weakly to $x$ in a
            Banach space, and if $x_{n}$ also converges weakly
            to $y$, then $x=y$. The uniform boundedness theorem
            also holds if $X$ is complete. The
            Banach-Alaoglu-Hilbert theorem fails in a general
            Banach space. For example, $\ell^{1}$. We now talk
            about weak* convergence. In $X'$, we say
            $f_{n}$ converges weak* to $f$ if
            $f_{n}(x)\rightarrow{f(x)}$ for all
            $x\in{X}$. Banach-Alaoglu holds if weak is replaced
            with weak* and if $X'$ is separable. The double
            dual, $X''$, is the dual of $X'$. $X$ is
            embedded in $X''$. That is, $X$ embeds naturally
            in $X''$ as follows: Define
            $C:X\rightarrow{X''}$ as follows. If $x\in{X}$,
            $Cx(f)=f(x)$ for all $f\in{X'}$. It's easy to
            show that $\norm{cx}=\norm{x}$. Indeed,
            $C$ is an isometry on $X$ to $X''$. If $C$ is
            onto, we say that $X$ is reflexive. There are
            Banach spaces that are not reflexive that can
            be isometrically embedded into their second dual,
            but the canonical map is not such an embedding.
            \begin{theorem}
                If $H$ is a Hilbert space, then $H$ is reflexive.
            \end{theorem}
            \begin{theorem}
                If $X$ is reflexive, then
                $X=X''$.
            \end{theorem}
            \begin{theorem}
                If $X$ is reflexive, weak* convergence
                implies weak convergence.
            \end{theorem}
            From topology, a subbasis for a topological space
            is a collection of sets such that every open set can
            be written as arbitrary unions and finite intersections
            of the sets. Choose as the subbasis:
            \begin{equation}
                \{f^{-1}(-\infty,a):a\in\mathbb{R},f\in{X'}\}
            \end{equation}
            If $X'$ is separable, then this space is metrizable.
            For let $A$ be a countable dense subset, and define
            the metric $d$ as:
            \begin{equation}
                d(f,g)=\sum_{x\in{A}}
                \frac{|f(x)-g(x)|}{1+|f(x)-g(x)|}2^{-n}
            \end{equation}
            \begin{theorem}[Banach-Alaoglu]
                If $X$ is a normed vector space, and if
                $\tau$ is the weak* topology, then
                $\overline{B_{1}(0)}$ is a compact subset of
                $(X',\tau)$.
            \end{theorem}
            Adams Sobolev Spaces.
    \section{Old Notes}
        \subsection{Summary of Lectures}
            The boundary of a circle
            in $\mathbb{R}^{2}$ is nowhere dense,
            with respect to the metric on $\mathbb{R}^{2}$.
            Any open ball about any point on the circle contains
            points not on the circle, and thus it has empty
            interior. Something about $\varepsilon$ nets.
            \subsubsection{Normed Spaces and Banach Spaces}
                There are notions of subspace,
                linear combination, independence, spanning,
                dimension, basis, Hamel basis, and
                \textit{convexity}.
                Open and closed balls are convex. 
                A subspace of a Banach space is complete
                iff closed. Schauder basis.
                A Schauder basis implies separable.
                If $\{x_{1},\hdots,x_{n}\}$ is independent,
                then there exists a $c>0$ such that, for all
                $\boldsymbol{\alpha}$,
                $|\boldsymbol{\alpha}\cdot\mathbf{x}|%
                 \geq{c}\norm{\boldsymbol{\alpha}}$.
                Finite dimensional subspaces are complete,
                as are closed subspaces. In finite dimensional
                normed spaces, a space is compact if and only
                if it is closed and bounded.
                Riesz's Lemma says that if $Z$ is a subspace
                of a normed space $X$, and if $Y$ is a proper
                closed subspace of $Z$, then there is a $z\in{Z}$
                such that $\norm{z}=1$ and $D(z,Y)\geq{1/2}$.
                A corollary of this is that $B_{1}(0)$ is compact
                if and only if $X$ is finite dimensional.
            \subsubsection{Linear Operators}
                Identity, zero, differentiation,
                and integration. Domain/Range
                of a linear operator, the null space.
                Inverse of a linear operator is linear.
                $(ST)^{-1}=T^{-1}S^{-1}$.
                In finite dimension all linear operators are
                continuous. An operator is bounded
                if and only if it is continuous. If a linear
                operator is continuous at some point, then it
                is continuous everywhere. An operator is bounded
                if and only if its null space is closed.
                There is something called the extension of a
                bounded linear operator. $B(X,Y)$ is the set of
                bounder linear operators
                from $X$ to $Y$. This is complete if and only if
                $Y$ is complete. A functional is a mapping from a
                vector space $X$ into the real numbers $\mathbb{R}$.
                For continuous linear functional, continuity at
                $0$ implies continuity everywhere.
                There is something called the dual space
                $X'$, which is itself a Banach space.
            \subsubsection{Inner Product and Hilbert Spaces}
                If $x_{n}\rightarrow{x}$ and $y_{n}\rightarrow{y}$,
                then
                $\langle{x_{n},y_{n}}\rangle\rightarrow\langle{x,y}\rangle$.
                There's a notion of orthogonal sets,
                and orthonormality. If $(e_{n})$ is orthonormal basis,
                then $x=\sum\langle{x,e_{k}}\rangle{e_{k}}$
                for all $x$. Gram-Schmidt procedure.
                $\sum\alpha_{k}e_{k}$ converges if and only if
                $\sum|\alpha_{k}|^{2}$ converges.
                A set $M$ is total in a Hilbert space $H$ is the
                span of the closure of $M$ is equal
                to $H$. If $M$ is complete, then it is
                total if and only if $M^{\perp}=0$.
                Parseval's theorem. Legendre, Hermite, and Laguerre
                polynomials are things.
                Self adjoint, unitary, and normal operators.
                $T^{*}=T$, $T^{*}=T^{-1}$, and $T^{*}T=TT^{*}$. If
                $X$ is a vector space over the complex numbers,
                and if $T$ is self adjoint, then
                $\langle{Tx,x}\rangle$ is a real number for all $x$.
            \subsubsection{Compact Linear Operators}
                If $T$ is compact and linear, then it is bounded and
                continuous. An operator is compact and linear
                if and only if
                for all bounded sequences $x_{n}$,
                $Tx_{n}$ has a convergent subsequence. Compact linear
                operators form a vector space. The rank of an
                operator is
                the dimension of its image. If $T$ is linear,
                bounded, and of finite rank, then it is compact.
                If $T_{n}$ is a sequence of compact linear
                operators, if $Y$ is
                complete, and if $\norm{T_{n}-T}\rightarrow{0}$, then
                $T$ is compact. A sequence $x_{n}$ converges weakly to
                $x$ if, for all $y$,
                $\langle{x_{n},y}\rangle\rightarrow\langle{x,y}\rangle$.
                If $x_{n}$ converges weakly to $x$, then
                and if $T$ is a compact linear operator, then
                $Tx_{n}\rightarrow{Tx}$. If $H$ is a Hilbert space,
                $T$ is a compact self-adjoint operator, and if
                $x_{n}$ converges weakly to $x$, then
                $\langle{Tx_{n},x_{n}}\rangle\rightarrow\langle{Tx,x}\rangle$.
                If $T:H\rightarrow{H}$ is compact and linear, then so
                is its adjoint. The Hilbert-Schmidt theorem says that
                compact self-adjoint operators on a Hilbert space $H$
                have an orthonormal basis of eigenvectors. All of this
                has applications to integral operators and
                Sturm-Liouville Theory.
            \subsubsection{Fundamental Theorems}
                Zorn's Lemma. Hahn-Banach Theorem. Sublinear functionals.
                If $X$ is a normed space, and $Z$ is a subspace, and if
                $f\in{Z'}$, then $f$ be extended to $X$ such that
                $\norm{f}_{X}=\norm{f}_{Z}$.
                This extends Hilbert spaces by Riesz.
                If $X$ is a normed space
                and $x\ne{0}$, then there is an $f\in{X'}$ such that
                $\norm{f}=1$ and $f(x_{0})=\norm{x_{0}}$.
                For all $x$,
                $\norm{x}=\sup\{\norm{f(x)}/\norm{f}:f\in{X'},f\ne{0}\}$.
                There's a thing called bounded variation.
                If $x\in{X}$ and
                $g_{x}(f)=f(x)$ for $f\in{X'}$, then
                $g_{x}\in{X''}$ and $\norm{g_{x}}=\norm{x}$.
                Reflexive implies complete.
                Finite and Hilbert implies reflexive.
                $X'$ separable implies $X$ is separable.
                $X$ separable and reflexive implies
                $X'$ is separable.
                Strong convergence implies weak convergence.
                The converse is not true. If $X$ is finite
                dimensional, then weak convergence
                implies strong convergence. Weak convergence implies
                $\norm{x_{n}}$ is bounded. If
                $x_{n}\rightarrow{x}$ weakly, and if
                $\norm{x_{n}}\rightarrow\norm{x}$, then
                $x_{n}\rightarrow{x}$ strongly.
                Open mapping theorem.
                Closed graph theorem.
                Differentiation is a closed operator on
                $C^{1}[a,b]\rightarrow{C[a,b]}$.
\end{document}