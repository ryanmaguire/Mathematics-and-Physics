\documentclass[crop=false,class=article,oneside]{standalone}
%----------------------------Preamble-------------------------------%
\input{../../../../preamble.tex}
\graphicspath{{../../../../images/}}    % Path to Image Folder.
%--------------------------Main Document----------------------------%
\begin{document}
    \ifx\ifmathcoursesfunctional\undefined
        \section*{Functional Analysis}
        \setcounter{section}{2}
        \renewcommand\thefigure{\arabic{section}.\arabic{figure}}
        \renewcommand\thesubfigure{%
            \arabic{section}.\arabic{figure}.\arabic{subfigure}}
    \else
        \section{Lecture Notes from Fall 2018 (UML)}
    \fi
    \subsection{Lecture 1: September 10, 2018}
        \subsubsection{Completeness}
            We will be talking about a bunch of different sets
            throughout the course, so let's begin with some of
            their notations:
            \begin{itemize}
                \item $\mathbb{N}$\quad The Natural Numbers.
                      We'll use this a lot.
                \item $\mathbb{Z}$\quad The Integers.
                      We'll never talk about these.
                \item $\mathbb{Q}$\quad The Rational Numbers.
                      Good for examples and counterexamples.
                \item $\mathbb{R}$\quad The Real Numbers.
                      These are the numbers we'll primarily
                      be concerned with.
                \item $\mathbb{C}$\quad The Complex Numbers.
                      See comment about $\mathbb{Z}$.
            \end{itemize}
            One of the fundamental properties of $\mathbb{R}$ is
            that is is ``Complete.'' This property is fundamental
            to many theorems involved in a standard calculus or
            real analysis course. The concepts of differentiation
            and convergence rely on completeness, and the
            intermediate value theorem fails without it. On the
            other hand, $\mathbb{Q}$ is not complete. $\mathbb{R}$
            is also something called a ``Field,'' and an ordered
            field at that. $\mathbb{Q}$ is also an ordered field.
            This means addition, subtraction, multiplication, and
            division are well defined operations (No dividing by
            zero, though), and that there is a sense of order on
            the set. For example, zero is less than one. The
            special fact about $\mathbb{R}$ is that it is a
            complete ordered field. In fact, $\mathbb{R}$ is the
            \textit{only} complete ordered field (Up to isomorphism,
            whatever that means). Completeness in $\mathbb{R}$ can
            be stated as the fact that the real numbers have the
            least upper bound property.
            \begin{definition}
                A subset of $\mathbb{R}$ that is bounded above is
                a nonempty set $S\subset{\mathbb{R}}$ such that
                there exists an $M\in\mathbb{R}$ such that for all
                $x\in{S}$, $x\leq{M}$.
            \end{definition}
            \begin{definition}
                An upper bound of a bounded above subset
                $S\subset\mathbb{R}$ is a real number $M\in\mathbb{R}$
                such that for all $x\in{S}$, $x\leq{M}$.
            \end{definition}
            For a subset $S$ of $\mathbb{R}$ is bounded above, then
            there exists infinitely many bounds. The real numbers
            have a special property that every bounded above set
            has a smallest upper bound.
            \begin{theorem}[Least Upper Bound Theorem]
                If $S\subset{\mathbb{R}}$ is bounded above, then
                there exists an $s\in\mathbb{R}$, called the least
                upper bound, such that $s$ and an upper bound
                and for all upper bounds $M$ of $S$, $s\leq{M}$.
            \end{theorem}
            \begin{theorem}
                There exist subsets $S$ of $\mathbb{Q}$ such that
                $S$ is bounded above, yet for all upper bounds
                $M$ there exists an $s$ such that $s$ is an upper
                bound of $S$ and $s<M$.
            \end{theorem}
            \begin{example}
                As an example, consider the set
                $\{x\in\mathbb{Q}:x^{2}\leq{2}\}$.
                This set has no least upper bound. The reason
                being is somewhat related to the fact that the
                least upper bound ``Wants,'' to be $\sqrt{2}$,
                but $\sqrt{2}$ is not a rational number. Thus
                there is no rational number to fill the gap.
                The rationals are incomplete.
            \end{example}
            The least upper bound property gives rise
            to many theorems, many of which are equivalent
            to this axiom.
            \begin{theorem}
                Bounded monotonic sequences converge.
            \end{theorem}
            \begin{proof}
                Let $x_{n}$ be a bounded monotonic sequence that is
                increasing in $\mathbb{R}$. If $x_{n}$ is decreasing, we
                replace the least upper bound with the greatest lower
                bound, and the proof is symmetric.
                Then $S=\{x_{n}:n\in\mathbb{N}\}$ is a
                non-empty subset of $\mathbb{R}$. But $x_{n}$ is
                a bounded sequence, and therefore $S$ is a bounded
                subset of $\mathbb{R}$.
                By the least upper bound property
                there exists a least upper bound $s\in\mathbb{R}$ of
                $S$. We now show that $x_{n}\rightarrow{s}$. Let
                $\varepsilon>0$ be given.
                Since $s$ is the least upper bound,
                $s-\varepsilon$ is not an upper bound of $S$, since
                $s-\varepsilon<s$. Therefore there exists a point
                $x_{N}\in{S}$ such that $s-\varepsilon<x_{n}$.
                Let $N\in\mathbb{N}$ be such that $s-\varepsilon<x_{N}$.
                But $x_{n}$ is monotonically increasing, and therefore
                for all $n>N$, $x_{N}<x_{n}$.
                But, as $s$ is a least upper
                bound of $S$, $x_{n}\leq{s}$. But then, for all $n>N$,
                $0<s-x_{n}<s-x_{N}<\varepsilon$.
                Therefore, $x_{n}\rightarrow{s}$
            \end{proof}
            The least upper bound is, in a sense, the
            reason why decimal expansions of
            real numbers work. For example, let $x_{n}$ be the
            sequence 3, 3.1, 3.14, 3.141, 3.1415, 3.14159, ...
            This sequence, which is the decimal expansion of $\pi$,
            is bounded by $4$. Therefore it has a least upper
            bound. We define the number $\pi$ to be the least upper
            bound of this sequence. Completeness is a very important
            property, but so far it relies on ordering.
            We want to find an equivalent definition of completeness
            that does not rely on ordering, so that we may speak of
            complete spaces, or sets, that have no notion of
            ordering on them.
            We start with a different definiton for the completeness
            of $\mathbb{R}$.
            \begin{definition}
                A Cauchy sequence in $\mathbb{R}$ is a sequence
                $x_{n}:\mathbb{N}\rightarrow\mathbb{R}$ such that
                for all $\varepsilon>0$ there is an $N\in\mathbb{N}$
                such that for all $n,m>N$, $|x_{n}-x_{m}|<\varepsilon$
            \end{definition}
            \begin{theorem}
                \label{%
                    FUNCTIONAL_ANALYSIS:CONVERGENT_SEEQUENCES_%
                    ARE_CAUCHY_SEQUENCES%
                }
                If $x_{n}:\mathbb{N}\rightarrow\mathbb{R}$ is a 
                convergent sequence, then it is a Cauchy-Sequence.
            \end{theorem}
            The converse of Thm.~\ref{%
                FUNCTIONAL_ANALYSIS:CONVERGENT_SEEQUENCES_%
                ARE_CAUCHY_SEQUENCES%
            }
            turns out to be a more general notion of completeness.
            That is, we can apply this to spaces that don't have
            a notion of order, but do have a notion of completeness.
            \begin{theorem}
                If $x_{n}:\mathbb{N}\rightarrow\mathbb{R}$ is
                a Cauchy sequence, then it converges.
            \end{theorem}
            \begin{proof}
                First we prove that Cauchy sequences are bounded.
                Let $\varepsilon=1$. Then $\varepsilon>0$. But
                as $x_{n}$ is a Cauchy sequence, there is an
                $N\in\mathbb{N}$ such that for all $n,m>N$,
                $|x_{n}-x_{m}|<\varepsilon$. That is, for all
                $n,m>N$, $-1<x_{n}-x_{m}<1$. Let $m=N+1$. Then,
                for all $n>N$, $x_{N+1}-1<x_{n}<x_{N+1}+1$.
                Then, for all $n\in\mathbb{N}$,
                $x_{n}\leq\max(\{x_{0},\hdots,1+x_{N+1}\})$. Therefore,
                $x_{n}$ is bounded. Next we need to talk about
                some fundamental properties of subsequences. Let
                $k:\mathbb{N}\rightarrow\mathbb{N}$ be a strictly
                increasing function. Then $x_{n_{k}}$ is a
                subsequence of $x_{n}$. The notation is somewhat
                strange here. We must first note that
                $x_{n}:\mathbb{N}\rightarrow\mathbb{R}$ is really just
                a function from the natural numbers to the real
                numbers. When we write $x_{n}$, what we really
                mean is $x(n)$. But nobody writes $x(0)$, $x(1),$
                $\hdots$ even if that's what they really mean.
                So a subsequence is merely function composition
                of the two functions
                $x:\mathbb{N}\rightarrow\mathbb{R}$
                and $k:\mathbb{N}\rightarrow\mathbb{N}$. So
                $x_{n_{k}}=x(k(n))$. Since $k$ is strictly increasing,
                the ``ordering'' of the sequence remains the same,
                we've simply skipped some
                (Perhaps a lot) of the elements.
                As an example, conside $k=n$. The most boring
                subsequence in the world: It's the exact same
                sequence. Or perhaps $k=2n$, which skips every other
                point. There is an important theorem associated
                to subsequences of bounded sequences, which we will
                use now but prove later. This is the
                Bolzano-Weierstrass theorem: Every bounded sequence
                has a convergent subsequence. But if $x_{n}$ is
                Cauchy, then it is bounded. By the
                Bolzano-Weiestrass theorem there is a convergent
                subsequence $x_{k_{n}}$. Let $x$ be the limit of
                $x_{k_{n}}$. We now must show that
                $x_{n}\rightarrow{x}$. Let $\varepsilon>0$ be given.
                As $x_{k_{n}}\rightarrow{x}$, there is an
                $N_{0}\in\mathbb{N}$ such that for all
                $n>N_{0}$, $|x_{k_{x}}-x|<\frac{\varepsilon}{2}$.
                But as $x_{n}$ is a Cauchy sequence, there is an
                $N_{1}$ such that for all $n,m>N_{1}$, 
                $|x_{n}-x_{m}|<\frac{\varepsilon}{2}$. Let
                $N=\max\{N_{0},N_{1}\}$. Then for all $n>N$,
                $|x-x_{n}|\leq|x-x_{k_{n}}|+|x_{k_{n}}-x_{n}|$.
                But as $k_{n}$ is a subsequence,
                $k_{n}\geq{n}$ and therefore $k_{n}>N$.
                But then $|x_{k_{n}}-x_{n}|<\frac{\varepsilon}{2}$.
                Therefore
                $|x-x_{n}|\leq|x-x_{k_{}}|+|x_{k_{n}}-x_{n}|\leq%
                 \frac{\varepsilon}{2}+\frac{\varepsilon}{2}%
                 =\varepsilon$. Therefore, etc.
            \end{proof}
            Now to prove the Bolzano-Weierstrass Theorem.
            \begin{theorem}
                Every sequence in $\mathbb{R}$ has a monotonic
                subsequence.
            \end{theorem}
            \begin{proof}
                Let $x_{n}$ be a sequence in $\mathbb{R}$. Call
                $n$ a ``peak point'' if $x_{n}\geq{x_{m}}$ for all
                ${m}\geq{n}$. If there are infinitely many of
                these ``peak points,'' then we have obtained
                a decreasing sequence, since the $n^{th}$ peak point
                will be greater than or equal to
                the $(n+1)^{th}$ peak point. We have thus obtained
                a monotonically increasing  subsequence.
                If there are finitely many,
                there are either $0$ or there is a last one,
                $x_{n_{0}}$. But then $x_{n_{0}+1}$ is not a
                peak point. But then there is a $k\in\mathbb{N}$
                such that $k>n_{0}+1$ and $x_{k}\geq{x_{n_{0}+1}}$, for
                otherwise $x_{n_{0}+1}$ would be a peak point.
                But $x_{k}$ is also not a peak point, and so there is
                a $k_{1}$ such that $k_{1}>k$ and
                $x_{k_{1}}\geq{x_{k}}$. This pattern continues, and
                we thus have a monotonically increasing subsequence.
                If there are zero peak points,
                repeat the argument above with $x_{n_{0}}=x_{0}$.
            \end{proof}
            There's probabbly some axiom of choice stuff going on
            here, but oh well.
            \begin{theorem}[Bolzano-Weierstrass Theorem]
                Bounded sequences have a convergent subsequence.
            \end{theorem}
            \begin{proof}
                By the previous theorem, all sequences have a
                monotonic subsequence. But bounded monotonic
                sequences converge. Therefore,
                there is a convergent subsequence.
            \end{proof}
            This notion is so important it has a name.
            \begin{definition}
                A sequentially compact space is a space such that
                every bounded sequence has a convergent subsequence.
            \end{definition}
            \begin{theorem}
                $\mathbb{R}$ is sequentially compact.
            \end{theorem}
            For shit's and giggles, let's prove the intermediate
            value theorem. A result used a lot in calculus, but
            never quite ``proved'' in the rigorous sense of the word.
            \begin{theorem}
                If $f:[a,b]\rightarrow\mathbb{R}$ is continuous and
                if $f(a)<f(b)$, then for all $z\in(f(a),f(b))$,
                there is a $c\in(a,b)$ such that $f(c)=z$.
            \end{theorem}
            \begin{proof}
                Let $x_{1}=\frac{a+b}{2}$. By trichotomy,
                which is one of the ordering properties, either
                $f(x_{1})=z$, $f(x_{1})<z$, or $f(x_{1})>z$. If
                $f(x_{1})=z$, we are done. If not, suppose
                $f(x_{1})<z$. The proof is symmetric for
                $f(x_{1})>z$. Let $x_{2}=\frac{x_{1}+b}{2}$. We
                continue checking whether $f(x_{2})=z$, and continue
                dividing the region in two. If $f(x_{2})<z$, 
                we set $x_{3}=\frac{x_{1}+x_{2}}{2}$, and if
                $f(x_{2})>z$ we set $x_{3}=\frac{x_{2}+b}{2}$.
                Note that $|x_{n+1}-x_{n}|=\frac{b-a}{2^{n+1}}$.
                Moreover, $x_{n}$ converges. Suppose it converges to
                $c$. Then $c\in[a,b]$. That is, the limit of a
                function $x_{n}:\mathbb{N}\rightarrow[a,b]$ is
                contained in $[a,b]$. This is related to the
                ``compactness'' of $[a,b]$. Moreover, it is related
                to the ``closedness'' of $[a,b]$. If there is an
                $N\in\mathbb{N}$ such that $f(x_{N})=z$, then we are
                done. Suppose not. Let $k_{n}$ be the
                elements such that $f(x_{k_{n}})<z$ and $\ell_{n}$
                be the elements such that $f(x_{\ell_{n}})>z$.
                Both of these must be infinite. For suppose not.
                Suppose there is a final $N$ such that
                $f(x_{N})<z$. Then for all $n>N$, $f(x_{n})>z$.
                But from how we've defined the sequence $x_{n}$,
                we have that $x_{n}\rightarrow{x_{N}}$. From the
                continuity of $f$, there is an open interval
                about $X_{N}$ such that for all elements $x$
                inside that interval, we have that$f(x)<z$.
                A contradiction, since eventually some of the
                $x_{n}$ will be in this interval, and thus
                $f(x_{n})<z$. So, both $k_{n}$ and $\ell_{n}$ are
                infinite. From continuity, we have
                $\lim_{n\rightarrow\infty}f(x_{k_{n}})\leq{z}$
                and
                $\lim_{n\rightarrow\infty}f(x_{\ell_{n}})\geq{z}$.
                Thus, $f(c)\leq{z}$ and $f(c)\geq{z}$, and therefore
                $f(c)=z$.
            \end{proof}
            This theorem fails in $\mathbb{Q}$, for it relies on
            the completeness of $\mathbb{R}$. For example,
            $f(x)=x^{2}$ defined on $[0,4]$. Then $2\in[0,4]$, but
            there is no rational such that $x^{2}=2$
            (We use the irrationality of $\sqrt{2}$ a lot, huh?)
            A cuter way to phrase this, in a more topological
            sense, is that the image of $[a,b]$, which is an
            interval, or a connected subset of $\mathbb{R}$,
            is again an interval, or a connected subset of
            $\mathbb{R}$. The proof that continuous functions take
            connected sets (Intervals) to connected sets
            (Again, intervals) is a lot easier than the one presented
            here, but relies on notions from topology.
            So we'll skip that.
            Another commonly used theorem in calculus
            (Again, usually not proved) is the extreme value theorem.
            The extreme value is used to proved Rolle's theorem,
            which says that if $f$ is differentiable on $(a,b)$
            and if $f(a)=f(b)$, then there is a point
            $c\in(a,b)$ such that $f'(c)=0$. This is used to
            prove the mean value theorem, which says that
            if $f$ is differentiable on $(a,b)$, then there is a point
            $c\in(a,b)$ such that
            $f'(c)=\frac{f(b)-f(a)}{b-a}$. This is in turned used to
            prove the Fundamental Theorem of Calculus. So some very
            important stuff going on here. First we prove that
            continuous functions on closed and bounded sets (That is,
            compact sets) are bounded. We stick to closed intervals
            for now.
            \begin{theorem}
                If $f:[a,b]\rightarrow\mathbb{R}$ is continuous,
                then it is bounded.
            \end{theorem}
            \begin{proof}
                Suppose not. Then for all $n\in\mathbb{N}$, there is
                an $x_{n}\in[a,b]$ such that $f(x_{n})>n$. But then
                $x_{n}$ is a bounded sequence, and thus by
                Bolzano-Weierstrass there is a convergent subsequence.
                Let $x$ be the limit of this convergent subsequence.
                But then $f(x_{k_{n}})\rightarrow{f(x)}$, this is
                the fundamental property of continuous functions.
                This is indeed equivalent to the standard
                $\varepsilon-\delta$ definition of continuity.
                But $f(x_{k_{n}})\rightarrow\infty$, and $f(x)<\infty$,
                a contradition. Therefore, etc.
            \end{proof}
            \begin{theorem}[Exreme Value Theorem]
                If $f:[a,b]\rightarrow\mathbb{R}$ is continuous,
                then there exists $c\in[a,b]$ such that
                for all $x\in[a,b]$, $f(x)\leq{f(c)}$
            \end{theorem}
            \begin{proof}
                By the previous theorem,
                $\{f(x):x\in[a,b]\}$ is bounded. By completeness,
                there is a least upper bound. Let $s$ be such
                a bound. If $s$ is the least upper bound, then
                $s-1$ is not a least upper bound. Therefore this is
                an $x_{0}\in[a,b]$ such that $s-1<f(x_{0})$. Similarly,
                for all $n\in\mathbb{N}$, there is an
                $x_{n}\in\mathbb{N}$ such that
                $s-\frac{1}{n}<f(x_{n})$. But $x_{n}$ is a bounded
                sequence, and bounded sequences have a convergent
                subsequence. Let $x$ be the limit of this
                subsequence. From continuity,
                $f(x)=\lim_{n\rightarrow\infty}f(x_{k_{n}})$.
                But $s-\frac{1}{n}\leq{f(x_{k_{n}})}\leq{s}$,
                and therefore $f(x_{k_{n}})\rightarrow{s}$.
                Thus, $f(x)=s$.
            \end{proof}
            Much the way the intermediate value theorem can be
            generalized to say that the continuous image of
            connected sets is connected, the extreme value
            theorem can be generalized to say that the
            continuous image of compact sets is compact.
            The proof is rather easy, but requires
            topology. So, we'll skip that too.
            The requirement of these previous theorems on
            continuity is crucial. Without continuity, functions
            on $[a,b]$ need not be bounded. Without continuity,
            functions on $(a,b)$ can just ``jump,'' right over
            other points. Continuity is very important. It's so
            important, let's talk about it for a moment.
        \subsubsection{Continuity}
            \begin{definition}
                A function $f:S\rightarrow\mathbb{R}$
                on a subset $S\subset\mathbb{R}$ continuous
                at a point $x\in{S}$ is a function such that
                for all $\varepsilon>0$ there is a $\delta>0$
                such that for all $x_{0}\in{S}$ where
                $|x-x_{0}|<\delta$, we have
                $|f(x)-f(x_{0}|<\varepsilon$.
            \end{definition}
            We have used this several times already. It is
            equivalent to the following.
            \begin{theorem}
                A function $f:S\rightarrow\mathbb{R}$
                is continuous at a point $x\in{S}$ if
                and only if for all sequences
                $x_{n}:\mathbb{N}\rightarrow{S}$ such that
                $x_{n}\rightarrow{x}$, we have
                $f(x_{n})\rightarrow{f(x)}$.
            \end{theorem}
            \begin{definition}
                A continuous function $f:S\rightarrow\mathbb{R}$
                is a function that is continuous at all $x\in{S}$.
            \end{definition}
            This definition comes from the fact that
            continuity is a point-wise property, and not a
            ``curve'' property. Continuous functions are
            functions that have point-wise continuity at
            every point. The statement ``A continuous function
            is a curve that you can draw,'' which many have
            heard in calculus is slightly misleading. There
            are functions that are continuous at one point and
            no where else. There are functions that are
            continuous on the irrationals and discontinuous
            on the rationals. For this beast, if $x$ is
            rational, write it as $x=\frac{p}{q}$ where $p$ and
            $q$ are integers and relatively prime. Define
            $f(x)=\frac{1}{q}$. If $x$ is irrational, define
            $f(x)=0$. This function is continuous at every
            irrational number and discontinuous at every
            rational number. There is no ``reverse,'' of this
            function. That is, there is no function continuous
            on $\mathbb{Q}$ and discontinuous on every
            irrational. Uniform continuity is a property of
            all points in the domain of a function. In
            ``fancy,'' notation, we can write the definition
            of a continuous function as follows:
            \begin{definition}
                A continuous function on a set $S$ is a
                function $f:S\rightarrow\mathbb{R}$ such that
                $\forall_{x\in{S}}\forall_{\varepsilon>0}%
                 \exists_{\delta>0}:\forall_{x_{0}\in{S}}:%
                 |x-x_{0}|<\delta%
                 \Rightarrow|f(x)-f(x_{0})|<\varepsilon$
            \end{definition}
            This says, give me a point $x$
            and a positive number
            $\varepsilon$ and I can find a $\delta$ satisfying
            this property. The key part is that you must
            specify the point first. That is, the $\delta$ I
            choose may be very dependent on the $x$ you choose.
            Uniform continuity occurs when a $\delta>0$ can be
            chosen regardless of the $x$. The $\delta$ is
            only dependent on the $\varepsilon$ chosen.
            \begin{definition}
                A uniformly continuous function on a set $S$
                is a function $f:S\rightarrow\mathbb{R}$ such that
                $\forall_{\varepsilon>0}\exists_{\delta>0}%
                 \forall_{x\in{S}}:\forall_{x_{0}\in{S}}:%
                 |x-x_{0}|<\delta,|f(x)-f(x_{0})|<\varepsilon$
            \end{definition}
            \begin{theorem}
                If $f:S\rightarrow\mathbb{R}$ is uniformly
                continuous, and if $x_{n}$ and $y_{n}$
                are sequences in $S$ such that
                $x_{n}-y_{n}\rightarrow{0}$, then
                $f(x_{n})-f(y_{n})\rightarrow{0}$.
            \end{theorem}
            The requirement of uniform continuity is crucial.
            Let $f:(0,1)\rightarrow\mathbb{R}$ be defined by
            $f(x)=\frac{1}{x}$. Then $f$ is continuous, but
            not uniformly continuous. Let $x_{n}=\frac{1}{n}$
            and $y_{n}=\frac{1}{2n}$. Then
            $y_{n}-x_{n}=\frac{1}{2n}\rightarrow{0}$, but
            $f(y_{n})-f(x_{n})=2n-n=n$, and that diverges.
            \begin{theorem}
                If $f:[a,b]\rightarrow\mathbb{R}$ is
                continuous, then it is uniformly continuous.
            \end{theorem}
            The above theorem relies on the fact that
            $[a,b]$ is closed and bounded. Indeed, this is
            the only thing it relies on, the fact that it's
            an interval (Or connected) it unnecessary. We can
            write a more general result.
            \begin{theorem}
                If $f:S\rightarrow\mathbb{R}$ is continuous
                and if $S$ is compact, then $f$ is
                uniformly continuous.
            \end{theorem}
            We end with a brief discussion on sequences of
            functions.
        \subsubsection{Sequences of Functions}
            \begin{definition}
                A sequence of functions
                $f_{n}:S\rightarrow\mathbb{R}$ converges
                point-wise to a function $f$ if
                for all $x\in{S}$, $f_{n}(x)\rightarrow{f(x)}$
            \end{definition}
            Rewriting this in ``fancy,'' notation:
            \begin{definition}
                $f_{n}\rightarrow{f}$ point-wise if
                $\forall_{x\in{S}}\forall_{\varepsilon>0}%
                 \exists_{N\in\mathbb{N}}:\forall_{n>N},%
                 |f(x)-f_{n}(x)|<\varepsilon$
            \end{definition}
            Uniform continuity requires that all of the
            points of the domain converge to $f(x)$ at
            the same speed. That is, given any $\varepsilon>0$
            there is an $N\in\mathbb{N}$ that works for
            all points. Point-Wise convergence does not
            have this property. It is possible for a sequence
            of functions to converge, point-wise, to zero,
            and yet there is a sequence $x_{n}$ such that
            $f_{n}(x_{n})\rightarrow\infty$. Uniform
            convergence does not allow this.
            \begin{definition}
                $f_{n}\rightarrow{f}$ uniformly if
                $\forall_{\varepsilon>0}%
                 \exists_{N\in\mathbb{N}}%
                 \forall_{x\in{S}}:\forall_{n>N},%
                 |f(x)-f_{n}(x)|<\varepsilon$
            \end{definition}
            \begin{theorem}
                If $f_{n}$ is a sequence of
                continuous functions and if
                $f_{n}\rightarrow{f}$ uniformly, then
                $f$ is continuous.
            \end{theorem}
            The word ``uniformly,'' is crucial in
            this definition. This theorem is not
            necessarily true of point-wise converging
            functions. Let
            $f_{n}:[0,1]\rightarrow[0,1]$ be defined by
            $f(x)=x^{n}$. Then $f_{n}$ converges to
            $0$ if $x\ne{1}$, and $1$ if $x=1$. That is,
            the limit function is discontinuous. This is
            possible because $f_{n}$ does not
            converge uniformly, only point-wise.
    \subsection{Lecture 2: September 17, 2018}
        \subsubsection{More on Continuity}
            A function $f$ is continuous at a point $x$ if
            for all $\varepsilon>0$ there exists a $\delta>0$
            such that for all $x_{0}$ such that
            $|x-x_{0}|<\delta$, we have that
            $|f(x)-f(x_{0})|<\varepsilon$. This defines
            continuity at a point. Continuity on a set
            $S$ simply means that $f$ is continuous
            at every point on the set. Uniform continuity,
            however, is a set property. You don't have
            uniform continuity at a point, you have this
            on a set. A function $f$ is uniformly continuous
            if for all $\varepsilon>0$ there is a $\delta>0$
            such that for all $x$ and $x_{0}$ such that
            $|x-x_{0}|<\delta$, it is true that
            $|f(x)-f(x_{0})|<\varepsilon$. Note that in
            the definition of continuity, the point $x$
            is the first thing that is mentioned, whereas in
            uniform continuity we really don't care about it
            until the end of the definition.
            \begin{theorem}
                If $x_{n}$ and $y_{n}$ are sequences such
                in a set $S$ such that $x_{n}-y_{n}\rightarrow{0}$,
                and if $f:{S}\rightarrow\mathbb{R}$, then
                $f$ is uniformly continuous if and only if
                $f(x_{n})-f(y_{n})\rightarrow{0}$
            \end{theorem}
            \begin{theorem}
                If $f:[a,b]\rightarrow\mathbb{R}$ is continuous,
                then $f$ is uniformly continuous.
            \end{theorem}
            \begin{proof}
                Let $x_{n}$, $y_{n}\in[a,b]$ be sequences such
                that $x_{n}-y_{n}\rightarrow{0}$. By the
                Bolzano-Weierstrass theorem there is a
                convergent subsequence $x_{k_{n}}$. Let $x$
                be the limit. But
                $y_{k_{n}}=x_{k_{n}}-(y_{k_{n}}-x_{k_{n}})$,
                and therefore $y_{k_{n}}\rightarrow{x}$.
                Let $X_{n}=x_{k_{n}}$, and $Y_{n}=y_{k_{n}}$.
                Then
                $f(X_{n})-f(Y_{n})%
                 =(f(X_{n})-f(x))-(f(Y_{n}-f(x))$.
                But from continuity, $f(x_{n})\rightarrow{f(x)}$
                and $f(y_{n})\rightarrow{f(x)}$. Therefore, $f$
                is uniformly continuous.
            \end{proof}
            A few notes on pointwise convergence, recapping
            from last week. We say that $f_{n}\rightarrow{f}$
            point-wise on a set $S$ if for all $x\in{S}$,
            $f_{n}(x)-f(x)\rightarrow{0}$. We say that
            $f_{n}\rightarrow{f}$ uniformly on $S$ if
            $\underset{x\in{S}}{\sup}%
             |f_{n}(x)-f(x)|\rightarrow{0}$.
            \begin{example}
                Let $f_{n}(x)=nxe^{-nx}$ on $[0,1]$.
                Then for $x\in[0,1]$, $f_{n}(x)\rightarrow{0}$
                and therefore $f_{n}(x)\rightarrow{0}$ poinwise
                on $[0,1]$. Note that
                $f_{n}'(x)=(n-n^{2}x)e^{-nx}$. This has a
                zero at $x=\frac{1}{n}$, so we see that
                $f_{n}(x)$ has a maximum
                which evaluates to $e^{-1}$. But
                $\sup|f_{n}(x)-f(x)|=\sup|f_{n}(x)|=e^{-1}$.
                So $f_{n}(x)$ does not
                \textit{uniformly} converge to $0$. The
                convergence is only \textit{pointwise}.
            \end{example}
            \begin{theorem}[Weierstrass Approximation Theorem]
                If $f:[a,b]\rightarrow\mathbb{R}$ is continuous,
                then there is a sequence of polynomials
                $f_{n}$ such that $f_{n}\rightarrow{f}$
                uniformly on $[a,b]$.
            \end{theorem}
            \begin{proof}
                We'll assume, for now, that $[a,b]=[0,1]$
                and that $f(0)=f(1)=0$. Extend $f$ to be
                zero outside of $[0,1]$. Let
                $Q_{n}(x)=c_{n}(1-x^{2})^{n}$ on $[-1,1]$,
                and choose $c_{n}$ such that
                $\int_{-1}^{1}Q_{n}(x)dx=1$. So we have:
                \begin{align*}
                    c_{n}\int_{-1}^{1}(1-x^{2})^{n}dx
                    &=2c_{n}\int_{0}^{1}(1-x^{2})^{n}dx
                    &
                    &\geq{2c_{n}}\int_{0}^{1}(1-x)^{n}dx\\
                    &=2c_{n}\int_{0}^{1}(1-x)^{n}(1+x)^{n}dx
                    &
                    &=\frac{2}{n+1}c_{n}
                \end{align*}
            From this we have that $c_{n}\leq{n+1}$. Let
            $f_{n}(x)=\int_{0}^{1}f(t)Q_{n}(x-t)dx$.
            Then $f_{n}(x)$ is a polynomial. Note that
            $f(t)Q_{n}(x-t)$ is roughly zero when $t$ differs
            from $x$ and $n$ is large enough. So we have:
            \begin{equation*}
                f_{n}(x)=\int_{0}^{1}f(t)Q_{n}(x-t)dt
                \approx{f(x)}\int_{0}^{1}Q_{n}(x-t)dt=f(x)
            \end{equation*}
            The remainder of the proof is to quantify this.
            Since $f$ is zero outside of $[0,1]$, if
            we let $s=t-x$, then:
            \begin{align*}
                f_{n}(x)&=\int_{-x}^{1-x}f(s+x)Q_{n}(s)ds
                =\int_{-1}^{1}f(s+x)Q_{n}(s)ds\\
                \Rightarrow|f_{n}(x)-f(x)|
                &=\bigg|\int_{-1}^{1}f(x+t)Q_{n}(t)dt
                -\int_{-1}^{1}f(x)Q_{n}(t)dt\bigg|\\
                \Rightarrow|f_{n}(x)-f(x)|
                &\leq\int_{-1}^{1}|f(x+t)-f(x)|Q_{n}(t)dt
            \end{align*}
            This comes for the fact that$\int_{-1}^{1}Q_{n}(t)=1$
            and from the integral version of the triangle
            inequaility.
            Suppose $\varepsilon>0$. Since $f$ is continuous
            on $[0,1]$, it is uniformly continuous. But
            if $f$ is uniformly continuous then there exists
            a $\delta>0$ such that
            $|f(x+t)-f(x)|<\frac{\varepsilon}{2}$ for all
            $t<\delta$. So we have:
            \begin{equation*}
                |f_{n}(x)-f(x)|\leq
                \int_{-1}^{-\delta}|f(x+t)-f(x)|Q_{n}(t)dt
                +\int_{-\delta}^{\delta}|f(x+t)-f(x)|Q_{n}(t)dt
                +\int_{\delta}^{1}|f(x+t)-f(x)|Q_{n}(t)dt
            \end{equation*}
            But $f$ is continuous on a closed and bounded
            set and therefore $f$ is bounded. Let $M$ be
            such a bound. Then $|(f(x+t)-f(x)|\leq{2M}$.
            We have:
            \begin{equation*}
                |f_{n}(x)-f(x)|\leq
                2M\int_{-1}^{-\delta}Q_{n}(t)dt
                +\frac{\varepsilon}{2}
                \int_{-\delta}^{\delta}Q_{n}(t)dt
                +2M\int_{\delta}^{1}Q_{n}(t)dt
            \end{equation*}
            But for all $t\in[-1,-\delta]$,
            $Q_{n}(t)\leq{Q_{n}(-\delta)}$. Similarly for
            $t$ in $[\delta,1]$. Since $Q_{n}(t)$
            is an even function:
            \begin{equation*}
                |f_{n}(x)-f(x)|\leq
                4MQ_{n}(\delta)+
                \frac{\varepsilon}{2}\int_{-1}^{1}Q_{n}(t)dt
                =4MQ_{n}(\delta)+\frac{\varepsilon}{2}
            \end{equation*}
            But since $\delta>0$, $Q_{n}(\delta)\rightarrow0$.
            Therefore, there is an $N\in\mathbb{N}$ such that
            for all $n>N$,
            $|Q_{n}(\delta)|<\frac{\varepsilon}{8M}$.
            But then $4MQ_{n}(\delta)<\frac{\varepsilon}{2}$.
            Therefore, etc.
        \end{proof}
            Another way to put this is that if $f$ is continuous
            on $[a,b]$ and if $\varepsilon>0$, then there is
            a polynomial $P$ such that for all $x\in[0,1]$,
            $|f(x)-P(x)|<\varepsilon$. There is a generalization
            of this. The set of functions need not be
            polynomials. The set needs to be closed
            under addition, multiplication, and scalar
            multiplication, it must separate points,
            and must not take every point to zero.
            This is the Stone-Weierstrass theorem.
        \subsubsection{Functional Analysis}
            Functional analysis is concerned with normed spaces.
            This is a vector space $V$ with a function, called
            a norm, from $V$ to $[0,\infty)$. This is usually
            written $\norm{\mathbf{x}}$ for an element
            $\mathbf{x}\in{V}$. This norm must satisfy the
            folllowing for all $\mathbf{x}$, $\mathbf{y}\in{V}$:
            \begin{enumerate}
                \item $\norm{\mathbf{x}}=0$ if and only
                      if $\mathbf{x}=\mathbf{0}$
                      \hfill[Definiteness]
                \item $\norm{c\mathbf{x}}=|c|\norm{\mathbf{x}}$
                      for all $c\in\mathbb{R}$.
                      \hfill[Positiveness]
                \item $\norm{\mathbf{x}+\mathbf{y}}%
                       \leq\norm{\mathbf{x}}+\norm{\mathbf{y}}$
                      \hfill[Triangle Inequality]
            \end{enumerate}
            \begin{example}
                The following are normed spaces:
                \begin{enumerate}
                    \begin{multicols}{2}
                        \item $\mathbb{R}$ with $\norm{x}=|x|$
                        \item $\mathbb{R}^{n}$ with
                              $\norm{\mathbf{x}}%
                               =\sqrt{\sum_{k=1}^{n}x_{k}^{2}}$
                        \item $\mathbb{R}^{n}$ with
                              $\norm{\mathbf{x}}%
                               =(\sum_{k=1}^{n}x_{k}^{p})^{1/p}$
                        \item $\ell^{p}$: sequences $x_{n}$
                              such that
                              $\sum_{k=1}^{\infty}|x_{n}|^P%
                               <\infty$.
                        \item $\mathbb{R}^{n}$ with
                              $\norm{\mathbf{x}}%
                               =\max\{x_{1},\hdots,x_{n}\}$
                        \item $\ell^{\infty}$: Bounded sequences
                              with $\norm{v}=\sup\{v_{n}\}$
                    \end{multicols}
                \end{enumerate}
            \end{example}
            From the fact that $\ell^{\infty}$ is a normed space
            we have that the set of convergent sequences,
            again with the $\norm{}_{\infty}$ norm, is also
            a normed space. The set of null sequences, which
            is the set of sequences that converge to zero,
            is also a normed space. A stranger normed space
            is the set of all bounded continuous functions
            $f:S\rightarrow\infty$ with norm
            $\norm{f}=\sup\{|f(x)|\}$. Furthermore, the
            set of all integrable functions with
            bounded integrals, with norm
            $(\int_{S}|f|^{p})^{1/p}$. If you allow integral
            to mean Lebesgue Integrable, then this becomes
            a special space denoted $L^{p}(S)$. As a final
            example, the set of functions
            $f:[a,b]\rightarrow\mathbb{R}$ such that
            $(\int_{a}^{b}\sum_{k=0}^{n}|f^{(k)}|^{p})^{(1/p)}$
            exists, denoted $W^{n,p}([a,b])$ is called
            the Sobolev space.
        \subsubsection{Metric Spaces}
            A lot of the things we wish to
            prove don't rely on the fact that all of these
            spaces are vector spaces. Really, we only care about
            the properties that the norm on the space has.
            What matters is that there's a set and a notion
            of distance on the set. This abstraction is the
            fundamental concept of a metric space.
            \begin{definition}
                A metric space is a set $S$ and a function
                $d:{X}\times{X}\rightarrow[0,\infty)$ such that:
                \begin{enumerate}
                    \item For all $x$, $y\in{X}$, $d(x,y)=0$
                          if and only if $x=y$.
                          \hfill[Definitenes]
                    \item For all $x$, $y\in{X}$,
                          $d(x,y)=d(y,x)$
                          \hfill[Symmetry]
                    \item For all $x$, $y\in{X}$,
                          $d(x,z)\leq{d(x,y)+d(y,z)}$
                          \hfill[Triangile Inequality]
                \end{enumerate}
            \end{definition}
            \begin{theorem}
                If $(V,\norm{})$ is a vector space,
                and if $d:{V}\times{V}\rightarrow[0,\infty)$
                is defined by
                $d(\mathbf{x},\mathbf{y})%
                 =\norm{\mathbf{x}-\mathbf{y}}$,
                then $(V,d)$ is a metric space.
            \end{theorem}
            \begin{proof}
                In order:
                \begin{enumerate}
                    \item If $\norm{\mathbf{x}-\mathbf{y}}=0$,
                          then $\mathbf{x}=\mathbf{y}$.
                          Similarly,
                          $\norm{\mathbf{x}-\mathbf{x}}%
                           =\norm{\mathbf{0}}=0$.
                    \item $d(\mathbf{x},\mathbf{y})%
                           =\norm{\mathbf{x}-\mathbf{y}}%
                           =\norm{(-1)(\mathbf{y}-\mathbf{x})}%
                           =|-1|\norm{\mathbf{y}-\mathbf{x}}%
                           =\norm{\mathbf{y}-\mathbf{y}}%
                           =d(y,x)$
                    \item The triangle inequality follows
                          from the triangle inequality that
                          norms have.
                \end{enumerate}
            \end{proof}
            There are metric spaces that have nothing to do
            with vector spaces or norms. Metric spaces are
            a more abstract object. Every normed space
            has an associated metric space since there
            is the ``induced'' metric.
            \begin{example}
                Let $X$ be a set and let
                $d(x,y)=\begin{cases}%
                            0,&x=y\\%
                            1,&{x}\ne{y}%
                        \end{cases}$
                This is the discrete metric on $X$.
            \end{example}
            \begin{example}
                Let $X=\{a,b,c\}$, and
                $d(a,b)=1$, $d(b,c)=2$. What value
                must $d(a,c)$ have if $d$ is a metric on $X$?
                We need $d(a,c)\leq{d(a,b)+d(b,c)}=1+2=3$.
                Also $d(a,b)\leq{d(a,c)+d(c,b)}=d(a,c)+2$, and
                therefore $-1\leq{d(a,c)}$. Finally, we need
                $d(b,c)\leq{d(b,a)+d(a,c)}=1+d(a,c)$. Together
                we have $d(a,c)\leq{3}$ and $1\leq{d(a,c)}$. So
                Pick $d(a,c)=2$.
                This makes $(X,d)$ a metric space.
            \end{example}
            \begin{example}
                Let $X=\mathbb{R}$ and $d(x,y)=|x-y|$.
                Then $(X,d)$ is a metric space.
            \end{example}
            \begin{example}
                $\mathbb{R}$ with
                $d(x,y)=|f(x)-f(y)|$, where
                $f$ is injective, is a metric space.
            \end{example}
            \begin{example}
                $\mathbb{R}$ with
                $d(x,y)=|\tan^{-1}(x)-\tan^{-1}(y)|$ is a
                metric. Moreover, $d(x,y)<\pi$ for all
                $x,y\in\mathbb{R}$. Thus, we have found
                a metric that makes $\mathbb{R}$ a bounded
                set. As a fun fact, $x_{n}=n$ is a Cauchy
                sequence in this metric space, but
                this sequence does not converge to anything.
                Thus we've found a metric on
                $\mathbb{R}$ such that
                $(\mathbb{R},d)$ is not complete.
            \end{example}
        \subsubsection{Topology}
            \begin{definition}
                The open ball of radius $r>0$
                about a point $x$ in a metric space
                $(X,d)$ is the set
                $B_{r}(x)=\{y\in{X}:d(x,y)<r\}$
            \end{definition}
            The picture for this is a ``circle'' around the
            point $x$ or radius $r$. However, this circle
            can look very strange for weird metrics.
            \begin{example}
                If $X$ is a set and $d$ is the discrete metric,
                then $B_{r}(x)$ is either the point $x$
                (If $r\leq{1}$), or it is the entire set $X$.
            \end{example}
            \begin{example}
                With $X=\mathbb{R}$ and $d$ the standard metric
                $d(x,y)=|x-y|$, we have $B_{r}(x)$ is simply
                the open interval $(x-r,x+r)$.
            \end{example}
            \begin{example}
                Let $X=\mathbb{R}^{2}$ and define
                $d_{p}(x,y)%
                 =(|x_{1}-y_{1}|^{p}+|x_{2}-y_{2}|^{p})^{1/p}$.
                For $p=2$, an open ball is a circle around
                the point $(x,y)$ of radius $r$. For $p=1$,
                we have ``diamonds'' around the point $x$.
                And for $p=\infty$ we have a square
                around $x$.
            \end{example}
            If you have a vector space and a norm on it,
            then the open balls about a point will have the
            property of convexity. Convexity is a vector space
            property, given two points the ``line'' between the
            two remains in the set. Metric spaces have no such
            notion. Since the balls of $\norm{}_{p}$ are not
            convex with $p<1$, we have that $\norm{}_{p}$ is
            a metric on $\mathbb{R}^{n}$
            if and only if $p\geq{1}$.
            \begin{definition}
                We say that a subset of $X$ is open if
                for all $x\in{S}$ there is an $r>0$
                such that $B_{r}(x)\subset{S}$.
            \end{definition}
            \begin{example}
                If $(X,d)$ is a metric space, then
                $X$ is open, $\emptyset$ is open
                (Vacuously true), and if $x\in{X}$ and
                $r>0$, then $B_{r}(x)$ is open.
            \end{example}
            \begin{theorem}
                If $(X,d)$ is a metric space, $x\in{X}$,
                and $r>0$, then $B_{r}(x)$ is an open
                subset of $X$.
            \end{theorem}
            \begin{proof}
                If $z\in{B_{r}(x)}$, let $t=d(x,z)$.
                Then $0\leq{t}<r$. Let $r'=r-t$.
                But if $y\in{B_{r'}(z)}$, then
                $d(x,y)\leq{d(x,z)+d(y,z)}<t+r'=t+r-t=r$.
                Therefore $B_{r'}(z)\subset{B_{r}(x)}$.
            \end{proof}
            \begin{theorem}
                A finite intersection of open sets is open.
            \end{theorem}
            \begin{proof}
                If $\mathcal{U}_{1},\hdots,\mathcal{U}_{n}$
                are open and if
                $x\in\cap_{k=1}^{n}\mathcal{U}_{k}$, then there
                exists $r_{1},\hdots,r_{n}$ such that
                $B_{r_{i}}(x)\subset\mathcal{U}_{i}$. Let
                $r=\min\{r_{1},\hdots,r_{n}\}$. Then
                $B_{r}(x)\subset\cap_{k=1}^{n}\mathcal{U}_{i}$
            \end{proof}
            \begin{theorem}
                Arbitrary unions of open sets are open.
            \end{theorem}
            Infinite intersections need not be open.
            The proof above would fail since the
            $r_{i}$ can form a sequence tending to zero.
            But indeed, let $X=\mathbb{R}$ and let
            $d(x,y)=|x-y|$, and take
            $\mathcal{U}_{n}=(-\frac{1}{n},\frac{1}{n})$.
            Then all of the $\mathcal{U}_{n}$ are open,
            yet the intersection, which is the set $\{0\}$,
            is not open. All of this mumbo-jumbo creates
            the more general notion of a topological space.
            \begin{definition}
                A topological space is a set $X$ and a
                subset $\tau\subset\mathcal{P}(X)$ such that:
                \begin{enumerate}
                    \item $\emptyset,X\in\tau$
                    \item Finite intersections of sets in $\tau$
                          are also sets in $\tau$.
                    \item Arbitrary unions of sets in $\tau$
                          are also sets in $\tau$.
                \end{enumerate}
            \end{definition}
            \begin{definition}
                An open subset of a topological space
                $(X,\tau)$ is a set $\mathcal{U}\in\tau$.
            \end{definition}
            Here, $\mathcal{P}(X)$ denotes the \textit{power set}
            of $X$. This is the set of all subsets of $X$.
            The notion of a topological space generalizes the
            notion of a metric space. There is no notion of
            distance in such spaces, and things can be weird.
            There are topological spaces that have no metric
            associated with them.
    \subsection{Lecture 3: September 24, 2018}
        \subsubsection{Some notes on Homework 1}
            Try to find a subsequence of $n$ such
            that $\cos(k_{n})$ and $\sin(k_{n})$
            have the same limit. Let's first just
            try to find a subsequence such that
            $\cos(k_{n})\rightarrow{1}$. If we can
            do that, we simply need to modify the
            argument so that
            $\cos(k_{n})\rightarrow{\frac{1}{\sqrt{2}}}$.
            Let $k_{n}$ be a sequence of integers
            such that $0<n-2\pi{k_{n}}<2\pi$.
            Let $\varepsilon>0$ and let $N\in\mathbb{N}$
            be such that $N>\frac{2\pi}{\varepsilon}$.
            Then the set
            $\{n-2\pi{k_{n}}:n=1,2,\hdots,N+1\}$
            has $N+1$ elements, and thus by the
            pidgeonhole principle there are two
            elements that are within
            $2\pi/\frac{2\pi}{\varepsilon}$ of each other.
            Let $n_{1}$ and $n_{2}$ be such numbers.
            Then:
            \begin{equation*}
                \cos(n_{2}-n_{1})
                =\cos(n_{2}-n_{1}-2\pi(k_{2}-k_{1}))
                =\cos((n_{2}-2\pi{k}_{2})
                       -(n_{1}-2\pi{k_{1}})
                =\cos(\xi)
            \end{equation*}
            Where $\xi$ is a number such that
            $0<|\xi|<\varepsilon$. But then
            $|1-\cos(\xi)|<\frac{\varepsilon^{2}}{2}$.
            And $n_{2}-n_{1}$ is a natural number,
            so we can find a subsequence of $n$ such
            that $\cos(k_{n})\rightarrow{1}$. Modifying
            this with $\frac{\pi}{4}$
            and $\frac{1}{\sqrt{2}}$ gives the
            desired result.
        \subsubsection{Back to Metric Spaces}
            $X$ is a set and
            $d:{x}\times{X}\rightarrow[0,\infty)$
            is a function such that for all $x$, $y\in{X}$,
            $d(x,y)=0$ if and only if $x=y$,
            $d(x,y)=d(y,x)$, and for all
            $x$, $y$, $z\in{X}$,
            $d(x,z)\leq{d(x,z)+d(z,y)}$. It turns out
            that we can actually write the following:
            \begin{definition}
                A metric space is a set $X$ and a function
                $d:{X}\times{X}\rightarrow\mathbb{R}$
                such that:
                \begin{enumerate}
                    \item $d(x,y)=0$ if and only if
                          $x=y$.
                    \item $d(x,z)\leq{d(x,y)+d(z,y)}$
                \end{enumerate}
            \end{definition}
            By writing the triangle inequality in this
            way, symmetry comes for free
            (The fact that $d(x,y)=d(y,x)$), as well
            as positivity (The fact that $d(x,y)\geq{0}$).
            Since it's easier to prove two things are
            true, rather than four things, it's nice to
            take this as the definition of a metric space,
            and then prove that the two definitions are
            equivalent. Some examples of metric spaces:
            \begin{example}
                If $(X,\norm{})$ is a normed space,
                then $(X,d)$ is a metric space with
                the metric
                $d(\mathbf{x},\mathbf{y})%
                 =\norm{\mathbf{x}-\mathbf{y}}$.
            \end{example}
            \begin{example}
                $\mathbb{R}^{n}$ (for $1\leq{p}<\infty$):
                \begin{equation*}
                    d_{p}(\mathbf{x},\mathbf{y})=
                    \big(
                        \sum_{k=1}^{n}|x_{k}-y_{k}|
                    \big)^{1/p}
                    =\norm{\mathbf{x}-\mathbf{y}}_{p}
                \end{equation*}
            \end{example}
            \begin{example}
                In $\ell^{p}$, which are sequences for
                which
                $\sum_{k=1}^{\infty}|x_{k}|^{p}<\infty$,
                $d_{p}(x,y)$ forms a metric, as well
                as
                $d_{\infty}(x,y)=\sup\{|x_{k}-y_{k}|\}$,
                which is called the supremum norm.
            \end{example}
            \begin{example}
                $C(S,\mathbb{R})$, which is the
                set of continuous functions from
                $S$ to $\mathbb{R}$, letting
                $L^{p}(S)$ be the set of of functions
                such that:
                \begin{equation*}
                    \int_{S}|x(t)|^{p}<\infty
                \end{equation*}
                Then the following is a metric:
                \begin{equation*}
                    d_{p}(x,y)=
                    \bigg(
                        \int_{S}|x(t)-y(t)|^{p}dt
                    \bigg)^{1/p}
                \end{equation*}
                Also,
                $d_{\infty}(x,y)=\sup\{|x(t)-y(t)|\}$,
                which is called the supremum norm.
            \end{example}
            \begin{example}
                Let $C$ be the set of sequences such that
                $x_{n}\rightarrow{0}$. Then, with
                $d_{p}$, this forms a metric space.
                If $C_{0}$ is set of sequences with
                only finitely many non-zero terms,
                then
                $C_{0}\subset{C}\subset{\ell^{\infty}}$.
                Is there a sequence $x\in{C}$ such
                that, for all $1\leq{p}<\infty$,
                $x\notin{\ell^{p}}$.
            \end{example}
        \subsubsection{Inequality of Minkowski and Holder}
            \begin{theorem}
                For non-negative sequences
                $a_{n}$, $b_{n}$, and for $p>1$:
                \begin{equation*}
                    \bigg(
                        \sum_{n=1}^{\infty}(a_{n}+b_{n})
                    \bigg)^{1/p}
                    \leq
                    \bigg(
                        \sum_{n=1}^{\infty}a_{n}^{p}
                    \bigg)^{1/p}
                    +
                    \bigg(
                        \sum_{n=1}^{\infty}b_{n}^{p}
                    \bigg)^{1/p}
                \end{equation*}
            \end{theorem}
            This says that
            $\norm{a+b}_{p}\leq\norm{a}_{p}+\norm{b}_{p}$.
            Which says that $\norm{}_{p}$ satisfies
            the triangle inequality. Proving this result
            requires H\"{o}lder's inequality.
            \begin{theorem}
                If $a_{n}$ and $b_{n}$ are nonnegative
                sequences, if $p>1$, and if $q$ is
                such that
                $\frac{1}{p}+\frac{1}{q}=1$, then:
                \begin{equation*}
                    \sum_{n=1}^{\infty}a_{n}b_{n}
                    \leq
                    \bigg(
                        \sum_{n=1}^{\infty}a_{n}^{p}
                    \bigg)^{1/p}
                    \bigg(
                        \sum_{n=1}^{\infty}b_{n}^{q}
                    \bigg)^{1/q}
                \end{equation*}
            \end{theorem}
            Numbers $p$ and $q$ such that
            $\frac{1}{p}+\frac{1}{q}=1$ are called
            conjugate exponents. When $p=q=2$, this
            is called the Cauchy-Schwartz inequality.
            That is,
            $|\mathbf{a}\cdot\mathbf{b}|%
             \leq\norm{\mathbf{a}}\norm{\mathbf{b}}$
            Proving H\"{o}lder's Inequality requires the
            following from Calc I:
            \begin{theorem}
                If $x$, $y>0$, $p>1$, and if
                $\frac{1}{p}+\frac{1}{q}=1$, then
                $y\leq{\frac{1}{p}x^{p}+\frac{1}{q}y^{q}}$.
            \end{theorem}
            Again, for $p=q=2$, this is easy.
            This says
            $0\leq{\frac{x^{2}+y^{2}}{2}-xy}%
             =\frac{(x-y)^{2}}{2}$, which is indeed true.
            A cute proof of this theorem comes from
            consider the area under the graph of
            $x^{p-1}$. There's an even more general
            result called Young's inequality.
        \subsubsection{Back to Open Sets}
            Let $(X,d)$ be a metric space.
            Recall that the open ball
            of radius $r>0$ centered about the
            point $x\in{X}$ is the set
            $B_{r}(x)=\{y\in{X}:d(x,y)<r\}$.
            We then defined open sets.
            A subset $S\subset{X}$ is a set such that
            for all $x\in{S}$, there is an $r_{x}>0$
            such $B_{r_{x}}(x)\subset{S}$. The subscript
            $r_{x}$ denotes the fact that the radius
            of the ball may depend on the point
            $x$. We then showed the open balls are indeed
            open sets. Moreover, all of $X$ is open,
            and the emptyset $\emptyset$ is open
            in a vacuous sense. That is, you cannot
            find a point that would make $\emptyset$
            \textit{not} open, and thus we say that it
            is open. We then proved the following, which
            led us into a brief discussion on topology:
            Finite unions of open sets are open, and
            arbitrary unions of open sets are open. A
            \textit{topology} on $X$ is a collection of
            subsets of $X$ that obey these two properties
            (And require that $X$ and $\emptyset$ are
            included in this collection).
            \begin{definition}
                Let $(X,d_{X})$ and $(Y,d_{Y})$ be
                metric spaces. A function
                $f:X\rightarrow{Y}$ is continuous
                at a point $x\in{X}$
                if for all $\varepsilon>0$ there is
                a $\delta>0$ such that for all
                $x_{o}\in{X}$ such that
                $d_{X}(x,x_{0})<\delta$, we have
                $d_{Y}(f(x),f(x_{0})<\varepsilon$
            \end{definition}
            \begin{definition}
                If $f:X\rightarrow{Y}$ is a function,
                then the pre-image of
                $S\subset{Y}$ is the set
                $f^{-1}(S)=\{x\in{X}:f(x)\in{S}\}$.
            \end{definition}
            A surprising theorem, and the entire
            basis of the study of topology, goes as
            follows:
            \begin{theorem}
                If $(X,d_{x})$ and $(Y,d_{Y})$
                are metric spaces, the
                $f:X\rightarrow{Y}$ is continuous
                at $x\in{X}$ if and only if
                for all open subsets
                of $S\subset{Y}$ such that
                $f(x)\in{S}$, the prem-image
                $f^{-1}(S)$ is an open subset of $X$.
            \end{theorem}
            This allows us to talk about continuous
            functions without a notion of metric.
            Thus, for topological spaces, this is
            the \textit{definition} of continuity.
            When the space we're discussing is a
            metric space, this theorem shows that the
            definition from topology and the defintition
            from real analysis are in fact equivalent.
            \begin{theorem}
                A function $f:X\rightarrow{Y}$ between
                metric spaces is continuous at a point
                $x\in{X}$ if and only if for all
                sequences $x_{n}$ such that
                $d_{X}(x,x_{n})\rightarrow{0}$, we have
                $d_{Y}(f(x),f(x_{n})\rightarrow{0}$.
            \end{theorem}
            We now have three different ways to talk
            about continuity.
            \begin{definition}
                A sequence $x_{n}$ is a metric space
                $X$ converges to a point $x\in{X}$
                if $d(x,x_{n})\rightarrow{0}$.
            \end{definition}
            \begin{theorem}
                If $(X,d)$ is a metric space,
                $x_{n}\rightarrow{x}$,
                and $x_{n}\rightarrow{y}$,
                then $x=y$.
            \end{theorem}
            \begin{proof}
                For
                $d(x,y)\leq{d(x_{n},x)+d(x_{n},y)}%
                 \rightarrow{0}$. Therefore, etc.
            \end{proof}
            This is not true in a topological space
            In a topological space you can have sequences
            which converge to every single point in the
            space simultaneously. The ability to
            ``Separate,'' points is a special property.
            Hausdorff spaces have this feature. But
            that's for a topology course.
            \begin{theorem}
                If $(X,d)$ is a metric space
                and $x_{n}\rightarrow{x}$, then
                for all $y\in{X}$,
                $d(x_{n},y)\rightarrow{d(x,y)}$.
            \end{theorem}
            \begin{proof}
                For
                $|d(x_{n},y)-d(x,y)|\leq{d(x_{n},x)}%
                 \rightarrow{0}$.
            \end{proof}
            \begin{theorem}
                If $(X,d)$ is a metric space,
                $y\in{X}$, then
                $f:X\rightarrow\mathbb{R}$ defined by
                $f(x)=d(x,y)$ is a continuous function.
                (In fact, it's uniformly continuous).
            \end{theorem}
        \subsubsection{Closed Sets}
            \begin{definition}
                A subset $S$ of a metric space $X$
                is closed if for all
                convergent sequences $x_{n}$
                in $S$, the limit of $x_{n}$ is also
                contained in $x$.
            \end{definition}
            This says that if $S$ is closed, and
            $x_{n}$ is a sequence in $S$ such
            that $x_{n}\rightarrow{x}$, then
            $x\in{S}$.
            \begin{example}
                In $\mathbb{R}$, with the standard
                metric, $(a,b)$ is open,
                $(-\infty,\infty)$ is open (and closed),
                $[a,b]$ is closed,
                $[a,\infty)$ is closed.
            \end{example}
            \begin{example}
                If $X=(0,1)$, and
                $d(x,y)=|x-y|$, then
                $(0,1)$ is closed. This is because
                there is no sequence that converges
                to a point in the space whose limit
                is not in the space. If you have a sequence
                converge to $0$ or $1$, then the
                limit is NOT in $(0,1)$, so we have not
                violated the definition of closedness.
            \end{example}
            \begin{theorem}
                If $(X,d)$ is a metric space,
                then a subset $S\subset{X}$ is open
                if and only if it's compliment $S^{c}$
                is closed.
            \end{theorem}
            \begin{proof}
                Suppose $S$ is open, and let
                $x_{n}$ be a sequence in $S^{c}$.
                Suppose $x_{n}\rightarrow{x}$ and
                $x\in{S}$. But $S$ is open, and thus
                there is an $\varepsilon>0$ such that
                $B_{\varepsilon}(x)\subset{S}$.
                But $x_{n}\rightarrow{x}$, and thus
                this is an $N\in\mathbb{N}$ such that
                for all $n>N$, $d(x,x_{n})<\varepsilon$.
                But then for all $n>N$,
                $x_{n}\in{B_{\varepsilon}(x)}$. But
                $x_{n}\in{S^{c}}$, a contradiction.
                Therefore, $S^{c}$ is closed. On the
                other hand, if $S^{c}$ is closed
                and there is an $x\in{S}$ such that
                for all $r>0$,
                $B_{r}(x)\cap{S}\ne\emptyset$, then
                for all $n\in\mathbb{N}$ there is
                an $x_{n}\in{S^{c}}$ such that
                $d(x,x_{n})<\frac{1}{n}$. But then
                $x_{n}\rightarrow{x}$, and therefore
                $x\in{S^{c}}$. But $x\in{S}$,
                a contradiction. Thus, $S$ is open.
            \end{proof}
            In topology we take the definition of
            closed sets to be the compliment of open
            sets. This theorem shows that the
            topological definition and the real analysis
            definition are equivalent when we consider
            metric spaces.
            \begin{definition}
                A point $x$ in a metric space
                $(X,d)$ is a limit point of a set
                $S\subset{X}$ if there is a sequence
                $x_{n}$ in $S$ such that
                $x_{n}\rightarrow{x}$.
            \end{definition}
            \begin{definition}
                The closure of a subset
                $S$ of a metric space
                $(X,d)$, denoted $\overline{S}$,
                is the set of all
                limit points of $S$.
            \end{definition}
            \begin{theorem}
                The closure of a subset
                $S$ of a metric space $(X,d)$
                is the intersection of all
                closed sets $\Delta$ such that
                $S\subset{\Delta}$.
            \end{theorem}
            With this theorem, we may loosely say that
            the closure of a set $S$ is the
            ``Smallest,'' closed set that contains $S$.
            \begin{definition}
                The closed ball of radius $r>0$ about
                a point $x$ in a metric space
                $(X,d)$ is the set
                $\overline{B}_{r}(x)%
                 =\{y\in{X}:d(x,y)\leq{r}\}$
            \end{definition}
            There exists metric spaces $(X,d)$
            such that
            $\overline{B}_{r}(x)\ne\overline{B_{r}(x)}$.
            For take the discrete metric, $r=1$.
            Then the closure of $B_{1}(x)$ is simply
            the point $x$. However, the closed ball
            $\overline{B}_{1}(x)$ is the entire space.
            Metric spaces can be very weird like this.
            They have a property, that given a nested
            sequence of closed balls whose radius
            tends to zero, there is precisely one
            point that lies in the intersection. However,
            if the radius does not tend to zero it is
            possible that the intersection is empty.
            This is very counter-intuitive.
        \subsubsection{Density}
            \begin{definition}
                A subset $S$ of a metric space $(X,d)$
                is dense if the $\overline{S}=X$.
            \end{definition}
            A subset $S$ is dense in $X$ if every point
            in $X$ can be approximated arbitrarily well
            by points in $S$. For any point $x\in{X}$
            there is a sequence $x_{n}\in{S}$
            such that $x_{n}\rightarrow{X}$. The
            classic example is $\mathbb{Q}$ and
            $\mathbb{R}$. Every real number can be
            approximated arbitrary well by a rational
            number. To see this, just take the continued
            fraction of a real number and stop once
            the approximation is less than
            $\varepsilon$. When we say $\mathbb{Q}$ is
            dense in $\mathbb{R}$, we of course mean with
            respect to the standard metric on $\mathbb{R}$.
            With the discrete metric, this is no longer
            true. 
            \begin{example}
                $\mathbb{Q}$ is dense in $\mathbb{R}$
                with respect to $d(x,y)=|x-y|$.
            \end{example}
            \begin{example}
                $\mathbb{Q}$ is dense in
                $\mathbb{R}$ with respect to
                $d_{p}(x,y)$ for $p\geq{1}$.
            \end{example}
            \begin{example}
                The set of polynomials on the interval
                $[a,b]$ are dense in the set of
                continuous functions on $[a,b]$ with
                respect to the $d_{\infty}$ metric.
                This comes from Weierstrass's Theorem.
            \end{example}
            \begin{example}
                The set of polynomials on $[a,b]$
                is dense in the set of continuous
                functions on $[a,b]$ with respect to
                the $d_{p}$ metric, for $p\geq{1}$. This
                comes from:
                \begin{align*}
                    d_{p}(P,x)&=
                    \bigg(
                        \int_{a}^{b}|P(t)-x(t)|^{p}dt
                    \bigg)^{1/p}\\
                    &\leq\bigg(
                        \int_{a}^{b}
                        |\max\{P(t)-x(t)\}|^{p}dt
                    \bigg)^{1/p}\\
                    &=\bigg(
                        d_{\infty}(P,x)^{p}\int_{a}^{b}dt
                    \bigg)^{1/p}\\
                    &=(b-a)^{1/p}d_{\infty}(P,x)
                \end{align*}
            \end{example}
            \begin{example}
                The continuous functions are not dense
                in the set of integrable functions,
                with respect to the supremum metric
                $d_{\infty}$. This is more or less
                because integrable functions can
                be discontinuous, or have jumps. This
                means, with respect to $d_{\infty}$,
                that no continuous functions could
                approximate such a discontinuous function
                arbitrary well.
            \end{example}
            \begin{definition}
                A countable s is a set
                $X$ such that there is a bijection
                $f:\mathbb{N}\rightarrow{X}$.
            \end{definition}
            Being countable means you can write
            the elements out in a list, or a
            one-to-one correspondence with all of
            the positive integers.
            \begin{example}
                $\mathbb{Q}$ is countable.
            \end{example}
            \begin{example}
                $\mathbb{R}$ is not countable.
            \end{example}
        \subsubsection{A Strange Divergence}
            For any set $A$, you can show
            that $A$ is strictly smaller
            than $\mathcal{P}(A)$. If you
            take $\mathcal{P}(\mathbb{N})$,
            you can show that this is the
            same size as $\mathbb{R}$. The
            question then becomes, is there
            any set whose ``Size,'' is in
            between $\mathbb{N}$ and $\mathbb{R}$?
            Continuing, you can ask the same thing
            about $\mathbb{R}$ and
            $\mathcal{P}(\mathbb{R})$, and so on
            This is called the continuum hypothesis.
            It turns out to be independent of
            the standard axioms of mathematics.
            Random stuff about $\gamma$.
            Transcendental numbers are cool.
        \subsubsection{Back to Reality}
            \begin{definition}
                A separable metric space
                is a metric space $(X,d)$ with
                a countable dense subset $S$.
            \end{definition}
            \begin{example}
                $\mathbb{R}$ is separable, with
                the standard metric, since
                $\mathbb{Q}$ is countable and also
                dense in $\mathbb{R}$.
            \end{example}
            \begin{theorem}
                A finite or countable union
                of countable sets is again countable.
            \end{theorem}
            \begin{example}
                The set of continuous functions on
                $[a,b]$ is separable. For
                take the set of polynomials with
                rational coefficients. This can
                be seen as a countable union of
                countably many elements. For let
                $P_{N}$ be the set of polynomials
                of degree $N$ with rational
                coefficients. This is countable,
                and the set of all polynomials with
                rational coefficients is simply the
                union of $P_{N}$ over all $N$. This
                is dense in the set of polynomials,
                and the set of polynomials is dense
                in $C[a,b]$, and thus
                the set of polynomials with rational
                coefficients is dense in $C[a,b]$. Thus
                $C[a,b]$ is separable.
            \end{example}
            \begin{example}
                $\ell^{p}$ is separable with the
                $d_{p}$ metric, simply use elements
                with rational entries. That is,
                sequences of rational numbers.
            \end{example}
            \begin{example}
                $\ell^{p}$ with the $d_{\infty}$ metric
                is NOT separable. Consider the real
                numbers in $(0,1)$.
            \end{example}
    \subsection{Lecture 4: October 1, 2018}
        \subsubsection{Completeness}
            \begin{definition}
                A complete metric space is a metric
                space $(X,d)$ such that every
                Cauchy sequence $x_{n}$
                in $X$ converges to a point in $X$
                with respect to $d$.
            \end{definition}
            Recall that a sequence $x_{n}$ is Cauchy if
            $\forall_{\varepsilon>0}\exists_{N\in\mathbb{N}}:%
             \forall_{n,m>N},d(x_{n},x_{m})<\varepsilon$.
            Convergence with respect to $d$ means that
            $d(x,x_{n})\rightarrow{0}$.
            \begin{example}
                $\mathbb{R}$ with the standard metric
                $d(x,y)=|x-y|$ is complete.
            \end{example}
            \begin{example}
                $(\mathbb{R}^{n},d_{p})$ is also complete
                for all $n\in\mathbb{N}$.
            \end{example}
            Completeness is both a property of the set
            and the metric itself. It is not a topological
            property.
            \begin{example}
                $(\mathbb{R},d)$, where
                $d(x,y)=|\tan^{-1}(x)-\tan^{-1}(y)|$
                is \textit{not} complete. For let
                $x_{n}=n$. This is a Cauchy sequence,
                as one can see from the graph
                of $\tan^{-1}(x)$. That is, because
                $\tan^{-1}(x)\rightarrow{\pi/2}$,
                $x_{n}=n$ is a Cauchy sequence in this
                metric. Being even more rigorous, let
                $\varepsilon>0$ and
                $N=\ceil{\tan(\pi/2-\varepsilon)}$.
                Then, for all $n,m>N$,
                $d(x_{n},x_{m})%
                 =|\tan^{-1}(n)-\tan^{-1}(m)|%
                 <|\pi/2-\tan^{-1}(\min\{n,m\})|%
                 <|\pi/2-(\pi/2-\varepsilon)|%
                 =\varepsilon$. But $x_{n}$ does not
                converge. For suppose not.,
                Suppose $x_{n}=n\rightarrow{x}$.
                Then for $n>x+1$,
                $d(x_{n},x)=|\tan^{-1}(n)-\tan^{-1}(x)|%
                 <|\tan^{-1}(x+1)-\tan^{-1}(x)|$,
                so $d(x_{n},x)\not\rightarrow{0}$.
                The sequence does not converge.
            \end{example}
            Let $X=\mathbb{R}\cup\{-\infty,\infty\}$.
            Let $d:X\times{X}\rightarrow\mathbb{R}$
            be defined by
            \begin{align*}
                d(x,y)
                &=|\tan^{-1}(x)-\tan^{-1}(y)|\\
                d(\infty,x)
                &=\frac{\pi}{2}-\tan^{-1}(x)
                &
                d(x,\infty)
                &=\frac{\pi}{2}-\tan^{-1}(x)\\
                d(-\infty,x)
                &=\frac{\pi}{2}+\tan^{-1}(x)
                &
                d(x,-\infty)
                &=\frac{\pi}{2}+\tan^{-1}(x)\\
                d(\infty,-\infty)
                &=\pi
                &
                d(-\infty,\infty)
                &=\pi
            \end{align*}
            Then $d$ is a metric on $X$, and moreover
            $(X,d)$ is complete. The counterexample
            we found for $(\mathbb{R},d)$ has been
            ``filled in,'' in a sense. The hole is
            no longer there. The sequence $x_{n}=n$
            now converges to $\infty$. Somewhat
            unsurpringly, $\mathbb{R}$ is
            dense in $X$, with respect to
            $d$. Every element in $X$ is the limit of
            a sequence of elements in $\mathbb{R}$.
            \begin{definition}
                A completion of a metric space
                $(X,d)$ is a complete metric space
                $(\tilde{X},\tilde{d})$
                such that
                $X\subset{\tilde{X}}$ and
                the restriction of
                $\tilde{d}$ onto $X$ is equal
                to $d$.
            \end{definition}
            \begin{theorem}
                Every metric space has
                a completion.
            \end{theorem}
            \begin{definition}
                An isometry between
                metric spaces
                $(X,d_{X})$ and
                $(Y,d_{Y})$ is a function
                $f:X\rightarrow{Y}$ such that
                $d_{X}(x,y)=d_{Y}(f(x),f(y))$
                for all $x,y\in{X}$.
            \end{definition}
            \begin{definition}
                Isometric metric spaces are metric spaces
                with an isometry between them.
            \end{definition}
            \begin{theorem}
                If $(X,d)$ is a metric space
                and $(\tilde{X}_{1},\tilde{d}_{1})$
                and $(\tilde{X}_{2},\tilde{d}_{2})$
                are completions of $(X,d)$, then
                $(\tilde{X}_{1},\tilde{d}_{1})$
                and $(\tilde{X}_{2},\tilde{d}_{2})$
                are isometric.
            \end{theorem}
            This says the completion of a metric space is
            unique up to isometry.
            The Lebesgue space $L^{p}(S)$
            can be defined to be the completion of
            $C(S)$ with respect to the $d_{p}$ metric.
            \begin{theorem}
                $(C(S),d_{\infty})$ is complete.
            \end{theorem}
            \begin{proof}
                Suppose $x_{n}$ is a Cauchy sequence
                and let $\varepsilon>0$. As $x_{n}$ is
                Cauchy, there exists $N\in\mathbb{N}$
                such that for all $n,m>N$,
                $\sup|x_{m}(t)-x_{n}(t)|<\frac{\varepsilon}{3}$.
                But then for all $t\in{S}$,
                $|x_{m}(t)-x_{n}(t)|<\frac{\varepsilon}{3}$,
                for all
                $n,m>N$. That is, if $x_{n}$ is
                a Cauchy sequence in $(C(S),d_{\infty})$,
                then it is a Cauchy sequence in
                $(\mathbb{R},d_{1})$. But
                $(\mathbb{R},d_{1})$ is complete, and
                therefore, for all $t\in{S}$, there is
                an $x(t)$ such that
                $x_{n}(t)\rightarrow{x(t)}$ with respect
                to the $d_{1}$ metric on $\mathbb{R}$. We
                now need to show that $x(t)$ is a continuous
                function. That is, that
                $x(t)\in{C(S)}$. Finally we need to show that
                $x_{n}\rightarrow{d}$ with respect to
                $d_{\infty}$. We need to show that
                for all $\varepsilon>0$ and all $t\in{S}$
                there is a $\delta>0$
                such that for all $|t-t_{0}|<\delta$,
                $|x(t)-x(t_{0})|<\varepsilon$. But for
                all $n,m>N$,
                $\sup\{x_{n}(t)-x_{m}(t)\}<\frac{\varepsilon}{3}$.
                Taking the limit on $m$, we have
                $|x(t)-x_{n}(t)|<\frac{\varepsilon}{2}$.
                But $x_{n}(t)$ is continuous, and thus
                there exists $\delta>0$ such that
                for all $|t-t_{0}|<\delta$,
                $|x_{n}(t)-x_{n}(t_{0})|<\frac{\varepsilon}{3}$.
                But
                $|x(t)-x(t_{0})|\leq%
                  |x(t)-x_{n}(t)|%
                 +|x_{n}(t)-x_{m}(t)|%
                 +|x(t_{0})-x_{n}(t_{0})$
                But
                $|x(t_{0})-x_{n}(t_{0})|<%
                 \sup\{|x(t)-x_{n}(t)|\}<\frac{\varepsilon}{3}$,
                and therefore
                $|x(t)-x(t_{0})|<\varepsilon$.
                So $x(t)$ is continuous.
            \end{proof}
            The Weierstrass Approximation Theorem says that,
            for closed finite intervals $S=[a,b]$,
            $(C(S),d_{\infty})$ is the completion
            of the set of polynomials with respect to
            the $d_{\infty}$ metric. On the other hand,
            $(C[0,1],d_{p}]$ is not complete when
            $1\leq{p}<\infty$. For define the following:
            \begin{equation*}
                H(x)=
                \begin{cases}
                    0,&0\leq{x}\leq{\frac{1}{2}}\\
                    1,&\frac{1}{2}<x\leq{1}
                \end{cases}
            \end{equation*}
            This is discontinuous, and cannot be
            approximated arbitrarily well everywhere
            by any continuous function. However, the
            \textit{area} underneath $H$ can be approximated
            arbitrarily well be continuous functions. For define:
            \begin{equation*}
                x_{n}(t)=
                \begin{cases}
                    0,&0\leq{x}\leq{\frac{1}{2}-\frac{1}{n}}\\
                    n(x-\frac{1}{2}+\frac{1}{n}),
                    &\frac{1}{2}-\frac{1}{n}\leq{x}
                     \leq{\frac{1}{2}}\\
                    1,&\frac{1}{2}<{x}\leq{1}
                \end{cases}
            \end{equation*}
            Then the area under $x_{n}(t)$
            is $\frac{1}{2}+\frac{1}{2n}$, and thus
            $d_{1}(x_{n}(t),x_{m}(t))%
             =|\frac{1}{2m}-\frac{1}{2n}|$,
            and therefore $x_{n}(t)$ is a Cauchy sequence.
            But $x_{n}(t)$ does not converge in
            $(C[0,1],d_{1})$. For suppose not, suppose
            $x_{n}(t)\rightarrow{x(t)}$, and
            $x(t)\in{C[0,1]}$.
            If $x(1/2)\geq{1/2}$, then, as $x(t)$ is
            continuous, there is a $\delta>0$ such that
            for all $|t-1/2|<\delta$,
            $x(t)>1/4$. But then
            $d(x_{n},x)=\int_{0}^{1}|x(t)-x_{n}(t)|dt%
            \geq\int_{1/2-\delta/2}^{1/2}|x(t)-x_{n}(t)|dt$.
            But $|x|=|(x-y)+y|\leq{|x-y|+|y|}$,
            and thus
            $|x|-|y|\leq{|x-y|}$. From this we have
            $d(x_{n}(t),x(t))\geq%
             \int_{1/2-\delta/2}^{1/2}(x(t)-x_{n}(t))dt%
             >\int_{1/2-\delta/2}^{1/2}\frac{1}{4}dt%
             -\int_{0}^{1/2}x_{n}(t)dt%
             =\frac{1}{4}\delta-\frac{1}{2n}%
             \rightarrow{\frac{1}{4}}\delta$.
            But then $d(x_{n}(t),x(t))\not\rightarrow{0}$.
            Therefore $x_{n}(t)$ does not converge.
            \begin{theorem}
                If $1\leq{p}<\infty$, then
                $(\ell^{p},d_{p})$ is complete.
            \end{theorem}
            \begin{proof}
                Let $x_{n}$ be a Cauchy sequence
                in $(e\ell^{p},d_{p})$,
                $x_{n}=x_{n}(1),x_{n}(2),\hdots,x_{n}(k),\hdots$
                Then, for $n,m\in\mathbb{N}$,
                $d_{p}(x_{n},x_{m})%
                 =(%
                    \sum_{k=0}^{\infty}|x_{n}(k)-x_{m}(k)|^{p}%
                  )^{1/p}$
                As $x_{n}$ is Cauchy, for all 
                $\varepsilon>0$ there is an $N\in\mathbb{N}$
                such that for all $n,m>N$,
                $d_{p}(x_{n},x_{m})<\varepsilon$.
                But then, for all $n,m>N$ and all
                $k\in\mathbb{N}$,
                $|x_{n}(k)-x_{m}(k)|^{p}<d_{p}(x_{n},x_{m})^{P}%
                 <\varepsilon^{p}$.
                But then
                $|x_{n}(k)-x_{m}(k)|<\varepsilon$. Therefore
                $x_{n}(k)$ is a Cauchy sequence in
                $(\mathbb{R},d)$, and this metric space is
                complete. Therefore, for all $k\in\mathbb{N}$,
                there is a $z_{k}$ such that
                $x_{n}(k)\rightarrow{z_{k}}$. We now need to
                show that $z_{k}$ is an element of
                $\ell^{p}$ and that
                $x_{n}\rightarrow{z_{k}}$ with respect to
                the $d_{p}$ metric. For let $N\in\mathbb{N}$.
                Then
                $\sum_{k=0}^{N}|x_{n}(k)-x_{m}(k)|^{p}%
                 \leq{\sum_{k=0}^{\infty}|x_{n}(k)-x_{m}(k)|^{p}}%
                 <\varepsilon^{p}$. Taking the limit on $m$,
                we have
                $\sum_{k=0}^{N}|z_{k}-x_{n}(k)|<\varepsilon^{p}$.
                The reason we have written a finite sum is to
                avoid getting into trouble with limits. An
                infinite sum is itself a limit, and taking
                limits of limits can get very messy very easily.
                For example,
                $f(n,m)=\frac{m}{n+m}$. Taking the limit on
                $m$ first results in $1$, whereas taking the
                limit on $n$ first gives you $0$.
                That is,
                $\lim_{n}\lim_{m}f(n,m)%
                 \ne\lim_{m}\lim_{n}f(n,m)$.
                You have to
                be careful when considering limits of limits.
                With this we have shown that
                $z_{k}-x_{n}(k)\in\ell^{p}$ for all
                $n\in\mathbb{N}$. But $x_{n}\in\ell^{p}$,
                and $\ell^{p}$ is closed under addition.
                Therefore $z_{k}\in\ell^{p}$. But also,
                for $n>N$, we have
                $d_{p}(x_{n},z)<\varepsilon$. Thus,
                $x_{n}$ converges.
            \end{proof}
            \begin{theorem}
                If $(X,d)$ is complete and $S$ is a closed
                subset of $X$, then $(S,d_{S})$ is complete,
                where $d_{S}$ is the restriction of
                $d$ onto $S$.
            \end{theorem}
            \begin{proof}
                Let $x_{n}$ be a Cauchy sequence in $S$. Then
                $x_{n}\rightarrow{x}$, $x\in{X}$,
                since $x_{n}$ is Cauchy in $X$
                and $X$ is complete. Since $S$ is closed,
                $x\in{S}$. Therefore, etc.
            \end{proof}
            \begin{theorem}
                If $(X,d)$ is complete and
                $S\subset{X}$ is not closed,
                then $(S,d_{S})$ is not complete.
            \end{theorem}
            \begin{proof}
                If $S$ is not closed then there
                is a convergent sequence $x_{n}\in{S}$
                whose limit it not in $S$. But
                then $x_{n}$ is a Cauchy sequence in
                $X$, and therefore is also a
                Cauchy sequence in $S$, but
                $x_{n}$ does not converge in $S$.
                Therefore $(S,d_{S})$ is not complete.
            \end{proof}
            Recall that $c_{0}$ is the set of sequences which
            tend to zero. That is, it is the set of
            null sequences.
            \begin{theorem}
                $c_{0}$ is a closed subset of
                $(\ell^{\infty},d_{\infty})$
            \end{theorem}
            \begin{proof}[proof 1]
                Let $x_{n}$ be a sequence in $c_{0}$
                that converges to $z\in\ell^{\infty}$
                with respect to $d_{\infty}$.
                Then
                $\sup\{|x_{n}(k)-z_{k}|\}\rightarrow{0}$.
                We need to show that $z\in{c_{0}}$.
                Let $\varepsilon>0$. Let $N_{1}\in\mathbb{N}$
                be such that
                $n>N$ implies
                $\sup\{|x_{n}(k)-z_{k}\}<\frac{\varepsilon}{2}$.
                But $x_{n}\in{c_{0}}$ for all $n$, and thus
                $x_{n}(k)\rightarrow{0}$ as $k\rightarrow\infty$.
                Thus, there is an $N_{2}\in\mathbb{N}$
                such that $n>N_{2}$ implies
                $|x_{n}(k)<\varepsilon$.
                But then for $n>\max\{N_{1},N_{2}\}$,
                $|z_{k}|\leq|z_{k}-x_{n}(k)|+|x_{n}(k)|%
                 <\varepsilon$.
            \end{proof}
            \begin{proof}[Proof 2]
                We can also show that
                $c_{0}^{C}$ is open.
                Let $x\in{c_{0}^{C}}$. Then there is
                an $r>0$ and a subsequence
                $x_{k_{n}}$ of $x$ such that
                $x_{k_{n}}>r$ for all $n$.
                But then $B_{r/2}(x)$ is
                an open ball contained in $c_{0}^{C}$.
                For if $y\in{B_{r/2}(x)}$, then
                $d_{\infty}(x,y)%
                 =\sup\{|x_{n}-y_{n}|\}<r<2$,
                and thus
                $|y_{k_{n}}-x_{k_{n}}|<r/2$,
                and there for $|y_{k_{n}}|>r/2$.
                Thus, $y$ is not a null sequence and
                $c_{0}^{C}$ is open. So
                $c_{0}$ is closed.
            \end{proof}
            Let $X$ be the set of sequences with only
            finitely many nonzero terms.
            Then $(X,d_{\infty}$ is not complete.
            Let $x_{1}=(1,0,0,\hdots)$,
            $x_{2}=(1,1/2,0,0,\hdots)$,
            $x_{n}=(1,1/2,\hdots,1/n,0,0,\hdots)$.
            Then
            $d_{\infty}(x_{n},x_{m})=1/\max\{n,m\}\rightarrow{0}$.
            But clearly
            $x_{n}\rightarrow(1,1/2,\hdots,1/n,\hdots)$, which
            is an element of $c_{0}$, but not an element
            of $X$. Thus $X$ is not closed, and therefore is
            not complete. Returning to $C[0,1]$, when we had
            that sequence of continuous functions that clearly
            converged to a discontinuous functions, we still
            needed to show that there is no continuous function
            that the $x_{n}(t)$ converged to. Here we've embedded
            $X$ into a bigger space, shown that the
            sequence converges to something outside of $X$,
            in our case an element of
            $c_{0}\setminus{X}$, and then used the uniqueness
            of limits to show that the limit does
            not converge in $X$.
    \subsection{Lecture 5: October 11, 2018}
        \subsubsection{Banach Fixed Point Theorem}
            If $(X,d)$ is a complete metric space,
            and if $T:X\rightarrow{X}$ satisfies
            the property that, for all $x$ and $y$
            in $X$, $d(T(x),T(y))<kd(x,y)$ for
            some $k<1$, then $T$ has a unique
            point $x$, called a fixed point,
            such that $T(x)=x$.
            \begin{definition}
                A contraction of a metric
                space $(X,d)$ is a function
                $T:{X}\rightarrow{X}$ such that there
                exists a $k\in(0,1)$ such that
                for all $x,y\in{X}$,
                $d(T(x),T(y))<kd(x,y)$.
            \end{definition}
            \begin{definition}
                A fixed point of a function
                $f:X\rightarrow{X}$ is a point
                $x\in{X}$ such that
                $f(x)=x$.
            \end{definition}
            \begin{theorem}[%
                Banach's Fixed Point Theorem%
            ]
                If $(X,d)$ is a complete
                metric space and $T:X\rightarrow{X}$
                is a contraction, then there is
                a unique fixed point $x\in{X}$
                with respect to $T$.
            \end{theorem}
            \begin{definition}
                A Lipschitz continuous function is a
                function $f:[a,b]\rightarrow\mathbb{R}$
                such that there is an $L\in\mathbb{R}$
                such that
                $|f(x)-f(y)|<L|x-y|$ for all
                $x,y\in[a,b]$.
            \end{definition}
            This says that the slopes of the
            secant lines of the
            function are bounded. The square root
            function $y=\sqrt{x}$ is an example
            of a function that is not Lipschitz. The
            slopes of secant lines go to infinity
            as the points tend towards the origin.
            \begin{theorem}[Picard's Theorem]
                If $f:[a,b]\times\mathbb{R}%
                    \rightarrow\mathbb{R}$
                is Lipschitz continuous,
                Then there is a unique function
                $x:[a,b]\rightarrow\mathbb{R}$
                such that
                $\frac{dx}{dt}=f(t,x(t))$ and $x(a)=a$.
            \end{theorem}
            \begin{proof}
                We prove Picard by using the
                Banach Fixed Point Theorem. First
                we write the problem as an integral
                equation.
                If $\dot{x}=f(t,x(t))$, then:
                \begin{equation*}
                    x(t)
                    =\int_{a}^{t}\frac{dx}{dt}dt
                    =x_{0}+\int_{a}^{t}f(t,x(t))dt
                \end{equation*}
                Let $(X,d)$ be $C[a,b]$ with the
                supremum norm $d_{\infty}$. Then
                $(x,d)$ is a complete metric space.
                Let $T:{X}\rightarrow{X}$ be defined
                by:
                \begin{equation*}
                    Tx=x_{0}+\int_{a}^{t}f(t,x(t))dt
                \end{equation*}
                All we need to do is show that $T$ is
                a contraction. Applying the
                Banach Fixed Point theorem then
                shows that there is a unique
                fixed point of $T$, thus showing
                that there is a unique solution
                to our original initial value problem.
                If $x,y\in{X}$, then:
                \begin{align*}
                    d(Tx,Ty)
                    &=\sup\{|Tx(t)-Ty(t)|\}\\
                    &=\sup\{
                        (x_{0}+
                         \int_{a}^{t}f(t,x(t))dt)
                       -(x_{0}+
                         \int_{a}^{t}f(t,y(t))dt)
                    \}\\
                    =&\sup\{
                        \int_{a}^{t}f(t,x(t))dt)-
                        \int_{a}^{t}f(t,y(t))dt)
                    \}\\
                    &\leq\int_{a}^{t}|
                        f(t,x(t))-f(t,y(t))|dt
                \end{align*}
                But from the Lipschitz continuity
                of $f$, we have:
                \begin{align*}
                    d(Tx,Ty)&\leq
                    L\int_{a}^{t}|x(t)-y(t)|dt\\
                    &\leq{L}(t-a)d(x,y)\\
                    &\leq{L}(b-a)d(x,y)
                \end{align*}
                So $T$ is a contraction for
                $L(b-a)<1$. Usually we can
                extend this solution by taking
                $b$ as the initial condition and
                stepping forward one interval
                at a time. We'll take a different
                approach. We have that
                $d(Tx,Ty)\leq{L}(b-a)d(x,y)$. From
                this, we obtain:
                \begin{align*}
                    d(T^{2}x,T^{2}y)
                    &\leq{L}\int_{a}^{b}d(Tx,Ty)dt\\
                    &\leq{L}\int_{a}^{t}
                        L(t-a)d(x,t)dt\\
                    &=\frac{L^{2}}{2}(t-a)^{2}d(x,y)\\
                    &\leq
                    \frac{L^{2}}{2}(b-a)^{2}d(x,y)
                \end{align*}
                Applying induction, we have:
                \begin{equation*}
                    d(T^{n}x,T^{n}y)
                    \leq\frac{L^{n}}{n!}(b-a)^{n}
                \end{equation*}
                But this tends to zero, and thus
                there is an $N$ such that,
                for all $n>N$, $T^{n}$ is a
                contraction. But then, by the
                Banach Fixed Point Theorem, there
                is a unique point $x$ such that
                $T^{n}x=x$. But then
                $Tx=T^{n}(Tx)$, and thus
                $Tx$ is a fixed point of
                $T^{n}$. But the fixed point of
                $T^{n}$ is unique, and $x$ is a
                fixed point. Therefore
                $Tx=x$. Therefore, etc.
            \end{proof}
            Without Lipschitz continuous you may
            lose uniqueness, but you still have
            existence. This is Peano's theorem.
            An example is $\dot{x}=\sqrt{x}$
            with $x(0)=0$.
            This has solutions $x(t)=0$ and
            $(t)=t^{2}/4$. Now back to compactness.
            \subsubsection{Compactness}
                \begin{definition}
                    A metric space $(X,d)$ is
                    sequentially compact if every
                    sequence in $X$ has a convergent
                    subsequence.
                \end{definition}
                In topology there is a difference
                between sequential compactness
                and regular compactness, but in
                metric spaces they turn out
                to be the same.
                A subset of $S$ of $X$ is
                compact if every sequence in
                $S$ has a subsequence which converges.
                That is, $(S,d)$ is compact.
                \begin{theorem}
                    A subset $S$ of a compact
                    metric space $(X,d)$ is compact
                    if and only if $S$ is closed.
                \end{theorem}
                \begin{proof}
                    For let $x_{n}$ be a sequence
                    in $S$. Then $x_{n}$ is a
                    sequence in $X$ and thus there
                    is a convergent subsequence
                    $x_{k_{n}}$ with a limit $x$.
                    But $x_{k_{n}}$ is in $S$ and
                    $S$ is closed, and therefore
                    $x$ is in $S$. Thus, $S$
                    is compact. Conversely, if
                    $S$ is compact, suppose it is
                    not closed. Then there is a point
                    $y\in{X}$ such that $y$ is a
                    limit point of $S$ but not
                    contained in $S$. Let
                    $x_{n}$ be a sequence that
                    converges to $y$. Then, as
                    $S$ is compact, there is
                    a convergent subsequence. But
                    the limit of this subsequence
                    is $y$, a contradiction as
                    $y\notin{S}$. Therefore $S$
                    is closed.
                \end{proof}
                \begin{theorem}
                    If $(X,d)$ is a compact metric
                    space, then
                    $(X,d)$ is complete.
                \end{theorem}
                \begin{proof}
                    If $x_{n}$ is Cauchy in $X$,
                    then there is a convergent
                    subsequence $x_{k_{n}}$
                    in $X$. But if $x_{k_{n}}$
                    converges to $x$, then
                    $x_{n}$ converges to $x$ as
                    well, as $x_{n}$ is Cauchy.
                    Therefore, $(X,d)$ is complete.
                \end{proof}
                \begin{theorem}[Heine-Borel Theorem]
                    A subset of
                    $\mathbb{R}^{n}$ is
                    compact if and only if
                    it is closed and bounded.
                \end{theorem}
                \begin{example}
                    The closed unit ball
                    of $\ell^{p}$ is not compact,
                    if $1\leq{p}\leq{\infty}$.
                    Let $x_{n}(m)$ be the sequence
                    (of sequences) such that
                    $x_{n}(m)=1$ if $n=m$, and
                    zero otherwise. Then
                    $d_{p}(x_{n},x_{m})=2^{1/p}$,
                    so $x_{n}$ has no subsequence
                    which is Cauchy. But then there
                    is no convergent subsequence
                    either, and therefore
                    $\ell^{p}$ is not compact.
                \end{example}
                \begin{example}
                    The closed unit ball in
                    $(C[0,1],d_{\infty})$ is
                    not compact. For let
                    $x_{n}(t)=t^{2^{n}}$. Then
                    (Do some calculus) the maximum of
                    $d(x_{n},x_{n+1})$ is always
                    $1/4$. So this has no subsequence
                    which is Cauchy, and thus no
                    convergent subsequence exists.
                \end{example}
                \begin{definition}
                    A metric space $X$ is totally
                    bounded if for all
                    $\varepsilon>0$ there is a finite
                    number of points $x_{n}$ such
                    that $B_{\varepsilon}(x_{n})$
                    covers the entirety of $X$.
                \end{definition}
                \begin{theorem}
                    A compact metric space is
                    totally bounded.
                \end{theorem}
                \begin{proof}
                    Suppose not. Then there is an
                    $\varepsilon>0$ such that
                    no finite collection
                    $B_{\varepsilon}(x_{n})$
                    is a covering of $X$. Let
                    $x_{1}\in{X}$. Then
                    $B_{\varepsilon}(x_{1})$ is not
                    $X$. Thus there is an $x_{2}$
                    such that
                    $x_{2}\notin%
                     B_{\varepsilon}(x_{1})$.
                    But also
                    $B_{\varepsilon}(x_{1})\cup%
                     B_{\varepsilon}(x_{2})$ is
                    not the entirety of $X$.
                    Continuing we have that there
                    is a sequence $x_{n}$ such that,
                    for all $n\ne{m}$,
                    $d(x_{n},x_{m})\geq{\varepsilon}$.
                    So there is no convergent
                    subsequence. But $X$ is
                    compact, a contradiction.
                    Therefore, etc.
                \end{proof}
                There are metric spaces that are
                bounded but not totally bounded.
                For let
                $X=\mathbb{R}$ and $d$ be the
                discrete metric. Then, for
                $\varepsilon=1/2$, the is no
                finite covering. Every point needs
                it's own ball, so the covering is
                uncountable.
                \begin{theorem}
                    If $(X,d)$ is complete and
                    totally bounded, then it
                    is compact.
                \end{theorem}
                \begin{proof}
                    Let $x_{n}$ be a sequence
                    in $X$. Let $\varepsilon=1$. Then
                    there are finitely many points
                    $y_{k}$ such that
                    $B_{\varepsilon}(y_{k})$ covers
                    $X$. Then one of these
                    balls has infinitely many of
                    the $x_{n}$. Similarly, for
                    $\varepsilon=\frac{1}{n}$, there
                    is a finite number of points
                    $y_{k}$ such that
                    $B_{\frac{1}{n}}(y_{k})$ covers
                    $X$. Thus there is a point with
                    infinitely many of the $x_{n}$
                    in it. So, we can find a
                    subsequence such that, for
                    $n,m>N$,
                    $d(x_{k_{n}},x_{k_{m}})<%
                     \frac{1}{N}$. But $(X,d)$ is
                    complete, and therefore
                    $x_{k_{n}}$ converges. Therefore
                    $x_{n}$ has a convergent
                    subsequence. Thus, $(X,d)$ is
                    compact.
                \end{proof}
                \begin{theorem}
                    Compact spaces are separable.
                \end{theorem}
                \begin{proof}
                    If $X$ is compact, then
                    it is totally bounded. But
                    then, for $\varepsilon=1/n$
                    there is a finite covering of
                    $X$ with balls of radius
                    $\varepsilon$. Then,
                    taking all of the
                    centers of all of the points
                    for all $n$ (Countable union
                    of finite points is countable),
                    we obtain a countable dense
                    subset.
                \end{proof}
                \begin{example}
                    There are ``infinite dimension''
                    sets that are also compact. Two
                    in particular worth mentioning.
                    The first is the hilbert Cube.
                    It's a subset of $\ell^{2}$
                    whose elements are such that
                    $|x_{n}|<1/n$. That is, elements
                    are sequences whose $n^{th}$
                    elements are less than
                    $1/n$. This is compact.
                    Arzela-Ascoli. Peano.
                \end{example}
    \subsection{Lecture 6: October 15, 2018}
        \subsubsection{Normed Spaces}
            We're finally going to put some structure on these
            sets, and talk about vector spaces. In a metric
            space, the only thing you can really talk about
            is the distance between points. In a vector space
            we have a lot more structure. We will start off
            with vector spaces over the reals $\mathbb{R}$.
            The main properties are that there is a
            $\mathbf{0}$ element, addition is well defined
            and is both associative and commutative,
            there is a notion of scalar multiplication that
            is associative, and the distributive law holds.
            \begin{example}
                $\mathbb{R}^{n}$, with it's usual notion
                of addition, and with scalar multiplication
                defined over $\mathbb{R}$, is a vector space.
            \end{example}
            \begin{definition}
                A norm on a vector space $X$ over $\mathbb{R}$
                is a function $\norm{}:X\rightarrow\mathbb{R}$
                such that:
                \begin{enumerate}
                    \item For all $\mathbf{x}\in{X}$,
                          $\norm{\mathbf{x}}\geq{0}$ and
                          $\norm{\mathbf{x}}=0$ if and only
                          if $\mathbf{x}=\mathbf{0}$.
                          \hfill[Positive Definiteness]
                    \item For all $\mathbf{x}\in{X}$ and
                          $c\in\mathbb{R}$,
                          $\norm{c\mathbf{x}}%
                           =|c|\norm{\mathbf{x}}$
                          \hfill[Homogeneity]
                    \item For all $\mathbf{x},\mathbf{y}\in{X}$,
                          $\norm{\mathbf{x}+\mathbf{y}}%
                           \leq\norm{\mathbf{x}}%
                           +\norm{\mathbf{y}}$
                          \hfill[Triangle Inequality]
                \end{enumerate}
            \end{definition}
            We have seen before that
            $d(\mathbf{x},\mathbf{y})%
             =\norm{\mathbf{x}-\mathbf{y}}$
            defines a metric, and thus $(X,d)$ is a metric space.
            Thus, for every vector space there is an associated
            metric space, the metric $d$ called the
            \textit{induced} metric.
            \begin{definition}
                A normed vector space is a vector space
                $X$ over $\mathbb{R}$ with a norm
                $\norm{}$ on $X$.
            \end{definition}
            \begin{example}
                $\mathbb{R}^{n}$ with
                $\norm{\mathbf{x}}_{p}$, for $p\geq{1}$,
                is a normed vector space.
            \end{example}
            \begin{example}
                $\ell^{p}$ with $\norm{x}_{p}$ is
                also a normed vector space.
            \end{example}
            \begin{example}
                $C[a,b]$ equipped with the supremum norm,
                $\norm{x(t)}_{\infty}$,
                is a normed vector space.
            \end{example}
        \subsubsection{Inner Product Spaces}
            \begin{definition}
                An inner product on a vector space
                $X$ over $\mathbb{R}$ is a function
                $\langle\rangle:X\rightarrow\mathbb{R}$
                such that:
                \begin{enumerate}
                    \item For all $x\in{X}$,
                          $\langle{\mathbf{x},\mathbf{x}}%
                           \rangle\geq{0}$
                          and
                          $\langle\mathbf{x},\mathbf{x}\rangle=0$
                          if and only
                          if $\mathbf{x}=\mathbf{0}$.
                          \hfill[Positive Definiteness]
                    \item For all $\mathbf{x},\mathbf{y}\in{X}$,
                          $\langle\mathbf{x},\mathbf{y}\rangle%
                           =\langle\mathbf{y},\mathbf{x}\rangle$
                          \hfill[Symmetry]
                    \item For all
                          $\mathbf{x},\mathbf{y},\mathbf{z}%
                           \in{X}$
                          and all $\alpha,\beta\in\mathbb{R}$,
                          $\langle\alpha\mathbf{x}%
                           +\beta\mathbf{y},\mathbf{z}\rangle%
                           =\alpha\langle\mathbf{x},\mathbf{z}%
                           \rangle+\beta\langle\mathbf{y},%
                           \mathbf{z}\rangle$
                          \hfill[Linearity]
                \end{enumerate}
            \end{definition}
            \begin{example}
                $\mathbb{R}^{2}$ with
                $\langle(x_{1},x_{2}),(y_{1},y_{2})\rangle%
                 =x_{1}y_{1}+x_{2}y_{2}$ is an inner product.
                Replacing this with $\mathbb{R}^{n}$ and doing
                $\sum_{k=1}^{n}x_{k}y_{k}$ is also an inner
                product. This is the usual dot product that one
                sees in a vector calculus course. In $\ell^{2}$,
                $\sum_{k=1}^{\infty}x_{k}y_{k}$ is an inner
                product as well. Note also that
                $\sum|x_{i}y_{i}|$ converges since
                $|x_{i}y_{i}|\leq\frac{1}{2}|x_{i}^{2}|%
                 +\frac{1}{2}|y_{i}|^{2}$.
            \end{example}
            \begin{example}
                In $C[a,b]$, let
                $\langle{x(t),y(t)}\rangle%
                 =\int_{a}^{b}x(t)y(t)dt$. This defines an
                inner product.
            \end{example}
            \begin{definition}
                An inner product space is a vector space
                $X$ over $\mathbb{R}$ with an inner product
                $\langle\rangle$.
            \end{definition}
            \begin{theorem}[Cauchy-Schwarz Inequality]
                If $X$ is an inner product space
                and $x,y\in{X}$, then
                $|\langle{x,y}\rangle<\norm{x}\norm{y}$
            \end{theorem}
            \begin{proof}
                For all $y\in\mathbb{R}$,
                $\langle{x+ty,x+ty}\rangle%
                 =\langle{x,x}\rangle%
                 +2t\langle{x,y}\rangle%
                 +t^{2}\langle{y,y}\rangle%
                 =\norm{x}^{2}+2t\langle{x,y}\rangle%
                 +t^{2}\norm{y}^{2}$. Thus we have a
                quadratic in $t$. But this is always positive,
                and thus the discriminant must be non-positive. Therefore
                $(2\langle{x,y})^{2}-4\norm{x}^{2}\norm{y}^{2}%
                 \leq{0}$
                and thus
                $|\langle{x,y})|\leq\norm{x}\norm{y}$.
            \end{proof}
            \begin{theorem}
                If $X$ is a vector space over $\mathbb{R}$
                and $\langle\rangle$ is an inner product,
                then
                $\norm{\mathbf{x}}%
                 =\sqrt{\langle\mathbf{x},\mathbf{y}\rangle}$
                is a norm on $X$.
            \end{theorem}
            \begin{proof}
                Positivity, homogeneity, and definiteness are
                pretty easy. The only tricky thing to check is
                the triangle inequality. We have that
                $\norm{x+y}=\langle{x+y,x+y}\rangle$,
                and this simplify to
                $\norm{x}^{2}+2\langle{x,y}\rangle+\norm{y}^{2}$.
                But from the Cauchy-Schwartz inequality, we
                have $\langle{x,y}\rangle\leq\norm{x}\norm{y}$.
                Thus
                $\norm{x+y}^{2}\leq\norm{x}^{2}%
                 +2\norm{x}\norm{y}+\norm{y}^{2}%
                 =(\norm{x}+\norm{y})^{2}$. Taking square roots
                 completes the theorem.
            \end{proof}
            In $\mathbb{R}^{n}$, the Cauchy-Schwartz inequality
            says that the dot product of two vectors is less
            than or equal to the product of the magnitude
            of the two vectors.
            This is obvious from the fact that the dot product
            of two vector is the product of the magnitudes and
            the \textit{cosine} of the angle between them.
            Since the cosine of a number is less than or equal
            to one, this would complete the theorem.
            In $\ell^{p}$ and $L^{p}$ spaces, this is the
            special case of the H\"{o}lder inequality for
            when $p=q=2$.
        \subsubsection{Convergence in Normed Spaces}
            In a metric space, convergence meant that
            $d(x_{n},x)\rightarrow{0}$. In a normed space
            we have the induced metric, and thus we may define
            convergence as $\norm{x_{n}-x}\rightarrow{0}$.
            \begin{definition}
                A convergent sequence in a normed space $X$
                is a sequence $x_{n}$ such that there is an
                $x\in{X}$ such that
                $\norm{x_{n}-x}\rightarrow{0}$.
            \end{definition}
            Since
            $\norm{y}=\norm{(y-x)+x}\leq\norm{y-x}+\norm{x}$,
            it follows that
            $|\norm{x}-\norm{y}|\leq\norm{x-y}$.
            But then if $x_{n}\rightarrow{x}$, then
            $|\norm{x_{n}}-\norm{x}|\leq\norm{x_{n}-x}$,
            and $\norm{x_{n}-x}\rightarrow{0}$. Therefore
            $\norm{x_{n}}\rightarrow\norm{x}$. That is,
            the norm function is a continuous function.
            Similarly, if $x_{n}\rightarrow{x}$, then
            $\langle{x_{n},y}\rangle\rightarrow%
             \langle{x,y}\rangle$.
            In fact, if $x_{n}\rightarrow{x}$ and
            $y_{n}\rightarrow{y}$, then
            $\langle{x_{n},y_{n}}\rangle%
             \rightarrow\langle{x,y}\rangle$. To see this, we
            have
            $\langle{x_{n},y_{n}}\rangle-\langle{x,y}\rangle%
             =\langle{x_{n}-x,y}\rangle+\langle{x,y-y_{n}}\rangle$
            and therefore
            $|\langle{x_{n},y_{n}}\rangle-\langle{x,y}\rangle%
             \leq\norm{x_{n}-x}\norm{y_{n}}%
             +\norm{x}\norm{y-y_{n}}$. But $\norm{x-x_{n}}\rightarrow{0}$
            and $\norm{y-y_{n}}\rightarrow{0}$. But also
            $\norm{y_{n}}=\norm{(y_{n}-y)+y}\leq\norm{y_{n}-y}+\norm{y}$,
            which is bounded. Therefore
            $\langle{x_{n},y_{n}}-\langle{x,y}\rangle\rightarrow{0}$.
            So inner product spaces and normed spaces are metric spaces
            and we can define everything we did for metric spaces and all
            of the previous results remain true. That is, the notions and
            theorems pertaining to convergence, completeness, compactness,
            the notion of open and closed. All of these still make sense in
            these new spaces.
        \subsubsection{Banach Spaces and Hilbert Spaces}
            \begin{definition}
                A Banach Space is a normed vector space $X$ that is
                complete with respect to the induced metric.
            \end{definition}
            \begin{definition}
                A Hilbert Space is an inner product space $X$ that is
                complete with respect to the induced metric.
            \end{definition}
        \subsubsection{Linear Operators}
            Let $X$ and $Y$ be normed spaces. A mapping
            $T:X\rightarrow{Y}$ is called a linear operator if, for
            all $x,y\in{X}$, and for all $\alpha,\beta\in\mathbb{R}$,
            $T(\alpha{x}+\beta{y})=\alpha{T(x)}+\beta{T(y)}$. Usually, with
            operators, we simply write $Tx$ and $Ty$. Similar to how
            we write matric multiplication over vectors. In $\mathbb{R}^{n}$,
            every $n\times{n}$ matrix defines a linear operator.
            \begin{definition}
                A linear operator from a normed vector space $X$ to
                a normed vector space $Y$ is a function
                $T:X\rightarrow{Y}$ such that, for all $x,y\in{X}$
                and for all $\alpha,\beta\in\mathbb{R}$,
                $T(\alpha{x}+\beta{y})=\alpha{Tx}+\beta{Ty}$.
            \end{definition}
            \begin{definition}
                A bounded linear operator from a normed vector space
                $X$ to a normed vector space $Y$ is a linear operator
                $T:X\rightarrow{Y}$ such that there is a $K\in\mathbb{R}$
                such that for all $x\in{X}$, $\norm{Tx}\leq{K}\norm{x}$
            \end{definition}
            In a just world, ``bounded'' would mean
            $\norm{Tx}\leq{K}$. However, the only linear mapping that does
            this is the zero mapping. For if $\norm{Tx}=1$,
            then $\norm{T(2x)}=2$, and so on, and thus no linear mapping
            is bounded (With the exception of the zero mapping).
            Boundedness of a norm $T:X\rightarrow{Y}$ depends on
            the norms of the space.
            \begin{theorem}
                Bounded linear operators are continuous.
            \end{theorem}
            \begin{proof}
                If $x_{n}\rightarrow{x}$, then
                $\norm{Tx_{n}-Tx}=\norm{T(x_{n}-x)}$. But
                $T$ is bounded, and thus there is a $K$ such that
                $\norm{T(x_{n}-x)}\leq{K}\norm{x_{n}-x}$. But
                $\norm{x_{n}-x}\rightarrow{0}$. Therefore, etc.
            \end{proof}
            The converse is also true.
            \begin{theorem}
                If $T$ is a continuous linear operator,
                than there exists a $\delta>0$ such that for
                all $x\in{B}_{\delta}(0)$,
                $\norm{Tx-T0}<1$. But from linearity,
                $T0=0$, and thus $\norm{Tx}<1$. Then for any
                $z\in{Z}$, we have
                $\norm{\frac{\delta}{2}\frac{z}{\norm{z}}}=\frac{\delta}{2}$,
                and thus $\norm{T(\frac{\delta}{2}\frac{z}{\norm{z}})}<1$.
                Letting $K=\delta$, we have
                $\norm{Tx}<K\norm{x}$. Thus, $T$ is bounded.
            \end{theorem}
            Continuity at 0 implies uniform continuity since
            if $x_{n}-y_{n}\rightarrow{0}$, then
            $\norm{Tx_{n}-Ty_{n}}=\norm{T(x_{n}-y_{n})}%
             \leq{K}\norm{x_{n}-y_{n}}\rightarrow{0}$.
            The set of bounded linear operators form a vector space,
            where addition is $(S+t)(x)=(Sx)+(Tx)$, and scalar multiplication
            is defined by $(\alpha{T})(x)=\alpha(Tx)$. We must show that
            when you add two bounded linear operators, the result is a
            bounded linear operator.
            \begin{theorem}
                If $T_{1}:X\rightarrow{Y}$ and $T_{2}:X\rightarrow{Y}$
                are bounded linear operators, then $T_{1}+T_{2}$ is a
                bounded linear operator.
            \end{theorem}
            \begin{proof}
                For let $T_{1}$ and $T_{2}$ be bounded. Then there are
                $K_{1},K_{2}$ such that, for all $x\in{X}$,
                $\norm{T_{1}x}\leq{K_{1}}\norm{x}$ and
                $\norm{T_{2}x}\leq{K_{2}}\norm{x}$. But then
                $\norm{(T_{1}+T_{2})x}=\norm{T_{1}x+T_{2}x}%
                 \leq\norm{T_{1}x}+\norm{T_{2}x}%
                 \leq{K_{1}}\norm{x}+K_{2}\norm{x}$. Let $K=K_{1}+K_{2}$.
            \end{proof}
            \begin{theorem}
                If $T:X\rightarrow{Y}$ is a bounded linear operator, and
                $\alpha\in\mathbb{R}$, then $\alpha{T}$ is a bounded
                linear operator.
            \end{theorem}
            \begin{proof}
                For
                $\norm{\alpha{Tx}}=|\alpha|\norm{Tx}%
                 \leq|\alpha|K\norm{x}=K\norm{\alpha{x}}$.
            \end{proof}
            We write $B(X,Y)$ to denote the set of bounded linear
            operators from $X$ to $Y$. That is, linear operators
            $T:X\rightarrow{Y}$.
            We can define a norm on $B(X,Y)$ as follows:
            $\norm{T}_{B}%
             =\sup_{x\in{X},x\ne{0}}\{\frac{\norm{Tx}}{\norm{x}}\}$.
            This is the ``Smallest $K$,'' used as a bounded for the linear
            operator $T$. This shows that
            $\norm{Tx}_{Y}\leq\norm{T}_{B}\norm{x}_{X}$.
    \subsection{Lecture 7:October 22, 2018}
        \subsubsection{Bounded Linear Operators}
            A bounded linear operator is a function
            $T:X\rightarrow{Y}$ between normed spaces
            $X$ and $Y$ such that $T$ is linear, and
            there exists a $K\in\mathbb{R}$ such that,
            for all $x\in{X}$,
            $\norm{Tx}_{Y}\leq{K}\norm{x}_{X}$. The
            norm of $T$, $\norm{T}$, is then defined
            as the smallest such $K$. Equivalently:
            \begin{equation*}
                \norm{T}=
                \sup\Big\{\frac{\norm{Tx}_{Y}}{\norm{x}_{X}}:
                          x\in{X},x\ne{0}\Big\}
                =\sup\{\norm{Tx}_{Y}:\norm{x}_{X}=1\}
            \end{equation*}
            The set of all bounded linear operators
            from a normed space $X$ to a normed space
            $Y$ is denoted $B(X,Y)$. This is a vector
            space with addition defined as
            $(T+S)x=(Tx)+(Sx)$ and $(aT)x=a(Tx)$.
            \begin{theorem}
                $\norm{T}$ defines a norm on
                $B(X,Y)$.
            \end{theorem}
            \begin{proof}
                For $\norm{T}\geq{0}$ and
                $\norm{Tx}=0$ if and only if
                $Tx=0$ for all $x\in{X}$, and thus
                $T$ is the zero operator. If
                $\alpha\in\mathbb{R}$, then:
                \begin{equation*}
                    \norm{\alpha{T}}
                    =\sup\Big\{
                        \frac{\norm{\alpha{T}x}_{Y}}{\norm{x}_{X}}:
                        x\in{X},x\ne{0}\Big\}
                    =|\alpha|\sup\Big\{
                        \frac{\norm{Tx}_{Y}}{\norm{x}_{X}}:
                        x\in{X},x\ne{0}\Big\}
                    =|\alpha|\norm{T}
                \end{equation*}
                Finally, if $S,T\in{B}(X,Y)$, then:
                \begin{align*}
                    \norm{S+T}&=\sup\Big\{
                        \frac{\norm{(S+t)x}_{Y}}{\norm{x}_{X}}:
                        x\in{X},x\ne{0}\}\\
                    &=\sup\Big\{
                        \frac{\norm{Sx+Tx}_{Y}}{\norm{x}_{X}}:
                        x\in{X},x\ne{0}\Big\}\\
                    &\leq\sup\Big\{
                        \frac{\norm{Sx}_{y}+\norm{Tx}_{Y}}
                             {\norm{x}_{X}}:
                        x\in{X},x\ne{0}\Big\}\\
                    &\leq\norm{T}+\norm{S}
                \end{align*}
            \end{proof}
            \begin{theorem}
                If $Y$ is a Banach space, and 
                if $X$ is a normed space, then
                $B(X,Y)$ is a Banach space.
            \end{theorem}
            \begin{proof}
                For let $T_{n}$ be a Cauchy sequence
                in $B(X,Y)$ and let $\varepsilon>0$.
                Then there exists $N_{0}\in\mathbb{N}$
                such that for all $n,m>N_{0}$,
                $\norm{T_{n}-T_{m}}<\varepsilon$. That is,
                for all $n,m>N_{0}$:
                \begin{align*}
                    \sup\Big\{
                        \frac{\norm{T_{n}x-T_{m}x}_{Y}}
                             {\norm{x}_{X}}:
                        x\in{X},x\ne{0}\Big\}
                    &\leq\varepsilon\\
                    \Rightarrow
                    \frac{\norm{T_{n}x-T_{m}y}_{Y}}
                         {\norm{x}_{X}}
                    &\leq\varepsilon
                \end{align*}
                That is, $T_{n}x$ is a Cauchy sequence
                in $Y$ for any fixed value $x\in{X}$.
                But $Y$ is a Banach space, and is therefore
                complete. But then if $T_{n}x$ is a Cauchy
                sequence in $Y$ it has a limit $y\in{Y}$.
                Let $Tx=\lim_{n\rightarrow\infty}T_{n}x$
                for all $x\in{X}$.
                Then $T\in{B(X,Y)}$. For:
                \begin{equation*}
                    T(x+y)
                    =\lim_{n\rightarrow\infty}T_{n}(x+y)
                    =\lim_{n\rightarrow\infty}(T_{n}x+T_{n}y)
                    =Tx+Ty
                \end{equation*}
                And similarly $(\alpha{T})x=\alpha{T}x$.
                Lastly, $T$ is bounded. For all $n,m>N$ we have
                $\norm{T_{n}x-T_{m}x}_{Y}/\norm{x}_{X}<\varepsilon$.
                Taking the limit on $m$, we have
                $\norm{Tx-T_{n}x}_{Y}/\norm{x}_{X}\leq\varepsilon$
                for all $n>N_{0}$. Thus,
                $\norm{T_{n}x-Tx}_{X}\leq\varepsilon\norm{x}_{X}$.
                But
                $\norm{Tx-T_{n}x}_{Y}=\norm{T_{n}x-(T_{n}-Tx)}_{Y}$,
                and therefore
                $\norm{Tx}\leq\varepsilon\norm{x}_{X}+\norm{T_{n}x}$,
                and $\norm{T_{n}x}\leq\norm{T_{n}}$, and therefore
                $\norm{Tx}\leq\varepsilon\norm{X}_{X}+%
                 \norm{T}\norm{x}_{X}$. But then
                $\norm{Tx}_{Y}\leq%
                 (\varepsilon+\norm{T_{n}})\norm{x}_{X}$.
                But $T_{n}$ is bounded, and therefore
                $T$ is bounded. Finally, we must show that
                $T_{n}\rightarrow{T}$ in $B(X,Y)$ with respect
                to the norm $\norm{T_{n}-T}$. That is, we must
                show that $\norm{T-T_{n}}\rightarrow{0}$. This
                follows since
                $\norm{Tx-T_{n}x}_{Y}/\norm{x}_{X}<\varepsilon$
                for $n>N_{0}$, and therefore
                $\norm{T-T_{n}}<\varepsilon$. Therefore, etc.
            \end{proof}
        \subsubsection{Dual Spaces}
            So if $Y$ is a Banach space, and $X$ is any normed
            space, then $B(X,Y)$ is a Banach space. One of the
            most important cases is $Y=(\mathbb{R},||)$, where
            $||$ is the normal absolute value ``norm.''
            $B(X,\mathbb{R})$ is a Banach space, and it is
            called the continuous dual space of $X$, written
            $X'$. Elements of $X'$ are called bounded linear
            functionals. These are bounded linear operators
            whose range of the operator is the real numbers.
            The characterization, or the representation, or
            realization, of these dual spaces is a major
            topic in functional analysis. A lot of these
            theorems are do to a mathematician by the name
            of Riesz.
            \begin{example}
                A functional takes an element of a normed
                space $X$ and spits out a real number. For
                example, if $X$ is the space of continuous
                functions, then the following are
                functionals:
                \begin{align*}
                    f_{1}(x)&=\int_{0}^{1}x(t)t^{2}\diff{t}
                    &
                    f_{2}(x)&=x(0.5)
                    &
                    f_{3}(x)&=0
                \end{align*}
            \end{example}
            Let $X=(\mathbb{R}^{2},\ell^{1}$. What does
            $X'$ look like? That is, what is the dual
            space of $X$? let $f:X\rightarrow\mathbb{R}$
            be defined by
            $f(x_{1},x_{2})=2x_{1}-5x_{2}$. Then
            $f\in{X'}$ and $\norm{f}=5$. More generally,
            every element of $\mathbb{R}^{2}$ defines
            and element of $X'$. Given
            $(a,b)\in\mathbb{R}^{2}$, we define
            $f(x_{1},x_{2})=ax_{1}+bx_{2}$. $f$ is then linear,
            and:
            \begin{align*}
                |f(x_{1},x_{2})|
                &=|ax_{1}+bx_{2}|\\
                &\leq|a||x_{1}|+|b||x_{2}|\\
                &\leq\max\{|a|,|b|\}(|x_{1}|+|x_{2}|)\\
                &=\norm{(a,b)}_{\infty}
                \norm{(x_{1},x_{2})}_{\ell^{1}}
            \end{align*}
            And therefore $f$ is bounded, as $\norm{(a,b)}_{\infty}$
            is a bound. That is, $\norm{f}\leq\norm{(a,b)}_{\infty}$.
            By choosing $x=(x_{1},x_{2})$, where $x_{1}=1$ and
            $x_{2}=0$ if $|b|\leq|a|$, and $x_{1}=0$ and
            $x_{2}=1$ otherwise, we ge
            $|f|=\max\{(a,b)\}=\norm{(a,b)}_{\infty}$.
            Therefore $\norm{f}=\norm{(a,b)}_{\infty}$. On
            the other hand, if $f\in{X'}$, let
            $a=f(1,0)$ and $b=f(0,1)$. Then, for all
            $(x_{1},x_{2})\in\mathbb{R}^{2}$:
            \begin{equation*}
                f(x_{1},x_{2})
                =f(x_{1}(1,0)+x_{2}(0,1))
                =x_{1}f(1,0)+x_{2}f(0,1)
                =ax_{1}+bx_{2}
            \end{equation*}
            So the dual of $(\mathbb{R}^{2},\ell^{1})$
            looks very much like $(\mathbb{R}^{2},\ell^{\infty})$.
            In fact, $(\mathbb{R}^{2},\ell^{1})'$ and
            $(\mathbb{R}^{2},\ell^{\infty})$ are isometric
            and isomorphic. That is, we really can't tell them
            apart and we can consider them as the same thing.
            More generally,
            $(\mathbb{R}^{n},\ell^{n})'=(\mathbb{R}^{n},\ell^{\infty})$.
            Even more general, if $p$ and $q$ are exponential
            conjugates of each other (That is,
            $\frac{1}{q}+\frac{1}{p}=1$), then
            $(\mathbb{R}^{n},\ell^{p})'=(\mathbb{R}^{n},\ell^{p})$
            for all $1\leq{p}\leq\infty$. Saying $p=\infty$ is
            equivalent to saying $q=1$. Setting $p=q=2$, we have
            $(\mathbb{R}^{n},\ell^{2})'=(\mathbb{R}^{n},\ell^{2})$.
            This is true of any Hilbert space: The dual of any
            Hilbert Space $\mathcal{H}$ is itself. That is,
            $\mathcal{H}'=\mathcal{H}$. This is one of the
            Riesz Representation Theorems. In infinite dimensions,
            $(\ell^{p})'=\ell^{q}$, where $p$ and $q$ are such that
            $\frac{1}{p}+\frac{1}{q}=1$, and $1\leq{p}<\infty$.
            Now, we cannot allow $p=\infty$. For
            $(\ell^{\infty})'$ is not equal to $\ell^{1}$.
            \begin{theorem}
                If $1\leq{p}<\infty$ and
                $\frac{1}{p}+\frac{1}{q}=1$, then
                $(\ell^{p})'=\ell^{q}$.
            \end{theorem}
            \begin{proof}
                If $(f_{1},f_{2},\hdots)\in\ell^{q}$, then let
                $f:\ell^{p}\rightarrow\mathbb{R}$ be defined by
                $f(x_{1},x_{2},\hdots)=\sum_{k=1}^{\infty}x_{k}f_{k}$.
                This converges from H\"{o}lder's inequality:
                \begin{equation*}
                    \sum_{k=1}^{\infty}|x_{k}f_{k}|
                    \leq
                    \Big(\sum_{k=1}^{\infty}f_{k}^{q}\Big)^{1/q}
                    \Big(\sum_{k=1}^{\infty}x_{i}^{p}\Big)^{1/p}
                \end{equation*}
                And therefore
                $|fx|=\norm{(f_{1},f_{2},\hdots)}_{q}%
                 \norm{(x_{1},x_{2},\hdots)}_{p}$. That is,
                Moreover $f$ is linear. Therefore
                $f\in(\ell^{p})'$ and
                $\norm{f}\leq\norm{(f_{1},f_{2},\hdots)}_{q}$.
                On the other hand, let
                $x_{i}=|f_{i}|^{q/p}\sgn(f_{i}$. Then
                \begin{equation*}
                    fx=\sum_{k=1}^{\infty}f_{k}x_{k}
                    =\sum_{k=1}^{\infty}|f_{k}|^{q/p+1}
                \end{equation*}
                But $\frac{1}{p}+\frac{1}{q}=1$, and thus
                $\frac{q}{p}+1=q$. Thus:
                \begin{align*}
                    |fx|
                    &=\sum_{k=1}^{\infty}|f_{k}|^{q}\\
                    &=\norm{(f_{1},f_{2},\hdots)}_{q}^{q}\\
                    &=\norm{(f_{1},f_{2},\hdots)}_{q}
                        \norm{(f_{1},f_{2},\hdots)}_{q}^{q-1}\\
                    &=\norm{(f_{1},f_{2},\hdots)}_{q}
                        \Big(\sum_{k=1}^{\infty}|f_{k}^{q}
                        \Big)^{\frac{q-1}{q}}
                    &=\norm{(f_{1},f_{2},\hdots)}_{q}
                        \Big(\sum_{k=1}^{\infty}|x_{k}|^{p}
                        \Big)^{1/p}\\
                    &=\norm{(f_{1},f_{2},\hdots)}_{q}\norm{x}_{p}
                \end{align*}
                Therefore, $\norm{f}=\norm{(f_{1},f_{2},\hdots)}_{q}$.
                Thus, for all $y\in\ell^{q}$ there is a bounded
                linear operator $f\in\ell^{p}$ such that
                $\norm{y}_{\ell^{q}}=\norm{f}_{(\ell^{p})'}$. That
                is, every $(f_{i})\in\ell^{q}$ defines an element
                of $(\ell^{p})'$ by
                $fx=\sum_{k=1}^{\infty}f_{k}x_{k}$, for any
                $(x_{i})\in\ell^{p}$. So $\ell^{q}$ can
                be \textit{embedded} into to $(\ell^{p})'$.
                Now we need to show that this embedding is
                the entirety of $(\ell^{p})'$. If
                $f\in(\ell^{p})'$, let
                $f_{i}=f(e_{i})$, where $e_{i}$ is the
                sequence $(0,0,\hdots,1,0,0,\hdots)$, where
                the 1 occurs in the $i^{th}$ spot. We need
                to show that $(f_{i})\in\ell^{q}$ and
                $fx=\sum_{k=1}^{\infty}f_{k}x_{k}$
                for all $x\in\ell^{p}$. If $x\in\ell^{p}$, then:
                \begin{align*}
                    x=\sum_{k=1}^{\infty}x_{k}e_{k}
                    \Rightarrow
                    fx=f\Big(\sum_{k=1}^{\infty}x_{k}e_{k}\Big)
                    =\sum_{k=1}^{\infty}f(x_{k}e_{k})
                    =\sum_{k=1}^{\infty}x_{k}f(x_{k})
                    =\sum_{k=1}^{\infty}x_{k}f_{k}
                \end{align*}
                Choosing $x_{k}=|f_{k}|^{q/p}\sgn(f_{k})$ and apply
                H\"{o}lder.
            \end{proof}
\end{document}