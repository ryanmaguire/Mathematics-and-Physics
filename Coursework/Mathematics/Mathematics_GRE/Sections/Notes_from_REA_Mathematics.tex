\documentclass[crop=false,class=article,oneside]{standalone}
%----------------------------Preamble-------------------------------%
\input{../../../../preamble.tex}
\graphicspath{{../../../../images/}}    % Path to Image Folder.
%--------------------------Main Document----------------------------%
\begin{document}
    \ifx\ifcoursesmathgre\undefined
        \section*{Mathematics GRE Subject Test}
        \setcounter{section}{3}
        \renewcommand\thefigure{\arabic{section}.\arabic{figure}}
        \renewcommand\thesubfigure{%
            \arabic{section}.\arabic{figure}.\arabic{subfigure}}
    \else
        \section{Notes from REA Mathematics}
    \fi
    \subsection{Polynomials and Equations}
        \begin{definition}
            A polynomomial in $x$, denoted $P(x)$,
            is a term or
            a sum of terms that are a
            real number of the product
            of a real number and a positive integral power
            of $x$.
        \end{definition}
        \begin{example}
            \
            \begin{enumerate}
                \begin{multicols}{2}
                    \item $5x^{3}+2x^{2}+3$ is a
                        polynomial in $x$.
                    \item $2^{2}+x^{\frac{1}{2}}-1$
                        is not a polynomial in $x$.
                    \item $9x^{3}+3x^{-2}+4$ is
                        not a polynomial in $x$.
                    \item $x^{2}+1$ is a polynomial in $x$.
                \end{multicols}
            \end{enumerate}
        \end{example}
        \begin{definition}
            A monomial is the product of a real number with
            variables raised to integral powers.
        \end{definition}
        \begin{definition}
            The degree of a monomial is the sum of the exponents
            of the variables. A monomial with no variables has
            degree $0$.
        \end{definition}
        \begin{example}
            \
            \begin{enumerate}
                \begin{multicols}{2}
                    \item $5x^{2}$ has degree $2$.
                    \item $3x^{3}y^{2}z$ has degree $6$.
                    \item $9$ has degree $0$.
                    \item $xyz$ has degree $3$.
                \end{multicols}
            \end{enumerate}
        \end{example}
        \begin{definition}
            The degree of a polynomial in $x$ is the largest
            exponent of non-zero terms.
        \end{definition}
        \begin{example}
            \
            \begin{enumerate}
                \begin{multicols}{2}
                    \item $5x^{4}+7x+12$ has degree $4$.
                    \item $x^{2}+1$ has degree $2$.
                    \item $4x+6x^{5}-3x^{2}+1$ has degree $5$.
                    \item $0x^{7}+x^{3}-x+x^{2}$ has degree $3$.
                \end{multicols}
            \end{enumerate}
        \end{example}
        The following operations can be
        performed on polynomials:
        \begin{enumerate}
            \item Addition: Add like terms
                that differ only in coefficients.
            \item Subtraction: Change the sign of the
                coefficients of the term being subtracted,
                and then add.
            \item Multiplication:
                Using the distributive property
                to multiply the terms of one polynomial to the
                term of another and then combine the terms.
        \end{enumerate}
        \begin{example}
            \
            \begin{enumerate}
                \item Addition:
                    \begin{align*}
                        (x^{2}-3x+5)+(4x^{2}+6x-3)
                        &=(x^{2}+4x^{2})+(-3x+6x)+(5+(-3))\\
                        &=5x^{2}+3x+2
                    \end{align*}
                \item Subtraction:
                    \begin{align*}
                        (x^{2}+1)-(3x^{2}-x+2)
                        &=(x^{2}+1)+(-3x^{2}+x-2)\\
                        &=(x^{2}-3x^{2})+(x)+(1-2)\\
                        &=-2x^{2}+x-1
                    \end{align*}
                \item Multiplication:
                    \begin{align*}
                        (x^{4}+3)(2x^{2}-1)
                        &=(x^{4}+3)2x^{2}+(x^{4}+3)(-1)\\
                        &=(x^{4})(2x^{2})+(3)(2x^{2})
                        +(x^{4})(-1)+(3)(-1)\\
                        &=2x^{6}-x^{4}+6x^{2}-3
                    \end{align*}
            \end{enumerate}
        \end{example}
        \begin{definition}
            Factors of a polynomial $P(x)$ are polynomials
            $Q_{k}(x)$ such that $P(x)=\Pi_{k=0}^{n}Q_{k}(x)$
        \end{definition}
        \begin{definition}
            A prime factor of a polynomial is a factor that
            cannot be factored further.
        \end{definition}
        \begin{example}
            $x^{2}+1$ is a prime factor of $x^{4}-1$.
            $x^{2}-1$ is not a prime factor since
            $x^{2}-1=(x-1)(x+1)$.
        \end{example}
        \begin{definition}
            The least common multiple of a set of numbers
            is the smallest number that is divisble by
            every element of the set.
        \end{definition}
        \begin{definition}
            The least common multiple of a set of polynomials
            is the polynomial with lowest degree and smallest
            numerical coefficients for which every element
            of the set is a factor.
        \end{definition}
        In a similar manner, then greatest common factor
        can be defined.
        \begin{theorem*}
            The following are true:
            \begin{enumerate}
                \begin{multicols}{2}
                    \item $a(c+d)=ac+ad$
                    \item $(a+b)(a-b)=a^{2}-b^{2}$
                    \item $(a+b)^{2}=a^{2}+2ab+b^{2}$
                    \item $(a-b)^{2}=a^{2}-2ab+b^{2}$
                    \item $(x+a)(x+b)=x^{2}+(a+b)x+ab$
                    \item $(ax+b)(cx+d)=acx^{2}+(ad+bc)x+bd$
                    \item $(a+b)(c+d)=ac+bc+ad+bd$
                    \item $(a+b)^{3}=%
                           a^{3}+3a^{2}b+3ab^{2}+b^{3}$
                    \item $(a-b)^{3}=%
                           a^{3}-3a^{2}b+3ab^{2}-b^{3}$
                    \item $(a-b)(a^{2}+ab+b^{2})=a^{3}-b^{3}$
                    \item $(a+b)(a^{2}-ab+b^{2})=a^{3}+b^{3}$
                    \item $(a+b+c)^{2}=%
                           a^{2}+b^{2}+c^{2}+2(ab+ac+bc)$
                    \item $(a-b)(a^{3}+a^{2}b+ab^{2}+b^{3})=%
                           a^{4}-b^{4}$
                    \item $(a-b)%
                           (a^{4}+a^{3}b+%
                            a^{2}b^{2}+ab^{3}+b^{4})%
                           =a^{5}-b^{5}`$
                \end{multicols}
            \end{enumerate}
        \end{theorem*}
        \begin{theorem*}
            If ${n}\geq{2}$, then
            $a^{n}-b^{n}=(a-b)\sum_{k=0}^{n-1}a^{n-1-k}b^{k}$
        \end{theorem*}
        \begin{theorem*}
            If $n$ is odd, then
            $a^{n}+b^{n}=%
             (a+b)\sum_{k=0}^{n-1}(-1)^{k}a^{n-1-k}b^{k}$
        \end{theorem*}
        To factor a polynomial completely,
        first compute the greatest common factor,
        if there is any. Then examine the remaining factors.
        Continue factoring until all of the factors are prime.
        \begin{example}
            $4-16x^{2}=4(1-4x^{2})=4(1-2x)(1+2x)$
        \end{example}
        \begin{definition}
            An equation is a statement of equality
            between two separate expressions.
        \end{definition}
        \begin{definition}
            A rational integral equation is an equation
            involving two expressions that have only rational
            coefficients and integral powers.
        \end{definition}
        Equations can be simplified by applying the following:
        \begin{enumerate}
            \item Adding or subtracting an expression to both
                sides of an equation results in an equivalent
                equation.
            \item Multiplying both sides of an equation by a
                non-zero expression results in an equivalent
                equation.
            \item The reciprocal of both sides of a non-zero
                equation are equivalent.
            \item When evaluating equations containing
                absolute values, note that $|P(x)|=Q(x)$ is
                equivalent to $P(x)=Q(x)$ OR $P(x)=-Q(x)$.
        \end{enumerate}
        \begin{example}
            \
            \begin{enumerate}
                \begin{multicols}{2}
                    \item $y+6={10}\Rightarrow{y+6-6}=10-6%
                           \Rightarrow{y}=4$
                    \item $3x={6}\Rightarrow{\frac{3x}{3}}=%
                           \frac{6}{3}\Rightarrow{x=2}$
                    \item $\frac{1}{x}=\frac{1}{3}%
                           \Rightarrow{x}=3$
                    \item $|x|=2\Rightarrow{x}=2$ OR
                        $x=-2$
                \end{multicols}
            \end{enumerate}
        \end{example}
        Both sides of an equation can also be raised to a power,
        but this may not be an equivalent expression.
        For example $x^{2}=1$ has the solution set $\{-1,1\}$,
        however if we take the square root of both sides
        we have $x=1$. You must be careful when using
        powers to make sure that both equations are equivalent.
    \subsection{Inequalities}
        \begin{definition}
            An inequality is a statement that the value of
            one expression is greater or less than that
            of another.
        \end{definition}
        \begin{example}
            $4<5$ means that 4 is less than 5.
            $5>4$ means that 5 is greater than 4.
        \end{example}
        \begin{definition}
            A conditional inequality is an inequality
            whose validity depends on the values
            of the variables in the expressions.
        \end{definition}
        \begin{definition}
            An absolute inequality is an inequality
            that is true of all values.
        \end{definition}
        \begin{definition}
            An inconsistent inequality is an inequality
            that is never true for any values.
        \end{definition}
        \begin{example}
            \
            \begin{enumerate}
                \item $x+3>3-x$ is a conditional inequality.
                    It is true for $x>0$, but false for
                    ${x}\leq{0}$.
                \item $x+2>x$ is an absolute inequality. This
                    is always true, regardless of $x$.
                \item $x>x+10$ is an inconsistent inequality.
            \end{enumerate}
        \end{example}
        \begin{theorem*}
            The following are true:
            \begin{enumerate}
                \begin{multicols}{2}
                    \item If $a<b$ and $b<c$, then $a<c$
                    \item If $a>b$ and $b>c$, then $a>c$
                    \item If $a>b$, then $a+c>b+c$
                    \item If $a>b$, then $a-c>b-c$
                    \item If $a>b$ and $c>0$, then $ac>bc$
                    \item If $a>b$ and $c<0$, then $ac<bc$
                    \item If $a>b>0$ and $n$ is positive,
                        then $a^{n}>b^{n}$
                    \item If $a>b>0$ and $n$ is negative,
                        then $a^{n}<b^{n}$
                    \item If $x>y$ and $p>q$, then
                        $x+p>y+q$
                    \item If $x>y>0$ and $p>q>0$,
                        then $xp>yq$
                    \item $|x|<a$ has solution set
                        $\{x:-a<x<a\}$
                    \item $|x|>a$ has solution set
                        $\{x:{x>a}\textrm{ or }{x<-a}\}$
                \end{multicols}
            \end{enumerate}
        \end{theorem*}
        \begin{definition}
            A linear inequality in two variables is
            an inequality of the form
            $ax+by<c$
        \end{definition}
        \begin{example}
            Solve for $2x-3y>6$. In the limiting case,
            ${2x-3y=6}\Rightarrow{y=\frac{2}{3}x-2}$.
            This is the equation of a line in the $xy$ plane.
            Choosing a point not on this line, say $(1,1)$,
            we have $2(1)-3(1)=2-3=-1<6$. Therefore the point
            $(1,1)$ is not contained in the region
            $2x-3y>6$. But then we also not that
            $(1,-\frac{4}{3})$ is on the boundary. So
            $2x-3y>6$ is the region
            \textit{below} the line $y=\frac{2}{3}x-2$
        \end{example}
    \subsection{Linear Algebra}
        A system of linear equations can be written
        using an equivalent matrix notation.
        \begin{example}
            \begin{align*}
                a_{11}x_{1}+a_{12}x_{2}+a_{13}x_{3}&=b_{1}\\
                a_{21}x_{1}+a_{22}x_{2}+a_{23}x_{3}&=b_{2}\\
                a_{31}x_{1}+a_{32}x_{2}+a_{33}x_{3}&=b_{3}\\
            \end{align*}
            This is equivalent to either of the following:
            \begin{align*}
                \begin{bmatrix}
                    a_{11}&a_{12}&a_{13}\\
                    a_{21}&a_{22}&a_{31}\\
                    a_{31}&a_{32}&a_{33}
                \end{bmatrix}
                \begin{bmatrix}
                    x_{1}\\
                    x_{2}\\
                    x_{3}
                \end{bmatrix}
                &=
                \begin{bmatrix}
                    b_{1}\\
                    b_{2}\\
                    b_{3}
                \end{bmatrix}
                &
                \mathbf{A}\mathbf{x}
                &=\mathbf{b}
            \end{align*}
        \end{example}
        Matrices can also be written as $\mathbf{A}=(a_{ij})$.
        The following rules are used to
        define matrix arithmetic.
        \begin{enumerate}
            \item Addition: To add two matrices, add their
                corresponding elements. That is, if
                $\mathbf{A}=(a_{ij})$ and $\mathbf{B}=(b_{ij})$,
                then $\mathbf{A}+\mathbf{B}=(a_{ij}+b_{ij})$.
                Matrix addition is only defined on matrices of
                the same size.
            \item Scale multiplication: To multiply a
                matrix by a real or complex number $c$,
                multiply this number to every element. That is,
                if $\mathbf{A}=(a_{ij})$, then
                $c\mathbf{A}=({c}\cdot{a_{ij}})$
            \item Matrix Multiplication: The product of
                and ${M}\times{N}$ matrix with an
                ${N}\times{P}$ matrix is defined by
                $\mathbf{C}=\mathbf{A}\mathbf{B}$, where
                $(c_{ij})=(\sum_{k=1}^{N}a_{ik}b_{kj})$. Note
                that it is possible for
                $\mathbf{A}\mathbf{B}\ne\mathbf{B}\mathbf{A}$.
                Indeed, it is possible for
                $\mathbf{A}\mathbf{B}$ to be defined, whereas
                $\mathbf{B}\mathbf{A}$ is undefined.
        \end{enumerate}
        \begin{example}
            Let the following be true:
            \begin{align*}
                A&=
                \begin{bmatrix}
                    1&2\\
                    3&4
                \end{bmatrix}
                &
                B&=
                \begin{bmatrix}
                    5&6\\
                    7&8
                \end{bmatrix}
            \end{align*}
            Then, using the defined rules, we have:
            \begin{align*}
                A+B&=
                \begin{bmatrix}
                    6&8\\
                    10&12
                \end{bmatrix}
                &
                5A&=
                \begin{bmatrix}
                    5&10\\
                    15&20
                \end{bmatrix}
                \\
                AB&=
                \begin{bmatrix}
                    19&22\\
                    43&50
                \end{bmatrix}
                &
                BA&=
                \begin{bmatrix}
                    23&34\\
                    31&46
                \end{bmatrix}
            \end{align*}
        Note that even in this trivial example,
        ${AB}\ne{BA}$.
        \end{example}
        \begin{definition}
            The ${n}\times{n}$ identity matrix is the matrix
            $I_{n}=(I_{ij})$, where
            $I_{ij}=%
            \begin{cases}%
             0,&{i}\ne{j}\\%
             1,&{i}={j}%
            \end{cases}$
        \end{definition}
        \begin{definition}
            An inverse matrix of an ${n}\times{n}$ matrix
            $A$ is a matrix $A^{-1}$ such that
            $AA^{-1}=A^{-1}A=I_{n}$
        \end{definition}
        Not every matrix has an inverse matrix. If one
        does exists, there are many properties it contains.
        \begin{theorem*}
            The following are true:
            \begin{enumerate}
                \item If $\mathbf{A}$ and $\mathbf{B}$
                    are invertible ${n}\times{n}$ matrices,
                    then $\mathbf{A}\mathbf{B}$ is invertible
                    and
                    $\mathbf{A}\mathbf{B}^{-1}%
                     =\mathbf{B}^{-1}\mathbf{A}^{-1}$
                \item If $\mathbf{A}$ is an invertible matrix,
                    then $\mathbf{A}^{-1}$ is an invertible
                    matrix and
                    $(\mathbf{A}^{-1})^{-1}=\mathbf{A}$
            \end{enumerate}
        \end{theorem*}
        \begin{definition}
            The trace of an ${n}\times{n}$ matrix
            $A$ is the sum of
            it's diagonal: $\Tr(A)=\sum_{i=1}^{n}a_{ii} $
        \end{definition}
        \begin{example}
            \begin{equation*}
                \Tr\Bigg(
                \begin{bmatrix}
                    4&5&6\\
                    1&2&3\\
                    8&8&3
                \end{bmatrix}
                \Bigg)
                =4+2+3=9
            \end{equation*}
        \end{example}
        \begin{definition}
            The determinant of a ${2}\times{2}$ matrix
            $A=%
             \begin{bmatrix}%
                a&b\\%
                c&d%
             \end{bmatrix}$
            is $\det(A)=ad-bc$
        \end{definition}
        \begin{definition}
            The minor of the $i^{th}$ row and $j^{th}$
            column of an ${n}\times{n}$ matrix $\mathbf{A}$,
            denoted $M_{ij}$, is the determinant of the
            ${(n-1)}\times{(n-1)}$ matrix formed by
            removing the $i^{th}$ row and $j^{th}$ column
            from $\mathbf{A}$.
        \end{definition}
        \begin{definition}
            The cofactor of the minor $M_{ij}$ of an
            ${n}\times{n}$ matrix $\mathbf{A}$,
            denoted $C_{ij}$, is $(-1)^{i+j}M_{ij}$.
        \end{definition}
        \begin{example}
            \begin{align*}
                A&=
                \begin{bmatrix}
                    7&1&3\\
                    1&3&5\\
                    17&4&20
                \end{bmatrix}
                &
                M_{11}
                &=
                \det\Bigg(\begin{bmatrix}
                         3&5\\
                         4&20
                     \end{bmatrix}
                    \Bigg)
                =40
                &
                C_{11}
                &=(-1)^{1+1}M_{11}=40
            \end{align*}
        \end{example}
        \begin{definition}
            The determinant of an ${n}\times{n}$ matrix
            $\mathbf{A}$ is
            $\det(A)=\sum_{j=1}^{n}a_{1j}C_{1j}$
        \end{definition}
        \begin{theorem*}
            If $\mathbf{A}$ is an ${n}\times{n}$ matrix
            and ${1}\leq{i}\leq{n}$, then
            $\det(A)=\sum_{j=1}^{n}a_{ij}C_{ij}$
        \end{theorem*}
        \begin{definition}
            The transpose of an ${n}\times{m}$ matrix
            $\mathbf{A}$, denoted $\mathbf{A}^{T}$,
            is the ${m}\times{n}$ matrix formed by
            swapping the rows and columns of $\mathbf{A}$
            with each other. That is $(a_{ij})^{T}=(a_{ji})$.
        \end{definition}
        \begin{definition}
            A symmetric matrix is a matrix $\mathbf{A}$
            such that $\mathbf{A}^{T}=\mathbf{A}$
        \end{definition}
        \begin{theorem*}
            If $\mathbf{A}$ is an ${n}\times{m}$ matrix and
            $\mathbf{B}$ is an ${m}\times{p}$ matrix, then
            the following are true:
            \begin{enumerate}
                \begin{multicols}{2}
                    \item $(\mathbf{A}^{T})^{T}=\mathbf{A}$
                    \item $(\mathbf{A}+\mathbf{B})^{T}%
                           =\mathbf{A}^{T}+\mathbf{B}^{T}$
                    \item $(k\mathbf{A})^{T}=k\mathbf{A}^{T}$
                    \item $(\mathbf{A}\mathbf{B})^{T}%
                           =\mathbf{B}^{T}\mathbf{A}^{T}$
                \end{multicols}
            \end{enumerate}
        \end{theorem*}
        \begin{definition}
            The adjoint of an ${n}\times{n}$ matrix
            $\mathbf{A}$, denoted $\adjoint(\mathbf{A})$,
            is the matrix $(C_{ij})^{T}$.
        \end{definition}
        \begin{theorem*}
            If ${\det(\mathbf{A})}\ne{0}$, then $\mathbf{A}$
            is invertible and
            $\mathbf{A}^{-1}=%
             \frac{1}{\det(\mathbf{A})}\adjoint(\mathbf{A})$
        \end{theorem*}
        \begin{theorem*}
            If $\mathbf{A}$ and $\mathbf{B}$ are
            ${n}\times{n}$ matrices, then the following
            are true:
            \begin{enumerate}
                \begin{multicols}{3}
                    \item $\det(\mathbf{A})%
                           =\det(\mathbf{A}^{T})$
                    \item $\det(k\mathbf{A})%
                           =k^{n}\det(\mathbf{A})$
                    \item $\det(\mathbf{A}\mathbf{B})%
                           =\det(\mathbf{A})\det(\mathbf{B})$
                \end{multicols}
            \end{enumerate}
        \end{theorem*}
        \begin{theorem*}
            A matrix $\mathbf{A}$ is invertible if and only
            if ${\det(\mathbf{A})}\ne{0}$
        \end{theorem*}
        \begin{theorem*}
            If $\mathbf{A}$ is invertible, then
            $\det(\mathbf{A}^{-1})=\frac{1}{\det(\mathbf{A})}$
        \end{theorem*}
        The differential equation
        $\sum_{k=0}^{n}a_{k}y^{(k)}(x)$ Can be expression
        in terms of the characteristic polynomial
        $\sum_{k=0}^{n}a_{k}D^{k}$. Factoring this linear
        operator into $\Pi_{k=0}^{n}(D-r_{k})$,
        the general solution is
        $y(x)=\sum_{k=1}^{n}c_{k}e^{r_{k}x}$. If some of the
        $r_{k}$ repeat, we have $c_{k}x^{m_{k}-1}e^{r_{k}x}$,
        where $m_{k}$ is the number of repetitions.
        In general, if we have
        $\Pi_{k=0}^{n}(D-r_{k})^{m_{k}}$, the general
        solution is
        $y(x)=%
         \sum_{k=1}^{n}c_{k}e^{r_{k}x}%
         (\sum_{j=0}^{m_{k}-1}x^{j})$
        \begin{example}
            \
            \begin{enumerate}
                \item $y'''-4y''+4y'=0$ has the characteristic
                    polynomial $D(D-2)^{2}$, so
                    $y(x)=c_{1}+c_{2}e^{2x}+c_{3}xe^{2x}$
            \end{enumerate}
        \end{example}
        In linear algebra, the determinant
        $\det(\mathbf{A}-\lambda{I})$ is the characteristic
        polynomial of the square matrix $\mathbf{A}$.
        \begin{definition}
            A vector space $V$ over a Field (Set of scalars)
            $F$ is a set $V$ with two operations
            $+$ and $\cdot$
            such that the following are true:
            \begin{enumerate}
                \begin{multicols}{3}
                    \item $\forall_{{\mathbf{a},%
                                     \mathbf{b}}\in{V}}$
                          ${\mathbf{a}+\mathbf{b}}\in{V}$
                    \item $\mathbf{a}+\mathbf{b}%
                           =\mathbf{b}+\mathbf{a}$
                    \item $\mathbf{a}+(\mathbf{b}+\mathbf{c})%
                           =(\mathbf{a}+\mathbf{b})+\mathbf{c}$
                    \item $\forall_{\mathbf{a}\in{V}}%
                           \exists_{\mathbf{b}\in{V}}:%
                           \mathbf{a}+\mathbf{b}=\mathbf{0}$
                    \item $\forall_{{k}\in{F},\mathbf{a}\in{V}}$
                          $k\mathbf{a}\in{V}$
                    \item $k(\mathbf{a}+\mathbf{b})%
                           =k\mathbf{a}+k\mathbf{b}$
                    \item $(k_{1}+k_{2})\mathbf{a}%
                           =k_{1}\mathbf{a}+k_{2}\mathbf{a}$
                    \item $1\mathbf{a}=\mathbf{a}$
                    \item $k_{1}(k_{2}\mathbf{a})%
                           =(k_{1}k_{2})\mathbf{a}$
                \end{multicols}
            \end{enumerate}
        \end{definition}
        \begin{theorem*}
            If $V$ is a vector space, then there is a
            $\mathbf{0}\in{V}$ such that for all
            $\mathbf{a}\in{V}$,
            $\mathbf{a}+\mathbf{0}=\mathbf{a}$
        \end{theorem*}
        \begin{definition}
            A linearly dependent subset of a vector space
            $V$ (Over $\mathbb{R}$)
            is a subset ${S}\subset{V}$ such that
            there exists an $N\in\mathbb{N}$, a non-zero
            $a_{n}:\mathbb{Z}_{N}\rightarrow\mathbb{R}$
            and an injective function
            $\mathbf{v}_{n}:\mathbb{Z}_{N}\rightarrow{V}$
            such that
            $\sum_{k=1}^{N}a_{n}\mathbf{v}_{n}=\mathbf{0}$
        \end{definition}
        \begin{definition}
            A linearly independent subset of a vector space
            $V$ is a subset ${S}\subset{V}$ that is not
            linearly dependent.
        \end{definition}
        \begin{theorem*}
            If $V\subset\mathbb{R}^{n}$ has more than
            $n$ vectors, then $V$ is linearly dependent.
        \end{theorem*}
        \begin{definition}
            The rank of a matrix is the number
            of linearly independent columns of
            the matrix.
        \end{definition}
        \begin{example}
            Let
            $\mathbf{A}=[A_{1}\ A_{2}]$
            where $A_{1}=(1,2)^{T}$ and
            $A_{2}=(2,4)^{T}$. So
            $2A_{1}+(-1)A_{2}=(0,0)^{T}=\mathbf{0}$. Therefore
            $\{A_{1},A_{2}\}$ is a linearly independent
            subset. Thus, $\rk(\mathbf{A})=1$.
        \end{example}
        \begin{definition}
            A matrix with full rank is a square
            ${n}\times{n}$ matrix $\mathbf{A}$ such that
            $\rk(\mathbf{A})=n$.
        \end{definition}
        \begin{theorem*}
            If $\mathbf{A}$ is a square matrix and
            $\det(\mathbf{A})\ne{0}$, then $\mathbf{A}$
            has full rank.
        \end{theorem*}
        \begin{theorem*}
            If $\mathbf{A}$ is a square matrix
            with full rank, then it is invertible.
        \end{theorem*}
        \begin{definition}
            A finite basis of a vector space $V$ is a
            linearly independent subset ${S}\subset{V}$
            where
            $S=\{\mathbf{v}_{k}\}_{k=0}^{n}$
            and for all
            $\mathbf{x}\in{V}$ there is an
            $a_{n}:\mathbb{Z}_{n}\rightarrow\mathbb{R}$
            such that
            $\mathbf{x}=\sum_{k=1}^{n}a_{k}\mathbf{v}_{k}$
        \end{definition}
        \begin{theorem*}
            All bases of a vector space $V$ have the
            same number of elements.
        \end{theorem*}
        \begin{definition}
            The dimension of a vector space
            $V$ is the number of elements in any
            basis of $V$.
        \end{definition}
        \begin{definition}
            An inner product on a vector space $V$ is a function
            $\langle|\rangle:{V}\times{V}\rightarrow\mathbb{R}$
            such that:
            \begin{enumerate}
                \begin{multicols}{3}
                    \item $\langle{v_{1},v_{2}}\rangle%
                           =\langle{v_{2},v_{3}}\rangle$
                    \item $\langle{v_{1},v_{1}}\rangle\geq{0}$
                    \item $\langle{v_{1}+v_{2},v_{3}}\rangle%
                           =\langle{v_{1},v_{3}}\rangle%
                           +\langle{v_{2},v_{3}}\rangle$
                \end{multicols}
            \end{enumerate}
        \end{definition}
        \begin{definition}
            The Euclidean inner product on $\mathbb{R}^{n}$
            is defined as:
            $\langle{\mathbf{x},\mathbf{y}}\rangle%
             =\sum_{k=1}^{n}x_{k}y_{k}$
        \end{definition}
        \begin{definition}
            An eigenvector of an ${n}\times{n}$ matrix
            $\mathbf{A}$ is a vector
            $\mathbf{x}\in\mathbb{R}^{n}$
            such that there exists a $\lambda\in\mathbb{R}$
            such that
            $\mathbf{A}\mathbf{x}=\lambda\mathbf{x}$
        \end{definition}
        \begin{definition}
            An eigenvalue of an ${n}\times{n}$ matrix
            $\mathbf{A}$ is a real number
            $\lambda\in\mathbb{R}$ such that there is
            a vector $\mathbf{x}\in\mathbf{R}^{n}$ such
            that $\mathbf{A}\mathbf{x}=\lambda\mathbf{x}$
        \end{definition}
        \begin{definition}
            The characteristic equation, or
            the characteristic polynomial, of an
            ${n}\times{n}$ matrix $\mathbf{A}$
            is $\det(\lambda{I}-\mathbf{A})=0$
        \end{definition}
        \begin{definition}
            A diagonalizable matrix is an
            ${n}\times{n}$ matrix
            $\mathbf{A}$ such that there exists
            an invertible matrix $\mathbf{B}$
            such that
            $\mathbf{A}=\mathbf{B}^{-1}\mathbf{A}\mathbf{B}$
        \end{definition}
        \begin{theorem*}
            The following are true:
            \begin{enumerate}
                \item If $\mathbf{A}$ is an ${n}\times{n}$
                    diagonable matrix, then $\mathbf{A}$
                    has $n$ linearly independent
                    eigenvectors.
                \item If $\mathbf{A}$ is an ${n}\times{n}$
                    matrix with $n$ linearly independent
                    eigenvectors, then $\mathbf{A}$
                    is diagonalizable.
                \item A symmetric matrix has all real
                    eigenvalues.
            \end{enumerate}
        \end{theorem*}
    \subsection{Abstract Algebra}
        \begin{definition}
            A group is a set $G$ and a binary relation $*$
            on $G$, denoted $(G,*)$, such that:
            \begin{enumerate}
                \item For all ${a,b,c}\in{G}$, $(a*b)*c=a*(b*c)$
                \item There is an ${e}\in{G}$ such that for all
                    ${a}\in{G}$, $a*e=e*a=a$.
                \item For all ${a}\in{G}$ there is a ${b}\in{G}$
                    such that $a*b=e$
            \end{enumerate}
        \end{definition}
        \begin{definition}
            An Abelian group is a grou $(G,*)$ such that
            $*$ is commutative.
        \end{definition}
        \begin{example}
            $G=\{1\}$ is an Abelian group
            under multiplication.
            This is the trivial group.
        \end{example}
        \begin{theorem*}
            If $(G,*)$ is a group, then the following are true:
            \begin{enumerate}
                \begin{multicols}{2}
                    \item The identity ${e}\in{G}$ is unique.
                    \item If $a*b=a*c$, then $b=c$.
                    \item If $b*a=c*a$, then $b=c$.
                    \item Inverses $a^{-1}$ are unique.
                    \item $\forall_{{a,b}\in{G}}%
                        \exists_{{x}\in{G}}:a*x=b$
                    \item $(a*b)^{-1}=b^{-1}*a^{-1}$
                \end{multicols}
            \end{enumerate}
        \end{theorem*}
        \begin{definition}
            The order of a group is number of elements in the
            group.
        \end{definition}
        \begin{definition}
            A group of finite order, or a finite group,
            is a group with finitely many elements.
        \end{definition}
        \begin{definition}
            The direct product of two groups $(G,*)$ and
            $(H,\circ)$ is the group  $({G}\times{H},\star)$
            where $\star$ is the binary operation defined by
            $(g_{1},h_{1})\star(g_{2},h_{2})%
             =(g_{1}*g_{2},{h_{1}}\circ{h_{2}})$
        \end{definition}
        \begin{definition}
            A permutation group on $n$ elements is a
            group whose elements are permutations of
            $n$ elements.
        \end{definition}
        \begin{definition}
            The symmetric group on $n$ elements,
            denoted $S_{n}$, is the group formed by
            permuting $n$ elements.
        \end{definition}
        \begin{definition}
            A homomorphism from a group $(G,*)$ to
            a group $(H,\circ)$ is a function
            $h:{G}\rightarrow{H}$ such that for all
            ${a,b}\in{G}$, $h(a*b)={h(a)}\circ{h(b)}$
        \end{definition}
        \begin{definition}
            An epimorphism from a group $(G,*)$ to
            a group $(H,\circ)$ is a homomorphism
            $h:{G}\rightarrow{H}$ such that
            $h$ is surjective.
        \end{definition}
        \begin{definition}
            A monomorphism from a group $(G,*)$ to
            a group $(H,\circ)$ is a homomorphism
            $h:{G}\rightarrow{H}$ such that
            $h$ is injective.
        \end{definition}
        \begin{definition}
            An isomorphism from a group $(G,*)$ to
            a group $(H,\circ)$ is a homomorphism
            $h:{G}\rightarrow{H}$ such that
            $h$ is bijective.
        \end{definition}
        \begin{definition}
            A ring is a set $R$ and two binary operations
            on $R$, denoted $(R,\cdot,+)$, such that:
            \begin{enumerate}
                \begin{multicols}{3}
                \item $(R,+)$ is an Abelian group.
                \item $a\cdot({b}\cdot{c})%
                       =({a}\cdot{b})\cdot{c}$
                \item ${a}\cdot(b+c)%
                       ={a}\cdot{b}+{a}\cdot{c}$
                \end{multicols}
            \end{enumerate}
        \end{definition}
        \begin{definition}
            A ring with identity is a ring $(R,\cdot,+)$
            such that there is a ${1}\in{R}$ such that for
            all ${a}\in{R}$, ${a}\cdot{1}={1}\cdot{a}=a$.
        \end{definition}
        \begin{remark}
            Left and right identities are elements such
            that ${e_{L}}\cdot{a}=a$ and ${e_{R}}\cdot{a}=a$.
            If inverses $a_{L}^{-1}$ and $a_{R}^{-1}$ exist
            for $a$, then $a_{L}^{-1}=a_{R}^{-1}$. That is,
            the inverse is the same for both right and left
            identities.
        \end{remark}
        \begin{definition}
            A commutative ring is a ring $(R,\cdot,+)$ such that
            $\cdot$ is commutative.
        \end{definition}
        \begin{definition}
            A commutative ring with identity is a
            ring with identity such that $\cdot$
            is commutative.
        \end{definition}
        \begin{definition}
            A Field is a commutative ring with identity
            $(F,\cdot,+)$
            such that for all ${a}\in{F}$ such that
            $a$ is not an identity with respect to $+$,
            there is a $b\in{F}$ such that ${a}\cdot{b}=1$.
        \end{definition}
        \begin{definition}
            Equivalent sets are sets $A$ and $B$ such that
            there exists a bijective function
            $f:{A}\rightarrow{B}$
        \end{definition}
        \begin{definition}
            A finite set is a set $A$ such that there
            is an ${n}\in{\mathbb{N}}$ such that $A$
            is equivalent to $\mathbb{Z}_{n}$.
        \end{definition}
        \begin{definition}
            A countable set (Or a denumerable set) is a
            set $A$ that is equivalent to $\mathbb{N}$.
        \end{definition}
        \begin{definition}
            An uncountable set is a set that is neither finite
            nor countable.
        \end{definition}
        \begin{theorem*}
            Set Equivalence is an equivalence relation.
        \end{theorem*}
        This equivalence allows to classify all sets by the
        number of elements they contain or, more generally,
        by their cardinality. We say that two sets $A$ and
        $B$ have the same cardinality, denoted
        $\Card(A)$, if and only if $A$ and $B$ are equivalent.
        \begin{theorem*}
            The following are true:
            \begin{enumerate}
                \begin{multicols}{2}
                    \item $\Card(A)=0$ if and only if
                          $A=\emptyset$.
                    \item If ${A}\sim{\mathbb{Z}_{n}}$, then
                          $\Card(A)=n$.
                \end{multicols}
            \end{enumerate}
        \end{theorem*}
        \begin{definition}
            A finite cardinal number is a cardinal
            number of a finite set.
        \end{definition}
        \begin{definition}
            The standard ordering on the finite cardinal
            number is $0<1<\hdots<n<n+1<\hdots$
        \end{definition}
        Thus, if $A$ and $B$ are finite sets, then we write
        $\Card(A)<\Card(B)$ if $A$ is equivalent to a
        subset of $B$ but not equivalent to $B$.
        We take this notion and generalize to
        all sets. For $A$ and $B$, we write
        $\Card(A)<\Card(B)$ if $A$ is equivalent to a subset
        of $B$ but is not equivalent to $B$. This is the
        same as saying $A$ is equivalent to a subset of $B$,
        but $B$ is not equivalent to a subset of $A$.
        We write that
        $\Card(A)\leq\Card(B)$ is $A$ is equivalent to a
        subset of $B$.
        \begin{theorem*}[Schr\"{o}der-Bernstein Theorem]
            If $A$ and $B$ are sets such that
            $\Card(A)\leq\Card(B)$ and
            $\Card(B)\leq\Card(A)$, then
            $\Card(A)=\Card(B)$.
        \end{theorem*}
        \begin{theorem*}
            The following are true:
            \begin{enumerate}
                \item If $\Card(A)\leq\Card(B)$ and
                      $\Card(B)\leq\Card(A)$, then
                      $\Card(A)\leq\Card(C)$.
                \item If $\Card(A)\leq\Card(B)$, then
                      $\Card(A)+\Card(C)\leq\Card(B)+\Card(C)$
            \end{enumerate}
        \end{theorem*}
        \begin{theorem*}
            If ${A}\subset{B}\subset{C}$, and
            $\Card(A)=\Card(C)$, then $\Card(B)=\Card(C)$
        \end{theorem*}
        \begin{theorem*}
            If $f:{X}\rightarrow{Y}$ is a function,
            then $\Card(f(X))\leq\Card(X)$.
        \end{theorem*}
        \begin{proof}
            Note that $f^{-1}(\{y\})$ creates a set of
            mutually disjoint subsets of $X$. By the
            axiom of choice there is a function
            $F:{f(X)}\rightarrow{X}$
            such that for all ${y}\in{f(X)}$,
            ${F(y)}\in{f^{-1}(\{y\})}$. But since these
            sets are disjoint, $F$ is injective.
            Thus, $f(X)$ is equivalent to a subset of $X$.
            Therefore, $\Card(f(X))\leq\Card(X)$.
        \end{proof}
        The Schr\"{o}der-Bernstein theorem can be restated
        equivalently as ``If $A$ is equivalent to a subset
        of $B$ and $B$ is equivalent to a subset of $A$,
        then $A$ is equivalent to $B$.''
        Addition and multiplication of finite cardinals
        follows directly from the standard arithmetic
        for the natural numbers. For cardinals of infinite
        sets, the arithmetic becomes a little more complicated.
        \begin{definition}
            The sum of two cardinal numbers is the
            cardinality of the union of two disjoint sets $A$
            and $B$. That is, if ${A}\cap{B}=\emptyset$, then
            $\Card(A)+\Card(B)=\Card({A}\cup{B})$.
        \end{definition}
        \begin{theorem*}
            If $a$ and $b$ are distinct cardinal numbers,
            then there exists sets $A$ and $B$ such that
            ${A}\cap{B}=\emptyset$, $\Card(A)=a$, and
            $\Card(B)=b$.
        \end{theorem*}
        \begin{theorem*}
            If $A,B,C,$ and $D$ are sets such that
            $\Card(A)=\Card(C)$, $\Card(B)=\Card(D)$,
            and if ${A}\cap{B}=\emptyset$ and
            ${C}\cap{D}=\emptyset$, then
            $\Card({A}\cup{B})=\Card({C}\cup{D})$.
        \end{theorem*}
        \begin{theorem*}
            If $x,y,$ and $z$ are cardinal numbers, then
            $x+y=y+x$ and $x+(y+z)=(x+y)+z$.
        \end{theorem*}
        \begin{notation}
            The carinality of the set of natural numbers
            is denoted $\aleph_{0}$. That is,
            $\Card(\mathbb{N})=\aleph_{0}$
        \end{notation}
        \begin{example}
            Find the cardinal sum of $2$ and $5$. Let
            $N_{2}=\{1,2\}$ and $N_{5}=\{3,4,5,6,7\}$.
            Then $N_{2}$ and $N_{5}$ are disjoint,
            $\Card(N_{2})=2$ and $\Card(N_{5})=5$.
            Therefore $2+5=\Card(N_{2}\cup{N_{5}})$.
            But ${N_{2}}\cup{N_{5}}$ is just $\mathbb{Z}_{7}$,
            and $\Card(\mathbb{Z}_{7})=7$. Thus, $2+5=7$.
        \end{example}
        \begin{theorem*}
            If $n$ and $m$ are finite cardinalities,
            then the cardinal sum of $n$ and $m$ is the
            integer $n+m$, where $+$ is the usual
            arithmetic addition.
        \end{theorem*}
        \begin{example}
            Compute the cardinal sum
            $\aleph_{0}+\aleph_{0}$. Let
            $\mathbb{N}_{e}$ be the set of even natural
            numbers, and let $\mathbb{N}_{o}$ be the set
            of odd natural numbers. Then
            $\Card(\mathbb{N}_{e})=\aleph_{0}$,
            $\Card(\mathbb{N}_{o})=\aleph_{0}$, and
            ${\mathbb{N}_{o}}\cap{\mathbb{N}_{e}}=\emptyset$.
            Thus
            $\aleph_{0}+\aleph_{0}%
             =\Card({\mathbb{N}_{o}}\cup{\mathbb{N}_{e}})$.
            But
            ${\mathbb{N}_{o}}\cup{\mathbb{N}_{e}}%
             =\mathbb{N}$ and $\Card(\mathbb{N})=\aleph_{0}$.
            Therefore, $\aleph_{0}+\aleph_{0}=\aleph_{0}$.
        \end{example}
        \begin{example}
            Find $n+\aleph_{0}$, where $n\in\mathbb{N}$.
            We have that
            $\Card(\mathbb{Z}_{n}z)=n$ and
            $\Card(\mathbb{N}\setminus\mathbb{Z}_{n})%
             =\aleph_{0}$
            But then
            $n+\aleph_{0}=%
             \Card(\mathbb{Z}_{n}\cup%
             \mathbb{N}\setminus\mathbb{Z}_{n})%
             =\Card(\mathbb{N})=\aleph_{0}$.
            Therefore, $n+\aleph_{0}=\aleph_{0}$.
        \end{example}
        \begin{definition}
            The cardinality of the continuum,
            denoted $\mathfrak{c}$, is the
            cardinality of the set of real numbers.
            That is, $\mathfrak{c}=\Card(\mathbb{R})$.
        \end{definition}
        \begin{theorem*}
            $\Card([0,1])=\mathfrak{c}$.
        \end{theorem*}
        \begin{theorem*}
            $\Card\big((0,1)\big)=\mathfrak{c}$.
        \end{theorem*}
        \begin{theorem*}
            $\mathbb{R}$ is uncountable. That is,
            $\mathfrak{c}>\aleph_{0}$.
        \end{theorem*}
        \begin{theorem*}
            $\mathfrak{c}+\aleph_{0}=\mathfrak{c}$.
        \end{theorem*}
        \begin{proof}
            We have $\Card((0,1))=\mathfrak{c}$ and
            $\Card(\mathbb{N})=\aleph_{0}$. But
            $(0,1)\cap\mathbb{N}=\emptyset$, and thus
            $\aleph_{0}+\mathfrak{c}%
             =\Card((0,1)\cup\mathbb{N})$.
            But $\mathbb{R}\sim(0,1)$ and
            $\mathbb{N}\cup(0,1)\subset\mathbb{R}$.
            By the Schr\"{o}der-Bernstein theorem,
            $\mathbb{N}\cup(0,1)\sim\mathbb{R}$.
            Therefore, etc.
        \end{proof}
        \begin{definition}
            The product of two cardinal numbers $a$ and $b$
            is the cardinality of the cartesian product
            of two set $A$ and $B$ such that
            $\Card(A)=a$ and $\Card(B)=b$. That is,
            ${a}\times{b}=\Card({A}\times{B})$.
        \end{definition}
        \begin{theorem*}
            The following are true of cardinal numbers:
            \begin{enumerate}
                \begin{multicols}{3}
                    \item $xy=yx$
                    \item $x(yz)=(xy)z$
                    \item $x(y+z)=xy+xz$
                \end{multicols}
            \end{enumerate}
        \end{theorem*}
        \begin{proof}[Proof of Part 3]
            Let $A,B,$ and $C$ be disjoint.
            Then
            ${A}\times{({B}\cup{C})}%
             =({A}\times{B})\cup({A}\times{C})$, and thus
            $\Card({A}\times{({B}\cup{C})})%
             =\Card(({A}\times{B})\cup({A}\times{C}))$.
            But ${A}\times{B}$ and ${A}\times{C}$ are disjoint.
            Thus we have
            $\Card(({A}\times{B})\cup({A}\times{C}))%
             =\Card({A}\times{B})+\Card({A}\times{C})$.
            Therefore, etc.
        \end{proof}
        \begin{theorem*}
            If $\Card(T)=x$ and
            $F:{T}\rightarrow{\mathcal{P}(T)}$
            is a set-valued mapping such that for all
            ${t}\in{T}$ we have that
            $\Card(F(t))=y$ and
            for all ${t}\ne{t}$,
            ${F(t)}\cap{F(t')}=\emptyset$, then
            $\Card(\cup_{t=1}^{N}F(t))=xy$
        \end{theorem*}
        \begin{example}
            Let $f:{\mathbb{N}^{2}}\rightarrow{\mathbb{N}}$
            be defined by $f(n,m)=2^{n}3^{m}$.
            Then $f$ is injective, since $2$ and $3$
            are coprime. Therefore,
            $\aleph_{0}\times\aleph_{0}=\aleph_{0}$.
        \end{example}
        \begin{example}
            Show that $\mathbb{R}^{2}\sim\mathbb{R}$.
            Let $f:\mathbb{R}^{2}\rightarrow\mathbb{R}$
            be the rather bizarre function defined by the image
            $f(x_{0}.x_{1}x_{2}\hdots,y_{0}.y_{1}y_{2}\hdots)%
             =x_{0}y_{0}.x_{0}y_{0}x_{1}y_{1}\hdots$ Then
            $f$ is inective. But the mapping
            $g:\mathbb{R}\rightarrow\mathbb{R}^{2}$
            defined by $g(x)=(x,0)$ is also injective.
            By Schr\"{o}der-Bernstein,
            $\mathbb{R}^{2}\sim\mathbb{R}$.
        \end{example}
        \begin{definition}
           Order isomorphic set are two sets $A$ and $B$
           with well orders $<_{A}$ and $<_{B}$ such that
           there exists a bijection $f:{B}A\rightarrow{B}$
           such that for all $a_{1},a_{2}\in{A}$ such that
           $a_{1}<_{A}a_{2}$, $f(a_{1})<_{B}f(a_{2})$.
        \end{definition}
        \begin{theorem*}
           Order-Isomorphism is an equivalence relation.
        \end{theorem*}
        To every well ordered set, an ordinal number is
        assigned, denoted $\Ord(A,<_{A})$. Conversely,
        for every ordinal number there is a set with a
        well order corresponding to it. Two ordinal numbers
        are equal if and only if the well-ordered sets
        corresponding to them are order isomorphic.
        That is,
        $\Ord(A,<_{A})=\Ord(B,<_{B})$ if and only if
        $(A,<_{A})$ and $(B,<_{B})$ are order isomorphic.
        \begin{theorem*}
           If $(A,<_{A})$ and $(B,<_{B})$ are well ordered
           sets, and if $\Card(A)=\Card(B)$, then
           $(A,<_{A})$ and $(B,<_{B})$ are order
           isomorphic.
        \end{theorem*}
        The ordinal number of the empty set is $0$. The
        ordinal number of a finite set of $n$ elements with
        a well ordering is denoted $n\in\mathbb{N}$.
        The ordinal for the natural numbers $\mathbb{N}$
        with their usual well-ordering is denoted $\omega$.
        A given well-ordered set has only one cardinal number,
        but it is possible for it to have two ordinal numbers.
        \begin{definition}
           An ordinal number $\alpha$ is less than or equal
           to an ordinal number $\beta$ if there are
           well-ordered sets $(A,<_{A})$ and $(B,<_{B})$
           such that $\alpha=\Ord((A,<_{A}))$ and
           $\beta=\Ord(B,<_{B})$, and $(A,<_{B})$ is
           order isomorphic to subset of
           $(B,<_{B})$.
        \end{definition}
        \begin{theorem*}
           The only order isomorphism from a well ordered
           set $(A,<_{A})$ to itself is the identity
           isomorphism.
        \end{theorem*}
        \begin{theorem*}
           If $\alpha$ and $\beta$ are ordinal numbers and
           ${\alpha}\leq{\beta}$ and ${\beta}\leq{\alpha}$,
           then $\alpha=\beta$.
        \end{theorem*}
        \begin{theorem*}
           If $\alpha$ and $\beta$ are ordinal numbers,
           either ${\alpha}\leq{\beta}$, or
           ${\beta}\leq{\alpha}$.
        \end{theorem*}
        \begin{theorem*}
           If $\alpha$ and $\beta$ are ordinal numbers,
           either $\alpha<\beta$, $\beta<\alpha$, or
           $\alpha=\beta$.
        \end{theorem*}
        \begin{definition}
           The total ordering relation of a
           well-ordered set $(A,<_{A})$ with respect
           to a well-ordered set $(B,<_{B})$ is the ordering
           on the set ${A}\cup{B}$ defined as: For all
           $a_{1},a_{2}\in{A}$ such that $a_{1}<_{A}a_{2}$,
           $a_{1}<_{*}a_{2}$, for all $b_{1},b_{2}\in{B}$
           such that $b_{1}<_{B}b_{2}$, $b_{1}<_{*}b_{2}$,
           and for all ${a}\in{A}$ and ${b}\in{B}$,
           ${a}<_{*}{b}$.
        \end{definition}
        \begin{theorem*}
           The total ordering relation $<_{*}$ on the set
           ${A}\cup{B}$ is a well-ordering.
        \end{theorem*}
        \begin{definition}
           The ordinal sum of two ordinal numbers
           $\Ord((A,<_{A}))$ and $\Ord((B,<_{B}))$,
           where $A$ and $B$ are disjoint,
           is the ordinal number
           $\Ord(({A}\cup{B},<_{*}))$.
        \end{definition}
        \begin{theorem*}
           The following are true of ordinal numbers:
           \begin{enumerate}
               \begin{multicols}{3}
                   \item $\alpha<\beta\Rightarrow%
                          \alpha+\gamma<\beta+\gamma$
                   \item $(\alpha+\beta)+\gamma%
                          =\alpha+(\beta+\gamma)$
                   \item $\alpha+\beta=\alpha+\gamma%
                          \Rightarrow\beta=\gamma$.
               \end{multicols}
           \end{enumerate}
        \end{theorem*}
        \begin{definition}
           The lexicographic ordering on the cartesian
           product of well ordered set $(A,<_{A})$ and
           $(B,<_{B})$ is the ordering on
           ${A}\times{B}$ defined by: If ${a}<_{A}{x}$,
           then $(a,b)<_{*}(x,y)$ for all $b,y\in{B}$, and
           if $a=x$ and $b<_{B}y$, then $(a,b)<_{*}(x,y)$.
        \end{definition}
        \begin{theorem*}
           If $(A,<_{A})$ and $(B,<_{B})$ are well ordered
           sets, then the lexicographic ordering
           on ${A}\times{B}$ is a well ordering.
        \end{theorem*}
        \begin{definition}
           The ordinal product of two ordinal numbers
           $\Ord((A,<_{A}))$ and $\Ord((B,<_{B}))$,
           is $\Ord(({A}\times{B},<_{*}))$
        \end{definition}
        \begin{theorem*}
           The following are true of ordinal numbers:
           \begin{enumerate}
               \begin{multicols}{2}
                   \item $\alpha(\beta\gamma)%
                          =(\alpha\beta)\gamma$
                   \item $\alpha(\beta+\gamma)%
                          =\alpha\beta+\alpha\gamma$
               \end{multicols}
           \end{enumerate}
        \end{theorem*}
        \begin{definition}
           Relatively prime integers are integers
           $a,b\in\mathbb{N}$ such that $\gcd(a,b)=1$.
        \end{definition}
        \begin{theorem*}
           If $p$ is prime and $a\in\mathbb{N}$ is
           such that $p$ does not divide $a$, then $a$ and $p$
           are relatively prime.
        \end{theorem*}
        \begin{theorem*}
           There are infinitely many prime numbers.
        \end{theorem*}
        \begin{theorem*}
           If $a\in\mathbb{N}$, $a>1$, then either
           $a$ is a prime number, or $a$ is the product
           of finitely many primes.
        \end{theorem*}
        \begin{theorem*}
           If $a\in\mathbb{N}$, $a>1$, and if $a$ is not
           prime, then the prime expansion of $a$ is
           unique.
        \end{theorem*}
        \begin{definition}
           A diophantine equation is an equation whose
           solutions are required to be integers.
        \end{definition}
        \begin{definition}
           A linear diophantine equation in two variables
           $x$ and $y$ is an equation
           $ax+by=c$, where $a,b,c\in\mathbb{Z}$.
        \end{definition}
        \begin{theorem*}
           If $a,b,c\in\mathbb{Z}$ $d=\gcd(a,b)$,
           and if $d$ does not divide $c$,
           then $ax+by=c$ has no integral solutions.
        \end{theorem*}
        \begin{theorem*}
           If $a,b,c\in\mathbb{Z}$ $d=\gcd(a,b)$,
           and if $d$ divides $c$,
           then $ax+by=c$ has infinitely many solutions.
        \end{theorem*}
    \subsection{Functions}
        \begin{definition}
            A function, denoted $f:X\rightarrow{Y}$, is a
            correspondence between two set $X$ and $Y$.
            $X$ is called the domain and $Y$ is called
            the co-domain. For every element $x\in{X}$ there
            is exactly one element $y\in{Y}$ such that
            $f(x)=y$.
        \end{definition}
        The sum of two functions is defined as
        $(f+g)(x)=f(x)+g(x)$. Similarly, the difference is
        defined as $(f-g)(x)=f(x)-g(x)$. The product is defined
        as $(fg)(x)=f(x)g(x)$, and the quotient is defined,
        for non-zero $g(x)$, as $(f/g)(x)=f(x)/g(x)$. An important
        concept is that of the composition of functions.
        \begin{definition}
            If $f:X\rightarrow{Y}$ and $g:Y\rightarrow{Z}$ are
            functions, then the composition
            $g\circ{f}:X\rightarrow{Z}$ is defined by
            $(g\circ{f})(x)=g(f(x))$.
        \end{definition}
        \begin{definition}
            A periodic function
            $f:\mathbb{R}\rightarrow\mathbb{R}$ is a function
            such that there exists a $p\in\mathbb{R}$ such that,
            for all $x\in\mathbb{R}$,
            $f(x+p)=f(x)$. The period of a periodic function
            is the smallest such $p$.
        \end{definition}
        \begin{definition}
            A one-to-one function, or an injection, is a function
            such that $f(x_{1})=f(x_{2})$ if and only if
            $x_{1}=x_{2}$.
        \end{definition}
        \begin{definition}
            The inverse of a one-to-one function
            $f:X\rightarrow{Y}$ is the function
            $f^{-1}:Y\rightarrow{X}$ defined by
            $f^{-1}(y)=x$, where $x$ is such that
            $f(x)=y$. If is, for all
            $y\in{Y}$, $f(f^{-1}(y))=y$, and for
            all $x\in{X}$, $f^{-1}(f(x))=x$.
        \end{definition}
        \begin{definition}
            A root of a function
            $f:X\rightarrow\mathbb{R}$, or
            $f:X\rightarrow\mathbb{C}$,
            is a value $x\in{X}$ such that
            $f(x)=0$
        \end{definition}
        \begin{definition}
            A fixed point of a function
            $f:X\rightarrow{X}$ is a point
            $x\in{X}$ such that $f(x)=x$.
        \end{definition}
        \begin{example}
            Find the fixed points and the inverse
            of $f(x)=(3x-2)/(x-1)$.
            A fixed point is a value $x$ such that
            $f(x)=x$. So, we have:
            $x=(3x-2)/(x-1)$, and thus $x^{2}-x=3x-2$.
            This implies $x^{2}-4x+2=0$. Using the quadratic
            formula, we have $x=2\pm\sqrt{2}$. To find the
            inverse we solve $y=(3x-2)/(x-1)$ for $x$. So
            we have: $y=(3x-2)/(x-1)$ which implies that
            $yx-y=3x-2$, and thus
            $x(3-y)=2-y$. Therefore $x=(2-y)/(3-y)$. The
            inverse function is
            $f^{-1}(x)=(2-x)/(3-x)$.
        \end{example}
    \subsection{Coordinate Geometry}
        Coordinate geometry, or Cartesian geometry, is
        the study of geometry using basic notion from
        elementary algebra. The Cartesian plane consists of
        two perpendicular lines, the $x$ axis and the $y$
        axis. The intersection of these lines is called the
        origin. This is denoted $(0,0)$. Every point to the
        right of the origin corresponds to a positive value
        $x$, and every value to the left corresponds to a
        negative value. Similarly, every value about the
        origin corresponds to a positive value $y$ and
        every value below corresponds to a negative value
        $y$. Each point in the plane is identified by the
        ordered pair $(x,y)$.
        \begin{definition}
            The abscissa of a coordinate $(x,y)$ in the
            Cartesian plane is the value $x$.
        \end{definition}
        \begin{definition}
            The ordinate of a coordinate $(x,y)$ in the
            Cartesian plane is the value $y$.
        \end{definition}
        The plane is divided into four quadrants. The
        First Quadrant is the set of all points
        $(x,y)$ such that $x$ and $y$ are positive.
        The second, third, and fourth quadrants are then
        labelled in counter-clockwise order around the
        origin.
        \begin{definition}
            The distance between
            $(x_{1},y_{1})$ and $(x_{2},y_{2})$ is
            $\sqrt{(x_{2}-x_{1})^{2}+(y_{2}-y_{1})^{2}}$
        \end{definition}
        This distance formula comes from
        Pythagoras' Theorem. Given two points in the
        Cartesian plane, form a right triangle by
        drawing lines perpendicular to the $x$ and
        $y$ axis that contain these two points. The
        height is then $y_{2}-y_{1}$ and the length
        is $x_{2}-x_{1}$.
        \begin{theorem}
            The midpoint between $(x_{1},y_{1})$
            and $(x_{2},y_{2})$, that is the point
            whose distance to either point is equal
            and lies on the line containing these two
            points, is
            $(\frac{x_{1}+x_{2}}{2},\frac{y_{1}+y_{2}}{2})$.
        \end{theorem}
        \begin{theorem}
            If $A=(x_{0},y_{0})$ is a point in the Cartesian plane,
            and if $\ell$ is a line defined by
            $ax+by+c=0$, then the minimum distance between $P$
            and $\ell$ is:
            \begin{equation*}
                d=\Big|
                    \frac{ax_{0}+by_{0}+c}{\sqrt{a^{2}+b^{2}}}
                \Big|
            \end{equation*}
        \end{theorem}
        \begin{definition}
            A vertical line is a line $\ell$ such that
            $ay+b=0$ for all $(x,y)$ that lie on $\ell$.
        \end{definition}
        \begin{definition}
            The slope of a non-vertical line with points
            $(x_{1},y_{2})$ and $(x_{2},y_{2})$ is:
            \begin{equation*}
                m=\frac{y_{2}-y_{1}}{x_{2}-x_{1}}
            \end{equation*}
        \end{definition}
        \begin{definition}
            An intercept of a line $ax+by+c=0$ is point on the
            line such that either $x=0$ or $y=0$. If $x=0$ this
            is called an $x$ intercept, and if $y=0$ this is
            called a $y$ intercept.
        \end{definition}
        \begin{theorem}
            Two non-vertical lines are perpendicular if and only if
            $m_{1}=-1/m_{2}$.
        \end{theorem}
        \begin{theorem}
            Two non-vertical lines are parallel if and only if
            $m_{1}=m_{2}$.
        \end{theorem}
        \begin{theorem}
            If $\ell$ is a line that passes through the
            origin with slope $m$, then $y=mx$.
        \end{theorem}
        \begin{theorem}
            If $\ell$ is a line with $y$ intercept
            $b$ and slope $m$, then
            $y=mx+b$.
        \end{theorem}
        \begin{theorem}
            If $\ell$ is a line with $x$ intercept
            $a$ and $y$ intercept $b$, then
            $x/a+y/b=1$.
        \end{theorem}
        \begin{theorem}
            If $\ell$ is a line containing
            $(x_{1},y_{1})$ and $(x_{2},y_{2})$, then:
            \begin{equation*}
                y=\frac{x-x_{1}}{x_{2}-x_{1}}(y_{2}-y_{1})+y_{1}
            \end{equation*}
        \end{theorem}
        \begin{theorem}
            If $\ell$ is a line containing $(x_{0},y_{0})$
            with slope $m$, then $y=m(x-x_{0})+y_{0}$.
        \end{theorem}
        A locus is a set of points satifsying a certain contiditon.
        For example, the locus of points that are a fixed distance
        $r$ away from the point $P$ is the circle of radius $r$
        centered at $P$. The locus of points that are equidistant
        from two line that intersect at an angle is the
        angle bisector.
        \begin{definition}
            A parabola is the locus, or set of all points, such that
            the distance to a fixed point (Called the focus)
            is equal to the distance to a fixed line
            (Called the directrix).
        \end{definition}
        \begin{definition}
            An ellipse is the locus of points such that
            the sum of the distances to two other points
            (Called the foci) are equal. If the two foci
            are the same, then we have a circle.
        \end{definition}
        \begin{definition}
            A hyperbola is the locus of points such that
            the difference of the distance between two
            other points (Called the foci) is constant.
        \end{definition}
        \begin{theorem}
            The equation of a circle centered at
            $(x_{0},y_{0})$ or radius $r$ is:
            $(x-x_{0})^{2}+(y-y_{0})^{2}=r^{2}$.
        \end{theorem}
        \begin{theorem}
            The equation of a parabola with vertex
            $V=(x_{0},y_{0})$ and directrix $d$ such that
            the signed distance from $(x_{0},y_{0})$ to
            $d$ is $p$ is $(y-y_{0})^{2}=4p(x-x_{0})$.
        \end{theorem}
        \begin{theorem}
            An ellipse centered at $(x_{0},y_{0})$
            has an equation of the form:
            \begin{equation*}
                \frac{(x-x_{0})^{2}}{a^{2}}+
                \frac{(y-y_{0})^{2}}{b^{2}}
                =1
            \end{equation*}
        \end{theorem}
        \begin{definition}
            The eccentricity of an ellipse is:
            \begin{equation*}
                \varepsilon=\sqrt{1-\frac{b^{2}}{a^{2}}}
            \end{equation*}
        \end{definition}
        \begin{theorem}
            A hyperbola centered at $(x_{0},y_{0})$
            has an equation of the form:
            \begin{equation*}
                \frac{(x-x_{0})^{2}}{a^{2}}-
                \frac{(y-y_{0})^{2}}{b^{2}}
                =1
            \end{equation*}
        \end{theorem}
    \subsection{Trigonometry}
        \begin{definition}
            Given a right angle triangle with height $y$,
            width $x$, hypotenuse $r$, and given the
            angle $\theta$ which is oppositive to the heigh
            and adjacent to the width of the triangle, the
            following functions are defined:
            \begin{align*}
                \sin(\theta)&=\frac{y}{r}
                &
                \cos(\theta)&=\frac{x}{r}
                &
                \tan(\theta)&=\frac{y}{x}
            \end{align*}
        \end{definition}
        \begin{example}
            In radians, we have the following:
            \begin{enumerate}
                \begin{multicols}{4}
                    \item $\sin(0)=0$
                    \item $\cos(0)=1$
                    \item $\tan(0)=0$
                    \item $\sin(\frac{\pi}{2})=1$
                    \item $\cos(\frac{\pi}{2})=0$
                    \item $\tan(\frac{\pi}{2})=\infty$
                    \item $\sin(\frac{\pi}{6})=\frac{1}{2}$
                    \item $\cos(\frac{\pi}{6})=\frac{\sqrt{3}}{2}$
                    \item $\tan(\frac{\pi}{6})=\frac{1}{\sqrt{3}}$
                    \item $\sin(\frac{\pi}{4})=\frac{1}{\sqrt{2}}$
                    \item $\cos(\frac{\pi}{4})=\frac{1}{\sqrt{2}}$
                    \item $\tan(\frac{\pi}{4})=1$
                \end{multicols}
            \end{enumerate}
        \end{example}
        \begin{definition}
            The reciprocals of the trigonometric functions are:
            \begin{align*}
                \sec(\theta)&=\frac{1}{\cos(\theta)}
                &
                \csc(\theta)&=\frac{1}{\sin(\theta)}
                &
                \cot(\theta)&=\frac{1}{\tan(\theta)}
            \end{align*}
        \end{definition}
        \begin{theorem}
            The following are true:
            \begin{align*}
                \sin(\theta)\csc(\theta)&=1
                &
                \cos(\theta)\sec(\theta)&=1\\
                \tan(\theta)\cot(\theta)&=1
                &
                \sin^{2}(\theta)+\cos^{2}(\theta)&=1\\
                \tan(\theta)&=\frac{\sin(\theta)}{\cos(\theta)}
                &
                \cot(\theta)&=\frac{\cos(\theta)}{\sin(\theta)}\\
                \sec^{2}(\theta)&=1+\tan^{2}(\theta)
                &
                \csc^{2}(\theta)&=1+\cot^{2}(\theta)\\
                \sin(a\pm{b})&=\sin(a)\cos(b)\pm\cos(a)\sin(b)
                &
                \cos(a\pm{b})&=\cos(a)\cos(b)\mp\sin(a)\sin(b)\\
                \tan(a\pm{b})
                &=\frac{\tan(a)\pm\tan(b)}{1\mp\tan(a)\tan(b)}
                &
                \cot(a\pm{b})
                &=\frac{\cot(a)\cot(b)\mp1}{\cot(b)\pm\cot(a)}\\
                \sin(2x)&=2\sin(x)\cos(x)
                &
                \sin(3x)&=3\sin(x)-4\sin^{3}(x)\\
                \sin(4x)&=8\cos^{3}(x)\sin(x)-4\cos(x)\sin(x)
                &
                \cos(2x)&=\cos^{2}(x)-\sin^{2}(x)\\
                \cos(3x)&=4\cos^{3}(x)-3\cos(x)
                &
                \cos(4x)&=8\cos^{4}(x)-8\cos^{2}(x)+1\\
                \sin(a)\pm\sin(b)
                &=2\sin(\frac{a\pm{b}}{2})\cos(\frac{a\mp{b}}{2})
                &
                \cos(a)+\cos(b)
                &=2\cos(\frac{a+b}{2})\cos(\frac{a-b}{2})\\
                \cos(a)-\cos(b)
                &=2\sin(\frac{a+b}{2})\sin(\frac{b-a}{2})
                &
                \tan(a)\pm\tan(b)
                &=\frac{\sin(a\pm{b})}{\cos(a)\cos(b)}\\
                \cot(a)\pm\cot(b)
                &=\frac{\sin(a\pm{b})}{\sin(a)\sin(b)}
                &
                \sin^{2}\Big(\frac{x}{2}\Big)
                &=\frac{1-\cos(x)}{2}\\
                \cos^{2}\Big(\frac{x}{2}\Big)
                &=\frac{1+\cos(x)}{2}
                &
                \tan\Big(\frac{a\pm{b}}{2}\Big)
                &=\frac{\sin(a)\pm\sin(b)}{\cos(a)+\cos(b)}
            \end{align*}
        \end{theorem}
    \subsection{Limits}
        The limit of a function $f$ defined on an open interval
        as $x$ approaches some value $a$ in that interval is
        the value $f(x)$ approaches. The is,
        $\lim_{x\rightarrow{a}}f(x)$ is the value $f(x)$ gets
        near to as $x$ approaches $a$. It is possible that
        there is no such value, and the limit may not
        exists. When it does exist there are several theorems
        pertaining to the limit:
        \begin{theorem}
            If $\lim_{x\rightarrow{a}}f(x)$ exists, then the
            limit is unique.
        \end{theorem}
        \begin{theorem}
            If $\lim_{x\rightarrow{a}}f(x)$ and
            $\lim_{x\rightarrow{a}}g(x)$ exists, then
            $\lim_{x\rightarrow{a}}(f+g)(x)$ exists and:
            \begin{equation*}
                \lim_{x\rightarrow{a}}(f+g)(x)
                =\lim_{x\rightarrow{a}}f(x)
                +\lim_{x\rightarrow{a}}g(x)
            \end{equation*}
        \end{theorem}
        \begin{theorem}
            If $\lim_{x\rightarrow{a}}f(x)$ and
            $\lim_{x\rightarrow{a}}g(x)$ exists, then
            $\lim_{x\rightarrow{a}}(fg)(x)$ exists and:
            \begin{equation*}
                \lim_{x\rightarrow{a}}(fg)(x)
                =\big(\lim_{x\rightarrow{a}}f(x)\big)
                \big(\lim_{x\rightarrow{a}}g(x)\big)
            \end{equation*}
        \end{theorem}
        \begin{theorem}
            If $\lim_{x\rightarrow{a}}f(x)=L$ and $L\ne{0}$,
            then $\lim_{x\rightarrow{a}}1/f(x)=1/L$.
        \end{theorem}
        You can also factor out constants in limits, raise
        limits to positive integer powers, and raise to
        arbitrary powers if the limit is positive. There are
        also left and right sided limits, for when a general
        limit doesn't exist. This also functions with jumps
        to have well defined limits as $x$ approaches from
        the right, and for when $x$ approaches from the left.
        \begin{example}
            The following are true:
            \begin{enumerate}
                \begin{multicols}{2}
                    \item $\lim_{x\rightarrow{0}}\frac{\sin(x)}{x}=1$
                    \item $\lim_{x\rightarrow{0}}\frac{1-\cos(x)}{x}=0$
                \end{multicols}
            \end{enumerate}
        \end{example}
        \begin{example}
            Let's evaluate $\sqrt{x^{2}+2x}-x$ as $x\rightarrow\infty$.
            We have:
            \begin{align*}
                \sqrt{x^{2}+2x}-x
                &=\frac{x^{2}+2x-x^{2}}{\sqrt{x^{2}+2x}+x}\\
                &=\frac{2x}{\sqrt{x^{2}+2x}}\\
                &=\frac{2}{\sqrt{1+\frac{2}{x}}+1}\\
            \end{align*}
            So the limit is 1.
        \end{example}
    \subsection{Derivatives}
        The derivative of a function is a way of determining
        the rate of change of the function with respect to
        the independent variable. Another common way of
        writing this is that it is the slope of the tangent
        line of the graph of the function for any given
        point.
        \begin{definition}
            The derivative of a function
            $f:\mathcal{U}\rightarrow\mathbb{R}$
            defined on an open interval $\mathcal{U}$ is:
            \begin{equation*}
                f'(x)
                =\frac{\diff{f}}{\diff{x}}(x)
                =\lim_{h\rightarrow{0}}\frac{f(x+h)-f(x)}{h}
            \end{equation*}
            Equivalently, we may write:
            \begin{equation*}
                f'(x_{0})
                =\frac{\diff{f}}{\diff{x}}(x_{0})
                =\lim_{x\rightarrow{x_{0}}}
                \frac{f(x)-f(x_{0})}{x-x_{0}}
            \end{equation*}
        \end{definition}
        \begin{theorem}
            If $f$ is a constant, then $f'(x)=0$.
        \end{theorem}
        \begin{theorem}
            If $f(x)=ax^{n}$, then $f'(x)=anx^{n-1}$
        \end{theorem}
        \begin{theorem}
            If $f$ and $g$ are differentiable, then
            $(f+g)'(x)=f'(x)+g'(x)$
        \end{theorem}
        \begin{theorem}
            If $f$ is differentiable and $a$ is a constant,
            then $(af)'(x)=af'(x)$
        \end{theorem}
        \begin{theorem}
            If $f$ and $g$ are differentiable, then
            $(fg)'(x)=f'(x)g(x)+f(x)g'(x)$
        \end{theorem}
        \begin{theorem}
            If $f$ and $g$ are differentiable, and $g\ne{0}$,
            then:
            \begin{equation*}
                \Big(\frac{f}{g}\Big)'(x)
                =\frac{f'(x)g(x)-f(x)g'(x)}{g(x)^{2}}
            \end{equation*}
        \end{theorem}
        \begin{theorem}
            If $r\ne{0}$ and $f(x)=ax^{r}$, then
            $f'(x)=arx^{r-1}$.
        \end{theorem}
        \begin{theorem}[Chain Rule]
            If $f$ and $g$ are differentiable, then
            $(g\circ{f})'(x)=g'(f(x))f'(x)$.
        \end{theorem}
        \begin{theorem}
            The following is true:
            \begin{align*}
                \frac{\diff}{\diff{x}}\Big(\sin(x)\Big)
                &=\cos(x)
                &
                \frac{\diff}{\diff{x}}\Big(\cos(x)\Big)
                &=-\sin(x)
                &
                \frac{\diff}{\diff{x}}\Big(\tan(x)\Big)
                &=\sec^{2}(x)
                &
                \frac{\diff}{\diff{x}}\Big(\exp(x)\Big)
                &=\exp(x)
            \end{align*}
        \end{theorem}
        \begin{theorem}
            If $f$ is differentiable, then:
            \begin{equation*}
                \frac{\diff}{\diff{x}}\Big(e^{f(x)}\Big)
                =e^{f(x)}\frac{\diff{f}}{\diff{x}}
            \end{equation*}
        \end{theorem}
        \begin{theorem}
            $\frac{\diff}{\diff{x}}(\ln(x))=\frac{1}{x}$
        \end{theorem}
        \begin{theorem}
            If $f$ is differentiable, then:
            \begin{equation*}
                \frac{\diff}{\diff{x}}\Big(\ln(f)\Big)
                =\frac{1}{f(x)}\frac{\diff{f}}{\diff{x}}
            \end{equation*}
        \end{theorem}
        \begin{theorem}
            If $x$ is a real number, and $a>0$, then
            $a^{x}=\exp(x\ln(a))$
        \end{theorem}
        \begin{theorem}
            If $a>0$ and $f$ is differentiable, then:
            \begin{equation*}
                \frac{\diff}{\diff{x}}\Big(a^{f(x)}\Big)
                =a^{f(x)}\ln(a)\frac{\diff{f}}{\diff{x}}
            \end{equation*}
        \end{theorem}
        If $f:\mathbb{R}^{2}\rightarrow\mathbb{R}$ is a function
        of two variables, we can define the
        \textit{partial derivatives} of $f$ with respect to
        the two coordinates of $f$.
        \begin{definition}
            The partial derivatives of a function
            $f:\mathbb{R}^{2}\rightarrow\mathbb{R}$ are:
            \begin{align*}
                \frac{\partial{f}}{\partial{x}}
                &=\lim_{h\rightarrow{0}}\frac{f(x+h,y)-f(x,y)}{h}
                &
                \frac{\partial{f}}{\partial{y}}
                &=\lim_{h\rightarrow{0}}\frac{f(x,y+h)-f(x,y)}{h}  
            \end{align*}
        \end{definition}
        The partial derivatives of $f$ are also commonly written as
        $f_{x}$ and $f_{y}$, respectively. Partial derivatives can be
        computed by assuming that the other variable is a constant,
        and then use the various theorems that apply to normal
        derivatives. There is a multivariate form of the chain rule.
        \begin{theorem}
            If $z=f(x,y)$ is differentiable in both $x$ and $y$, and
            if $x=x(t)$ and $y=y(t)$ are differentiable functions, then:
            \begin{equation*}
                \frac{\diff{f}}{\diff{t}}
                =\frac{\partial{f}}{\partial{x}}\frac{\diff{x}}{\diff{t}}
                +\frac{\partial{f}}{\partial{y}}\frac{\diff{y}}{\diff{t}}
            \end{equation*}
        \end{theorem}
        Because of this, we often write
        $\diff{z}=\frac{\partial{z}}{\partial{x}}\diff{x}%
         +\frac{\partial{z}}{\partial{y}}\diff{y}$.
        Similar extensions of this theorem hold for functions
        $f:\mathbb{R}^{n}\rightarrow\mathbb{R}$ of $n$ variables.
        \begin{example}
            If $\exp(xy)+\sin(xy)+1=0$, compute $dy/dx$. We compute
            the partial derivatives of $f(x,y)=\exp(xy)+\sin(xy)+1$:
            \begin{align*}
                \frac{\partial{f}}{\partial{x}}
                &=y\exp(xy)+y\cos(xy)
                &
                \frac{\partial{f}}{\partial{y}}
                &=x\exp(xy)+x\cos(xy)\\
            \end{align*}
            Using this, we have:
            \begin{equation*}
                \frac{\diff{y}}{\diff{x}}
                =-\frac{\partial{f}/\partial{y}}{\partial{f}/\partial{x}}
                =-\frac{y(\exp(xy)+\cos(xy)}{x(\exp(xy)+\cos(xy)}
                =-\frac{y}{x}
            \end{equation*}
        \end{example}
    \subsection{Application of Derivatives}
        \begin{definition}
            A function $f:\mathbb{R}\rightarrow\mathbb{R}$ is said
            to have a local minima $a$ if there is an open interval
            $\mathcal{U}$ such that $a\in\mathcal{U}$ and, for all
            $x\in\mathbb{U}$, $f(a)\leq{f(x)}$.
        \end{definition}
        \begin{definition}
            A function $f:\mathbb{R}\rightarrow\mathbb{R}$ is said
            to have a local maxima $a$ if there is an open interval
            $\mathcal{U}$ such that $a\in\mathcal{U}$ and, for all
            $x\in\mathbb{U}$, $f(a)\geq{f(x)}$.
        \end{definition}
        \begin{theorem}
            If $f$ is differentiable and has a relative maximum
            or minimum at $x=a$, then $f'(a)=0$.
        \end{theorem}
        \begin{theorem}
            If $f'(a)=0$ and $f''(a)>0$, then
            $a$ is a relative minimum.
        \end{theorem}
        \begin{theorem}
            If $f'(a)=0$ and $f''(a)<0$, then
            $a$ is a relative maximum.
        \end{theorem}
        \begin{definition}
            A point of inflection for a twice differentiable
            function $f$ is a point $a$ such that
            $f''(a)=0$.
        \end{definition}
        Points of inflection are points where the concavity of
        the function may change. This is also points curve
        crosses the tangent line.
        \begin{theorem}[Rolle's Theorem]
            If $f$ is continuous on the closed interval
            $[a,b]$ and differentiable on the open interval
            $(a,b)$, and if $f(a)=f(b)=0$, there there is
            a point $c\in(a,b)$ such that $f'(c)=0$.
        \end{theorem}
        \begin{theorem}[Mean Value Theorem]
            If $f$ is continuous on $[a,b]$ and
            differentiable on $(a,b)$, there there
            is a $c\in(a,b)$ such that:
            \begin{equation*}
                f'(c)=\frac{f(b)-f(a)}{b-a}
            \end{equation*}
        \end{theorem}
        \begin{theorem}
            If $f(x)\rightarrow{0}$ and $g(x)\rightarrow{0}$ as
            $x\rightarrow{a}$, or if
            $f(x)\rightarrow\infty$ and $g(x)\rightarrow\infty$
            as $x\rightarrow{a}$, and if $f$ and $g$ are differentiable,
            then:
            \begin{equation*}
                \lim_{x\rightarrow{a}}\frac{f(x)}{g(x)}
                =\lim_{x\rightarrow{a}}\frac{f'(x)}{g'(x)}
            \end{equation*}
        \end{theorem}
        \begin{theorem}
            If $f$ is $n$ times differentiable on in interval
            $(a,b)$ and if $x_{0}\in(a,b)$, then for all
            $x\in(a,b)$ there is a point $c\in(x_{0},x)$
            or $c\in(x,x_{0})$ such that:
            \begin{equation*}
                f(x)=
                \frac{f^{(n)}(c)(x-x_{0})^{n}}{n!}
                +\sum_{k=0}^{n-1}f^{(k)}(x_{0})\frac{(x-x_{0})^{k}}{k!}
            \end{equation*}
        \end{theorem}
        The two dimensional analog is as follows:
        \begin{align*}
            f(x,y)&=R_{T}+f(a,b)+
            \sum_{1\leq{r+s}\leq{n}}\frac{\partial^{r}}{\partial{x}^{r}}
            \frac{\partial^{s}}{\partial{y}^{s}}\Big(f(a,b)\Big)
            \frac{(x-a)^{r}}{r!}\frac{(y-b)^{s}}{s!}\\
            R_{T}&=\sum_{1\leq{r+s}\leq{n}}\frac{\partial^{r}}{\partial{x}^{r}}
            \frac{\partial^{s}}{\partial{y}^{s}}\Big(f(c,d)\Big)
            \frac{(x-a)^{r}}{r!}\frac{(y-b)^{s}}{s!}
        \end{align*}
        Where $(c,d)$ lies on the line segment between $(a,b)$ and $(x,y)$.
        \begin{definition}
            If $f:\mathbb{R}^{2}\rightarrow\mathbb{R}^{2}$ and
            $g:\mathbb{R}^{2}\rightarrow\mathbb{R}^{2}$ are
            differentiable functions, then the Jacobian is:
            \begin{equation*}
                \frac{\partial(f,g)}{\partial(u,v)}=
                \det
                \begin{pmatrix}
                    \frac{\partial{f}}{\partial{u}}&
                    \frac{\partial{f}}{\partial{v}}\\
                    \frac{\partial{g}}{\partial{u}}&
                    \frac{\partial{g}}{\partial{v}}
                \end{pmatrix}
                =
                \begin{vmatrix}
                    \frac{\partial{f}}{\partial{u}}&
                    \frac{\partial{f}}{\partial{v}}\\
                    \frac{\partial{g}}{\partial{u}}&
                    \frac{\partial{g}}{\partial{v}}
                \end{vmatrix}
            \end{equation*}
        \end{definition}
        \begin{definition}
            If $f$ and $g$ are differentiable functions,
            then the Wronskian $W(x)$ is:
            \begin{equation*}
                W(x)=
                \det
                \begin{pmatrix}
                    f(x)&g(x)\\
                    f'(x)&g'(x)
                \end{pmatrix}
                =
                \begin{vmatrix}
                    f(x)&g(x)\\
                    f'(x)&g'(x)
                \end{vmatrix}
            \end{equation*}
        \end{definition}
    \subsection{Integration}
        \begin{definition}
            The indefinite integral of a differentiable function
            $f$ is $\int{f(x)}\diff{x}=F(x)+C$, where $F$ is such
            that $F'(x)=f(x)$, and $C$ is an arbitrary constant.
        \end{definition}
        \begin{theorem}
            The following are true:
            \begin{enumerate}
                \begin{multicols}{2}
                    \item $\int{x^{n}}\diff{x}%
                           =\frac{x^{n+1}}{n+1}+C$
                    \item $\int\frac{\diff{x}}{x}=\ln|x|+C$
                    \item $\int\frac{\diff{x}}{x-a}=\ln|x-a|+C$
                    \item $\int\frac{\diff{x}}{x^{2}+a^{2}}%
                           =\frac{1}{a}\tan^{-1}(\frac{x}{a})+C$
                    \item $\int\frac{x\diff{x}}{x^{2}+a^{2}}%
                           =\frac{1}{2}\ln|x^{2}+a^{2}|+C$
                    \item $\int\frac{\diff{x}}{\sqrt{a^{2}-x^{2}}}%
                           =\sin^{-1}(\frac{x}{a})+C$
                    \item $\int\sin(ax)\diff{x}=-\frac{1}{a}\cos(ax)+C$
                    \item $\int\cos(ax)\diff{x}=\frac{1}{a}\sin(ax)+C$
                    \item $\int\sec^{2}(x)\diff{x}=\tan(x)+C$
                    \item $\int\exp(ax)\diff{x}=\frac{1}{a}\exp(ax)+C$
                \end{multicols}
            \end{enumerate}
        \end{theorem}
        \begin{example}
            Find $\int\frac{x}{x+1}\diff{x}$:
            \begin{align*}
                \int\frac{x}{x+1}\diff{x}
                =\int\frac{x+1-1}{x+1}\diff{x}
                &=\int\frac{x+1}{x+1}\diff{x}
                -\int\frac{1}{x+1}\diff{x}\\
                &=\int\diff{x}-\int\frac{1}{x+1}\diff{x}
                =x-\ln|x+1|+C
            \end{align*}
        \end{example}
        \begin{theorem}
            If $F(x)=f(u(x))$,
            then $\int{F'(x)}\diff{x}=f(u(x))+C$.
        \end{theorem}
        This is called $u$ substituion. If we have
        $\int{f(x)\diff{x}}$ and we can write
        $f=g(u)$, $dx=g'(u)du$, then we can rewrite the
        integral as
        $\int{f(x)}\diff{x}=\int{g(u)du}$. This comes from
        reversing the chain rule found in the study of
        differentiation.
        \begin{example}
            Compute $\int\frac{x}{x^{2}+a^{2}}\diff{x}$.
            Let $u=x^{2}+a^{2}$. Then $du=2xdx$. We have:
            \begin{equation*}
                \int\frac{x}{x^{2}+a^{2}}\diff{x}
                =\frac{1}{2}\int\frac{\diff{u}}{u}
                =\frac{1}{2}\ln|u|+C
                =\frac{1}{2}\ln|x^{2}+u^{2}|+C
            \end{equation*}
        \end{example}
        The product rule says that
        $\diff(uv)=u\diff{v}+v\diff{u}$. Integrating, we have
        $uv=\int{u}\diff{v}+\int{v}\diff{u}$. This gives rise to
        the integration by parts formula:
        \begin{equation*}
            \int{u}\diff{v}=uv-\int{v}\diff{u}
        \end{equation*}
        \begin{definition}
            The definite integral of $f$ on $(a,b)$ is
            $\int_{a}^{b}f(x)\diff{x}=F(b)-F(a)$, where
            $F$ is a function such that $F'(x)=f(x)$.
        \end{definition}
    \subsection{Sequences and Series}
        \begin{definition}
            A sequence is a function $x:\mathbb{N}\rightarrow{X}$.
            That is, a function whose domain is the natural numbers.
        \end{definition}
        \begin{definition}
            A convergent sequence $a:\mathbb{N}\rightarrow\mathbb{R}$
            is a sequence such that there is an $\ell\in\mathbb{R}$ such
            that, for all $\varepsilon>0$, there is an $N\in\mathbb{N}$
            such that, for all $n>N$, $|a_{n}-\ell|<\varepsilon$.
        \end{definition}
        \begin{theorem}
            If $a_{n}\rightarrow{a}$, and
            $b_{n}\rightarrow{b}$, then:
            \begin{enumerate}
                \begin{multicols}{2}
                    \item $a_{n}+b_{n}\rightarrow{a+b}$
                    \item $a_{n}-b_{n}\rightarrow{a-b}$
                    \item $a_{n}b_{n}\rightarrow{ab}$
                    \item If $b\ne{0}$, $a_{n}/b_{n}\rightarrow{a/b}$
                \end{multicols}
            \end{enumerate}
        \end{theorem}
        \begin{theorem}[Squeeze Theorem]
            If $a_{n}$, $b_{n}$, and $c_{n}$ are sequences such
            that $a_{n}\leq{b_{n}}\leq{c_{n}}$ for all $n\in\mathbb{N}$,
            and if $a_{n}\rightarrow\ell$ and $c_{n}\rightarrow\ell$,
            then $b_{n}\rightarrow\ell$.
        \end{theorem}
        \begin{theorem}
            If $|a_{n}|\rightarrow{0}$, then
            $a_{n}\rightarrow{0}$.
        \end{theorem}
        \begin{definition}
            A monotonically increasing sequence is a sequence
            such that, for all $n$,
            $a_{n}\leq{a_{n+1}}$.
        \end{definition}
        \begin{definition}
            A monotonically decreasing sequence is a sequence
            such that, for all $n$,
            $a_{n}\geq{a_{n+1}}$.
        \end{definition}
        \begin{definition}
            A monotonic sequence is a sequence that is either
            monotonically increasing or monotonically
            decreasing.
        \end{definition}
        \begin{definition}
            A bounded sequence is a sequence $a:\mathbb{N}\rightarrow\mathbb{R}$
            such that there exists an $M\in\mathbb{R}$ such that, for all
            $n\in\mathbb{N}$, $|a_{n}|\leq{M}$.
        \end{definition}
        \begin{definition}
            The $N^{th}$ partial sum of a sequence
            $a:\mathbb{N}\rightarrow\mathbb{R}$ is
            $s_{N}=\sum_{n=1}^{N}a_{n}$.
        \end{definition}
        \begin{definition}
            An infinite series of a sequence $a_{n}$ is the
            limit of the partial sums $s_{N}$ of $a_{n}$.
        \end{definition}
        \begin{definition}
            A convergent infinite series is a series such that
            the limit of partial sums exists.
        \end{definition}
        \begin{definition}
            A divergent infinite series is a series that
            does not converge.
        \end{definition}
        \begin{definition}
            A geometric series is a sum of the form
            $\sum_{n=0}^{N}ar^{n}$, where $a$ and $r$ are constants.
        \end{definition}
        \begin{theorem}
            A series converges if and only if the sequence of partial
            sums forms a Cauchy sequence.
        \end{theorem}
        \begin{theorem}
            If $a_{n}$ is a positive sequence $(a_{n}>0)$
            and if $S_{N}=\sum_{n=1}^{N}$ is bounded above
            by some $M\in\mathbb{R}$ for all $N\in\mathbb{N}$,
            then $\sum_{n=1}^{\infty}a_{n}$ converges. If
            $S_{N}$ is not bounded above, then the sum diverges.
        \end{theorem}
        \begin{theorem}
            If $p>1$, then
            $\sum_{n=0}^{\infty}\frac{1}{p^{n}}$ converges. If
            $p\leq{1}$, the sum diverges.
        \end{theorem}
        \begin{theorem}[Comparison Test]
            If $0\leq{a_{n}}\leq{b_{n}}$ for all $n$,
            and if $\sum_{n=1}^{\infty}b_{n}$ converges,
            then $\sum_{n=1}^{\infty}a_{n}$ converges.
            If $\sum_{n=1}^{\infty}a_{n}$ diverges,
            then $\sum_{n=1}^{\infty}b_{n}$ diverges.
        \end{theorem}
        \begin{theorem}[Alternating Series Test]
            If $a_{n}$ is a strictly decreasing sequence
            of positive real numbers, and if
            $a_{n}\rightarrow{0}$, then
            $\sum_{n=1}^{\infty}(-1)^{n}a_{n}$ converges.
        \end{theorem}
        \begin{definition}
            An absolutely convergent series
            is a series such that
            $\sum_{n=1}^{\infty}|a_{n}|$ converges.
        \end{definition}
        \begin{theorem}
            If $\sum_{n=1}^{\infty}|a_{n}|$ converges,
            then $\sum_{n=1}^{\infty}a_{n}$ converges.
        \end{theorem}
        \begin{theorem}[Ratio Test]
            If $a_{n}$ is a sequence, and
            $|\frac{a_{n+1}}{a_{n}}|\rightarrow\alpha$,
            the $\sum_{n=1}^{\infty}a_{n}$ converges absolutely
            if $\alpha<1$, and diverges if $\alpha>1$.
        \end{theorem}
        \begin{theorem}[Root Test]
            If $\lim_{n\rightarrow\infty}\sqrt[n]{|a_{n}|}=\ell$,
            then $\sum_{n=1}^{\infty}a_{n}$ is absolutely convergent
            if $\ell<1$, and diverges if $\ell>1$.
        \end{theorem}
        \begin{definition}
            A power series is a function
            $f(x)=\sum_{n=0}^{N}c_{n}(x-x_{0})^{n}$
        \end{definition}
        \begin{theorem}[Taylor's Coefficient Theorem]
            If $f(x)=\sum_{n=0}^{N}c_{n}(x-x_{0})^{n}$,
            then $c_{n}=\frac{f^{(n)}(x_{0})}{n!}$.
        \end{theorem}
        \begin{definition}
            The Taylor Series of a function $f$ about $x_{0}$ is
            $f(x)=\sum_{n=0}^{\infty}f^{(n)}(x_{0})\frac{(x-x_{0})^{n}}{n!}$
        \end{definition}
        \begin{definition}
            An analytic function is a function $f$ whose
            Taylor series converges to $f$.
        \end{definition}
        \begin{definition}
            A MacLaurin series is a series of the form:
            $f(x)=\sum_{n=0}^{\infty}f^{(n)}(0)\frac{x^{n}}{n!}$
        \end{definition}
        \begin{example}
            $\sin$ and $\cos$ have the following MacLaurin Series:
            \begin{align*}
                \sin(x)
                &=\sum_{n=0}^{\infty}(-1)^{n}\frac{x^{2n+1}}{(2n+1)!}
                &
                \cos(x)
                &=\sum_{n=0}^{\infty}(-1)^{n}\frac{x^{2n}}{(2n)!}
            \end{align*}
        \end{example}
        \begin{theorem}[Binomial Theorem]
            If $n\in\mathbb{N}$, and $\binom{n}{m}$ is the
            binomial coefficient, then:
            \begin{equation*}
                (x+y)^{n}=
                \sum_{k=0}^{n}\binom{n}{k}x^{k}y^{n-k}=
                \sum_{k=0}^{n}\frac{n!}{(m-k)!k!}x^{k}y^{n-k}
            \end{equation*}
        \end{theorem}
        \begin{theorem}[Geometric Series]
            If $N\in\mathbb{N}$ and $r$ is a real number, then
            \begin{equation*}
                \sum_{n=0}^{N}ar^{n}=a\frac{1-r^{N+1}}{1-r}
            \end{equation*}
        \end{theorem}
        \begin{theorem}
            If $|r|<1$, then
            $\sum_{n=0}^{\infty}ar^{n}$ converges and:
            \begin{equation*}
                \sum_{n=0}^{\infty}ar^{n}=\frac{a}{1-r}
            \end{equation*}
        \end{theorem}
        \begin{definition}
            The Fourier Series of a function $f$ on an interval $[-\pi,\pi]$
            is:
            \begin{equation*}
                f(x)=\sum_{n=0}^{\infty}\Big(a_{n}\cos(nx)+b_{n}\sin(nx)\Big)
            \end{equation*}
            Where:
            \begin{align*}
                a_{0}&=\frac{1}{2\pi}\int_{-\pi}^{\pi}f(x)\diff{x}
                &
                a_{n}&=\frac{1}{\pi}\int_{-\pi}^{\pi}f(x)\cos(x)\diff{x}
                &
                b_{n}&=\frac{1}{\pi}\int_{-\pi}^{\pi}f(x)\sin(x)\diff{x}
            \end{align*}
        \end{definition}
    \subsection{Differential Equations}
        \begin{definition}
            A differential equation is an equation involving an unknown
            function and its derivatives.
        \end{definition}
        \begin{definition}
            An ordinary differential equation is a differential
            equation involving only ordinary derivatives.
        \end{definition}
        \begin{definition}
            A partial differential equation is an equation involving
            a function and its partial derivatives.
        \end{definition}
        \begin{definition}
            The order of a differential equation is the order
            of the highest derivative in the equation.
        \end{definition}
        \begin{definition}
            The degree of a differential equation is the highest exponent
            of the unknown function that appears in the equation.
        \end{definition}
        \begin{definition}
            A linear $n^{th}$ differential equation is an equation of the
            form $\sum_{k=0}^{n}a_{k}(x)y^{(n)}(x)=b(x)$, where
            $a_{n}(x)$ is not the zero function.
        \end{definition}
        First order linear differential equations have a simple method for solution:
        \begin{align*}
            \frac{\diff{y}}{\diff{x}}(x)+P(x)y(x)&=Q(x)\\
            \Rightarrow
            \exp\Big(\int^{x}P(\tau)\diff{\tau}\Big)
            \Big(\frac{\diff{y}}{\diff{x}}(x)+P(x)y(x)\Big)
            &=\exp\Big(\int^{x}P(\tau)\diff{\tau}\Big)Q(x)\\
            \Rightarrow
            \frac{\diff}{\diff{x}}\bigg(
                \exp\Big(\int^{x}P(\tau)\diff{\tau}\Big)
                y(x)\bigg)
            &=\exp\Big(\int^{x}P(\tau)\diff{\tau}\Big)Q(x)\\
            \Rightarrow
            \exp\Big(\int^{x}P(\tau)\diff{\tau}\Big)y(x)
            &=\int^{x}\exp\Big(\int^{\tau}P(\xi)\diff{\xi}\Big)
                Q(\tau)\diff{\tau}\\
            \Rightarrow
            y(x)&=\exp\Big(-\int^{x}P(\tau)\diff{\tau}\Big)
                \int^{x}\bigg(
                    \exp\Big(\int^{\tau}P(\xi)\diff{\xi}\Big)
                    Q(\tau)\diff{\tau}
                \bigg)
        \end{align*}
        The integration over $\tau$ and $\xi$ are just dummy variables.
        If we let $\mu(x)=\exp(\int^{x}P(\tau)\diff{\tau})$, we can write this as:
        \begin{equation*}
            y(x)=\frac{1}{\mu(x)}\int\mu(x)Q(x)\diff{x}+C
        \end{equation*}
        \begin{example}
            The case with constant coefficients is easy to study:
            \begin{align*}
                \frac{\diff{y}}{\diff{x}}+ay&=0\\
                \Rightarrow{y(x)}&=Ce^{-ax}\\
                \frac{\diff{y}}{\diff{x}}+ay&=g(x)\\
                \Rightarrow{y(x)}
                &=e^{-ax}\int{e^{ax}g(x)\diff{x}}
                +Ce^{-ax}
            \end{align*}
        \end{example}
        A separable equation is one of the form:
        \begin{align*}
            M(x,y)+N(x,y)\frac{\diff{y}}{\diff{x}}&=0\\
            \Rightarrow{M(x)M(y)\diff{x}}
            +N(x)N(y)\diff{y}&=0
        \end{align*}
        We can then rewrite this as:
        \begin{equation*}
            \frac{M(x)}{N(x)}\diff{x}+\frac{N(y)}{M(y)}\diff{y}=0
        \end{equation*}
        The general solution is then:
        \begin{equation*}
            \int^{x}\frac{M(x)}{N(x)}\diff{x}+
            \int^{x}\frac{N(y)}{M(y)}\diff{y}=C
        \end{equation*}
        \begin{example}
            Let's solve $xy^{2}\diff{x}-(x+5)\diff{y}=0$.
            This is separable, and we can write:
            \begin{equation*}
                \frac{x}{x+5}\diff{x}-\frac{1}{y^{2}}\diff{y}=0
            \end{equation*}
            Integrating, we obtain the general solution:
            \begin{align*}
                \int\frac{x}{x+5}\diff{x}-\int\frac{1}{y^{2}}
                &=C\\
                \Rightarrow
                x-5\ln|x+5|+\frac{1}{y}&=C\\
                \Rightarrow
                y&=\frac{1}{C-x+5\ln|x+5|}
            \end{align*}
        \end{example}
        \begin{definition}
            An exact equation is an equation of the form
            $M(x,y)\diff{x}+N(x,y)\diff{y}=0$ such that
            $\partial{M}/\partial{y}=\partial{N}/\partial{x}$.
        \end{definition}
        \begin{theorem}
            If $M(x,y)\diff{x}+N(x,y)\diff{y}=0$ is exact,
            then there exists an $F(x,y)$ such that:
            \begin{align*}
                \frac{\partial{F}}{\partial{x}}(x,y)&=M(x,y)
                &
                \frac{\partial{F}}{\partial{y}}(x,y)&=N(x,y)
            \end{align*}
        \end{theorem}
        We can integrate $F$ obtaining:
        \begin{equation*}
            F(x,y)=\int{M(x,y)\diff{x}}+g(y)
        \end{equation*}
        Where $g$ is a function of integration, obtained
        since we are taking partial derivatives.
        If $M(x,y)\diff{x}+N(x,y)\diff{y}$ is not exact, we
        often seek for a multiplication factor $\mu(x,y)$ such that
        $\mu(x,y)M(x,y)\diff{x}+\mu(x,y)N(x,y)\diff{y}$ is exact.
        \begin{theorem}
            If $M_{y}/N=N_{x}/N$ and $M_{y}/N$ is a function of
            $x$ alone, then $\mu(x)=\exp(\int{M_{y}/N}\diff{x}$ is
            the integrating factor.
        \end{theorem}
        \begin{theorem}
            If $M_{y}/M=N_{x}/M$, and
            $M_{y}/M$ is a function of $y$ alone, then
            $\mu(x)=\exp(-\int{M_{y}/M}\diff{y}$ is the
            integrating factor.
        \end{theorem}
        \begin{theorem}
            If $M_{x}+N_{y}\ne{0}$, then
            $\mu(x,y)=(M_{x}+N_{y})^{-1}$ is
            the integrating factor.
        \end{theorem}
        \begin{theorem}
            If $M(x,y)=yf(x,y)$, and
            $N(x,y)=xg(x,y)$, and if
            $f(x,y)\ne{g(x,y)}$, then the
            integrating factor is
            $\mu(x,y)=(M_{x}+N_{y})^{-1}$.
        \end{theorem}
        \begin{example}
            We can convert many types of equations into exact
            equations:
            \begin{equation*}
                x\diff{y}-y\diff{x}=0
                \Rightarrow\frac{x\diff{y}-y\diff{x}}{x^{2}}=0
                \Rightarrow\diff\big(\frac{y}{x}\big)=0
                \Rightarrow{y=Cx}
            \end{equation*}
            \begin{equation*}
                x\diff{y}-y\diff{x}=0
                \Rightarrow\frac{x\diff{y}-y\diff{x}}{xy}=0
                \Rightarrow\frac{\diff{y}}{y}-\frac{\diff{x}}{x}=0
                \Rightarrow\diff\ln\big(\frac{y}{x}\big)=0
                \Rightarrow{y=Cx}
            \end{equation*}
        \end{example}
        A homogeneous equation is an equation $y'(x)=f(x,y)$
        that depends only on the ratio $y/x$ or $x/y$.
        Letting $y=vx$, we have $y'(x)=xv'(x)+v(x)$.
        Substituting this back into our equation, we have:
        $xv'(x)+v(x)=F(v)$. This is separable and can be solved
        by direct integration. After solving, substitute
        $v=y/x$ again, and obtain the solution. A function $f(x,y)$
        is called homogeneous of degree $n$ if
        $f(\lambda{x},\lambda{y})=\lambda^{n}f(x,y)$.
        \begin{example}
            Solve $xy-y^{2}-x^{2}y'=0$. Dividing by $x^{2}$,
            we have:
            \begin{equation*}
                \frac{\diff{y}}{\diff{x}}(x)
                =\frac{y}{x}-\frac{y^{2}}{x^{2}}
            \end{equation*}
            Let $y=vx$. Then $y'(x)=xv'(x)+v$, so we have:
            \begin{equation*}
                x\frac{\diff{v}}{\diff{x}}+v=v-v^{2}
            \end{equation*}
            Simiplifying, we have:
            \begin{equation*}
                \frac{\diff{v}}{\diff{x}}+\frac{v^{2}}{x}=0
            \end{equation*}
            Solving, we get $v=(C-\ln|x|)^{-1}$.
            But $v=y/x$, so we have:
            \begin{equation*}
                y=\frac{x}{C-\ln(x)}
            \end{equation*}
        \end{example}
        A second order differential equation is one involving
        the second derivatives of the unknown function. A simple
        boundary value problem on the interval $(a,b)$ is
        $y''(x)+P(x)y'(x)+Q(x)y(x)=F(x)$, where
        $y(a)=y_{0}$ and $y(b)=y_{1}$. The homogeneous solution
        is $y''(x)+P(x)y'(x)+Q(x)y(x)=0$. From the linearity of
        the problem, if $y_{h}(x)$ is the homogeneous solution,
        and $y_{p}$ solves the original problem, then
        $C_{1}y_{p}+C_{2}y_{h}$ is a solution as well, for
        arbitrary constants $C_{1}$ and $C_{2}$. The Wronskian
        of these two solutions is $y_{h}y_{p}'-y_{h}'y_{p}$.
        The Wronskian is either the zero function, or is
        never zero. Reduction of order allows a second order
        differential equation to be converted into a first
        order differential equation, if a solution is already
        known. If $y''(x)+P(x)y'(x)+Q(x)y(x)=0$, and if
        $y_{1}(x)$ is a known solution, a second
        solution of the form $y_{2}=u(x)y_{1}(x)$ can be
        determined. Substituting this in, we have
        $u''(x)+(P(x)+2y'(x)/y(x))u'(x)=0$. This is a first
        order differential equation in terms of the variable
        $u'(x)$. The solution is
        $u'(x)=C\exp(-\int(p(x)+y'(x)/y(x))\diff{x})$.
        Integrating this gives the second solution.
        The general solution is then
        $y=C_{1}y_{1}(x)+C_{2}y_{2}(x)$. Another common
        second order differential equation is one with
        constant coefficients. If 
        $ay''+by'+cy=0$, we look at the characteristic
        polynomail of the equation:
        $ar^{2}+br+c=0$, and solve for the roots. We
        can do this using the quadratic formula. If
        the roots are real and distinct (That is,
        if $b^{2}-4ac>0$), then the general solution
        is $y=C_{1}\exp(r_{1}x)+C_{2}\exp(r_{2}x)$,
        where $r_{1}$ and $r_{2}$ are the roots. If
        both roots are identical (That is,
        if $b^{2}-4ac=0$), then the general solution
        is $y(x)=C_{1}\exp(rx)+C_{2}x\exp(rx)$. Finally,
        if the roots are complex (That is, if
        $b^{2}-4ac<0$), then the general solution is of
        the form
        $y(x)=\exp(\lambda{x})(C_{1}\sin(\mu{x})+C_{2}\cos(\mu{x})$,
        where $\lambda=b/2a$, and $\mu=\sqrt{b^{2}-4ac}/2a$.
        The last method to be discussed is that of
        variation of parameters. This is also called
        Lagrange's Method. If $y''(x)+P(x)y'(x)+Q(x)y(x)=f(x)$,
        and if $y_{1}$ and $y_{2}$ are solutions
        to the homogeneous equation $f(x)=0$, then the particular
        solution is of the form $y=u_{1}y_{1}+u_{2}y_{2}$ such
        that $u_{1}'(x)y_{1}(x)+u_{2}'(x)y_{2}(x)=0$
        and $u_{1}'(x)y_{1}'(x)+u_{2}'(x)y_{2}'(x)=f(x)$.
        The general solution is thus:
        \begin{equation*}
            y_{p}=
            -y_{1}(x)\int\frac{y_{2}(x)f(x)}{W(y_{1},y_{2})}\diff{x}
            +y_{2}(x)\int\frac{y_{1}(x)f(x)}{W(y_{1},y_{2})}\diff{x}
        \end{equation*}
    \subsection{Probability and Statistics}
        Probability is defined in sample spaces. The sample space of
        a random experiment is the set of all
        possible outcomes for the experiment.
        \begin{example}
            The sample space for tossing two coings randomly is
            $S=\{(H,H),(H,T),(T,H),(T,T)\}$.
        \end{example}
        \begin{definition}
            An event is an element of a sample space.
        \end{definition}
        \begin{definition}
            A probability function is a function
            $P:S\rightarrow[0,1]$, where $S$ is a sample
            space, such that:
            \begin{enumerate}
                \item For all $E\subset{S}$, $0\leq{P(E)}\leq{1}$.
                \item $P(S)=1$ and $P(\emptyset)=0$.
                \item If $A_{n}$ is a set of mutually disjoint
                      events in $S$, then
                      $P(\bigcup_{n=1}^{\infty}A_{n})%
                       =\sum_{n=1}^{\infty}P(A_{n})$
            \end{enumerate}
        \end{definition}
        The uniform probability model on a finite sample space with
        $n$ events gives the probability $1/n$ to every event.
        This is a useful model for many problems, such as flipping
        coins, or tossing dice. Given a collection of events, the probability
        is then the total number of times these events occur divided by
        the total number of possible events. If we take $k$ elements from
        a set of $n$, and if the ordering does not matter, the total number of
        ways to choose the $k$ elements is the number of permutations of $k$
        elements. This is:
        \begin{equation*}
            P(n,k)=\frac{n!}{(n-k)!}
        \end{equation*}
        If the ordering does matter, this is the number of combinations
        that are possible. This is:
        \begin{equation*}
            C(n,r)=\binom{n}{k}=\frac{n!}{k!(n-k)!}
        \end{equation*}
        Here $\binom{n}{k}$ is the \textit{binomial coefficient}.
        \begin{example}
            Suppose 10 men and 8 women are to be selected from to form
            a committee of 5 people. What is the probability that there
            are exactly 3 men? There are $\binom{10}{3}$ ways to
            select 3 men, and $\binom{8}{2}$ ways to select 2 women.
            There is a total of $\binom{18}{5}$ ways to select 5 people.
            Thus, we have:
            \begin{equation*}
                P=\frac{\binom{10}{3}\binom{8}{2}}{\binom{18}{5}}
                =\frac{20}{51}
            \end{equation*}
        \end{example}
        The probability point function, or probability mass function,
        is the function $Q(X)=P(X=x)$. The probability distribution
        function is given by
        $F_{X}(x)=P_{X}(X\leq{x})$. This can be expressed in
        terms of the probability mass function as
        $F_{X}(x)=\sum_{x<X}Q(x)$. For continuous random variables,
        the probability density function is defined as
        $f_{X}(x)=F_{X}'(x)$. In the study of statistics, we choose a sample
        from the total population which is intended to represent the
        entire population as closely as possible.
        \begin{definition}
            The arithmetic mean of a finite set
            $\{x_{1},\hdots,x_{N}\}$ is:
            \begin{equation*}
                \overline{x}=\frac{1}{N}\sum_{n=1}^{N}x_{n}
            \end{equation*}
        \end{definition}
        If $f_{i}$ is the probability of $x_{i}$, then the arithmetic mean is:
        \begin{equation*}
            \overline{x}=
            \frac{\sum_{n=1}^{N}x_{n}f_{n}}{\sum_{n=1}^{N}f_{n}}
        \end{equation*}
        The arithmetic mean is strongly effected by extreme values, or outliers.
        The deviation of $x_{i}$ from the mean $\overline{x}$
        is $d_{i}=x_{i}-\overline{x}$. This has the special property:
        \begin{equation*}
            \sum_{n=1}^{N}d_{n}
            =\sum_{n=1}^{N}(x_{i}-\overline{x})
            =0
        \end{equation*}
        \begin{theorem}
            If $x_{n}$ and $y_{n}$ are finite sequences of
            $N$ elements, and $z_{n}=x_{n}+y_{n}$, then
            $\overline{z}=\overline{x}+\overline{z}$.
        \end{theorem}
        \begin{definition}
            The geometric mean of a finite set
            $\{x_{1},\hdots,x_{N}\}$ is:
            \begin{equation*}
                g=\sqrt[N]{\prod_{n=1}^{N}x_{n}}
            \end{equation*}
            Where
            $\prod_{n=1}^{N}x_{n}=x_{1}\cdot{x_{2}}\cdots{x_{N}}$
        \end{definition}
        Logarithms can help calculate geometric means:
        \begin{theorem}
            Given a finite set $\{x_{1},\hdots,x_{N}\}$,
            the geometric mean is:
            \begin{equation*}
                g=\exp\Big(\frac{1}{N}\sum_{n=1}^{N}x_{n}\Big)
                =\exp(\overline{x})
            \end{equation*}
        \end{theorem}
        Given an initial deposit $A$ in a bank with yearly interest $q$,
        after $n$ years the account balance is given by the
        Compound Interest Formula:
        \begin{equation*}
            M=A(1+q)^{n}
        \end{equation*}
        \begin{definition}
            The harmonic mean of $\{x_{1},\hdots,x_{N}\}$
            is:
            \begin{equation*}
                h=\frac{1}{\frac{1}{N}\sum_{n=1}^{N}\frac{1}{x_{n}}}
                =\frac{n}{\sum_{n=1}^{N}\frac{1}{x_{n}}}
            \end{equation*}
        \end{definition}
        \begin{theorem}
            Given a set of numbers $\{x_{1},\hdots,x_{n}\}$,
            the harmonic mean $h$, and the geometric mean $g$,
            $h\leq{g}\leq\overline{x}$. Equality holds if
            and only if all of the $x_{i}$ are the same.
        \end{theorem}
        \begin{definition}
            The root-mean square (\textit{rms}), or the
            quadratic mean, of a data set $\{x_{1},\hdots,x_{N}\}$
            is:
            \begin{equation*}
                rms=\sqrt{\overline{x^{2}}}
                =\sqrt{\frac{1}{N}\sum_{n=1}^{N}x_{n}^2}
            \end{equation*}
        \end{definition}
        \begin{theorem}
            The quadratic mean of two numbers is greater
            than the geometric mean. That is:
            \begin{equation*}
                \sqrt{ab}\leq\sqrt{\frac{a^{2}+b^{2}}{2}}
            \end{equation*}
        \end{theorem}
        \begin{proof}
            For $(a-b)^{2}\geq{0}$, and thus
            $a^{2}+b^{2}-2ab\geq{0}$. Therefore, etc.
        \end{proof}
        The variation of data is a measurement of how much
        the data spreads about the average of the data.
        \begin{definition}
            The range of a finite data set is the maximum value
            minus the minimum value.
        \end{definition}
        The $n^{th}$ percentile of a data set is the value such that
        $n\%$ of the data set lies below said value, and
        $(100-n)\%$ lies above it. Percentiles are usually split into
        quartiles to better represent data sets. The interquartile range
        is the difference between the $75\%$ and the $25\%$ marks.
        The average deviation of a data set is:
        \begin{equation*}
            \overline{|\overline{x}-x_{i}|}
            =\frac{1}{N}\sum_{n=1}^{N}|\overline{x}-x_{i}|
        \end{equation*}
        Note the need for the absolute value signs. For without them,
        the average would always be zero.
        \begin{definition}
            The standard deviation of a finite data set
            $\{x_{1},\hdots,x_{N}\}$ is:
            \begin{equation*}
                \sigma=
                \sqrt{\frac{1}{N}\sum_{n=1}^{N}(\overline{x}-x_{n})^{2}}
            \end{equation*}
        \end{definition}
        \begin{definition}
            The variance of a data set is the square of the
            standard deviation. $V=\sigma^{2}$.
        \end{definition}
        \begin{theorem}
            The variance is equal to:
            \begin{equation*}
                V=\overline{x^{2}}-\overline{x}^{2}
            \end{equation*}
        \end{theorem}
        For a normal, or bell curve, the interval between
        $\overline{x}$ and $\overline{x}\pm\sigma$ contains
        roughly $68\%$ of the entire data set. $2\sigma$
        contains about $95\%$ of the data, and
        $3\sigma$ contains $99.5\%$.
        The $\chi$ square statistic is defined by:
        \begin{equation*}
            \chi^{2}=\frac{1}{\sigma^{2}}
                \sum_{n=1}^{N}(\overline{x}-x_{n})^{2}
        \end{equation*}
        The $\chi$ square distribution is:
        \begin{equation*}
            Y=Y_{0}\chi{\nu-2}e^{-\frac{1}{2}\chi^{2}}
        \end{equation*}
        Where $\nu=n-1$ is the umber of degrees of freedom,
        and $Y_{0}$ is a constant to make the area under the
        curve equal 1.
    \subsection{Combinatorics}
        If one event can happen $n$ ways, and another independent
        event can happen $m$ ways, then the total number
        of possibilities is $nm$. If there are 5 entrees
        and 3 sides at a restaurant, then there are
        15 total possible meals.
        \begin{definition}
            The factor of a positive integer
            $n$, denoted $n!$, is
            $n!=n\cdot(n-1)\cdots{2}\cdot{1}$
            We define $0!=1$.
        \end{definition}
        The factorial of a number $n$ is the number
        of ways to permute $n$ objects. The number of ways
        to permute $k$ objects from a set of $n$ is
        $P(n,k)=n!/(n-k)!$, and the number of ways to
        choose $k$ objects from a set of $n$ objects
        (Taking the order into account) is the
        binomial coefficient $\binom{n}{k}$. The number
        of permutations of $n$ objects into groups
        $n_{1},n_{2},\hdots,n_{N}$, where
        $n_{1}+n_{2}+\hdots+n_{N}=n$, is
        $n!/(n_{1}!n_{2}!\hdots{n_{N}!})$
        Stirling's Approximation says that, for large
        $n$, the factorial can be approximated as follows:
        \begin{equation*}
            n!\approx\sqrt{2\pi{n}}n^{n}e^{-n}
        \end{equation*}
        \begin{example}
            What is the probability of rolling
            four 4's out six tosses of a six sided
            dice? The number of ways to choose
            two numbers that are not 4 is
            $\binom{6}{2}=15$. The probability of
            an event with four 4's and two numbers
            that aren't 4 is
            $(\frac{1}{6})^{4}(\frac{5}{6})^{2}$.
            So the probability of rolling four 4's is
            $15(\frac{1}{6})^{4}(\frac{5}{6})^{2}=0.008$.
        \end{example}
    \subsection{Numerical Analysis}
        \begin{definition}
            If $a_{n}\rightarrow{a}$, then $a_{n}$ converges
            with order of converges $O(\beta_{n})$, where
            $\beta_{n}\ne{0}$, if, for large enough $N$, there
            is a $K$ such that for all $n<N$:
            \begin{equation*}
                \Big|\frac{a-a_{n}}{\beta_{n}}\Big|\leq{K}
            \end{equation*}
        \end{definition}
        Given the order of convergence $\beta_{n}$, we can
        write $a_{n}=a+O(\beta_{n})$. A similar definition
        holds for the limit of functions.
        If $f$ is continuous and $f(a)$ and $f(b)$ have
        opposite signs, then the intermediate value theorem
        says that there is a $c\in(a,b)$ such that
        $f(c)=0$. The bisection method is an algorithm for
        approximation such a value $c$. Let $p$ be the
        mean of $a$ and $b$ (The midpoint). If $f(p)$ has
        the same sign as $f(a)$, choose your next point
        to be the mean of $p$ and $b$. If $f(p)$ has
        the same sign as $f(b)$, choose your next point
        to be the midpoint of $a$ and $p$. If $f(p)=0$,
        you're done. Continuing on in this manner, we bisect
        the interval each time so that after $n$ steps we
        have an interval $(b-a)/2^{n+1}$ in width. The error
        of $p$ from the actual value is the bound by this
        number, which tends to zero very fast.
        The Newton-Raphson method is a method for computing
        the roots of various equations. If $f$ is twice
        differentiable, and $x_{0}$ is your initial guess,
        we can create a sequence that converges to the
        root (Provided $f$ has various properties) by:
        \begin{equation*}
            x_{n+1}=x_{n}-\frac{f(x_{n})}{f'(x_{n})}
        \end{equation*}
        In particular, $f'(x)$ can not be zero, or near
        zero. This can cause convergence to fail.
        If the root is a simple root, the convergence
        is quadratic. The fixed tangent method is:
        \begin{equation*}
            x_{n+1}=x_{n}-\frac{f(x_{n})}{f'(x_{0})}
        \end{equation*}
        Convergence is again subjeect to various criteria
        on $f$. For derivatives that are difficult to
        compute, we can use the secant method:
        \begin{equation*}
            x_{n+1}=x_{n}-\frac{f(x_{n})(x_{n}-x_{n-1})}
                               {f(x_{n})-f(x_{n-1})}
        \end{equation*}
        The order of convergence is approximately 1.62.
        The Regula Falsi method combines the secant
        method and the bisection method. We create the
        sequence:
        \begin{equation*}
            x_{n+1}=a_{n}-\frac{f(a_{n})(b_{n}-a_{n})}
                               {f(b_{n})-f(a_{n})}
        \end{equation*}
        \begin{theorem}
            If $P$ is a polynomial over the complex plane
            of degree $n\geq{1}$,
            then there is a $z\in\mathbb{C}$
            such that $P(z)=0$. That is,
            all non-constant polynomials have at least
            one root in the complex plane.
        \end{theorem}
        \begin{theorem}
            If $P$ is a polynomial of degree $n$,
            then there exists a unique complex number $\alpha$,
            and unique complex numbers $r_{i}$ called the roots,
            and integers $m_{i}$ called the degrees,
            such that $\sum{m_{i}}=n$, and
            $P=\alpha(x-r_{1})^{m_{1}}\cdots(x-r_{N})^{m_{N}}$
        \end{theorem}
        \begin{theorem}
            A polynomial of degree $n$ has
            $n$ roots (Though they may not be distinct).
        \end{theorem}
        \begin{theorem}
            If $P$ and $Q$ are polynomials
            of degree $n$, and if there are
            $N>n$ numbers $x_{i}$ such that
            $P(x_{i})=Q(x_{i})$, then
            $P(x)=Q(x)$.
        \end{theorem}
        \begin{theorem}
            If $P$ is a polynomial and
            $z=a+bi$ has multiplicity $m$, then
            $a-bi$ has multiplicity $m$ as well.
        \end{theorem}
        Horner's Method is used to quickly calculate
        polynomials:
        $P(x)=a_{0}+x(a_{1}+a_{2}x(a_{3}+x_{4}x(\hdots)))$
        \begin{theorem}[Weierstrass Approximation Theorem]
            If $f$ is continuous on $[a,b]$, then for all
            $\varepsilon>0$ there is a polynomial $P(x)$
            such that $|P(x)-f(x)|<\varepsilon$ for all
            $x\in[a,b]$.
        \end{theorem}
        Recalling the binomial theorem from before, we have
        several theorem pertaining to polynomials:
        \begin{theorem}
            The following are true:
            \begin{enumerate}
                \begin{multicols}{2}
                    \item $\sum_{i=0}^{n}%
                           \binom{n}{i}x^{i}(1-x)^{n-i}=1$
                    \item $\sum_{i=0}^{n}%
                           \binom{n}{i}x^{i}(1-x)^{n-i}%
                           (i-nx)=0$
                    \item $\sum_{i=0}^{n}%
                           \binom{n}{i}x^{i}(1-x)^{n-i}%
                           (i-nx)^{2}=nx(1-x)$
                \end{multicols}
            \end{enumerate}
        \end{theorem}
        \begin{definition}
            If $f$ is defined on $[0,1]$, then the Bernstein
            polynomial of $f$ of degree $N$ is:
            \begin{equation*}
                B_{N}(x)=\sum_{n=0}^{N}\binom{N}{n}
                    f\Big(\frac{n}{N}\Big)
                    x^{n}(1-x)^{N-n}
            \end{equation*}
        \end{definition}
        \begin{theorem}[Bernstein's Theorem]
            If $f[0,1]:\rightarrow\mathbb{R}$ is
            continuous, then $B_{N}(x)\rightarrow{f(x)}$
            uniformly.
        \end{theorem}
        The problem of finding polynomials that pass through
        fixed points in the plane involves the
        techniques of interpolation. Lagrange polynomials can
        be used for this problem. There are other methods such
        as Neville's method and Aitken's method.
        \begin{definition}
            Given a function $f$ defined on $[a,b]$, and
            nodes $a=x_{0}<x_{1}<\hdots<x_{n}=b$, a
            cubic spline $C$ is a function such that,
            for all $[x_{i},x_{i+1}]$, the restriction
            of $C$ to that interval, $C_{i}$,
            is a cubic polynomial and
            $C(x_{i})=f(x_{i})$ for all nodes. Also,
            $C_{i}(x_{i+1})=C_{i+1}(x_{i+1})$,
            $C_{i}'(x_{i+1})=C_{i+1}'(x_{i+1})$, and
            $C_{i}''(x_{i+1})=C_{i+1}''(x_{i+1})$
            That is, the cubic sections meet
            tangentially and share the same
            second derivative.
        \end{definition}
        Numerical integration is another technique that is
        important in numerical analysis. There are two popular
        kinds. Riemann sums approximate integrates with
        rectangles. We can instead approximate with
        lines (Forming trapezoids) of quadratics.
        The trapezoid rule has the formula:
        \begin{equation*}
            \int_{a}^{b}f(x)\diff{x}\approx
            \frac{b-a}{2n}[f_{0}+2f_{1}+2f_{2}+
            \hdots+2f_{n-2}+2f_{n-1}+f_{n}]
        \end{equation*}
        The approximation error for the trapezoid rule is:
        \begin{equation*}
            E_{n}=\Big|\frac{(b-a)^{3}f''(c)}{12n^{2}}\Big|
        \end{equation*}
        Where $c\in(a,b)$. Simpson's rule uses approximating
        quadratics between each node. This formula is:
        \begin{equation*}
            \int_{a}^{b}f(x)\diff{x}\approx
            \frac{b-a}{3n}[f(x_{0})+4f(x_{1})+2f(x_{2})
            +4f(x_{3})+\hdots+2f(x_{n-2})+4f(x_{n-1})+f(x_{n})]
        \end{equation*}
        The approximating error is:
        \begin{equation*}
            E_{n}=\Big|\frac{(b-a)^{5}f^{(4)}(c)}{180n^{4}}\Big|
        \end{equation*}
        Where $c\in(a,b)$. The trapazoidal and Simpson's rules
        can be obtained by integrating the Lagrange
        interpolating formula over
        $[a,b]$. Quadrature methods are obtained from
        integrating over the Lagrange polynomials:
        \begin{equation*}
            \int_{a}^{b}\sum_{n=0}^{N}f(x_{n})L_{n}(x)\diff{x}
            =\sum_{n=0}^{N}c_{i}f(x_{i})
        \end{equation*}
        There are many types of quadrature rules, such as
        Newton-Cotes methods, and Gaussian quadrature. Another
        type of problem that arises in numerical analysis is
        finding a continuous function that best fits a
        discrete set of data. Least squares approximations are
        one such method of finding a line of best fit.
        We wish to find a line $y=ax+b$ that minimizes the
        following: If $(x_{i},y_{i})$ are the data points,
        \begin{equation*}
            M=\sum_{n=1}^{N}\big(y_{n}-(ax_{n}+b_{n})\big)^{2}
        \end{equation*}
        Thus we have:
        \begin{align*}
            \frac{\partial{M}}{\partial{a}}&=0
            &
            \frac{\partial{M}}{\partial{b}}&=0
        \end{align*}
        Solving this set of equations gives us
        our $a$ and $b$:
        \begin{align*}
            a&=
            \frac{N\sum_{n=1}^{N}x_{n}y_{n}-
                  \Big(\sum_{n=1}^{N}x_{n}\Big)
                  \Big(\sum_{n=1}^{N}y_{n}\Big)}
                 {N\sum_{n=1}^{N}x_{n}^{2}-
                  \Big(\sum_{n=1}^{N}x_{n}\Big)^{2}}\\
            b&=
            \frac{\Big(\sum_{n=1}^{N}x_{n}^{2}\Big)
                  \Big(\sum_{n=1}^{N}y_{n}\Big)-
                  \Big(\sum_{n=1}^{N}x_{n}y_{n}\Big)
                  \Big(\sum_{n=1}^{N}x_{n}\Big)}
                 {N\sum_{n=1}^{N}x_{n}^{2}-
                  \Big(\sum_{n=1}^{N}x_{n}\Big)^{2}}\\
        \end{align*}
        This least squares method uses a line to best fit
        the data. However we may use arbitrary polynomials
        and exponential functions, as well.
        \begin{definition}
            A sequence of polynomials $P_{n}(x)$ is
            orthogonal on an interval $[a,b]$ with
            respect to a weight $w$ if:
            \begin{equation*}
                \int_{a}^{b}w(x)P_{n}(x)P_{m}(x)\diff{x}
                =\delta_{nm}
            \end{equation*}
            Where $\delta_{nm}$ is the Kronecker Delta Function.
            That is, $\delta_{nm}=1$ if $n=m$, and
            is equal to zero otherwise.
        \end{definition}
        \begin{theorem}
            If $\psi_{n}$ is a set of orthogonal
            polynomials with weight $w$ on the
            interval $[a,b]$, then the least squares
            solution for approximating a continuous
            function $f$ with $\psi_{n}$ is
            $f(x)=\sum_{n=0}^{N}c_{n}\psi_{n}(x)$,
            where:
            \begin{equation*}
                c_{n}=
                \frac{\int_{a}^{b}w(x)\psi_{n}(x)f(x)\diff{x}}
                     {\int_{a}^{b}w(x)\psi_{n}^{2}(x)\diff{x}}
            \end{equation*}
        \end{theorem}
    \subsection{Real Variables}
        The real line, or real number system, is a complete ordered
        field. That is, it is complete in the sense that all
        Cauchy sequences converge, has a total order structure
        on it, and has a field structure (That of addition,
        multiplication, subtraction, and division).
        An open subset of the real line is a set $S$ such that
        for all $x\in{S}$ there is a $\varepsilon>0$ such that
        $(x-\varepsilon,x+\varepsilon)\subset{S}$. The entire
        space $\mathbb{R}$ is open, as is the empty set
        $\emptyset$. The union of
        an arbitrary collection of open sets is open, and the
        intersection of finitely many open sets is open. The
        intersection of infinitely many open sets may not be
        open, however. A set is closed if its complement is
        open. The Euclidean plane is the set of all ordered
        pairs $(a,b)$. That is,
        $\mathbb{R}^{2}=\mathbb{R}\times\mathbb{R}$. Euclidean
        space, or 3-space, is
        $\mathbb{R}^{3}=\mathbb{R}\times\mathbb{R}\times\mathbb{R}$.
        This is the set of all ordered triplets $(x,y,z)$. Similarly,
        $n$ dimensional Euclidean space is the set of all
        $n$ tuples. This is denoted $\mathbb{R}^{n}$. The distance
        between two points $\mathbf{x}$ and $\mathbf{y}$ is defined
        by the generalized Pythagorean Theorem:
        \begin{equation*}
            d(\mathbf{x},\mathbf{y})=
            \sqrt{\sum_{k=1}^{n}(x_{k}-y_{k})^{2}}
        \end{equation*}
        \begin{definition}
            A metric on a set $X$ is a function
            $d:X\times{X}\rightarrow\mathbb{R}$ such that:
            \begin{enumerate}
                \item $d(x,y)\geq{0}$ for all $x,y\in{X}$.
                \item $d(x,y)=0$ if and only if $x=y$.
                \item $d(x,y)=d(y,x)$ for all $x,y\in{X}$.
                \item $d(x,z)\leq{d(x,y)+d(y,z)}$
                      for all $x,y,z\in{X}$.
            \end{enumerate}
        \end{definition}
        There are two types of integrals defined for functions
        of a real variable: Riemann Integration and Lebesgue Integration.
        Lebesgue integration requires the notion of \textit{measure}.
    \subsection{Complex Variables}
        A complex function is a function whose argument is a complex
        variable $z=x+iy$, where $i$ is the imaginary unit. Complex
        functions can have the problem of being multi-valued, which
        is a cause for caution when dealing with them. For example,
        in the complex realm every non-zero complex number $z$
        has two square roots $\sqrt{z}$. So the square root
        function is multi-valued. Any complex function $f(z)$ can
        be written as $f(z)=u(x,y)+iv(x,y)$, where $u$ and $v$ are
        purely real functions. The function $w=f(z)$ can be seen
        as a mapping, or transformation, of the $z$ plane to
        the $w$ plane. That is, $f$ is a transformation of
        its domain onto its range, or image. A compound complex
        function is one of the form $F(z)=g(f(z))$. Since complex
        functions are functions of two variables, in a sense, one
        must be careful when considering limits of complex functions.
        \begin{example}
            What is the limit of $z/\overline{z}$ as $z\rightarrow{0}$?
            This is undefined. For:
            \begin{equation*}
                \frac{z}{\overline{z}}=\frac{x+iy}{x-iy}
            \end{equation*}
            Letting $x=0$ and taking the limit on $y$,
            we get:
            \begin{equation*}
                \frac{0+iy}{0-iy}=-1
            \end{equation*}
            Letting $y=0$ and taking the limit on $x$,
            we get:
            \begin{equation*}
                \frac{x+0i}{x-0i}=1
            \end{equation*}
            So the limit does not exist.
        \end{example}
        Continuity and the various properties of limits
        are defined similarly on $\mathbb{C}$ as for
        $\mathbb{R}$, with distance between points being
        defined by
        $d(z_{1},z_{2})=\sqrt{(x_{2}-x_{1})^{2}+(y_{2}-y_{1})^{2}}$.
        Differentiation is defined as:
        \begin{equation*}
            f'(z_{0})=\lim_{z\rightarrow{z_{0}}}\frac{f(z)-f(z_{0})}{z-z_{0}}
        \end{equation*}
        \begin{theorem}
            A complex function $f(z)=u(x,y)+iv(x,y)$ is
            differentiable if and only if it satisfies
            the Cauchy-Riemann equations:
            \begin{align*}
                \frac{\partial{u}}{\partial{x}}
                &=\frac{\partial{v}}{\partial{y}}
                &
                \frac{\partial{u}}{\partial{y}}
                &=-\frac{\partial{v}}{\partial{x}}
            \end{align*}
        \end{theorem}
        \begin{theorem}
            If $f(z)=u(x,y)+iv(x,y)$ is differentiable,
            then:
            \begin{equation*}
                f'(z)=u_{x}(x,y)+iv_{y}(x,y)
            \end{equation*}
        \end{theorem}
        \begin{definition}
            A complex function $f(z)$ is analytic,
            or holomorphic, at a point $z_{0}$ if
            it is differentiable in some neighborhood of
            $z_{0}$.
        \end{definition}
        \begin{definition}
            An entire function is a complex function
            $f(z)$ such that $f$ is analytic at every
            point $z\in\mathbb{C}$.
        \end{definition}
        \begin{definition}
            A harmonic function is a function
            $A(x,y)$ such that all of its second
            partial derivatives exists, and it
            satisfies the Laplace Equation:
            \begin{equation*}
                \nabla^{2}A
                =A_{xx}(x,y)+A_{yy}(x,y)
                =0
            \end{equation*}
        \end{definition}
        \begin{theorem}
            If $f(z)=u(x,y)+iv(x,y)$ is differentiable
            on a domain $D$, then $u$ and $v$ are
            harmonic on the domain.
        \end{theorem}
        \begin{theorem}
            A function $f(z)$ is analytic if and only if
            its real and complex parts are harmonic
            conjugates of each other.
        \end{theorem}
        \begin{definition}
            A level curve of a function $f(x,y)$ is
            a curve in $\mathbb{R}^{2}$ such that
            $f$ is constant on that curve.
        \end{definition}
        One of the most basic and fundamental results from
        complex variables is Euler's Formula:
        \begin{equation*}
            \exp(i\theta)=\cos(\theta)+i\sin(\theta)
        \end{equation*}
    \subsection{Set Theory}
        A set is a collection of objects. The objects in a
        set are called the elements of the set. The emptyset
        $\emptyset$ is the set containing no elements. A set
        is finite if there is a bijection between it
        and $\mathbb{Z}_{n}=\{1,2,\hdots,n\}$ for some
        $n\in\mathbb{N}$. A set is infinite if it is
        not finite and non-empty.
        A set is countably infinite if there is a bijection
        between it and $\mathbb{N}$. A set is uncountably
        infinite if it is infinite and not
        countably infinite. Sets can be represented by
        Venn diagrams, which are essentially blobs in
        the plane. Two sets are equal if and only if they
        contain exactly the same elements. A subset $A$ of
        a set $B$, denoted $A\subset{B}$, is a set such that
        for all $x\in{A}$, $x\in{B}$ as well. That is, every
        element of $A$ is also an element of $B$. For any
        set $A$, $A\subset{A}$. A proper subset is a susbet
        $A\subset{B}$ such that $A\ne{B}$. That is, there is
        an $x\in{B}$ such that $x\notin{A}$. Disjoint sets
        are sets that have no common elements.
        \begin{theorem}
            If $A\subset{B}$ and $B\subset{C}$,
            then $A\subset{C}$.
        \end{theorem}
        \begin{theorem}
            If $A\subset{B}$ and $B\subset{A}$, then
            $A=B$.
        \end{theorem}
        \begin{theorem}
            If $A$ is a set, then $\emptyset\subset{A}$.
        \end{theorem}
        The universe set $U$ is the set of all objects under
        consideration. If $A$ is a subset of a universe $U$,
        then the complement of $A$, denoted $A^{C}$, is the
        set of all elements in $U$ that are NOT contained
        in $A$. This is also known as set difference:
        $A^{C}=U\setminus{A}$. The power set of a set $A$,
        denoted $\mathcal{P}(A)$ is the set of all subsets
        of $A$.
        \begin{definition}
            A partial ordering on a set $A$ is a
            relation $\leq$ such that if
            $A\leq{B}$ and $B\leq{C}$, then
            $A\leq{C}$, and if $A\leq{B}$ and
            $B\leq{A}$, then $A=B$.
        \end{definition}
        Set inclusion is a partial ordering on the power
        set of a set. The union of two sets $A$ and $B$
        is the set $A\cup{B}$ containing all of the elements
        of $A$ and all of the elements of $B$.
        The intersection of $A$ and $B$
        is the set $A\cap{B}$ containing only the elements
        that are in both $A$ and $B$.
        \begin{theorem}
            The following are true:
            \begin{enumerate}
                \begin{multicols}{3}
                    \item $A\cup{A}=A$
                    \item $A\cup{B}=B\cup{A}$
                    \item $A\cup\emptyset=A$
                    \item $(A\cup{B})\cup{C}=A\cup(B\cup{C})$
                    \item $A\cup{A}=A$
                    \item $A\subset{B}\Rightarrow{A=A\cup{B}}$
                    \item $A\subset{A\cup{B}}$
                    \item $A\cap{B}=B\cap{A}$
                    \item $A\cap\emptyset=\emptyset$
                    \item $(A\cap{B})\cap{C}=A\cap(B\cap{C})$
                    \item $A\cap{B}\subset{A}$
                    \item $A\subset{B}\Rightarrow{A=A\cap{B}}$
                \end{multicols}
            \end{enumerate}
        \end{theorem}
        \begin{theorem}
            If $U$ is a universe set, and $A\subset{U}$, then:
            \begin{enumerate}
                \begin{multicols}{5}
                    \item $\emptyset^{C}=U$
                    \item $U^{C}=\emptyset$
                    \item $(A^{C})^{C}=A$
                    \item $A\cup{A^{C}}=U$
                    \item $A\cap{A^{C}}=\emptyset$
                \end{multicols}
            \end{enumerate}
        \end{theorem}
        \begin{theorem}[DeMorgan's Theorem]
            If $U$ is a universe and $A,B\subset{U}$,
            then:
            \begin{enumerate}
                \begin{multicols}{2}
                    \item $(A\cup{B})^{C}=A^{C}\cap{B^{C}}$
                    \item $(A\cap{B})^{C}=A^{C}\cup{B^{C}}$
                \end{multicols}
            \end{enumerate}
        \end{theorem}
        \begin{theorem}[Distributive Laws]
            If $A$ and $B$ are sets, then:
            \begin{enumerate}
                \begin{multicols}{2}
                    \item $A\cup(B\cap{C})=(A\cup{B})\cap(A\cup{C})$
                    \item $A\cap(B\cup{C})=(A\cap{B})\cup(A\cap{C})$
                \end{multicols}
            \end{enumerate}
        \end{theorem}
        \begin{definition}
            The difference of two sets $A$ and $B$
            is: $A\setminus{B}=\{x\in{A}:x\notin{B}\}$
        \end{definition}
        \begin{theorem}
            If $U$ is a universe, and $A,B\subset{U}$,
            then $A\setminus{B}=A\cap{B^{C}}$
        \end{theorem}
        \begin{definition}
            The symmetric difference of two set
            $A$ and $B$ is
            $A\oplus{B}=(A\cup{B})\setminus(A\cap{B})$
        \end{definition}
        The symmetric difference is the set of all elements
        that are in either $A$ or in $B$, but not contained
        in both. A function or a mapping from a set $A$ to
        a set $B$ is a rule which assigns to every element
        $a\in{A}$ a unique element $b\in{B}$. An injective
        mapping, or a one-to-one mapping, is a function
        $f:A\rightarrow{B}$ such that
        $f(a_{1})=f(a_{2})$ if and only if $a_{1}=a_{2}$.
        A surjective mapping, or a correspondence, or an
        onto mapping, is a function $f:A\rightarrow{B}$
        such that for all $b\in{B}$, there is an
        $a\in{A}$ such that $f(a)=b$. A bijection is a
        function $f:A\rightarrow{B}$ that is both
        injective and surjective. Two sets are said to
        have the same size, or the same cardinality,
        if there is a bijection between them.
        \begin{theorem}
            The rational numbers $\mathbb{Q}$ are countable.
            That is, there is a bijection
            $f:\mathbb{N}\rightarrow\mathbb{Q}$.
        \end{theorem}
        \begin{theorem}
            The real numbers $\mathbb{R}$ are uncountable.
        \end{theorem}
    \subsection{Topology}
        \begin{definition}
            A topology $\tau$ on a set $X$ is a subset
            of $\mathcal{P}(X)$ such that:
            \begin{enumerate}
                \item $\emptyset\subset\tau$ and $X\subset\tau$
                \item For any subset $\mathcal{O}\subset\tau$, the
                      union of all elements of $\mathcal{O}$ is an
                      element of $\tau$.
                \item For any finite subset of $\tau$, the intersection
                      is also an element of $\tau$.
            \end{enumerate}
        \end{definition}
        \begin{definition}
            A topological space, denoted $(X,\tau)$, is a set
            $X$ and a topology $\tau$ on $X$.
        \end{definition}
        Elements of a topological space are called points.
        Elements of the topology are called open subset of $X$.
        A neighborhood of a point $x$ is a set $V$ that contains
        an open subste $U$ such that $x\in{U}$. An open neighborhood
        of $x$ is an open set $U$ such that $x\in{U}$.
        \begin{example}
            The chaotic topology on a set
            $X$ is the set $\tau=\{\emptyset,X\}$.
        \end{example}
        \begin{example}
            The discrete topology on a set $X$ is
            $\tau=\mathcal{P}(X)$.
        \end{example}
        \begin{example}
            The Sierpinski topology
            on $\{0,1\}$ is the set
            $\{\emptyset,\{0\},\{0,1\}\}$.
        \end{example}
        \begin{theorem}
            If $T_{\omega}$ is a set of topologies on
            a topological space $X$, then
            $\bigcap{T_{\omega}}$ is a topology on $X$.
        \end{theorem}
        However, the union of topologies may not be a
        topology. A topology $\tau_{1}$ is set to
        be finer than a topology $\tau_{0}$ if
        $\tau_{0}\subset\tau_{1}$. An accumulation point
        of a set $A$ is a point $x$ such that, for
        all open neighborhoods $U$ of $A$,
        $U\cap{A}\ne\emptyset$.
        \begin{theorem}[Bolzano-Weierstrass Theorem]
            If $X$ is a bounded, infinite subset of
            $\mathbb{R}$, then $X$ has at least
            one accumulation point.
        \end{theorem}
        \begin{definition}
            The Euclidean topology on
            $\mathbb{R}$ is the set of
            all open sets in the sense that
            $U$ is open if, for all $x\in{U}$,
            there is an $\varepsilon>0$ such
            that $(x-\varepsilon,x+\varepsilon)\subset{U}$.
        \end{definition}
        \begin{definition}
            A closed subset of a topological space
            $(X,\tau)$ is a set $A$ such that
            $A^{C}\in\tau$.
        \end{definition}
        \begin{theorem}
            The intersection of an arbitrary collection of
            closed sets is closed. The union of finitely
            many closed sets is closed.
        \end{theorem}
        \begin{proof}
            Apply DeMorgan's theorem to the properties
            of a topological space $\tau$.
        \end{proof}
        \begin{definition}
            The closure of a set $A$,
            denoted $\overline{A}$, is the
            intersection of all closed sets
            containing $A$.
        \end{definition}
        \begin{theorem}
            If $A$ is a set in a topological space
            $(X,\tau)$, then $A\subset\overline{A}$.
        \end{theorem}
        There's also something called the derived
        set of a set $A$. The interior of $A$
        is the union of all open subset of $A$.
        The boundary of $A$ is the set difference
        of the closure of $A$ and the interior of
        $A$.
        \begin{theorem}
            If $A$ is a set, then
            the interior of $A$ is equal to
            $(\overline{A^{C}})^{C}$
        \end{theorem}
        \begin{definition}
            A dense subset of a topological
            space $(X,\tau)$ is a set $A$
            such that $\overline{A}=X$.
        \end{definition}
        \begin{definition}
            The neighborhood system of a point
            $x$ in a topological space $(X,\tau)$
            is the set of all neighborhoods of
            $x$.
        \end{definition}
        \begin{definition}
            A sequence in a topological space
            $a_{n}$ converges to a point $a$ if,
            for all open neighborhoods $U$ of $a$,
            there is an $N\in\mathbb{N}$ such that,
            for all $n>N$, $a_{n}\in{U}$.
        \end{definition}
        Limits of sequences in topological spaces are NOT
        necessarily unique. This is different from convergence
        in $\mathbb{R}$, where convergence is always unique.
        \begin{definition}
            The relative topology of a
            topological space $(X,\tau)$ with
            respect to a subset $A\subset{X}$
            is $\tau_{A}=\{A\cap{U}:U\in\tau\}$
        \end{definition}
        \begin{theorem}
            If $(X,\tau)$ is a topological space and
            $A\subset{X}$, then
            $(A,\tau_{A})$ is a topological space.
        \end{theorem}
        $(A,\tau_{A})$ is called a subspace of
        $(X,\tau)$.
        \begin{definition}
            A basis of a topological space
            $(X,\tau)$ is a subset $B$ of
            $\tau$ such that every element
            of $\tau$ is the union of some of the
            elements of $B$.
        \end{definition}
        \begin{theorem}
            A subset $B\subset\tau$ is a basis
            for $\tau$ if and only if for all
            $U\in\tau$ and all $x\in{U}$, there is
            a $V\in{B}$ such that
            $x\in{V}\subset{U}$.
        \end{theorem}
        \begin{theorem}
            If $B$ is a basis of $\tau$, then
            $U$ is open if and only if for all
            $x\in{U}$ there is a $V\in{B}$ such that
            $x\in{V}\subset{U}$.
        \end{theorem}
        \begin{theorem}
            $\mathbb{R}$ has a countable basis.
        \end{theorem}
        \begin{proof}
            For the set of open intervals
            $(p,q)$, where $p$ and $q$ are rational
            numbers, forms a basis for the standard
            topology on $\mathbb{R}$. Moreover, this
            is countable.
        \end{proof}
        \begin{definition}
            If $(X,\tau)$ is a topological space
            and $S\subset\tau$, then $S$ is a subbase
            if a finite intersection of elements of $S$
            forms a base of $\tau$.
        \end{definition}
        \begin{definition}
            A local base for a point
            $x$ in a topological space $(X,\tau)$
            is a set of open neighborhods $B_{x}$ of
            $x$ such that for all open neighborhoods $G$
            of $x$, there is a $G_{x}\in{B_{x}}$ such that
            $x\in{G_{x}}\subset{G}$.
        \end{definition}
        \begin{theorem}
            If $(X,\tau)$ is a topological space, $x\in{X}$,
            and if $B$ is a base for $\tau$, then
            the set of elements $G_{x}$ in $B$ such that
            $x\in{G_{x}}$ is a local base for $x$.
        \end{theorem}
\end{document}