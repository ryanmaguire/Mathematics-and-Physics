\documentclass[crop=false,class=book,oneside]{standalone}
%----------------------------Preamble-------------------------------%
\input{../../../preamble.tex}
\graphicspath{{../../../images/}}       % Path to Image Folder.
%----------------------------GLOSSARY-------------------------------%
\makeglossaries
\loadglsentries{../../../glossary}
\loadglsentries{../../../acronym}
%--------------------------Main Document----------------------------%
\begin{document}
\chapter{Mathematical Physics I}
\section{Cheat Sheet}
\subsection{Series}
\begin{theorem*}[The Comparison Test]
If $a_n$ and $b_n$ are sequences of non-negative real numbers, if there is an $N_0\in \mathbb{N}$ such that for all $n>N_0$, $a_n\leq b_n$, and if $\sum_{n=0}^{N}b_n$ converges, then $\sum_{n=0}^{N}a_n$ converges.
\end{theorem*}
\begin{proof}
Let $B_N=\sum_{n=0}^{N}b_n$, $S = \sum_{n=0}^{N_{0}-1}a_n$ and $B=\lim_{n\rightarrow \infty}B_{n}$. As $b_n$ is non-negative, for all $N\in \mathbb{N}$, $B_{N} \leq B$. Then for $N>N_{0}$, $\sum_{n=0}^{N}a_n = S+\sum_{n=N_{0}}^{N}a_n \leq S+\sum_{n=N_{0}}^{N}b_n \leq S+B_{N} \leq S+B$. That is, $\sum_{n=0}^{N}a_n$ bounded by $S+B$. And as $a_n$ is non-negative, $\sum_{n=0}^{N}a_n$ increasing monotonically, and is thus a monotonically increases sequence that is bounded above, and therefore converges.
\end{proof}
\begin{theorem*}[Generalized Geometric Series Theorem]
If $r$ is a real number, then $\sum_{n=0}^{N} r^n = \frac{1-r^{N+1}}{1-r}$
\end{theorem*}
\begin{proof}
By induction. The base case is $1+r = (1+r)\frac{1-r}{1-r} = \frac{1-r^2}{1-r}$. Suppose it is true for $N \in \mathbb{N}$. Then $\sum_{n=0}^{N+1}r^n=\sum_{n=0}^{N}r^n+r^{N+1}= \frac{1-r^{N+1}}{1-r}+r^{N+1}=\frac{1-r^{N+1}+r^{N+1}(1-r)}{1-r}=\frac{1-r^{N+2}}{1-r}$.
\end{proof}
\begin{theorem}[Geometric Series Theorem]
If $|r|<1$, then $\sum_{n=0}^{\infty} r^n = \frac{1}{1-r}$
\end{theorem}
\begin{proof}
For $\sum_{n=0}^{N}r^{n}=\frac{1-r^{N+1}}{1-r}$. As $|r|<1$, $\lim_{N\rightarrow \infty}r^{N}=0$. Therefore $\sum_{n=0}^{\infty}=\frac{1}{1-r}$
\end{proof}
\begin{theorem*}[First Root Test Theorem]
If $a_n$ is positive and $\lim_{n\rightarrow \infty} \sqrt[n]{a_n}<1$, then $\sum_{n=0}^{\infty} a_n$ converges.
\end{theorem*}
\begin{proof}
If $\lim_{n\rightarrow \infty}\sqrt[n]{a_n}<1$, then there is a $c\in (0,1)$ and an $N\in \mathbb{N}$ such that for all $n>N$, $\sqrt[n]{a_n}<c$. But then $a_n<c^n<1$, so $\sum_{n=0}^{\infty}c^n = \frac{1}{1-c}$. Thus, by the comparison test, $\sum_{n=0}^{N}a_n$ converges.
\end{proof}
\begin{theorem*}[Second Root Test Theorem]
If $a_n$ is positive and $\lim_{n\rightarrow \infty} \sqrt[n]{a_n}>1$, then $\sum_{n=0}^{\infty} a_n$ diverges.
\end{theorem*}
\begin{proof}
If $\lim_{n\rightarrow \infty}\sqrt[n]{a_n}>1$, then there is a $c\in (1,\infty)$ and an $N\in \mathbb{N}$ such that for all $n>N$, $\sqrt[n]{a_n}>c$. But then $a_n>c^n>1$, so $\sum_{n=0}^{\infty}c^n$ diverges. By the comparison test, $\sum_{n=0}^{N}a_n$ diverges.
\end{proof}
\begin{theorem*}
There exists sequences of positive real numbers $a_n$ such that $\sqrt[n]{a_n} \rightarrow 1$ and $\sum_{n=1}^{N} a_n$ diverges.
\end{theorem*}
\begin{proof}
Let $a_n = \frac{1}{n}$. Then $\ln(\sqrt[n]{a_n}) = \frac{\ln(\frac{1}{n})}{n} = -\frac{\ln(n)}{n} \rightarrow 0$. So $\sqrt[n]{a_n} \rightarrow 1$. But $\sum_{n=1}^{N} \frac{1}{n}$ diverges.
\end{proof}
\begin{theorem*}
There exists positive sequences $a_{n}$ such that $\sqrt[n]{a_n} \rightarrow 1$ and $\sum_{n=1}^{N}a_n$ converges.
\end{theorem*}
\begin{proof}
Let $a_{n}=\frac{1}{n^2}$. Then $\ln(\sqrt[n]{a_n})=-2\frac{\ln(n)}{n}\rightarrow 0$. Therefore $\sqrt[n]{a_n}\rightarrow 1$. But $\sum_{n=1}^{N}\frac{1}{n^2}$ converges.
\end{proof}
\begin{theorem*}
If $a_n$ is a sequences of positive real numbers, and if $a(x)$ is a monotonically decreasing function such that for all $n\in \mathbb{N}$, $a(n) = a_n$, then $\sum_{n=1}^{\infty} a_n$ converges if and only if $\int_{1}^{\infty} a(x)dx$ converges.
\end{theorem*}
\begin{proof}
As $a(x)$ is decreasing, for all $n\in \mathbb{N}$ and for all $x\in [n,n+1]$, $a_{n+1} \leq a(x) \leq a_{n}$. Therefore $\int_{n}^{n+1}a_{n+1}dx\leq\int_{n}^{n+1}a(x)dx\leq \int_{n}^{n+1} a_{n}dn\Rightarrow a_{n+1}\leq\int_{n}^{n+1}a(x)dx\leq a_{n}$. But for all $N\in \mathbb{N}$, $\sum_{n=1}^{N}\int_{n}^{n+1}a(x)dx=\int_{n=1}^{N}a(x)dx$, and thus $\sum_{n=1}^{N}a_{n+1}\leq\int_{n=1}^{N}a(x)dx\leq\sum_{n=1}^{N}a_n$. Let $S_{N}=\sum_{n=1}^{N}a_{n}$. Then $S_{N+1}-a_{1}\leq\int_{n=1}^{N}a(x)dx\leq S_{N}$. If $\int_{n=1}^{N}a(x)dx$ converges, say to $I$, then $S_{N+1} \leq I+a_{1}$. But $S_{N+1}$ is a monotonically increasing sequence that is bounded, and therefore converges. That is, if $\int_{n=1}^{N}a(x)dx$ converges, then $\sum_{n=1}^{N}a_n$ converges. If $\sum_{n=1}^{N} a_n$ converges, say to $S$, then $\int_{n=1}^{N}a(x)dx \leq S$. But $a(x)$ is monotonically decreasing and positive, and thus $I_{N} = \int_{n=1}^{N}a(x)dx$ is a bounded monotonically increasing sequence, and therefore converges. That is, if $\sum_{n=1}^{N}a_n$ converges, then $\int_{1}^{N}a(x)dx$ converges. 
\end{proof}
\begin{theorem*}
If $a_n$ is a positive, monotonically decreasing, and if $a_n \rightarrow 0$, then $\sum_{n=1}^{\infty} (-1)^{n}a_n$ converges.
\end{theorem*}
\begin{proof}
Let $S_N = \sum_{n=1}^{N}(-1)^n a_n$. If $N$ is even, then $S_N = \sum_{n=1}^{N/2}(a_{2n}-a_{2n-1})$. But $a_n$ is decreasing monotonically, so $a_{N+2} - a_{N+1} \leq 0$. That is, if $N$ is even then $S_{N} \geq S_{N+2}$. So $S_{2k}$ is a monotonically decreasing subsequence of $S_{N}$. Moreover, $S_{2k} \geq a_{2} - a_{1}$. Thus, $S_{2k}$ converges, say to $S_1$. If $N$ is odd, then $S_{N} = -a_{N}+\sum_{n=1}^{(N-1)/2}(a_{2n} - a_{2n-1})$. But then $S_{N+2} = -(a_{N+2}-a_{N+1})+S_{N} \geq S_{N}$. That is, $S_{2k-1}$ is a monotonically increasing subsequence. Moreover, $S_{2k-1} \leq a_{2}$. So, $S_{2k-1}$ converges, say to $S_2$. Let $\epsilon>0$ be given. As $a_n \rightarrow 0$, there is an $N_{0}\in \mathbb{N}$ such that for all $n \geq N_{0}$, $a_{n} < \frac{\epsilon}{2}$. There is also an $N_{1}$ such that for $n>N_{1}$, $|S_1 - S_{2n}|<\frac{\varepsilon}{4}$. Finally there is an $N_2$ such that for $n>N_{2}$, $|S_2 - S_{2n-1}|<\frac{\varepsilon}{4}$. Let $N = \max\{N_0,N_1,N_2\}$. Then for $n>N$, we have $|S_1 - S_2| \leq |S_1 - S_{2m}|+|S_2 - S_{2m-1}| + |S_{2n} - S_{2n-1}|< \frac{\varepsilon}{4}+\frac{\varepsilon}{4} + |a_{2n}|<\epsilon$. But $S_1$ and $S_2$ are real numbers, and $\varepsilon$ is arbitrary, so $S_1 = S_2$. Therefore $S_N \rightarrow S_1$.
\end{proof}
\begin{theorem*}
There exists positive sequences $a_n$ such that $a_n \rightarrow 0$ and $\sum_{n=1}^{N}(-1)^n a_n$ diverges.
\end{theorem*}
\begin{proof}
For let $a_{n}=\frac{1}{n}$ for odd $n$ and $a_{n}=\frac{1}{n^{2}}$ for even $n$ and let $S_N = \sum_{n=1}^{N}(-1)^{n}a_n$. Then $S_{2N} = \sum_{n=1}^{N} (\frac{1}{n^2} - \frac{1}{n}) = \sum_{n=1}^{N}\frac{1}{n^2} - \sum_{n=1}^{N}\frac{1}{n}$. But $\sum_{n=1}^{N} \frac{1}{n^2}$ converges and $\sum_{n=1}^{N}\frac{1}{n}$ diverges, and therefore $S_{2N}$ diverges. But if $S_{2N}$ diverges, then $S_{N}$ diverges.
\end{proof}
\begin{theorem*}
If $a_n$ is a sequence of real numbers and $\sum_{n=1}^{N}|a_n|$ converges, then $\sum_{n=1}^{N}a_n$ converges.
\end{theorem*}
\begin{proof}
So let $T_{N} = \sum_{n=1}^{N}|a_n|$ and let $S_{N} = \sum_{n=1}^{N}a_{n}$. As $T_{N}$ converges, it is a Cauchy Sequence. That is, for all $\varepsilon>0$ there is an $N_0 \in \mathbb{N}$ such that for all $n,m>N_{0}$, $|T_{n} - T_{m}|<\varepsilon$. But then for $n,m>N_{0}$, $|S_{n} - S_{m}| = |\sum_{n=m+1}^{n}a_n| \leq \sum_{n=m+1}^{n}|a_n| = |T_{n} - T_{m}| <\varepsilon$. That is $S_{N}$ forms a Cauchy sequence. But Cauchy sequences converge. Therefore $S_{N}$ converges.
\end{proof}
\subsection{Complex Variables}
\begin{theorem}
If $x\in\mathbb{C}$, then $e^{ix}=\cos(x)+i\sin(x)$
\end{theorem}
\begin{proof}
For $e^{ix}$ is the solution to $y'=iy$, $y(0)=1$. But $\frac{d}{dx}(\cos(x)+i\sin(x))i(\cos(x)+i\sin(x))$. Moreover, $\cos(0)+i\sin(0)=1$. From the uniqueness of solutions, $e^{ix}=\cos(x)+i\sin(x)$. 
\end{proof}
\begin{theorem}
If $x\in \mathbb{C}$, then $\cos(x)=\frac{1}{2}(e^{ix}+e^{-ix})$
\end{theorem}
\begin{proof}
For $\cos(x)=\cos(-x)$ and $\sin(x)=-\sin(-x)$. F rom the previous theorem $e^{ix}+e^{-ix} = 2\cos(x)$.
\end{proof}
\begin{definition}
The hyperbolic cosine of $x\in \mathbb{R}$ is $\cosh(x)=\cos(ix)$.
\end{definition}
\begin{theorem}
If $x\in\mathbb{R}$, then $\cosh(x)=\frac{e^{ix}+e^{-ix}}{2}$.
\end{theorem}
\begin{proof}
Apply the previous theorem to $ix$.
\end{proof}
\begin{theorem}
If $x\in\mathbb{C}$, then $\sin(x)=\frac{1}{2i}(e^{i\theta}-e^{i\theta})$
\end{theorem}
\begin{proof}
For $e^{ix} = \cos(x)+i\sin(x)$, and thus $e^{ix}-e^{-ix}=2i\sin(x)$
\end{proof}
\begin{definition}
The hyperbolic sine of $x\in\mathbb{R}$ is $\sinh(x)=-i\sin(ix)$
\end{definition}
\begin{theorem}
If $x\in\mathbb{R}$, then $\sinh(x)=\frac{e^{x}-e^{-x}}{2}$
\end{theorem}
\begin{proof}
Apply the previous theorem to $ix$ and multiply by $-i$.
\end{proof}
\begin{definition}
A complex function that is differentiable at a point $z_{0}\in\mathbb{C}$ is a function $f:\mathbb{C}\rightarrow\mathbb{C}$ such that $\underset{z\rightarrow z_{0}}{\lim}\frac{f(z)-f(z_{0})}{z-z_{0}}$
\end{definition}
\begin{definition}
A differentiable complex function is a function $f:\mathbb{C}\rightarrow\mathbb{C}$ that is differentiable for all $z\in\mathbb{C}$.
\end{definition}
\begin{theorem}[The Cauchy-Riemann Theorem]
A function $f:\mathbb{C}\rightarrow\mathbb{C}$, $f(z)=u(x,y)+iv(x,y)$, where $u,v:\mathbb{R}^{2}\rightarrow \mathbb{R}$, is differentiable if and only if $\frac{\partial u}{\partial x}=\frac{\partial v}{\partial y}$ and $\frac{\partial u}{\partial y} = 0\frac{\partial v}{\partial x}$.
\end{theorem}
\begin{theorem}
If $z\in\mathbb{R}$, then there is a unique $r>0$ and $\theta\in[0,2\pi)$ such that $z=re^{i\theta}$.
\end{theorem}
\begin{theorem}[Cauchy's Integral Theorem]
If $\mathcal{U}\subset\mathbb{C}$ is simply connected, if $f:\mathcal{U}\rightarrow \mathbb{C}$ is differentiable, and if $\gamma:I\rightarrow \mathcal{U}$ is a closed path of finite measure (Length), then $\oint_{\gamma}f(z)dz = 0$.
\end{theorem}
\begin{theorem}[Cauchy's Integral Formula]
If $\mathcal{U}\subset\mathbb{C}$ is open, $z_{0}\in\mathcal{U}$, and if $B_{r}(z_{0})\subset\mathcal{U}$, then for all $a\in B_{r}(z_{0})$, $\oint_{\partial B_{r}(z_{0})}\frac{f(z)}{z-a}dz = f(a)$
\end{theorem}
\subsection{Matrices}
\begin{definition}
The transpose of a matrix $A = (a_{ij})$ is $A^{T} = (a_{ji})$.
\end{definition}
\begin{definition}
The complex transpose of a matrix $A = (a_{ij})$ is $A^{\dagger} = (\overline{a_{ji}})$.
\end{definition}
\begin{definition}
An orthogonal matrix is a matrix $A$ such that $A^T = A^{-1}$.
\end{definition}
\begin{definition}
A Hermitian Matrix is a matrix $A$ such that $A^{\dagger} = A$.
\end{definition}
\begin{definition}
A unitary matrix is a matrix $A$ such that $A^{\dagger} = A^{-1}$.
\end{definition}
\begin{definition}
A singular matrix is a matrix with no inverse.
\end{definition}
\begin{definition}
The Commutator of two matrices $A$ and $B$ is $[A,B] = AB - BA$.
\end{definition}
\begin{definition}
A normal matrix is a matrix $A$ such that $[A,A^{\dagger}] = 0$.
\end{definition}
\begin{definition}
The Levi-Civita symbol is defined as:
\begin{equation*}
    \varepsilon_{ijk} = \begin{cases} 1, & ijk\textrm{ is an even permutation} \\ 0, & i=j\textrm{ OR }i=k\textrm{ OR }j=k\\ -1, & ijk\textrm{ is an odd permutation}\end{cases}
\end{equation*}
\end{definition}
\begin{definition}
The Matrix Representation of $a+ib$ is the matrix $\begin{bmatrix}a & -b \\ b & a \end{bmatrix}$
\end{definition}
\begin{theorem}
$\mathbb{C}$, with it's usual arithmetic, is homomorphic to $\mathbb{R}^{2\times 2}$, with it's usual arithmetic.
\end{theorem}
\begin{proof}
For let $f:\mathbb{C}\rightarrow\mathbb{R}^{2\times 2}$ be defined by $f(z) = f(a+ib) = \begin{bmatrix}a & -b\\b & a\end{bmatrix}$
\begin{align*}
    f(z+w)&=\begin{bmatrix}a+b & -(b+d)\\b+d& a+c\end{bmatrix}=\begin{bmatrix}a&-b\\b&a\end{bmatrix}+\begin{bmatrix}c&-d\\d&c\end{bmatrix}=f(z)+f(w)\\
    f(z\cdot w)&=\begin{bmatrix}ac-bd&-(ad+bc)\\ad+bc&ac-bd\end{bmatrix}=\begin{bmatrix}a&-b\\b&a\end{bmatrix}\begin{bmatrix}c&-d\\d&c\end{bmatrix}=f(z)\cdot f(w)
\end{align*}
\end{proof}
\begin{definition}
The $ij$ minor of an $n\times m$ matrix $A$ is the matrix $M_{ij}=\{(k,\ell,a_{k\ell}),k\ne i,j\ne\ell\}$. That is, it is the matrix formed by removing the $i^{th}$ row and $j^{th}$ column from $A$.
\end{definition}
\begin{definition}
The cofactor matrix of an $n\times n$ matrix $A$ is the matrix $C=(C_{ij})$, where $C_{ij}=(-1)^{i+j}M_{ij}$.
\end{definition}
\begin{theorem}
If $A$ is an $n\times n$ matrix, and $\det(A) \ne 0$, then $A^{-1}= \frac{1}{\det(A)}C^{T}$.
\end{theorem}
\subsection{Vectors}
\begin{theorem}
If $\mathbf{A},\mathbf{B},\mathbf{C}\in \mathbb{R}^{3}$, then $\langle \mathbf{A},\mathbf{B}\times \mathbf{C}\rangle = \langle \mathbf{B}, \mathbf{C}\times \mathbf{A}\rangle$.
\end{theorem}
\begin{proof}
For $\mathbf{B}\times \mathbf{C} = (B_{y}C_{z} - B_{z}C_{y},B_{z}C_{x}-B_{x}C_{z},B_{x}C_{y} - B_{y}C_{x})$, and thus: \begin{align*}
    \langle \mathbf{A},\mathbf{B}\times \mathbf{C}\rangle &= A_{x}(B_{y}C_{z} - B_{z}C_{y}) + A_{y}(B_{z}C_{x} - B_{x}C_{z}) + A_{z}(B_{x}C_{y}-B_{y}C_{x})\\
    &= A_{x}B_{y}C_{z} - A_{x}B_{z}C_{y} + A_{y}B_{z}C_{x} - A_{y}B_{x}C_{z} + A_{z}B_{x}C_{y} - A_{z}B_{y}C_{x}\\
    &= B_{x}(A_{z}C_{y} - A_{y}C_{z}) + B_{y}(A_{x}C_{z} - A_{z}C_{x}) + B_{z}(A_{y}C_{x} - A_{x}C_{y})\\
    &= B_{x}(C_{y}A_{z} - C_{z}A_{y}) + B_{y}(C_{z}A_{x} - C_{x}A_{z}) + B_{z}(C_{x}A_{y} - C_{y}A_{x})\\
    &= \langle \mathbf{B}, (C_{y}A_{z} - C_{z}A_{y}, C_{z}A_{x} - C_{x}A_{z}, C_{x}A_{y} - C_{y}A_{x})\rangle\\
    &= \langle \mathbf{B},\mathbf{C}\times \mathbf{A}\rangle
\end{align*}
\end{proof}
\begin{theorem}
If $\mathbf{A},\mathbf{B},\mathbf{C}\in \mathbb{R}^3$, then $ \mathbf{A}\times(\mathbf{B}\times \mathbf{C}) = \langle \mathbf{A},\mathbf{C}\rangle \mathbf{B} - \langle \mathbf{A},\mathbf{B}\rangle \mathbf{C}$
\end{theorem}
\begin{proof}
For $\mathbf{B}\times \mathbf{C} = (B_{y}C_{z} - B_{z}C_{y},B_{z}C_{x}-B_{x}C_{z},B_{x}C_{y} - B_{y}C_{x})$, and thus:
\begin{align*}
    \mathbf{A}\times(\mathbf{B}\times\mathbf{C})\phantom{+}&=\phantom{+-}\mathbf{A}\times(B_{y}C_{z}-B_{z}C_{y},B_{z}C_{x}-B_{x}C_{z},B_{x}C_{y}-B_{y}C_{x})\\
    &=\phantom{+-}\big(A_{y}(B_{x}C_{y}-B_{y}C_{x})-A_{z}(B_{z}C_{x}-B_{x}C_{z})\big)\hat{\mathbf{x}}\\
    &\phantom{=-}+\big(A_{z}(B_{y}C_{z}-B_{z}C_{y})-A_{x}(B_{x}C_{y}-B_{y}C_{x})\big)\hat{\mathbf{y}}\\
    &\phantom{=-}+\big(A_{x}(B_{z}C_{x}-B_{x}C_{z})-A_{y}(B_{y}C_{z}-B_{z}C_{y})\big)\hat{\mathbf{z}}\\
    &=\phantom{+-}\big(B_{x}(A_{x}C_{x}+A_{y}C_{y}+A_{z}C_{z})-C_{x}(A_{x}B_{x}+A_{y}B_{y}+A_{z}B_{z})\big)\hat{\mathbf{x}}\\
    &\phantom{=-}+\big(B_{y}(A_{x}C_{x}+A_{y}C_{y}+A_{z}C_{z})-C_{y}(A_{x}B_{x}+A_{y}B_{y}+A_{z}B_{z})\big)\hat{\mathbf{y}}\\
    &\phantom{=-}+\big(B_{z}(A_{x}C_{x}+A_{y}C_{y}+A_{z}C_{z})-C_{z}(A_{x}B_{x}+A_{y}B_{y}+A_{z}B_{z})\big)\hat{\mathbf{z}}\\
    &=\phantom{+-}(A_{x}C_{x}+A_{y}C_{y}+A_{z}C+{z})(B_{x},B_{y},B_{z})\\
    &\phantom{=+}-(A_{x}B_{x}+A_{y}B_{y}+A_{z}B_{z})(C_{x},C_{y},C_{z})\\
    &=\phantom{+-}\langle\mathbf{A},\mathbf{C}\rangle\mathbf{B}-\langle \mathbf{A},\mathbf{C}\rangle\mathbf{C}
\end{align*}
\end{proof}
\begin{theorem}[Divergence Theorem]
If $\Omega$ is a closed bounded subset of $\mathbb{R}^{n}$ with a smooth boundary $\partial \Omega$, and if $\mathbf{A}$ is a smooth vector field, then $\iiint_{\Omega} (\nabla \cdot \mathbf{A})d\tau = \oiint_{\partial \Omega}\mathbf{A}\cdot \boldsymbol{d\sigma}$ 
\end{theorem}
\begin{theorem}[Stokes' Theorem]
If $\Sigma$ is a compact, simply connected subset of $\mathbb{R}^{3}$ bounded by a smooth Jordan Curve $\partial \Sigma$, and if $\mathbf{A}$ is a smooth vector field, then $\iint_{\Sigma}(\nabla\times \mathbf{A})\cdot \boldsymbol{d\sigma} = \oint_{\partial \Sigma}\mathbf{A}\cdot \boldsymbol{d\ell}$
\end{theorem}
\begin{theorem}
If $\phi:\mathbb{R}^{3}\rightarrow \mathbb{R}$ is a smooth function, then $\nabla \times \nabla(\phi) = \boldsymbol{0}$.
\end{theorem}
\begin{proof}
For $\nabla(\phi) = \frac{\partial \phi}{\partial x}\hat{\mathbf{x}}+\frac{\partial \phi}{\partial y}\hat{\mathbf{y}}+\frac{\partial \phi}{\partial z}\hat{\mathbf{z}}$, and therefore:
\begin{equation*}
    \nabla \times \nabla(\phi) = \bigg(\frac{\partial^{2} \phi}{\partial y \partial z} - \frac{\partial^{2}\phi}{\partial z \partial y}\bigg)\hat{\mathbf{x}}+\bigg(\frac{\partial^{2} \phi}{\partial z \partial x} - \frac{\partial^{2}\phi}{\partial x \partial z}\bigg)\hat{\mathbf{y}}+\bigg(\frac{\partial^{2} \phi}{\partial x \partial y} - \frac{\partial^{2}\phi}{\partial y \partial x}\bigg)\hat{\mathbf{z}}
\end{equation*}
But, as $\phi$ is smooth, $\frac{\partial^{2}\phi}{\partial x_{i}\partial x_{j}} = \frac{\partial^{2}\phi}{\partial x_{j}\partial x_{i}}$. Therefore, $\nabla \times \nabla(\phi) = \boldsymbol{0}$.
\end{proof}
\begin{theorem}
If $\mathbf{A}:\mathbb{R}^{3}\rightarrow \mathbb{R}^{3}$ is smooth, then $\nabla\cdot(\nabla \times \mathbf{A}) = 0$.
\end{theorem}
\begin{proof}
For $\nabla \times \mathbf{A} = (\frac{\partial A_{z}}{\partial y} - \frac{\partial A_{y}}{\partial z})\hat{\mathbf{x}}+(\frac{\partial A_{x}}{\partial z} - \frac{\partial A_{z}}{\partial x})\hat{\mathbf{y}}+(\frac{\partial A_{y}}{\partial x} - \frac{\partial A_{x}}{\partial y})\hat{\mathbf{z}}$, and thus:
\begin{align*}
    \nabla \cdot (\nabla \times \mathbf{A}) &=\nabla \cdot \bigg(\big(\frac{\partial A_{z}}{\partial y} - \frac{\partial A_{y}}{\partial z}\big)\hat{\mathbf{x}}+\big(\frac{\partial A_{x}}{\partial z} - \frac{\partial A_{z}}{\partial x}\big)\hat{\mathbf{y}}+\big(\frac{\partial A_{y}}{\partial x} - \frac{\partial A_{x}}{\partial y}\big)\hat{\mathbf{z}}\bigg)\\
    &=\bigg(\frac{\partial^{2}A_{z}}{\partial x \partial y} - \frac{\partial^{2}A_{y}}{\partial x \partial z}\bigg)+\bigg(\frac{\partial^{2}A_{x}}{\partial y \partial z} - \frac{\partial^{2}A_{z}}{\partial y \partial x}\bigg) + \bigg(\frac{\partial^{2} A_{y}}{\partial z \partial x} - \frac{\partial^{2} A_{x}}{\partial z \partial y}\bigg)\\
    &= \bigg(\frac{\partial^{2} A_{z}}{\partial x \partial y} - \frac{\partial^{2} A_{z}}{\partial y \partial x}\bigg) + \bigg(\frac{\partial^{2} A_{y}}{\partial z \partial x} - \frac{\partial^{2} A_{y}}{\partial x \partial z}\bigg) + \bigg(\frac{\partial^{2}A_{x}}{\partial y \partial z} - \frac{\partial^{2} A_{x}}{\partial z \partial y}\bigg)
\end{align*}
But $A_{x},A_{y}$, and $A_{z}$ are smooth, and thus $\frac{\partial^{2} A_{k}}{\partial x_{i} \partial x_{j}} = \frac{\partial^{2} A_{k}}{\partial x_{j} \partial x_{i}}$. Therefore, $\nabla \cdot (\nabla \times \mathbf{A}) = \boldsymbol{0}$.
\end{proof}
\end{document}