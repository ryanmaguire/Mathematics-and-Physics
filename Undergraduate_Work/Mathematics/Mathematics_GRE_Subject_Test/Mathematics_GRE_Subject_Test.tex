\documentclass[crop=false,class=book,oneside]{standalone}
%----------------------------Preamble-------------------------------%
\input{../../preamble.tex}
%----------------------------GLOSSARY-------------------------------%
\makeglossaries
\loadglsentries{../../glossary}
\loadglsentries{../../acronym}
%--------------------------Main Document----------------------------%
\begin{document}
    \ifx\ifundergraduatemathematicswork\undefined
        \pagenumbering{gobble}
        \title{GRE Mathematics Subject Test}
        \author{Ryan Maguire}
        \date{\vspace{-5ex}}
        \maketitle
        \tableofcontents
        \clearpage
        \pagenumbering{arabic}
        \chapter*{GRE Mathematics Subject Test}
        \addcontentsline{toc}{chapter}{GRE Mathematics Subject Test}
            \markboth{}{GRE MATHEMATICS SUBJECT TEST}
        \setcounter{chapter}{1}
    \else
        \chapter{GRE Mathematics Subject Test}
    \fi
    \section{Theorems and Definitions for the GRE}
        \begin{definition*}
                The tangent line of a differentiable function
                $y:\mathbb{R}\rightarrow\mathbb{R}$ at a point
                $x_{0}\in\mathbb{R}$ is the function
                $y_{T}:\mathbb{R}\rightarrow\mathbb{R}$ defined by
                $y_{T}(x)=y'(x_0)(x-x_0)+y(x_0)$ 
            \end{definition*}
        \begin{definition*}
            If $\Gamma(t)=\big(x(t),y(t)\big)$, for $a\leq t\leq b$,
            and $\Gamma'(t)=\big(x'(t),y'(t)\big)$ exists for
            $a<t<b$, then the length of $\Gamma$ from $a$ to $b$ is:
            \begin{equation}
                L=\int_{a}^{b}\sqrt{
                    \bigg(\frac{dx}{dt}\bigg)^{2}+
                    \bigg(\frac{dy}{dt}\bigg)^{2}
                }dt
            \end{equation}
        \end{definition*}
        \begin{definition*}
            The dimension of a vector space is the cardinality of
            any basis of the space. 
        \end{definition*}
        \begin{remark*}
            By the Dimension Theorem, all bases of a vector space
            have the same cardinality.
        \end{remark*}
        \begin{definition*}
            The absolute value of $x$ is
            $|x|=\begin{cases}%
                x,&x\geq 0\\ 
                -x,&x<0
            \end{cases}$
        \end{definition*}
        \begin{theorem*}[Mean Value Theorem]
            If $f:(a,b)\rightarrow\mathbb{R}$ is continuous and
            bounded, and if $x\in(a,b)$, then there is a $c\in(a,x)$
            such that $\int_{a}^{x}f=(x-a)f(c)$.
        \end{theorem*}
        \begin{theorem*}
            [Generalized Fundamental Theorem of Calculus]
            If $\mathcal{U}$ is an open non-empty subset of
            $\mathbb{R}$, $a\in\mathcal{U}$, and if
            $f:\mathcal{U}\rightarrow\mathbb{R}$
            is bounded and continuous, then
            $F:\mathcal{U}\rightarrow\mathbb{R}$
            defined by $F(x)=\int_{\mathcal{U}\cap (a,x)}f$ is
            differentiable and $F'(x)=f(x)$
        \end{theorem*}
        \begin{proof}
            For let $x\in\mathcal{U}$. Let
            $\{x_n\}_{n=1}^{\infty}\subset\mathcal{U}$
            be a sequence such that $x_{n}\rightarrow x$,
            $x\notin\{x_{n}\}_{n=1}^{\infty}$.
            As $\mathcal{U}$ is open and $x\in\mathcal{U}$,
            there is an $\varepsilon>0$ such that
            $B_{\varepsilon}(x)\subset\mathcal{U}$. But, as
            $x_{n}\rightarrow x$, there is an $N\in \mathbb{N}$ such
            that for all $n>N$, $x_{n}\in B_{\varepsilon}(x)$.
            But then for all $n>N$:
            \begin{equation*}
                \int_{\mathcal{U}\cap(a,x)}f-%
                \int_{\mathcal{U}\cap(a,x_{n})}f=%
                \int_{x_{n}}^{x}f
            \end{equation*}
            But, as $f$ is continuous, by the mean value theorem for
            all $n>N$ there is a $c_{n}\in(x_n,x)$ such that
            $\int_{x_{n}}^{x}f=(x-x_{n})f(c_{n})$. But then 
            \begin{equation*}
                \Big|\frac{\int_{x_{n}}^{x}f}{x-x_{n}}-f(x)\Big|
                =|f(c_{n})-f(x)|
            \end{equation*}
            But $c_{n}\in(x_{n},x)$, and $x_{n}\rightarrow x$, and
            therefore $c_{n} \rightarrow x$. But $f$ is continuous,
            and therefore $f(c_{n})\rightarrow f(x)$. Therefore, by
            the definition of the derivative of $F$ at $x$,
            $F'(x)=f(x)$. 
        \end{proof}
        \begin{theorem*}
            If $V$ is a vector space and $A,B\subset V$ are
            subspaces, then $A\cap B$ is a subspace and
            $\dim(A\cap B)\leq\min\{\dim(A),\dim(B)\}$
        \end{theorem*}
        \begin{theorem*}
            If $f:\mathbb{R}\rightarrow \mathbb{R}$
            is differentiable
            and $f'(x)>0$ for all $x$,
            then $f$ is strictly increasing.
        \end{theorem*}
        \begin{theorem*}
            If $f:(a,b)\rightarrow\mathbb{R}$ is continuous and
            $f(a)<0<f(b)$, then there is a $c\in (a,b)$ such that
            $f(c)=0$.
        \end{theorem*}
        \begin{theorem*}
            If $f$ is integrable on $(a,b)$, and if $c\in(a,b)$, then
            $\int_{a}^{b}f=\int_{a}^{c}f+\int_{c}^{b}f$
        \end{theorem*}
    \section{GR0568}
        \begin{problem}
            What is the length of the curve with
            parametric equations
            $x=\cos(t)$, $y=\sin(t)$, for $0\leq t\leq\pi$.
        \end{problem}
        \begin{proof}[Solution]
            We have $x(t)=\cos(t)$ and $y(t)=\sin(t)$, so
            $\Gamma(t)=\big(\cos(t),\sin(t)\big)$. This is the
            parameterization of the unit circle for
            $0\leq t\leq 2\pi$. Knowing this we see that the length
            of $\Gamma$ from $0$ to $\pi$ is simply half the
            circumference of the unit circle, which is $\pi$. We can
            also use the definition of length to obtain:
            \begin{align*}
                x(t)&=\cos(t)&
                \Big(\frac{dx}{dt}\Big)^{2}+
                \Big(\frac{dy}{dt}\Big)^{2}
                &=\sin^{2}(t)+\cos^{2}(t)\\                
                y(t)&=\sin(t)
                &\sin^{2}(t)+\cos^{2}(t)&=1\\
                \frac{dx}{dt}&=-\sin(t)
                &\int_{0}^{\pi}\sqrt{\Big(\frac{dx}{dt}\Big)^{2}+
                \Big(\frac{dy}{dt}\Big)^{2}}dt
                &=\int_{0}^{\pi}dt\\
                \frac{dy}{dt}&=\cos(t)&
                \int_{0}^{\pi}dt&=\pi
            \end{align*}
        \end{proof}
        \begin{problem}
            What is the equation of the line tangent to $y=x+e^x$
            at $x=0$?
        \end{problem}
        \begin{proof}[Solution]
            The tangent line of $y$ at $x_{0}$
            is a line that touches
            and lies tangent to $y$ at the point $x_{0}$. Using the
            definition we have:
            \begin{align*}
                y(x)&=x+e^{x}\Rightarrow y(0)=1\\
                y'(x)&=1+e^{x}\Rightarrow y'(0)=2\\
                \Rightarrow y_{T}(x)&=2(x-0)+1\\
                &=2x+1
            \end{align*}
        \end{proof}
        \begin{problem}
            If $V$ and $W$ are $2-$dimensional subspaces in
            $\mathbb{R}^{4}$, what are the possible dimensions of
            $V\cap W$.
        \end{problem}
        \begin{proof}[Solution]
            If $V$ and $W$ are subspaces, then ${V}\cap{W}$ is
            subspace, and
            $\dim({V}\cap{W}\leq\min(\{\dim(V),\dim(W)\})$
            we have in our problem that $\dim\{V\cap W\}\leq 2$. We
            now must show that $V\cap W$ can have dimensions 0,1, or
            2. If $V=\{(x,y,0,0):x,y\in\mathbb{R}\}$ and
            $W=\{(0,0,z,w):z,w\in \mathbb{R}\}$, then
            ${V}\cap{W}=\{(0,0,0,0)\}$ which has dimension $0$.
            If $V=\{(x,y,0,0):x,y\in\mathbb{R}\}$ and
            $W=\{(0,y,z,0):y,z\in\mathbb{R}\}$,
            then ${V}\cap{W}=\{(0,y,0,0):y\in\mathbb{R}\}$
            which has dimension $1$. Finally, if $V=W$ then
            ${V}\cap{W}=V$, which has dimension $2$. So, the only
            possibilities are $0,1$, or $2$.
        \end{proof}
        \begin{problem}
            Let $k$ be the number of solutions of $e^{x}+x-2=0$ in
            the interval $[0,1]$ and let $n$ be the number of
            solutions not in $[0,1]$ What are the values of $k$
            and $n$?
        \end{problem}
        \begin{proof}[Solution]
            If $y=e^{x}+x-2$, then $y'(x)=e^{x}+1>1>0$, for all
            $x\in \mathbb{R}$. Therefore $y'(x)>0$ for all $x$, and
            thus $y$ is strictly increasing. Recall that
            $e\approx 2.71$. We have:
            \begin{align*}
                y(0)&=e^{0}+0-2=1-2=-1<0\\
                y(1)&=e^{1}+1-2=e-1\approx 1.71>0.
            \end{align*}
            So $y(0)<0<y(1)$, and therefore there is a $c\in (0,1)$
            such that $y(c)=0$. But $y$ is strictly increasing, and
            therefore for all $x>c$ we have $y(x)>y(c)=0$, and for
            all $x<c$ we have $y(x)<y(c)=0$. Thus $c$ is the only
            solution. The answer is $k=1$ and $n=0$.
        \end{proof}
        \begin{problem}
            If $b\in\mathbb{R}$ and $f(x)=3x^{2}+bx+12$ has its
            vertex at $x=2$, what is $f(5)$?
        \end{problem}
        \begin{proof}[Solution]
            If the vertex of $f$ is at $x=2$, then $f'(2)=0$.
            \begin{align*}
                f(x)&=2x^{2}+bx+12&f'(2)&=12+b\\
                f'(x)&=6x+b&12+b&=0\\
                f'(2)&=0&b&=-12
            \end{align*}
            So $f(x)=3x^{2}-12x+12$. Thus
            $f(5)=3\cdot 5^{2}-12\cdot 5+12=75-60+12=15+12=27$.
        \end{proof}
        \begin{problem}
            Which of the following circles has
            the greatest number of
            points of intersections with the parabola $x^{2}=y+4$:
            \begin{enumerate}
                \begin{multicols}{5}
                    \item[A.)] $x^{2}+y^{2}=1$
                    \item[B.)] $x^{2}+y^{2}=2$
                    \item[C.)] $x^{2}+y^{2}=9$
                    \item[D.)] $x^{2}+y^{2}=16$
                    \item[E.)] $x^{2}+y^{2}=25$
                \end{multicols}
            \end{enumerate}
        \end{problem}
        \begin{proof}[Solution]
            We wish to solve the system of equations:
            \begin{align*}
                x^{2}+y^{2}&=r^{2}\\
                x^{2}-y&=4
            \end{align*}
            The solutions are of the form $(\pm \sqrt{r^2-y^2},y)$.
            For $(x,y)$ to be a solution
            the second equation requires
            that $y\geq -4$. Substituting $x$, we have
            $y^{2}+y+4-r^{2}=0$. Using the quadratic formula
            we obtain
            $y=\frac{-1\pm\sqrt{1+4(r^{2}-4)}}{2}$.
            For $y$ to be real
            we need $1+4(r^{2}-4)\geq 0$. Solving for $r^2$,
            we have$r^{2}\geq\frac{15}{4}$.
            Thus there are no solutions for
            $r^{2}<\frac{15}{4}$. If $r^{2}=\frac{15}{4}$,
            then there are two solutions:
            $(\pm\frac{\sqrt{3}}{2},\frac{-1}{2})$. If
            $r^{2}>\frac{15}{4}$, then we are
            also constrained by $y\geq-4$. This means 
            \begin{equation*}
                \frac{-1\pm\sqrt{1+4(r^{2}-4)}}{2}\geq-4
                \Rightarrow -1\pm\sqrt{1+4(r^{2}-4)}\geq-8
                \Rightarrow 7\pm\sqrt{1+4(r^{2}-4)}\geq 0
            \end{equation*}
            For $r^2 \geq \frac{15}{4}$, $7+\sqrt{1+4(r^2-4)}>0$
            is always true. Now we consider $7-\sqrt{1+4(r^2-4)}:$
            \begin{align*}
                7-\sqrt{1+4(r^{2}-4)}&\geq 0
                \Rightarrow 7\geq\sqrt{1+4(r^{2}-4)}\\
                \Rightarrow 49&\geq 1+4(r^{2}-4)\Rightarrow
                48\geq 4(r^{2}-4)\\
                \Rightarrow 12&\geq r^{2}-4\Rightarrow 16\geq r^{2}
            \end{align*}
            Thus if $\frac{15}{4}<r^2<16$, we have two $y$ values, with two $x$ values corresponding to each for a total of $4$ solutions. If $r^2 = 16$, then we have three solutions: $(0,-4)$, $(\pm \sqrt{7},3)$. If $r^2>16$ then there is only one possible value for $y$, with two corresponding $x$ values. Thus, if $r^2>16$ there are only two solutions. The maximum number of solutions is attained when $\frac{15}{4} < r^2 <16$. The only value listed that has this property is $r^2 = 9$. The correct answer is C.
            \end{proof}
            \begin{problem}
                Solve $\int_{-3}^{3}|x+1|dx$
            \end{problem}
            \begin{proof}[Solution]
                Using this, we see that
                $|x+1|=\begin{cases}%
                    x+1,&x\geq -1\\%
                    -(x+1),&x< -1%
                \end{cases}$.
                So from the theorem
                $\int_{-3}^{3}|x+1|dx=%
                -\int_{-3}^{-1}(x+1)dx+\int_{-1}^{3}(x+1)dx$.
                Evaluating this, we get:
                \begin{align*}
                    -\bigg[\frac{x^2}{2}+x\bigg]_{-3}^{-1}
                    +\bigg[\frac{x^2}{2}+x\bigg]_{-1}^{3}
                    &=-\bigg[(\frac{1}{2}-1)-(\frac{9}{2}-3)\bigg]
                    +\bigg[(\frac{9}{2}+3)-(\frac{1}{2}-1)\bigg]\\
                    &=-\bigg[-\frac{1}{2}-\frac{3}{2}\bigg]
                    +\bigg[\frac{15}{2}-(-\frac{1}{2})\bigg]\\
                    &=2+8\\ 
                    &=10
                \end{align*}
            \end{proof}
            \begin{problem}
            What is the greatest area of a triangular region with one vertex at the center of a circle of radius $1$, and the other two on the circle?
            \end{problem}
            \begin{proof}[Solution]
            \begin{theorem*}
            Area is invariant under translation and rotation.
            \end{theorem*}
            We can use this theorem to place the center at $(0,0)$, one of the points at $(1,0)$, and then solve for the third point. The third point lies on the circle $x^2+y^2 = 1$, so we have $y = \sqrt{1-x^2}$. The area of the triangle is $\frac{1}{2}bh$. The base is the distance from $(0,0)$ to $(1,0)$, which is $1$, and the height is $\sqrt{1-x^2}$. So we have $A = \frac{1}{2}\sqrt{1-x^2}$. To find the extremum we take the derivative with respect to $x$ and set it to $0$.
            \begin{align*}
                A&=\frac{1}{2}\sqrt{1-x^{2}}\\
                =\frac{dA}{dx}&=0\\
                \Rightarrow\frac{1}{2}\frac{-x}{\sqrt{1-x^{2}}}&=0\\
                \Rightarrow x&=0
            \end{align*}
            So the extremum occurs when $x=0$. The area is then $\frac{1}{2}\sqrt{1-0^2} = \frac{1}{2}$. The answer is $\frac{1}{2}$.
            \end{proof}
            \begin{problem}
            Order the following equations from least to greatest:
            \begin{align*}
                I&=1\\
                J&=\int_{0}^{1}\sqrt{1-x^{4}}dx\\
                K&=\int_{0}^{1}\sqrt{1+x^{4}}dx\\
                L&=\int_{0}^{1}\sqrt{1-x^{8}}dx
            \end{align*}
            \end{problem}
            \begin{proof}[Solution]
            \begin{theorem*}
            If $f$ and $g$ are continuous on $(a,b)$ and $f>g$, then $\int_{a}^{b}f>\int_{a}^{b}g$
            \end{theorem*}
            All three integrals occur over the interval $(0,1)$. For $0 < x < 1$, we have the following:
            \begin{equation*}
                0<x^{8}<x^{4}<x<1
            \end{equation*}
            Using this, we have have:
            \begin{equation*}
                0<\sqrt{1-x^{4}}<\sqrt{1-x^{8}}<1<\sqrt{1+x^{4}}
            \end{equation*}
            Using the theorem, we have:
            \begin{equation*}
                0<\int_{0}^{1}\sqrt{1-x^4}dx<\int_{0}^{1}\sqrt{1-x^8}dx<1<\int_{0}^{1}\sqrt{1+x^4}dx
            \end{equation*}
            So $J<L<1<K$
            \end{proof}
            \begin{problem}
            Which is the best estimate of $\sqrt{1.5}(266)^{3/2}$?
            \begin{enumerate}
                \begin{multicols}{5}
                \item[A.)] 1,0000
                \item[B.)] 2,700
                \item[C.)] 3,200
                \item[D.)] 4,100
                \item[E.)] 5,300
                \end{multicols}
            \end{enumerate}
            \end{problem}
            \begin{proof}[Solution]
            First note that $1.5 = \frac{3}{2}$ and $266 = 2\cdot 133$. So we have $\sqrt{1.5}(266)^{3/2} = \sqrt{\frac{3}{2}}\sqrt{266}\cdot 266 = \frac{\sqrt{3}}{\sqrt{2}}\cdot \sqrt{2}\cdot \sqrt{133}\cdot 266 = \sqrt{3}\cdot \sqrt{133}\cdot 266 = \sqrt{3\cdot 133}\cdot 266 = \sqrt{399}\cdot 266$. Now for the approximating. We note that $399 \approx 400$. So $\sqrt{399} \approx \sqrt{400} = 20$. So, with this approximation we have $20\cdot 266 = 5320$. The closest option is E.) $5,300$.
            \end{proof}
            \begin{problem}
            If $A$ is a $2\times 2$ matrix who's columns and rows add up to a constant $k$, which of the following must be an eigenvector:
            \begin{enumerate}
                \begin{multicols}{3}
                \item[I] $\begin{pmatrix}1 \\ 0 \end{pmatrix}$
                \item[II] $\begin{pmatrix} 0 \\ 1 \end{pmatrix}$
                \item[III] $\begin{pmatrix} 1 \\ 1 \end{pmatrix}$
                \end{multicols}
            \end{enumerate}
            \begin{enumerate}
                \begin{multicols}{5}
                \item[A.)] I only
                \item[B.)] II only
                \item[C.)] III only
                \item[D.)] I and II only
                \item[E.)] I, II, and III
                \end{multicols}
            \end{enumerate}
            \end{problem}
            \begin{proof}[Solution]
            We have $A = \begin{bmatrix} a & b \\ c & d\end{bmatrix}$ With the condition that:
            \begin{align*}
                a+b&=k\\
                a+c&=k\\
                b+d&=k\\
                c+d&=k
            \end{align*}
            From this we can see that $a = d$ and $b=c = k=a$.
            So, we have $A = \begin{bmatrix} a & k-a \\ k-a & a\end{bmatrix}$. We could solve directly for the eignvectors, or we could be more time efficient and check the possibilities we were given. $A\begin{bmatrix}1 \\ 0 \end{bmatrix} = \begin{bmatrix} a \\ k-a \end{bmatrix}$. $A\begin{bmatrix} 0 \\ 1 \end{bmatrix} = \begin{bmatrix} k-a \\ a \end{bmatrix}$. So I and II are only solutions if $a = k = 0$. $A\begin{bmatrix} 1 \\ 1 \end{bmatrix} = \begin{bmatrix} k \\ k\end{bmatrix} = k \begin{bmatrix} 1 \\ 1 \end{bmatrix}$. Thus we see that $\begin{bmatrix} 1 \\ 1 \end{bmatrix}$ is always an eigenvector. The answer is C.
            \end{proof}
            \begin{problem}
            A total of $x$ fee of fencing is to form three sides of a level rectangular yard. What is the maximum possible area in terms of $x$?
            \end{problem}
            \begin{proof}[Solution]
            Let $ax$ be the length of one side of the rectangular and $bx$ be the length of the adjacent side, so we have $ax+2bx = x$. From this we have $a+2b = 1$, or $a = 1-2b$. The area of the rectangle is the product of these lengths, so $A = ax\cdot bx = abx^2$. Substituting $a = 1-2b$, we have $A = (1-2b)bx^2$. To find the extremum we differentiate with respect to $b$ and set equal to $0$.
            \begin{align}
                A&=(1-2b)bx^2\\
                \frac{dA}{db}&=0\\
                \Rightarrow (1-4b)x^{2}&=0\\
                \Rightarrow b&=\frac{1}{4}
            \end{align}
            So we have $b = \frac{1}{4}$, and $a = 1-2b = \frac{1}{2}$. The corresponding area is $abx^2 = \frac{1}{2}\cdot \frac{1}{4} x^2 = \frac{x^2}{8}$. The answer is $\frac{x^2}{8}$.
            \end{proof}
            \begin{problem}
            What is the unit digit in the decimal expansion of $7^{25}$?
            \end{problem}
            \begin{proof}[Solution]
                Asking what is the unit digit of $7^{25}$ is equivalent to asking what is $7^{25} \mod 10$.
            \begin{align*}
                7^{25} &= 7\cdot7^{24}\\
                &= 7\cdot(7^2)^{12}\\
                &= 7\cdot(49)^{12}\\
                &\equiv 7\cdot(9)^{12}\mod 10\\
                &= 7\cdot (9^2)^{6}\\
                &= 7 \cdot (81)^{6}\\
                &\equiv 7 \cdot (1)^6 \mod 10\\
                &= 7
            \end{align*}
            So, $7^{25} \cong 7 \mod 10$. The answer is $7$.
            \end{proof}
            \begin{problem}
            Let $f:[-2,3]\rightarrow \mathbb{R}$ be continuous. Which of the following is NOT necessarily true?
            \begin{enumerate}
                \item[A.)] $f$ is bounded.
                \item[B.)] $\int_{-2}^{3}f$ exists.
                \item[C.)] For each $c$ between $f(-2)$ and $f(3)$ there is an $x\in (-2,3)$ such that $f(x) = c$.
                \item[D.)] There is an $M$ in $f([-2,3])$ such that $\int_{-2}^{3}f = 5M$.
                \item $\underset{h\rightarrow 0}\lim \frac{f(h)-f(0)}{h}$ exists.
            \end{enumerate}
            \end{problem}
            \begin{proof}[Solution]
            \begin{theorem*}
            A subset $\mathcal{C}\subset \mathbb{R}$ is compact if and only if it is closed and bounded.
            \end{theorem*}
            \begin{theorem*}
            If $\mathcal{C}$ is compact and $f:\mathcal{C}\rightarrow \mathbb{R}$ is continuous, then f is bounded.
            \end{theorem*}
            \begin{theorem*}
            If is continuous and bounded, then f is integrable.
            \end{theorem*}
            \begin{theorem*}
            If $f:[a,b] \rightarrow \mathbb{R}$ is continuous, then for all $c$ between $f(a)$ and $f(b)$ there is an $x\in (a,b)$ such that $f(x) = c$.
            \end{theorem*}
            \begin{theorem*}
            If $f:[a,b]\rightarrow \mathbb{R}$ is continuous then there is an $M\in f\big([a,b]\big)$ such that $\int_{a}^{b}f = M(b-a)$
            \end{theorem*}
            Since $[-2,3]$ is closed and bounded, it is compact, and if $f$ is continuous then it must be bounded so A is true. Since $f$ is continuous and bounded, it is integrable so B is true. C is simply the Intermediate Value Theorem and D is simply the Mean Value Theorem. By process of elimination, E is not necessarily true. Let us find a counterexample. Let $f=|x|$. Then $\underset{h\rightarrow 0^+}\lim \frac{|h|-|0|}{h} = 1$, but $\underset{h\rightarrow 0^{-}}\lim \frac{|h|-|0|}{h} = -1$. Thus the limit does not exist for this function. The answer is E.
            \end{proof}
            \begin{problem}
            What is the volume of the solid formed by revolving about the $x-$axis the region in the first quadrant of the $xy-$plane bounded by the coordinate axes and the graph of $x = \frac{1}{\sqrt{1+x^2}}$?
            \end{problem}
            \begin{proof}[Solution]
            Working by definition, we could simply use the formula $V = \int_{0}^{\infty}\pi r^2(x)dx$, where $r(x) = \frac{1}{\sqrt{1+x^2}}$. But memorizing formulas is usually not the best idea. Instead let's try to do this intuitively. Suppose we had a small cylinder centered at $x$. The volume of a cylinder is $\pi r^2 h$, so our tiny slab will be $dV = \pi r^2(x) dx$. Integrating over all of these infinitesimal discs gives us $V = \pi \int_{0}^{\infty}r^2(x)dx$. For our problem, $r(x) = \frac{1}{\sqrt{1+x^2}}$, so we have $V = \pi \int_{0}^{\infty} \frac{1}{1+x^2}dx = \pi \tan^{-1}(x)\big|_{0}^{\infty} = \pi \cdot \frac{\pi}{2} = \frac{\pi^2}{2}$. The answer is $\frac{\pi^2}{2}$.
            \end{proof}
            \begin{problem}
            How many real zeros does the polynomial $3x^5+8x-7$ have?
            \end{problem}
            \begin{proof}[Solution]
            \begin{theorem*}
            If $f = \sum_{k=0}^{n} a_k x^k$, then the number of positive zeroes of $f(x)$ is equal to the number of sign changes of $f$ or is an even number less than that.
            \end{theorem*}
            \begin{theorem*}
            If $f=\sum_{k=0}^{n} a_k x^k$, then the number of negative zeroes is equal to the number of sign changes of $f(-x)$ or equal to an even number less than that.
            \end{theorem*}
            We have $f(x) = 2x^5+8x - 7$. So, the set of coefficients is $\{-7,8,0,0,0,2\}$. There is only one sign change, from $-7$ to $8$, so there is one positive zero. $f(-x) = -2x^5 - 8x - 7$, which has the set of coefficients $\{-7,-8,0,0,0,0,-2\}$. This has no sign changes, so there are no negative zeroes. Also, $f(0) = -7$, so $0$ is not a root of $f$. Thus $f$ has $1$ zero. The answer is $1$.
            \end{proof}
            \begin{problem}
            If $T$ is a linear transformation of $V = \mathbb{R}^{2\times 3}$ $\textbf{onto}$ $W = \mathbb{R}^4$, what is $\dim(\{v\in V:T(v) = 0\})$?
            \end{problem}
            \begin{proof}[Solution]
            \begin{theorem*}
            If $V,W$ are vector spaces and $T:V\rightarrow W$ is a linear transformation, then $\dim(\Im(T)) + \dim(\ker(T)) = \dim(V)$.
            \end{theorem*}
            This set is called the kernel of $T$, denoted $\ker(T)$. Now, as $T$ is onto, the image of $T$ is $V$, which has dimension $4$. Also, $\dim(V) = 6$. So we have $\dim(\ker(T)) + 4 = 6$, so $\dim(\ker(T)) = 2$. The answer is $2$.
            \end{proof}
            \begin{problem}
            If $f,g:\mathbb{R}\rightarrow \mathbb{R}$ are twice differentiable, and if for all $x>0$, $f'(x)>g'(x)$, then which of the following must be true for all $x>0$?
            \begin{enumerate}
                \begin{multicols}{3}
                \item[A.)] $f(x)>g(x)$
                \item[B.)] $f''(x)>g''(x)$
                \item[C.)] $f(x)-f(0)>g(x)-g(0)$
                \item[D.)] $f'(x)-f'(0)>g'(x)-g'(0)$
                \item[E.)] $f''(x) - f''(0)>g''(x)-g''(0)$
                \end{multicols}
            \end{enumerate}
            \end{problem}
            \begin{proof}[Solution]
            \begin{theorem*}
            If $f:(a,b)\rightarrow \mathbb{R}$ is continuous and $x\in (a,b)$, then $f(x)-f(a) = \int_{a}^{x}f'$ 
            \end{theorem*}
            \begin{theorem*}
            If $f,g$ are continuous and bounded on $(a,b)$, and if $f>g$, then $\int_{a}^{b}f > \int_{a}^{b}g$
            \end{theorem*}
            \begin{theorem*}
            If $f,g$ are continuous and bounded on $(a,b)$, $f'>g'$, and if $x\in (a,b)$, then $f(x) - f(0) > g(x) - g(0)$.
            \end{theorem*}
            From this we have that C is true. There are counterexamples for the others:
            \begin{enumerate}
                \item[A.)] Let $f = x$ and $g = 2$. Then $f'(x) = 1$ and $g'(x) = 0$, but $f(x)<g(x)$ for $x\in [0,2)$.
                \item[B.)] Let $f = x$, $g(x) = \int_{0}^{x} \frac{1}{2}\sin(t^2)dt$. Then $f'(x) = 1$, $g'(x) = \frac{1}{2}\sin(x^2)$, $f''(x) = 0$, $g''(x) = x\cos(x^2)$.
                \item[D.)] Let $f(x) = x$, $g(x) = \frac{1}{2}x^2$. Then $f'(x) = 1$, $g'(x) = x$, $f(x)-f(0) = 0$, $g(x)-g(0) = x$.
                \item[E.)] Use the same example from B.
            \end{enumerate}
            \end{proof}
            \begin{problem}
            Where is the function $f(x) = \begin{cases} \frac{x}{2}, & x\in \mathbb{Q} \\ \frac{x}{3}, & x \notin \mathbb{Q}\end{cases}$ disccontinuous?
            \end{problem}
            \begin{proof}[Solution]
            Let $x\in \mathbb{Q}\setminus \{0\}$ and let $x_n$ be any sequence of irrationals such that $x_n \rightarrow x$. Then $f(x_n) = \frac{x_n}{3} \rightarrow \frac{x}{3}$, but $f(x) = \frac{x}{2}$. Therefore $f$ is not continuous in $\mathbb{Q}\setminus \{0\}$. If $x\in \mathbb{R}\setminus \mathbb{Q}$, let $x_n$ be any sequence of rationals such that $x_n \rightarrow x$. Then $f(x_n) = \frac{x_n}{2} \rightarrow \frac{x}{2}$, but $f(x) = \frac{x}{3}$. So $f$ is discontinuous on $\mathbb{R}\setminus \mathbb{Q}$. If $x= 0$, let $\varepsilon>0$ and choose $\delta = \varepsilon$. Then for $|x|<\delta$ we have $|f(0) - f(x)| = |f(x)| < \frac{|x|}{2} < \frac{\varepsilon}{2}<\varepsilon$. Thus, $f$ is continuous at $0$. $f$ is discontinuous at all non-zero real numbers.
            \end{proof}
            \begin{problem}
            Let $P_1 = \{2,3,5,7,11,\hdots\}$ and $P_n = \{2n,3n,5n,7n,11n,\hdots\}$. Which of the following is non-empty?
            \begin{enumerate}
            \begin{multicols}{5}
                \item[A.)] $P_1\cap P_{23}$
                \item[B.)] $P_7\cap P_{21}$
                \item[C.)] $P_{12}\cap P_{20}$
                \item[D.)] $P_{20}\cap P_{24}$
                \item[E.)] $P_{5}\cap P_{25}$
            \end{multicols}
            \end{enumerate}
            \end{problem}
            \begin{proof}[Solution]
            \
            \begin{enumerate}
                \item[A.)] If $x\in P_{1}\cap P_{23}$, $x = p = 23\cdot q$ for primes $p$ and $q$. But then $p = 23\cdot q$, a contradiction as $p$ is prime. So A is empty.
                \item[B.)] If $x\in P_{7}\cap P_{21}$, then $x= 7p = 21q$. So $7p = 7\cdot 3q$, and thus $p = 3q$, again a contradiction.
                \item[C.)] If $x\in P_{12}\cap P_{20}$, then $x = 12p = 20q$, so we have $4\cdot 3 p = 4\cdot 5 q$, and thus $3p = 5q$. So $p=5$ and $q = 3$. $60 \in P_{12}\cap P_{20}$.
                \item[D.)] If $x\in P_{20}\cap P_{24}$, then $20p = 24q$. So $4\cdot 5p = 4\cdot 6q$, and thus $5p = 6q$. The left-side is even, and thus the only possibility is $p=5$. But then $10 = 6p$, a contradiction as $p$ is prime.
                \item[E.)] If $x\in P_{5}\cap P_{25}$, then $x = 5p = 25q$, so $5p = 5\cdot 5q$, and therefore $p=5q$, a contradiction as $p$ is prime.
            \end{enumerate}
            C is the only non-empty set. The answer is C.
            \end{proof}
            \begin{problem}
            Let $C(\mathbb{R})$ be the set of continuous function $f:\mathbb{R}\rightarrow \mathbb{R}$. Then $C(\mathbb{R})$ is a vector space with addition defined as $(f+g)(x) = f(x)+g(x)$ for all $f,g\in C(\mathbb{R})$, and scalar multiplication defined $(rf)(x) = r\cdot f(x)$, for all $x,r\in \mathbb{R}$. Which of the following are subspaces of $C(\mathbb{R})$?
            \begin{enumerate}
                \item[I] $\{f:f''\textrm{ exists and }f'' - 2f'+3f = 0\}$
                \item[II] $\{g:g''\textrm{ exists and }g'' = 3g' \}$
                \item[III] $\{h:h''\textrm{ exists and }h'' = h+1\}$
            \end{enumerate}
            \begin{enumerate}
                \begin{multicols}{5}
                \item[A.)] I only
                \item[B.)] I and II only
                \item[C.)] I and III only
                \item[D.)] II and III only
                \item[E.)] I, II, and III
                \end{multicols}
            \end{enumerate}
            \end{problem}
            \begin{proof}[Solution]
            We have to check for closure under vector addition and scalar multiplication.
            \begin{enumerate}
                \item[I] For addition, $(f_1+f_2)'' - 2(f_1+f_2)' +3(f_1+f_2) = (f''_1 - 2f'_1 +3f_1) + (f''_2 - 2f'_2 +3f_2) = 0+0 = 0$. For scalar multiplication $(rf)'' - 2(rf)' +3(rf) = r(f'' - 2f' + 3f) = r\cdot 0 = 0$. So I is closed under vector addition and scalar multiplication, and thus I is a subspace.
                \item For addition, $(g_1+g_2)'' = g''_1 +g''_2 = 3g_1 + 3g_2 = 3(g_1+g_2)$. For scalar multiplication, $(rg)'' =rg'' = r\cdot 3g = 3(rg)$. Thus II is closed under vector addition and scalar multiplication. II is a subspace.
                \item For addition, $(h_1+h_2)'' = h''_1 + h''_2 = h_1+1 + h_2+1 = (h_1+h_2) + 2 \ne (h_1+h_2)+1$. Therefore III is not closed under vector addition. III is not a subspace.
            \end{enumerate}
            We see that I and II are subspaces, but III is not. The answer is B.
            \end{proof}
            \begin{problem}
            For what value of $b$ does the line $y_1=10x$ lie tangent to the curve $y_2=e^{bx}$?
            \end{problem}
            \begin{proof}[Solution]
            If the two curves lie tangent at $x$, then $y'_1(x) = y'_2(x)$. Thus we have $10 = be^{bx}$. But also $y_1(x) = y_2(x)$. So $e^{bx} = 10x$. So we have $10 = be^{bx} = b\cdot (10x) = 10bx$. Therefore we have $bx = 10$, and $x = \frac{1}{b}$. But $10 = b e^{bx} = b e^{b\cdot \frac{1}{b}} = be$. Therefore $b = \frac{10}{e}$. The answer is $b = \frac{10}{e}$.
            \end{proof}
            \begin{problem}
            Let $h(x)=\int_{0}{x^{2}}e^{x+t}dt$. What is $h'(1)$?
            \end{problem}
            \begin{proof}[Solution]
            \begin{theorem*}
            If $\alpha$ and $\beta$ are differentiable, $f$ is continuous, and if $F(x) = \int_{\alpha(x)}^{\beta(x)}f$, then:
            \begin{equation*}
                F'(x)=f\big(\beta(x)\big)\beta'(x)-
                    f\big(\alpha(x)\big)\alpha'(x)
            \end{equation*}
            \end{theorem*}
            We have $h(x) = \int_{0}^{x^2}e^{x+t}dt = e^x \int_{0}^{x^2}e^t dt$. Using the product rule and the above theorem we have:
            \begin{align*}
                h'(x) &= e^x \int_{0}^{x^2} e^t dt + e^{x} \big(e^{x^2}\frac{d}{dx}(x^2)\big) \\
                &= e^x\big(e^{x^2}-1\big) + 2xe^xe^{x^2} \\
                \Rightarrow h'(1) &= e(e-1) + 2e^2 = \\
                &= 3e^2 - e
            \end{align*}
            The answer is $3e^{2}-e$
            \end{proof}
            \begin{problem}
            Let $\{a_n\}_{n=1}^{\infty}$ be recursively defined by $a_1 = 1$, and for all $n\in \mathbb{N}$, $a_{n+1} = \frac{n+2}{n}a_n$. What is $a_{30}$?
            \end{problem}
            \begin{proof}[Solution]
            Let us try to find a closed form for this sequence. For $n>2$ we hav:
            \begin{align*}
                a_{n+1} &= \frac{n+2}{n}a_n \\
                &= \frac{n+2}{n}\frac{n+1}{n-1}a_{n-1}\\
                &= \frac{n+2}{n}\frac{n+1}{n-1}\frac{n}{n-2}a_{n-2}
            \end{align*}
            The pattern seems to be $a_{n+1} = \frac{(n+2)!}{(n+1-k)!}\frac{(n-1-k)!}{n!}a_{n-k}$. When $k=n-1$, we have $a_{n} = \frac{(n+1)!}{2!(n-1)!}a_1$. We can confirm this by induction. The base case of $n=1$ is $\frac{2!}{2!0!}a_1 = a_1$. Suppose it is true for $n\in \mathbb{N}$. Then $a_{n+1} = \frac{n+2}{n} a_n = \frac{n+2}{n} \frac{(n+1)!}{2!(n-1)!}a_1 = \frac{(n+2)!}{2!n!}a_1$. This proves the induction step. Therefore, $a_n = \frac{(n+1)!}{2!n!}a_1$. So $a_{30} = \frac{31!}{2!29!} = \frac{31\cdot 30}{2} = 31\cdot 15$. The answer is $(15)(31)$.
            \end{proof}
    \section{GR1268}
    \newpage
    \section{Notes from REA Mathematics}
        \subsection{Polynomials and Equations}
            \begin{definition}
                A polynomomial in $x$, denoted $P(x)$,
                is a term or
                a sum of terms that are a
                real number of the product
                of a real number and a positive integral power
                of $x$.
            \end{definition}
            \begin{example}
                \
                \begin{enumerate}
                    \begin{multicols}{2}
                        \item $5x^{3}+2x^{2}+3$ is a
                            polynomial in $x$.
                        \item $2^{2}+x^{\frac{1}{2}}-1$
                            is not a polynomial in $x$.
                        \item $9x^{3}+3x^{-2}+4$ is
                            not a polynomial in $x$.
                        \item $x^{2}+1$ is a polynomial in $x$.
                    \end{multicols}
                \end{enumerate}
            \end{example}
            \begin{definition}
                A monomial is the product of a real number with
                variables raised to integral powers.
            \end{definition}
            \begin{definition}
                The degree of a monomial is the sum of the exponents
                of the variables. A monomial with no variables has
                degree $0$.
            \end{definition}
            \begin{example}
                \
                \begin{enumerate}
                    \begin{multicols}{2}
                        \item $5x^{2}$ has degree $2$.
                        \item $3x^{3}y^{2}z$ has degree $6$.
                        \item $9$ has degree $0$.
                        \item $xyz$ has degree $3$.
                    \end{multicols}
                \end{enumerate}
            \end{example}
            \begin{definition}
                The degree of a polynomial in $x$ is the largest
                exponent of non-zero terms.
            \end{definition}
            \begin{example}
                \
                \begin{enumerate}
                    \begin{multicols}{2}
                        \item $5x^{4}+7x+12$ has degree $4$.
                        \item $x^{2}+1$ has degree $2$.
                        \item $4x+6x^{5}-3x^{2}+1$ has degree $5$.
                        \item $0x^{7}+x^{3}-x+x^{2}$ has degree $3$.
                    \end{multicols}
                \end{enumerate}
            \end{example}
            The following operations can be
            performed on polynomials:
            \begin{enumerate}
                \item Addition: Add like terms
                    that differ only in coefficients.
                \item Subtraction: Change the sign of the
                    coefficients of the term being subtracted,
                    and then add.
                \item Multiplication:
                    Using the distributive property
                    to multiply the terms of one polynomial to the
                    term of another and then combine the terms.
            \end{enumerate}
            \begin{example}
                \
                \begin{enumerate}
                    \item Addition:
                        \begin{align*}
                            (x^{2}-3x+5)+(4x^{2}+6x-3)
                            &=(x^{2}+4x^{2})+(-3x+6x)+(5+(-3))\\
                            &=5x^{2}+3x+2
                        \end{align*}
                    \item Subtraction:
                        \begin{align*}
                            (x^{2}+1)-(3x^{2}-x+2)
                            &=(x^{2}+1)+(-3x^{2}+x-2)\\
                            &=(x^{2}-3x^{2})+(x)+(1-2)\\
                            &=-2x^{2}+x-1
                        \end{align*}
                    \item Multiplication:
                        \begin{align*}
                            (x^{4}+3)(2x^{2}-1)
                            &=(x^{4}+3)2x^{2}+(x^{4}+3)(-1)\\
                            &=(x^{4})(2x^{2})+(3)(2x^{2})
                            +(x^{4})(-1)+(3)(-1)\\
                            &=2x^{6}-x^{4}+6x^{2}-3
                        \end{align*}
                \end{enumerate}
            \end{example}
            \begin{definition}
                Factors of a polynomial $P(x)$ are polynomials
                $Q_{k}(x)$ such that $P(x)=\Pi_{k=0}^{n}Q_{k}(x)$
            \end{definition}
            \begin{definition}
                A prime factor of a polynomial is a factor that
                cannot be factored further.
            \end{definition}
            \begin{example}
                $x^{2}+1$ is a prime factor of $x^{4}-1$.
                $x^{2}-1$ is not a prime factor since
                $x^{2}-1=(x-1)(x+1)$.
            \end{example}
            \begin{definition}
                The least common multiple of a set of numbers
                is the smallest number that is divisble by
                every element of the set.
            \end{definition}
            \begin{definition}
                The least common multiple of a set of polynomials
                is the polynomial with lowest degree and smallest
                numerical coefficients for which every element
                of the set is a factor.
            \end{definition}
            In a similar manner, then greatest common factor
            can be defined.
            \begin{theorem*}
                The following are true:
                \begin{enumerate}
                    \begin{multicols}{2}
                        \item $a(c+d)=ac+ad$
                        \item $(a+b)(a-b)=a^{2}-b^{2}$
                        \item $(a+b)^{2}=a^{2}+2ab+b^{2}$
                        \item $(a-b)^{2}=a^{2}-2ab+b^{2}$
                        \item $(x+a)(x+b)=x^{2}+(a+b)x+ab$
                        \item $(ax+b)(cx+d)=acx^{2}+(ad+bc)x+bd$
                        \item $(a+b)(c+d)=ac+bc+ad+bd$
                        \item $(a+b)^{3}=%
                               a^{3}+3a^{2}b+3ab^{2}+b^{3}$
                        \item $(a-b)^{3}=%
                               a^{3}-3a^{2}b+3ab^{2}-b^{3}$
                        \item $(a-b)(a^{2}+ab+b^{2})=a^{3}-b^{3}$
                        \item $(a+b)(a^{2}-ab+b^{2})=a^{3}+b^{3}$
                        \item $(a+b+c)^{2}=%
                               a^{2}+b^{2}+c^{2}+2(ab+ac+bc)$
                        \item $(a-b)(a^{3}+a^{2}b+ab^{2}+b^{3})=%
                               a^{4}-b^{4}$
                        \item $(a-b)%
                               (a^{4}+a^{3}b+%
                                a^{2}b^{2}+ab^{3}+b^{4})%
                               =a^{5}-b^{5}`$
                    \end{multicols}
                \end{enumerate}
            \end{theorem*}
            \begin{theorem*}
                If ${n}\geq{2}$, then
                $a^{n}-b^{n}=(a-b)\sum_{k=0}^{n-1}a^{n-1-k}b^{k}$
            \end{theorem*}
            \begin{theorem*}
                If $n$ is odd, then
                $a^{n}+b^{n}=%
                 (a+b)\sum_{k=0}^{n-1}(-1)^{k}a^{n-1-k}b^{k}$
            \end{theorem*}
            To factor a polynomial completely,
            first compute the greatest common factor,
            if there is any. Then examine the remaining factors.
            Continue factoring until all of the factors are prime.
            \begin{example}
                $4-16x^{2}=4(1-4x^{2})=4(1-2x)(1+2x)$
            \end{example}
            \begin{definition}
                An equation is a statement of equality
                between two separate expressions.
            \end{definition}
            \begin{definition}
                A rational integral equation is an equation
                involving two expressions that have only rational
                coefficients and integral powers.
            \end{definition}
            Equations can be simplified by applying the following:
            \begin{enumerate}
                \item Adding or subtracting an expression to both
                    sides of an equation results in an equivalent
                    equation.
                \item Multiplying both sides of an equation by a
                    non-zero expression results in an equivalent
                    equation.
                \item The reciprocal of both sides of a non-zero
                    equation are equivalent.
                \item When evaluating equations containing
                    absolute values, note that $|P(x)|=Q(x)$ is
                    equivalent to $P(x)=Q(x)$ OR $P(x)=-Q(x)$.
            \end{enumerate}
            \begin{example}
                \
                \begin{enumerate}
                    \begin{multicols}{2}
                        \item $y+6={10}\Rightarrow{y+6-6}=10-6%
                               \Rightarrow{y}=4$
                        \item $3x={6}\Rightarrow{\frac{3x}{3}}=%
                               \frac{6}{3}\Rightarrow{x=2}$
                        \item $\frac{1}{x}=\frac{1}{3}%
                               \Rightarrow{x}=3$
                        \item $|x|=2\Rightarrow{x}=2$ OR
                            $x=-2$
                    \end{multicols}
                \end{enumerate}
            \end{example}
            Both sides of an equation can also be raised to a power,
            but this may not be an equivalent expression.
            For example $x^{2}=1$ has the solution set $\{-1,1\}$,
            however if we take the square root of both sides
            we have $x=1$. You must be careful when using
            powers to make sure that both equations are equivalent.
        \subsection{Inequalities}
            \begin{definition}
                An inequality is a statement that the value of
                one expression is greater or less than that
                of another.
            \end{definition}
            \begin{example}
                $4<5$ means that 4 is less than 5.
                $5>4$ means that 5 is greater than 4.
            \end{example}
            \begin{definition}
                A conditional inequality is an inequality
                whose validity depends on the values
                of the variables in the expressions.
            \end{definition}
            \begin{definition}
                An absolute inequality is an inequality
                that is true of all values.
            \end{definition}
            \begin{definition}
                An inconsistent inequality is an inequality
                that is never true for any values.
            \end{definition}
            \begin{example}
                \
                \begin{enumerate}
                    \item $x+3>3-x$ is a conditional inequality.
                        It is true for $x>0$, but false for
                        ${x}\leq{0}$.
                    \item $x+2>x$ is an absolute inequality. This
                        is always true, regardless of $x$.
                    \item $x>x+10$ is an inconsistent inequality.
                \end{enumerate}
            \end{example}
            \begin{theorem*}
                The following are true:
                \begin{enumerate}
                    \begin{multicols}{2}
                        \item If $a<b$ and $b<c$, then $a<c$
                        \item If $a>b$ and $b>c$, then $a>c$
                        \item If $a>b$, then $a+c>b+c$
                        \item If $a>b$, then $a-c>b-c$
                        \item If $a>b$ and $c>0$, then $ac>bc$
                        \item If $a>b$ and $c<0$, then $ac<bc$
                        \item If $a>b>0$ and $n$ is positive,
                            then $a^{n}>b^{n}$
                        \item If $a>b>0$ and $n$ is negative,
                            then $a^{n}<b^{n}$
                        \item If $x>y$ and $p>q$, then
                            $x+p>y+q$
                        \item If $x>y>0$ and $p>q>0$,
                            then $xp>yq$
                        \item $|x|<a$ has solution set
                            $\{x:-a<x<a\}$
                        \item $|x|>a$ has solution set
                            $\{x:{x>a}\textrm{ or }{x<-a}\}$
                    \end{multicols}
                \end{enumerate}
            \end{theorem*}
            \begin{definition}
                A linear inequality in two variables is
                an inequality of the form
                $ax+by<c$
            \end{definition}
            \begin{example}
                Solve for $2x-3y>6$. In the limiting case,
                ${2x-3y=6}\Rightarrow{y=\frac{2}{3}x-2}$.
                This is the equation of a line in the $xy$ plane.
                Choosing a point not on this line, say $(1,1)$,
                we have $2(1)-3(1)=2-3=-1<6$. Therefore the point
                $(1,1)$ is not contained in the region
                $2x-3y>6$. But then we also not that
                $(1,-\frac{4}{3})$ is on the boundary. So
                $2x-3y>6$ is the region
                \textit{below} the line $y=\frac{2}{3}x-2$
            \end{example}
        \subsection{Linear Algebra}
            A system of linear equations can be written
            using an equivalent matrix notation.
            \begin{example}
                \begin{align*}
                    a_{11}x_{1}+a_{12}x_{2}+a_{13}x_{3}&=b_{1}\\
                    a_{21}x_{1}+a_{22}x_{2}+a_{23}x_{3}&=b_{2}\\
                    a_{31}x_{1}+a_{32}x_{2}+a_{33}x_{3}&=b_{3}\\
                \end{align*}
                This is equivalent to either of the following:
                \begin{align*}
                    \begin{bmatrix}
                        a_{11}&a_{12}&a_{13}\\
                        a_{21}&a_{22}&a_{31}\\
                        a_{31}&a_{32}&a_{33}
                    \end{bmatrix}
                    \begin{bmatrix}
                        x_{1}\\
                        x_{2}\\
                        x_{3}
                    \end{bmatrix}
                    &=
                    \begin{bmatrix}
                        b_{1}\\
                        b_{2}\\
                        b_{3}
                    \end{bmatrix}
                    &
                    \mathbf{A}\mathbf{x}
                    &=\mathbf{b}
                \end{align*}
            \end{example}
            Matrices can also be written as $\mathbf{A}=(a_{ij})$.
            The following rules are used to
            define matrix arithmetic.
            \begin{enumerate}
                \item Addition: To add two matrices, add their
                    corresponding elements. That is, if
                    $\mathbf{A}=(a_{ij})$ and $\mathbf{B}=(b_{ij})$,
                    then $\mathbf{A}+\mathbf{B}=(a_{ij}+b_{ij})$.
                    Matrix addition is only defined on matrices of
                    the same size.
                \item Scale multiplication: To multiply a
                    matrix by a real or complex number $c$,
                    multiply this number to every element. That is,
                    if $\mathbf{A}=(a_{ij})$, then
                    $c\mathbf{A}=({c}\cdot{a_{ij}})$
                \item Matrix Multiplication: The product of
                    and ${M}\times{N}$ matrix with an
                    ${N}\times{P}$ matrix is defined by
                    $\mathbf{C}=\mathbf{A}\mathbf{B}$, where
                    $(c_{ij})=(\sum_{k=1}^{N}a_{ik}b_{kj})$. Note
                    that it is possible for
                    $\mathbf{A}\mathbf{B}\ne\mathbf{B}\mathbf{A}$.
                    Indeed, it is possible for
                    $\mathbf{A}\mathbf{B}$ to be defined, whereas
                    $\mathbf{B}\mathbf{A}$ is undefined.
            \end{enumerate}
            \begin{example}
                Let the following be true:
                \begin{align*}
                    A&=
                    \begin{bmatrix}
                        1&2\\
                        3&4
                    \end{bmatrix}
                    &
                    B&=
                    \begin{bmatrix}
                        5&6\\
                        7&8
                    \end{bmatrix}
                \end{align*}
                Then, using the defined rules, we have:
                \begin{align*}
                    A+B&=
                    \begin{bmatrix}
                        6&8\\
                        10&12
                    \end{bmatrix}
                    &
                    5A&=
                    \begin{bmatrix}
                        5&10\\
                        15&20
                    \end{bmatrix}
                    \\
                    AB&=
                    \begin{bmatrix}
                        19&22\\
                        43&50
                    \end{bmatrix}
                    &
                    BA&=
                    \begin{bmatrix}
                        23&34\\
                        31&46
                    \end{bmatrix}
                \end{align*}
            Note that even in this trivial example,
            ${AB}\ne{BA}$.
            \end{example}
            \begin{definition}
                The ${n}\times{n}$ identity matrix is the matrix
                $I_{n}=(I_{ij})$, where
                $I_{ij}=%
                \begin{cases}%
                 0,&{i}\ne{j}\\%
                 1,&{i}={j}%
                \end{cases}$
            \end{definition}
            \begin{definition}
                An inverse matrix of an ${n}\times{n}$ matrix
                $A$ is a matrix $A^{-1}$ such that
                $AA^{-1}=A^{-1}A=I_{n}$
            \end{definition}
            Not every matrix has an inverse matrix. If one
            does exists, there are many properties it contains.
            \begin{theorem*}
                The following are true:
                \begin{enumerate}
                    \item If $\mathbf{A}$ and $\mathbf{B}$
                        are invertible ${n}\times{n}$ matrices,
                        then $\mathbf{A}\mathbf{B}$ is invertible
                        and
                        $\mathbf{A}\mathbf{B}^{-1}%
                         =\mathbf{B}^{-1}\mathbf{A}^{-1}$
                    \item If $\mathbf{A}$ is an invertible matrix,
                        then $\mathbf{A}^{-1}$ is an invertible
                        matrix and
                        $(\mathbf{A}^{-1})^{-1}=\mathbf{A}$
                \end{enumerate}
            \end{theorem*}
            \begin{definition}
                The trace of an ${n}\times{n}$ matrix
                $A$ is the sum of
                it's diagonal: $\Tr(A)=\sum_{i=1}^{n}a_{ii} $
            \end{definition}
            \begin{example}
                \begin{equation*}
                    \Tr\Bigg(
                    \begin{bmatrix}
                        4&5&6\\
                        1&2&3\\
                        8&8&3
                    \end{bmatrix}
                    \Bigg)
                    =4+2+3=9
                \end{equation*}
            \end{example}
            \begin{definition}
                The determinant of a ${2}\times{2}$ matrix
                $A=%
                 \begin{bmatrix}%
                    a&b\\%
                    c&d%
                 \end{bmatrix}$
                is $\det(A)=ad-bc$
            \end{definition}
            \begin{definition}
                The minor of the $i^{th}$ row and $j^{th}$
                column of an ${n}\times{n}$ matrix $\mathbf{A}$,
                denoted $M_{ij}$, is the determinant of the
                ${(n-1)}\times{(n-1)}$ matrix formed by
                removing the $i^{th}$ row and $j^{th}$ column
                from $\mathbf{A}$.
            \end{definition}
            \begin{definition}
                The cofactor of the minor $M_{ij}$ of an
                ${n}\times{n}$ matrix $\mathbf{A}$,
                denoted $C_{ij}$, is $(-1)^{i+j}M_{ij}$.
            \end{definition}
            \begin{example}
                \begin{align*}
                    A&=
                    \begin{bmatrix}
                        7&1&3\\
                        1&3&5\\
                        17&4&20
                    \end{bmatrix}
                    &
                    M_{11}
                    &=
                    \det\Bigg(\begin{bmatrix}
                             3&5\\
                             4&20
                         \end{bmatrix}
                        \Bigg)
                    =40
                    &
                    C_{11}
                    &=(-1)^{1+1}M_{11}=40
                \end{align*}
            \end{example}
            \begin{definition}
                The determinant of an ${n}\times{n}$ matrix
                $\mathbf{A}$ is
                $\det(A)=\sum_{j=1}^{n}a_{1j}C_{1j}$
            \end{definition}
            \begin{theorem*}
                If $\mathbf{A}$ is an ${n}\times{n}$ matrix
                and ${1}\leq{i}\leq{n}$, then
                $\det(A)=\sum_{j=1}^{n}a_{ij}C_{ij}$
            \end{theorem*}
            \begin{definition}
                The transpose of an ${n}\times{m}$ matrix
                $\mathbf{A}$, denoted $\mathbf{A}^{T}$,
                is the ${m}\times{n}$ matrix formed by
                swapping the rows and columns of $\mathbf{A}$
                with each other. That is $(a_{ij})^{T}=(a_{ji})$.
            \end{definition}
            \begin{definition}
                A symmetric matrix is a matrix $\mathbf{A}$
                such that $\mathbf{A}^{T}=\mathbf{A}$
            \end{definition}
            \begin{theorem*}
                If $\mathbf{A}$ is an ${n}\times{m}$ matrix and
                $\mathbf{B}$ is an ${m}\times{p}$ matrix, then
                the following are true:
                \begin{enumerate}
                    \begin{multicols}{2}
                        \item $(\mathbf{A}^{T})^{T}=\mathbf{A}$
                        \item $(\mathbf{A}+\mathbf{B})^{T}%
                               =\mathbf{A}^{T}+\mathbf{B}^{T}$
                        \item $(k\mathbf{A})^{T}=k\mathbf{A}^{T}$
                        \item $(\mathbf{A}\mathbf{B})^{T}%
                               =\mathbf{B}^{T}\mathbf{A}^{T}$
                    \end{multicols}
                \end{enumerate}
            \end{theorem*}
            \begin{definition}
                The adjoint of an ${n}\times{n}$ matrix
                $\mathbf{A}$, denoted $\adjoint(\mathbf{A})$,
                is the matrix $(C_{ij})^{T}$.
            \end{definition}
            \begin{theorem*}
                If ${\det(\mathbf{A})}\ne{0}$, then $\mathbf{A}$
                is invertible and
                $\mathbf{A}^{-1}=%
                 \frac{1}{\det(\mathbf{A})}\adjoint(\mathbf{A})$
            \end{theorem*}
            \begin{theorem*}
                If $\mathbf{A}$ and $\mathbf{B}$ are
                ${n}\times{n}$ matrices, then the following
                are true:
                \begin{enumerate}
                    \begin{multicols}{3}
                        \item $\det(\mathbf{A})%
                               =\det(\mathbf{A}^{T})$
                        \item $\det(k\mathbf{A})%
                               =k^{n}\det(\mathbf{A})$
                        \item $\det(\mathbf{A}\mathbf{B})%
                               =\det(\mathbf{A})\det(\mathbf{B})$
                    \end{multicols}
                \end{enumerate}
            \end{theorem*}
            \begin{theorem*}
                A matrix $\mathbf{A}$ is invertible if and only
                if ${\det(\mathbf{A})}\ne{0}$
            \end{theorem*}
            \begin{theorem*}
                If $\mathbf{A}$ is invertible, then
                $\det(\mathbf{A}^{-1})=\frac{1}{\det(\mathbf{A})}$
            \end{theorem*}
            The differential equation
            $\sum_{k=0}^{n}a_{k}y^{(k)}(x)$ Can be expression
            in terms of the characteristic polynomial
            $\sum_{k=0}^{n}a_{k}D^{k}$. Factoring this linear
            operator into $\Pi_{k=0}^{n}(D-r_{k})$,
            the general solution is
            $y(x)=\sum_{k=1}^{n}c_{k}e^{r_{k}x}$. If some of the
            $r_{k}$ repeat, we have $c_{k}x^{m_{k}-1}e^{r_{k}x}$,
            where $m_{k}$ is the number of repetitions.
            In general, if we have
            $\Pi_{k=0}^{n}(D-r_{k})^{m_{k}}$, the general
            solution is
            $y(x)=%
             \sum_{k=1}^{n}c_{k}e^{r_{k}x}%
             (\sum_{j=0}^{m_{k}-1}x^{j})$
            \begin{example}
                \
                \begin{enumerate}
                    \item $y'''-4y''+4y'=0$ has the characteristic
                        polynomial $D(D-2)^{2}$, so
                        $y(x)=c_{1}+c_{2}e^{2x}+c_{3}xe^{2x}$
                \end{enumerate}
            \end{example}
            In linear algebra, the determinant
            $\det(\mathbf{A}-\lambda{I})$ is the characteristic
            polynomial of the square matrix $\mathbf{A}$.
            \begin{definition}
                A vector space $V$ over a Field (Set of scalars)
                $F$ is a set $V$ with two operations
                $+$ and $\cdot$
                such that the following are true:
                \begin{enumerate}
                    \begin{multicols}{3}
                        \item $\forall_{{\mathbf{a},%
                                         \mathbf{b}}\in{V}}$
                              ${\mathbf{a}+\mathbf{b}}\in{V}$
                        \item $\mathbf{a}+\mathbf{b}%
                               =\mathbf{b}+\mathbf{a}$
                        \item $\mathbf{a}+(\mathbf{b}+\mathbf{c})%
                               =(\mathbf{a}+\mathbf{b})+\mathbf{c}$
                        \item $\forall_{\mathbf{a}\in{V}}%
                               \exists_{\mathbf{b}\in{V}}:%
                               \mathbf{a}+\mathbf{b}=\mathbf{0}$
                        \item $\forall_{{k}\in{F},\mathbf{a}\in{V}}$
                              $k\mathbf{a}\in{V}$
                        \item $k(\mathbf{a}+\mathbf{b})%
                               =k\mathbf{a}+k\mathbf{b}$
                        \item $(k_{1}+k_{2})\mathbf{a}%
                               =k_{1}\mathbf{a}+k_{2}\mathbf{a}$
                        \item $1\mathbf{a}=\mathbf{a}$
                        \item $k_{1}(k_{2}\mathbf{a})%
                               =(k_{1}k_{2})\mathbf{a}$
                    \end{multicols}
                \end{enumerate}
            \end{definition}
            \begin{theorem*}
                If $V$ is a vector space, then there is a
                $\mathbf{0}\in{V}$ such that for all
                $\mathbf{a}\in{V}$,
                $\mathbf{a}+\mathbf{0}=\mathbf{a}$
            \end{theorem*}
            \begin{definition}
                A linearly dependent subset of a vector space
                $V$ (Over $\mathbb{R}$)
                is a subset ${S}\subset{V}$ such that
                there exists an $N\in\mathbb{N}$, a non-zero
                $a_{n}:\mathbb{Z}_{N}\rightarrow\mathbb{R}$
                and an injective function
                $\mathbf{v}_{n}:\mathbb{Z}_{N}\rightarrow{V}$
                such that
                $\sum_{k=1}^{N}a_{n}\mathbf{v}_{n}=\mathbf{0}$
            \end{definition}
            \begin{definition}
                A linearly independent subset of a vector space
                $V$ is a subset ${S}\subset{V}$ that is not
                linearly dependent.
            \end{definition}
            \begin{theorem*}
                If $V\subset\mathbb{R}^{n}$ has more than
                $n$ vectors, then $V$ is linearly dependent.
            \end{theorem*}
            \begin{definition}
                The rank of a matrix is the number
                of linearly independent columns of
                the matrix.
            \end{definition}
            \begin{example}
                Let
                $\mathbf{A}=[A_{1}\ A_{2}]$
                where $A_{1}=(1,2)^{T}$ and
                $A_{2}=(2,4)^{T}$. So
                $2A_{1}+(-1)A_{2}=(0,0)^{T}=\mathbf{0}$. Therefore
                $\{A_{1},A_{2}\}$ is a linearly independent
                subset. Thus, $\rk(\mathbf{A})=1$.
            \end{example}
            \begin{definition}
                A matrix with full rank is a square
                ${n}\times{n}$ matrix $\mathbf{A}$ such that
                $\rk(\mathbf{A})=n$.
            \end{definition}
            \begin{theorem*}
                If $\mathbf{A}$ is a square matrix and
                $\det(\mathbf{A})\ne{0}$, then $\mathbf{A}$
                has full rank.
            \end{theorem*}
            \begin{theorem*}
                If $\mathbf{A}$ is a square matrix
                with full rank, then it is invertible.
            \end{theorem*}
            \begin{definition}
                A finite basis of a vector space $V$ is a
                linearly independent subset ${S}\subset{V}$
                where
                $S=\{\mathbf{v}_{k}\}_{k=0}^{n}$
                and for all
                $\mathbf{x}\in{V}$ there is an
                $a_{n}:\mathbb{Z}_{n}\rightarrow\mathbb{R}$
                such that
                $\mathbf{x}=\sum_{k=1}^{n}a_{k}\mathbf{v}_{k}$
            \end{definition}
            \begin{theorem*}
                All bases of a vector space $V$ have the
                same number of elements.
            \end{theorem*}
            \begin{definition}
                The dimension of a vector space
                $V$ is the number of elements in any
                basis of $V$.
            \end{definition}
            \begin{definition}
                An inner product on a vector space $V$ is a function
                $\langle|\rangle:{V}\times{V}\rightarrow\mathbb{R}$
                such that:
                \begin{enumerate}
                    \begin{multicols}{3}
                        \item $\langle{v_{1},v_{2}}\rangle%
                               =\langle{v_{2},v_{3}}\rangle$
                        \item $\langle{v_{1},v_{1}}\rangle\geq{0}$
                        \item $\langle{v_{1}+v_{2},v_{3}}\rangle%
                               =\langle{v_{1},v_{3}}\rangle%
                               +\langle{v_{2},v_{3}}\rangle$
                    \end{multicols}
                \end{enumerate}
            \end{definition}
            \begin{definition}
                The Euclidean inner product on $\mathbb{R}^{n}$
                is defined as:
                $\langle{\mathbf{x},\mathbf{y}}\rangle%
                 =\sum_{k=1}^{n}x_{k}y_{k}$
            \end{definition}
            \begin{definition}
                An eigenvector of an ${n}\times{n}$ matrix
                $\mathbf{A}$ is a vector
                $\mathbf{x}\in\mathbb{R}^{n}$
                such that there exists a $\lambda\in\mathbb{R}$
                such that
                $\mathbf{A}\mathbf{x}=\lambda\mathbf{x}$
            \end{definition}
            \begin{definition}
                An eigenvalue of an ${n}\times{n}$ matrix
                $\mathbf{A}$ is a real number
                $\lambda\in\mathbb{R}$ such that there is
                a vector $\mathbf{x}\in\mathbf{R}^{n}$ such
                that $\mathbf{A}\mathbf{x}=\lambda\mathbf{x}$
            \end{definition}
            \begin{definition}
                The characteristic equation, or
                the characteristic polynomial, of an
                ${n}\times{n}$ matrix $\mathbf{A}$
                is $\det(\lambda{I}-\mathbf{A})=0$
            \end{definition}
            \begin{definition}
                A diagonalizable matrix is an
                ${n}\times{n}$ matrix
                $\mathbf{A}$ such that there exists
                an invertible matrix $\mathbf{B}$
                such that
                $\mathbf{A}=\mathbf{B}^{-1}\mathbf{A}\mathbf{B}$
            \end{definition}
            \begin{theorem*}
                The following are true:
                \begin{enumerate}
                    \item If $\mathbf{A}$ is an ${n}\times{n}$
                        diagonable matrix, then $\mathbf{A}$
                        has $n$ linearly independent
                        eigenvectors.
                    \item If $\mathbf{A}$ is an ${n}\times{n}$
                        matrix with $n$ linearly independent
                        eigenvectors, then $\mathbf{A}$
                        is diagonalizable.
                    \item A symmetric matrix has all real
                        eigenvalues.
                \end{enumerate}
            \end{theorem*}
        \subsection{Abstract Algebra}
            \begin{definition}
                A group is a set $G$ and a binary relation $*$
                on $G$, denoted $(G,*)$, such that:
                \begin{enumerate}
                    \item For all ${a,b,c}\in{G}$, $(a*b)*c=a*(b*c)$
                    \item There is an ${e}\in{G}$ such that for all
                        ${a}\in{G}$, $a*e=e*a=a$.
                    \item For all ${a}\in{G}$ there is a ${b}\in{G}$
                        such that $a*b=e$
                \end{enumerate}
            \end{definition}
            \begin{definition}
                An Abelian group is a grou $(G,*)$ such that
                $*$ is commutative.
            \end{definition}
            \begin{example}
                $G=\{1\}$ is an Abelian group
                under multiplication.
                This is the trivial group.
            \end{example}
            \begin{theorem*}
                If $(G,*)$ is a group, then the following are true:
                \begin{enumerate}
                    \begin{multicols}{2}
                        \item The identity ${e}\in{G}$ is unique.
                        \item If $a*b=a*c$, then $b=c$.
                        \item If $b*a=c*a$, then $b=c$.
                        \item Inverses $a^{-1}$ are unique.
                        \item $\forall_{{a,b}\in{G}}%
                            \exists_{{x}\in{G}}:a*x=b$
                        \item $(a*b)^{-1}=b^{-1}*a^{-1}$
                    \end{multicols}
                \end{enumerate}
            \end{theorem*}
            \begin{definition}
                The order of a group is number of elements in the
                group.
            \end{definition}
            \begin{definition}
                A group of finite order, or a finite group,
                is a group with finitely many elements.
            \end{definition}
            \begin{definition}
                The direct product of two groups $(G,*)$ and
                $(H,\circ)$ is the group  $({G}\times{H},\star)$
                where $\star$ is the binary operation defined by
                $(g_{1},h_{1})\star(g_{2},h_{2})%
                 =(g_{1}*g_{2},{h_{1}}\circ{h_{2}})$
            \end{definition}
            \begin{definition}
                A permutation group on $n$ elements is a
                group whose elements are permutations of
                $n$ elements.
            \end{definition}
            \begin{definition}
                The symmetric group on $n$ elements,
                denoted $S_{n}$, is the group formed by
                permuting $n$ elements.
            \end{definition}
            \begin{definition}
                A homomorphism from a group $(G,*)$ to
                a group $(H,\circ)$ is a function
                $h:{G}\rightarrow{H}$ such that for all
                ${a,b}\in{G}$, $h(a*b)={h(a)}\circ{h(b)}$
            \end{definition}
            \begin{definition}
                An epimorphism from a group $(G,*)$ to
                a group $(H,\circ)$ is a homomorphism
                $h:{G}\rightarrow{H}$ such that
                $h$ is surjective.
            \end{definition}
            \begin{definition}
                A monomorphism from a group $(G,*)$ to
                a group $(H,\circ)$ is a homomorphism
                $h:{G}\rightarrow{H}$ such that
                $h$ is injective.
            \end{definition}
            \begin{definition}
                An isomorphism from a group $(G,*)$ to
                a group $(H,\circ)$ is a homomorphism
                $h:{G}\rightarrow{H}$ such that
                $h$ is bijective.
            \end{definition}
            \begin{definition}
                A ring is a set $R$ and two binary operations
                on $R$, denoted $(R,\cdot,+)$, such that:
                \begin{enumerate}
                    \begin{multicols}{3}
                    \item $(R,+)$ is an Abelian group.
                    \item $a\cdot({b}\cdot{c})%
                           =({a}\cdot{b})\cdot{c}$
                    \item ${a}\cdot(b+c)%
                           ={a}\cdot{b}+{a}\cdot{c}$
                    \end{multicols}
                \end{enumerate}
            \end{definition}
            \begin{definition}
                A ring with identity is a ring $(R,\cdot,+)$
                such that there is a ${1}\in{R}$ such that for
                all ${a}\in{R}$, ${a}\cdot{1}={1}\cdot{a}=a$.
            \end{definition}
            \begin{remark}
                Left and right identities are elements such
                that ${e_{L}}\cdot{a}=a$ and ${e_{R}}\cdot{a}=a$.
                If inverses $a_{L}^{-1}$ and $a_{R}^{-1}$ exist
                for $a$, then $a_{L}^{-1}=a_{R}^{-1}$. That is,
                the inverse is the same for both right and left
                identities.
            \end{remark}
            \begin{definition}
                A commutative ring is a ring $(R,\cdot,+)$ such that
                $\cdot$ is commutative.
            \end{definition}
            \begin{definition}
                A commutative ring with identity is a
                ring with identity such that $\cdot$
                is commutative.
            \end{definition}
            \begin{definition}
                A Field is a commutative ring with identity
                $(F,\cdot,+)$
                such that for all ${a}\in{F}$ such that
                $a$ is not an identity with respect to $+$,
                there is a $b\in{F}$ such that ${a}\cdot{b}=1$.
            \end{definition}
            \begin{definition}
                Equivalent sets are sets $A$ and $B$ such that
                there exists a bijective function
                $f:{A}\rightarrow{B}$
            \end{definition}
            \begin{definition}
                A finite set is a set $A$ such that there
                is an ${n}\in{\mathbb{N}}$ such that $A$
                is equivalent to $\mathbb{Z}_{n}$.
            \end{definition}
            \begin{definition}
                A countable set (Or a denumerable set) is a
                set $A$ that is equivalent to $\mathbb{N}$.
            \end{definition}
            \begin{definition}
                An uncountable set is a set that is neither finite
                nor countable.
            \end{definition}
            \begin{theorem*}
                Set Equivalence is an equivalence relation.
            \end{theorem*}
            This equivalence allows to classify all sets by the
            number of elements they contain or, more generally,
            by their cardinality. We say that two sets $A$ and
            $B$ have the same cardinality, denoted
            $\Card(A)$, if and only if $A$ and $B$ are equivalent.
            \begin{theorem*}
                The following are true:
                \begin{enumerate}
                    \begin{multicols}{2}
                        \item $\Card(A)=0$ if and only if
                              $A=\emptyset$.
                        \item If ${A}\sim{\mathbb{Z}_{n}}$, then
                              $\Card(A)=n$.
                    \end{multicols}
                \end{enumerate}
            \end{theorem*}
            \begin{definition}
                A finite cardinal number is a cardinal
                number of a finite set.
            \end{definition}
            \begin{definition}
                The standard ordering on the finite cardinal
                number is $0<1<\hdots<n<n+1<\hdots$
            \end{definition}
            Thus, if $A$ and $B$ are finite sets, then we write
            $\Card(A)<\Card(B)$ if $A$ is equivalent to a
            subset of $B$ but not equivalent to $B$.
            We take this notion and generalize to
            all sets. For $A$ and $B$, we write
            $\Card(A)<\Card(B)$ if $A$ is equivalent to a subset
            of $B$ but is not equivalent to $B$. This is the
            same as saying $A$ is equivalent to a subset of $B$,
            but $B$ is not equivalent to a subset of $A$.
            We write that
            $\Card(A)\leq\Card(B)$ is $A$ is equivalent to a
            subset of $B$.
            \begin{theorem*}[Schr\"{o}der-Bernstein Theorem]
                If $A$ and $B$ are sets such that
                $\Card(A)\leq\Card(B)$ and
                $\Card(B)\leq\Card(A)$, then
                $\Card(A)=\Card(B)$.
            \end{theorem*}
            \begin{theorem*}
                The following are true:
                \begin{enumerate}
                    \item If $\Card(A)\leq\Card(B)$ and
                          $\Card(B)\leq\Card(A)$, then
                          $\Card(A)\leq\Card(C)$.
                    \item If $\Card(A)\leq\Card(B)$, then
                          $\Card(A)+\Card(C)\leq\Card(B)+\Card(C)$
                \end{enumerate}
            \end{theorem*}
            \begin{theorem*}
                If ${A}\subset{B}\subset{C}$, and
                $\Card(A)=\Card(C)$, then $\Card(B)=\Card(C)$
            \end{theorem*}
            \begin{theorem*}
                If $f:{X}\rightarrow{Y}$ is a function,
                then $\Card(f(X))\leq\Card(X)$.
            \end{theorem*}
            \begin{proof}
                Note that $f^{-1}(\{y\})$ creates a set of
                mutually disjoint subsets of $X$. By the
                axiom of choice there is a function
                $F:{f(X)}\rightarrow{X}$
                such that for all ${y}\in{f(X)}$,
                ${F(y)}\in{f^{-1}(\{y\})}$. But since these
                sets are disjoint, $F$ is injective.
                Thus, $f(X)$ is equivalent to a subset of $X$.
                Therefore, $\Card(f(X))\leq\Card(X)$.
            \end{proof}
            The Schr\"{o}der-Bernstein theorem can be restated
            equivalently as ``If $A$ is equivalent to a subset
            of $B$ and $B$ is equivalent to a subset of $A$,
            then $A$ is equivalent to $B$.''
            Addition and multiplication of finite cardinals
            follows directly from the standard arithmetic
            for the natural numbers. For cardinals of infinite
            sets, the arithmetic becomes a little more complicated.
            \begin{definition}
                The sum of two cardinal numbers is the
                cardinality of the union of two disjoint sets $A$
                and $B$. That is, if ${A}\cap{B}=\emptyset$, then
                $\Card(A)+\Card(B)=\Card({A}\cup{B})$.
            \end{definition}
            \begin{theorem*}
                If $a$ and $b$ are distinct cardinal numbers,
                then there exists sets $A$ and $B$ such that
                ${A}\cap{B}=\emptyset$, $\Card(A)=a$, and
                $\Card(B)=b$.
            \end{theorem*}
            \begin{theorem*}
                If $A,B,C,$ and $D$ are sets such that
                $\Card(A)=\Card(C)$, $\Card(B)=\Card(D)$,
                and if ${A}\cap{B}=\emptyset$ and
                ${C}\cap{D}=\emptyset$, then
                $\Card({A}\cup{B})=\Card({C}\cup{D})$.
            \end{theorem*}
            \begin{theorem*}
                If $x,y,$ and $z$ are cardinal numbers, then
                $x+y=y+x$ and $x+(y+z)=(x+y)+z$.
            \end{theorem*}
            \begin{notation}
                The carinality of the set of natural numbers
                is denoted $\aleph_{0}$. That is,
                $\Card(\mathbb{N})=\aleph_{0}$
            \end{notation}
            \begin{example}
                Find the cardinal sum of $2$ and $5$. Let
                $N_{2}=\{1,2\}$ and $N_{5}=\{3,4,5,6,7\}$.
                Then $N_{2}$ and $N_{5}$ are disjoint,
                $\Card(N_{2})=2$ and $\Card(N_{5})=5$.
                Therefore $2+5=\Card(N_{2}\cup{N_{5}})$.
                But ${N_{2}}\cup{N_{5}}$ is just $\mathbb{Z}_{7}$,
                and $\Card(\mathbb{Z}_{7})=7$. Thus, $2+5=7$.
            \end{example}
            \begin{theorem*}
                If $n$ and $m$ are finite cardinalities,
                then the cardinal sum of $n$ and $m$ is the
                integer $n+m$, where $+$ is the usual
                arithmetic addition.
            \end{theorem*}
            \begin{example}
                Compute the cardinal sum
                $\aleph_{0}+\aleph_{0}$. Let
                $\mathbb{N}_{e}$ be the set of even natural
                numbers, and let $\mathbb{N}_{o}$ be the set
                of odd natural numbers. Then
                $\Card(\mathbb{N}_{e})=\aleph_{0}$,
                $\Card(\mathbb{N}_{o})=\aleph_{0}$, and
                ${\mathbb{N}_{o}}\cap{\mathbb{N}_{e}}=\emptyset$.
                Thus
                $\aleph_{0}+\aleph_{0}%
                 =\Card({\mathbb{N}_{o}}\cup{\mathbb{N}_{e}})$.
                But
                ${\mathbb{N}_{o}}\cup{\mathbb{N}_{e}}%
                 =\mathbb{N}$ and $\Card(\mathbb{N})=\aleph_{0}$.
                Therefore, $\aleph_{0}+\aleph_{0}=\aleph_{0}$.
            \end{example}
            \begin{example}
                Find $n+\aleph_{0}$, where $n\in\mathbb{N}$.
                We have that
                $\Card(\mathbb{Z}_{n}z)=n$ and
                $\Card(\mathbb{N}\setminus\mathbb{Z}_{n})%
                 =\aleph_{0}$
                But then
                $n+\aleph_{0}=%
                 \Card(\mathbb{Z}_{n}\cup%
                 \mathbb{N}\setminus\mathbb{Z}_{n})%
                 =\Card(\mathbb{N})=\aleph_{0}$.
                Therefore, $n+\aleph_{0}=\aleph_{0}$.
            \end{example}
            \begin{definition}
                The cardinality of the continuum,
                denoted $\mathfrak{c}$, is the
                cardinality of the set of real numbers.
                That is, $\mathfrak{c}=\Card(\mathbb{R})$.
            \end{definition}
            \begin{theorem*}
                $\Card([0,1])=\mathfrak{c}$.
            \end{theorem*}
            \begin{theorem*}
                $\Card\big((0,1)\big)=\mathfrak{c}$.
            \end{theorem*}
            \begin{theorem*}
                $\mathbb{R}$ is uncountable. That is,
                $\mathfrak{c}>\aleph_{0}$.
            \end{theorem*}
            \begin{theorem*}
                $\mathfrak{c}+\aleph_{0}=\mathfrak{c}$.
            \end{theorem*}
            \begin{proof}
                We have $\Card((0,1))=\mathfrak{c}$ and
                $\Card(\mathbb{N})=\aleph_{0}$. But
                $(0,1)\cap\mathbb{N}=\emptyset$, and thus
                $\aleph_{0}+\mathfrak{c}%
                 =\Card((0,1)\cup\mathbb{N})$.
                But $\mathbb{R}\sim(0,1)$ and
                $\mathbb{N}\cup(0,1)\subset\mathbb{R}$.
                By the Schr\"{o}der-Bernstein theorem,
                $\mathbb{N}\cup(0,1)\sim\mathbb{R}$.
                Therefore, etc.
            \end{proof}
            \begin{definition}
                The product of two cardinal numbers $a$ and $b$
                is the cardinality of the cartesian product
                of two set $A$ and $B$ such that
                $\Card(A)=a$ and $\Card(B)=b$. That is,
                ${a}\times{b}=\Card({A}\times{B})$.
            \end{definition}
            \begin{theorem*}
                The following are true of cardinal numbers:
                \begin{enumerate}
                    \begin{multicols}{3}
                        \item $xy=yx$
                        \item $x(yz)=(xy)z$
                        \item $x(y+z)=xy+xz$
                    \end{multicols}
                \end{enumerate}
            \end{theorem*}
            \begin{proof}[Proof of Part 3]
                Let $A,B,$ and $C$ be disjoint.
                Then
                ${A}\times{({B}\cup{C})}%
                 =({A}\times{B})\cup({A}\times{C})$, and thus
                $\Card({A}\times{({B}\cup{C})})%
                 =\Card(({A}\times{B})\cup({A}\times{C}))$.
                But ${A}\times{B}$ and ${A}\times{C}$ are disjoint.
                Thus we have
                $\Card(({A}\times{B})\cup({A}\times{C}))%
                 =\Card({A}\times{B})+\Card({A}\times{C})$.
                Therefore, etc.
            \end{proof}
            \begin{theorem*}
                If $\Card(T)=x$ and
                $F:{T}\rightarrow{\mathcal{P}(T)}$
                is a set-valued mapping such that for all
                ${t}\in{T}$ we have that
                $\Card(F(t))=y$ and
                for all ${t}\ne{t}$,
                ${F(t)}\cap{F(t')}=\emptyset$, then
                $\Card(\cup_{t=1}^{N}F(t))=xy$
            \end{theorem*}
            \begin{example}
                Let $f:{\mathbb{N}^{2}}\rightarrow{\mathbb{N}}$
                be defined by $f(n,m)=2^{n}3^{m}$.
                Then $f$ is injective, since $2$ and $3$
                are coprime. Therefore,
                $\aleph_{0}\times\aleph_{0}=\aleph_{0}$.
            \end{example}
            \begin{example}
                Show that $\mathbb{R}^{2}\sim\mathbb{R}$.
                Let $f:\mathbb{R}^{2}\rightarrow\mathbb{R}$
                be the rather bizarre function defined by the image
                $f(x_{0}.x_{1}x_{2}\hdots,y_{0}.y_{1}y_{2}\hdots)%
                 =x_{0}y_{0}.x_{0}y_{0}x_{1}y_{1}\hdots$ Then
                $f$ is inective. But the mapping
                $g:\mathbb{R}\rightarrow\mathbb{R}^{2}$
                defined by $g(x)=(x,0)$ is also injective.
                By Schr\"{o}der-Bernstein,
                $\mathbb{R}^{2}\sim\mathbb{R}$.
            \end{example}
             \begin{definition}
                Order isomorphic set are two sets $A$ and $B$
                with well orders $<_{A}$ and $<_{B}$ such that
                there exists a bijection $f:{B}A\rightarrow{B}$
                such that for all $a_{1},a_{2}\in{A}$ such that
                $a_{1}<_{A}a_{2}$, $f(a_{1})<_{B}f(a_{2})$.
             \end{definition}
             \begin{theorem*}
                Order-Isomorphism is an equivalence relation.
             \end{theorem*}
             To every well ordered set, an ordinal number is
             assigned, denoted $\Ord(A,<_{A})$. Conversely,
             for every ordinal number there is a set with a
             well order corresponding to it. Two ordinal numbers
             are equal if and only if the well-ordered sets
             corresponding to them are order isomorphic.
             That is,
             $\Ord(A,<_{A})=\Ord(B,<_{B})$ if and only if
             $(A,<_{A})$ and $(B,<_{B})$ are order isomorphic.
             \begin{theorem*}
                If $(A,<_{A})$ and $(B,<_{B})$ are well ordered
                sets, and if $\Card(A)=\Card(B)$, then
                $(A,<_{A})$ and $(B,<_{B})$ are order
                isomorphic.
             \end{theorem*}
             The ordinal number of the empty set is $0$. The
             ordinal number of a finite set of $n$ elements with
             a well ordering is denoted $n\in\mathbb{N}$.
             The ordinal for the natural numbers $\mathbb{N}$
             with their usual well-ordering is denoted $\omega$.
             A given well-ordered set has only one cardinal number,
             but it is possible for it to have two ordinal numbers.
             \begin{definition}
                An ordinal number $\alpha$ is less than or equal
                to an ordinal number $\beta$ if there are
                well-ordered sets $(A,<_{A})$ and $(B,<_{B})$
                such that $\alpha=\Ord((A,<_{A}))$ and
                $\beta=\Ord(B,<_{B})$, and $(A,<_{B})$ is
                order isomorphic to subset of
                $(B,<_{B})$.
             \end{definition}
             \begin{theorem*}
                The only order isomorphism from a well ordered
                set $(A,<_{A})$ to itself is the identity
                isomorphism.
             \end{theorem*}
             \begin{theorem*}
                If $\alpha$ and $\beta$ are ordinal numbers and
                ${\alpha}\leq{\beta}$ and ${\beta}\leq{\alpha}$,
                then $\alpha=\beta$.
             \end{theorem*}
             \begin{theorem*}
                If $\alpha$ and $\beta$ are ordinal numbers,
                either ${\alpha}\leq{\beta}$, or
                ${\beta}\leq{\alpha}$.
             \end{theorem*}
             \begin{theorem*}
                If $\alpha$ and $\beta$ are ordinal numbers,
                either $\alpha<\beta$, $\beta<\alpha$, or
                $\alpha=\beta$.
             \end{theorem*}
             \begin{definition}
                The total ordering relation of a
                well-ordered set $(A,<_{A})$ with respect
                to a well-ordered set $(B,<_{B})$ is the ordering
                on the set ${A}\cup{B}$ defined as: For all
                $a_{1},a_{2}\in{A}$ such that $a_{1}<_{A}a_{2}$,
                $a_{1}<_{*}a_{2}$, for all $b_{1},b_{2}\in{B}$
                such that $b_{1}<_{B}b_{2}$, $b_{1}<_{*}b_{2}$,
                and for all ${a}\in{A}$ and ${b}\in{B}$,
                ${a}<_{*}{b}$.
             \end{definition}
             \begin{theorem*}
                The total ordering relation $<_{*}$ on the set
                ${A}\cup{B}$ is a well-ordering.
             \end{theorem*}
             \begin{definition}
                The ordinal sum of two ordinal numbers
                $\Ord((A,<_{A}))$ and $\Ord((B,<_{B}))$,
                where $A$ and $B$ are disjoint,
                is the ordinal number
                $\Ord(({A}\cup{B},<_{*}))$.
             \end{definition}
             \begin{theorem*}
                The following are true of ordinal numbers:
                \begin{enumerate}
                    \begin{multicols}{3}
                        \item $\alpha<\beta\Rightarrow%
                               \alpha+\gamma<\beta+\gamma$
                        \item $(\alpha+\beta)+\gamma%
                               =\alpha+(\beta+\gamma)$
                        \item $\alpha+\beta=\alpha+\gamma%
                               \Rightarrow\beta=\gamma$.
                    \end{multicols}
                \end{enumerate}
             \end{theorem*}
             \begin{definition}
                The lexicographic ordering on the cartesian
                product of well ordered set $(A,<_{A})$ and
                $(B,<_{B})$ is the ordering on
                ${A}\times{B}$ defined by: If ${a}<_{A}{x}$,
                then $(a,b)<_{*}(x,y)$ for all $b,y\in{B}$, and
                if $a=x$ and $b<_{B}y$, then $(a,b)<_{*}(x,y)$.
             \end{definition}
             \begin{theorem*}
                If $(A,<_{A})$ and $(B,<_{B})$ are well ordered
                sets, then the lexicographic ordering
                on ${A}\times{B}$ is a well ordering.
             \end{theorem*}
             \begin{definition}
                The ordinal product of two ordinal numbers
                $\Ord((A,<_{A}))$ and $\Ord((B,<_{B}))$,
                is $\Ord(({A}\times{B},<_{*}))$
             \end{definition}
             \begin{theorem*}
                The following are true of ordinal numbers:
                \begin{enumerate}
                    \begin{multicols}{2}
                        \item $\alpha(\beta\gamma)%
                               =(\alpha\beta)\gamma$
                        \item $\alpha(\beta+\gamma)%
                               =\alpha\beta+\alpha\gamma$
                    \end{multicols}
                \end{enumerate}
             \end{theorem*}
             \begin{definition}
                Relatively prime integers are integers
                $a,b\in\mathbb{N}$ such that $\gcd(a,b)=1$.
             \end{definition}
             \begin{theorem*}
                If $p$ is prime and $a\in\mathbb{N}$ is
                such that $p$ does not divide $a$, then $a$ and $p$
                are relatively prime.
             \end{theorem*}
             \begin{theorem*}
                There are infinitely many prime numbers.
             \end{theorem*}
             \begin{theorem*}
                If $a\in\mathbb{N}$, $a>1$, then either
                $a$ is a prime number, or $a$ is the product
                of finitely many primes.
             \end{theorem*}
             \begin{theorem*}
                If $a\in\mathbb{N}$, $a>1$, and if $a$ is not
                prime, then the prime expansion of $a$ is
                unique.
             \end{theorem*}
             \begin{definition}
                A diophantine equation is an equation whose
                solutions are required to be integers.
             \end{definition}
             \begin{definition}
                A linear diophantine equation in two variables
                $x$ and $y$ is an equation
                $ax+by=c$, where $a,b,c\in\mathbb{Z}$.
             \end{definition}
             \begin{theorem*}
                If $a,b,c\in\mathbb{Z}$ $d=\gcd(a,b)$,
                and if $d$ does not divide $c$,
                then $ax+by=c$ has no integral solutions.
             \end{theorem*}
             \begin{theorem*}
                If $a,b,c\in\mathbb{Z}$ $d=\gcd(a,b)$,
                and if $d$ divides $c$,
                then $ax+by=c$ has infinitely many solutions.
             \end{theorem*}
\end{document}