%-----------------------------------LICENSE------------------------------------%
%   This file is part of Mathematics-and-Physics.                              %
%                                                                              %
%   Mathematics-and-Physics is free software: you can redistribute it and/or   %
%   modify it under the terms of the GNU General Public License as             %
%   published by the Free Software Foundation, either version 3 of the         %
%   License, or (at your option) any later version.                            %
%                                                                              %
%   Mathematics-and-Physics is distributed in the hope that it will be useful, %
%   but WITHOUT ANY WARRANTY; without even the implied warranty of             %
%   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the              %
%   GNU General Public License for more details.                               %
%                                                                              %
%   You should have received a copy of the GNU General Public License along    %
%   with Mathematics-and-Physics.  If not, see <https://www.gnu.org/licenses/>.%
%------------------------------------------------------------------------------%
\documentclass{article}
\usepackage{graphicx}                           % Needed for figures.
\usepackage{amsmath}                            % Needed for align.
\usepackage{amssymb}                            % Needed for mathbb.
\usepackage{amsthm}                             % For the theorem environment.
\usepackage[font=scriptsize,
            labelformat=simple,
            labelsep=colon]{subcaption} % Subfigure captions.
\usepackage[font={scriptsize},
            hypcap=true,
            labelsep=colon]{caption}    % Figure captions.
\usepackage{hyperref}
\hypersetup{colorlinks=true, linkcolor=blue}
\renewcommand\thesubfigure{\arabic{figure}.\arabic{subfigure}}

% Define theorem style for default spacing and normal font.
\newtheoremstyle{normal}
    {\topsep}               % Amount of space above the theorem.
    {\topsep}               % Amount of space below the theorem.
    {}                      % Font used for body of theorem.
    {}                      % Measure of space to indent.
    {\bfseries}             % Font of the header of the theorem.
    {}                      % Punctuation between head and body.
    {.5em}                  % Space after theorem head.
    {}

% Define default environments.
\theoremstyle{normal}
\newtheorem{examplex}{Example}

\newenvironment{example}{%
    \pushQED{\qed}\renewcommand{\qedsymbol}{$\blacksquare$}\examplex%
}{%
    \popQED\endexamplex%
}

\title{Advanced Calculus: Tutorial 1}
\author{Ryan Maguire}
\date{\today}

% No indent and no paragraph skip.
\setlength{\parindent}{0em}
\setlength{\parskip}{0em}

\begin{document}
    \maketitle
    \begin{abstract}
        The following notes are from my tutorials for advanced calculus during
        the spring 2021 semester at Dartmouth college. The nature of tutorial
        sessions makes the handwritten notes very scattered as students ask
        questions about different aspects of the course. These typed notes
        reflect this scattering of information as well.
    \end{abstract}
    \begin{example}[\textbf{Basics of Linear Functions}]
        A linear function $f:\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}$ is a
        function that respects vector addition. Formally this means that if
        $\mathbf{x}_{1},\mathbf{x}_{2}\in\mathbb{R}^{n}$ are vectors, then
        \begin{equation}
            f(\mathbf{x}_{1}+\mathbf{x}_{2})
            =f(\mathbf{x}_{1})+f(\mathbf{x}_{2})
        \end{equation}
        Moreover it respects scalar multiplication. That is, if
        $a\in\mathbb{R}$ and $\mathbf{x}\in\mathbb{R}^{n}$, then:
        \begin{equation}
            f(a\cdot\mathbf{x})=a\cdot{f}(\mathbf{x})
        \end{equation}
        The multiplication symbol is often omitted in the notation, and this
        may be rewritten as:
        \begin{equation}
            f(a\mathbf{x})=a\,f(\mathbf{x})
        \end{equation}
        Another way to put this uses bases. If
        $\{\,\mathbf{e}_{1},\,\cdots,\,\mathbf{e}_{n}\,\}$ is a basis for
        $\mathbb{R}^{n}$ and if $\mathbf{x}\in\mathbb{R}^{n}$ is given by:
        \begin{equation}
            \mathbf{x}=\sum_{k=1}^{n}a_{k}\mathbf{e}_{k}
        \end{equation}
        where $a_{k}\in\mathbb{R}$ are scalars, then:
        \begin{equation}
            f(\mathbf{x})
            =f\Big(\sum_{k=1}^{n}a_{k}\mathbf{e}_{k}\Big)
            =\sum_{k=1}^{n}a_{k}\,f(\mathbf{e}_{k})
        \end{equation}
        Linear functions $f:\mathbb{R}\rightarrow\mathbb{R}$ are just functions
        of the form $f(x)=ax$. We can prove this using elementary calculus.
        Let $a=f(1)$, the value of $f$ at $x=1$. We have:
        \begin{align}
            \frac{f(x+h)-f(x)}{h}
            &=\frac{f(x)+f(h)-f(x)}{h}
            \tag{Linearity}\\
            &=\frac{f(h)}{h}
            \tag{Simplify}\\
            &=\frac{h\,f(1)}{h}
            \tag{Linearity}\\
            &=\frac{ah}{h}
            \tag{Definition of $a$}\\
            &=a
            \tag{Simplify}
        \end{align}
        If we take limits we see that $f'(x)=a$ for all $x\in\mathbb{R}$.
        Integration tells us $f(x)=ax+b$ for some $b\in\mathbb{R}$. We now show
        that $b=0$. We have:
        \begin{equation}
            f(0)=f(0\cdot{0})=0\cdot{f}(0)=0
        \end{equation}
        And hence $f(0)=0$. Since $f(x)=ax+b$, this tells us that $b=0$. So the
        equation for $f$ is $f(x)=ax$.
    \end{example}
    \begin{example}[\textbf{Basis of a Vector Space}]
        A basis for $\mathbb{R}^{n}$ is a set of
        \textit{linearly independent vectors}
        $\{\,\mathbf{e}_{1},\,\cdots,\,\mathbf{e}_{n}\,\}$ that
        \textit{span} the entirety of $\mathbb{R}^{n}$. That is, every
        vector $\mathbf{x}\in\mathbb{R}^{n}$ can be uniquely written as:
        \begin{equation}
            \mathbf{x}=\sum_{k=1}^{n}a_{k}\mathbf{e}_{k}
        \end{equation}
        for (unique) scalars $a_{1},\,\dots,\,a_{n}$. As a simple example,
        consider $\mathbb{R}^{2}$ and define
        $\mathbf{e}_{1}=(1,\,0)$ and $\mathbf{e}_{2}=(0,\,1)$. Then given a
        vector $\mathbf{x}=(x_{1},\,x_{2})$ we can write
        $\mathbf{x}=x_{1}\mathbf{e}_{1}+x_{2}\mathbf{e}_{2}$.
    \end{example}
    \begin{example}[\textbf{Linear Functions Defined Using Bases}]
        A linear function is uniquely determined by its behavior on a basis.
        That is, if $f:\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}$ is a linear
        function, and if we know the value of $f(\mathbf{e}_{k})$ for all
        $\mathbf{e}_{k}$ in a basis set
        $\{\,\mathbf{e}_{1},\,\cdots,\,\mathbf{e}_{n}\,\}$, then we can compute
        $f(\mathbf{x})$ for any $\mathbf{x}\in\mathbb{R}^{n}$. That is, if we
        know the vectors $\mathbf{v}_{k}\in\mathbb{R}^{m}$ such that
        $f(\mathbf{e}_{k})=\mathbf{v}_{k}$, we know all there is to know about
        the function. For let $\mathbf{x}\in\mathbb{R}^{n}$. Then there are
        (unique) scalars $a_{k}\in\mathbb{R}$ such that:
        \begin{equation}
            \mathbf{x}=\sum_{k=1}^{n}a_{k}\mathbf{e}_{k}
        \end{equation}
        This is the definition of a basis. Since $f$ is linear, and since we
        know $f(\mathbf{e}_{k})=\mathbf{v}_{k}$ for each $k$, we have:
        \begin{equation}
            f(\mathbf{x})
            =f\Big(\sum_{k=1}^{n}a_{k}\mathbf{e}_{k}\Big)
            =\sum_{k=1}^{n}a_{k}\,f(\mathbf{e}_{k})
            =\sum_{k=1}^{n}a_{k}\mathbf{v}_{k}
        \end{equation}
        allowing us to compute $f(\mathbf{x})$ for any arbitrary vector
        $\mathbf{x}\in\mathbb{R}^{n}$.
    \end{example}
    \begin{example}[\textbf{Reflections}]
        A reflection $f:\mathbb{R}^{n}\rightarrow\mathbb{R}^{n}$ (note the
        domain and co-domain are the same here) about a \textbf{non-zero}
        vector $\mathbf{v}\in\mathbb{R}^{n}$ is the linear function defined by:
        \begin{equation}
            f(\mathbf{x})
            =\mathbf{x}-2
            \frac{\langle\,\mathbf{x}\,|\,\mathbf{v}\,\rangle}
                 {\langle\,\mathbf{v}\,|\,\mathbf{v}\,\rangle}
            \mathbf{v}
        \end{equation}
        where $\langle\,\mathbf{x}\,|\,\mathbf{v}\,\rangle$ denotes the
        standard Euclidean inner product, also called the Euclidean
        \textit{dot product}, between vectors. To be explicit, if
        $\mathbf{x}=(x_{1},\,\dots,\,x_{n})$ and
        $\mathbf{y}=(y_{1},\,\dots,\,y_{n})$, then we have:
        \begin{equation}
            \langle\,\mathbf{x}\,|\,\mathbf{y}\,\rangle
            =\sum_{k=1}^{n}x_{k}y_{k}
        \end{equation}
        Let's derive this. A reflection should negate any vector parallel to
        the vector we are reflecting. This is $\mathbf{v}$ in our example.
        That is, $f(\mathbf{x})=-\mathbf{x}$ for any vector $\mathbf{x}$ that
        is a scalar multiple of $\mathbf{v}$. Secondly, any vector that is
        perpendicular to $\mathbf{v}$ should be untouched. Think of the plane
        $\mathbb{R}^{2}$ reflecting the $x$ axis. The positive $x$ axis is
        flipped to the negative $x$ axis, and the $y$ axis remains stationary.
        \par\hfill\par
        The set of all vectors perpendicular to $\mathbf{v}$ forms a
        \textit{vector subspace} of $\mathbb{R}^{n}$. Moreover, a basis
        $\{\,\mathbf{e}_{1},\,\cdots,\,\mathbf{e}_{n-1}\,\}$ for this subspace,
        together with $\mathbf{v}$, forms a basis for all of $\mathbb{R}^{n}$.
        Let's write this as $\{\,\mathbf{e}_{1},\,\cdots,\,\mathbf{e}_{n}\,\}$
        where we define $\mathbf{e}_{n}=\mathbf{v}$. Our previous conditions
        now translate to a formula for the basis elements:
        \begin{equation}
            f(\mathbf{e}_{k})=
            \begin{cases}
                \mathbf{e}_{k}&k\ne{n}\\
                -\mathbf{e}_{k}&k=n
            \end{cases}
        \end{equation}
        Given $\mathbf{x}\in\mathbb{R}$ we may write:
        \begin{equation}
            \mathbf{x}=\sum_{k=1}^{n}a_{k}\mathbf{e}_{k}
        \end{equation}
        for appropriate scalars $a_{k}$. Since $f$ is linear, and since we know
        how it works on a basis, we have:
        \begin{equation}
            f(\mathbf{x})=\sum_{k=1}^{n}a_{k}\,f(\mathbf{e}_{k})
            =\Big(\sum_{k=1}^{n-1}a_{k}\mathbf{e}_{k}\Big)-a_{n}\mathbf{e}_{n}
        \end{equation}
        Very close to the original formula for $\mathbf{x}$. Let us add back
        the minus component (and then subtract it out so as to not change the
        formula). We get:
        \begin{align}
            f(\mathbf{x})
            &=\Big(
                \sum_{k=1}^{n-1}a_{k}\mathbf{e}_{k}
            \Big)-a_{n}\mathbf{e}_{n}\\
            &=\Big(
                \sum_{k=1}^{n-1}a_{k}\mathbf{e}_{k}
            \Big)+(a_{n}\mathbf{e}_{n}-a_{n}\mathbf{e}_{n})
            -a_{n}\mathbf{e}_{n}\\
            &=\Big(
                \sum_{k=1}^{n}a_{k}\mathbf{e}_{k}
            \Big)-2a_{n}\mathbf{e}_{n}\\
            &=\mathbf{x}-2a_{n}\mathbf{e}_{n}
        \end{align}
        We'll complete our formula if we can rewrite $2a_{n}\mathbf{e}_{n}$ in
        a different manner. Remember that $\mathbf{e}_{n}$ is
        orthogonal to $\mathbf{e}_{k}$ for all $k\ne{n}$. From the linearity of
        the inner product we get:
        \begin{align}
            \langle\,\mathbf{x}\,|\,\mathbf{e}_{n}\,\rangle
            &=
            \langle\,
                \sum_{k=1}^{n}a_{k}\mathbf{e}_{k}\,|\,\mathbf{e}_{k}
            \rangle\\
            &=
            \sum_{k=1}^{n}\langle\,
                a_{k}\mathbf{e}_{k}\,|\,\mathbf{e}_{k}
            \rangle\\
            &=\sum_{k=1}^{n}a_{k}\langle\,
                \mathbf{e}_{k}\,|\,\mathbf{e}_{k}
            \rangle\\
            &=a_{n}\langle\,\mathbf{e}_{n}\,|\,\mathbf{e}_{n}\,\rangle
        \end{align}
        We hence obtain:
        \begin{equation}
            a_{n}=\frac{\langle\,\mathbf{x}\,|\,\mathbf{e}_{n}\,\rangle}
                       {\langle\,\mathbf{e}_{n}\,|\,\mathbf{e}_{n}\,\rangle}
        \end{equation}
        Our previous formula then becomes:
        \begin{equation}
            f(\mathbf{x})
            =\mathbf{x}-2
            \frac{\langle\,\mathbf{x}\,|\,\mathbf{e}_{n}\,\rangle}
                 {\langle\,\mathbf{e}_{n}\,|\,\mathbf{e}_{n}\,\rangle}
            \mathbf{e}_{n}
        \end{equation}
        Recalling that we set $\mathbf{v}=\mathbf{e}_{n}$, we obtain:
        \begin{equation}
            f(\mathbf{x})
            =\mathbf{x}-2
            \frac{\langle\,\mathbf{x}\,|\,\mathbf{v}\,\rangle}
                 {\langle\,\mathbf{v}\,|\,\mathbf{v}\,\rangle}
            \mathbf{v}
        \end{equation}
        Precisely as claimed.
    \end{example}
    \begin{example}[\textbf{Reflections in the Plane}]
        Suppose $f:\mathbb{R}^{2}\rightarrow\mathbb{R}^{2}$ is a reflection of
        the plane. So it is linear, and there is some \textit{unit} vector
        $\mathbf{u}\in\mathbb{R}^{2}$ such that
        $f(\mathbf{u})=-\mathbf{u}$ and
        $f(\mathbf{u}^{\perp})=\mathbf{u}^{\perp}$. If
        $\mathbf{u}=(x,\,y)$, we can write down $\mathbf{u}^{\perp}=(-y,\,x)$,
        which follows the right hand rule in the plane. This is indeed
        orthogonal:
        \begin{equation}
            \langle\mathbf{u}\,|\,\mathbf{u}^{\perp}\rangle
            =\langle(x,\,y)\,|\,(-y,\,x)\rangle
            =-xy+yx
            =0
        \end{equation}
        Moreover, $\mathbf{u}$ and $\mathbf{u}^{\perp}$ form a \textit{basis}
        for $\mathbb{R}^{2}$. Given $(\alpha,\,\beta)\in\mathbb{R}^{2}$
        we can write:
        \begin{align}
            \langle
                \mathbf{u}\,|\,(\alpha,\,\beta)
            \rangle\mathbf{u}+
            \langle
                \mathbf{u}^{\perp}\,|\,(\alpha,\,\beta)
            \rangle\mathbf{u}^{\perp}
            &=
            (\alpha{x}+\beta{y})\mathbf{u}+
                (-\alpha{y}+\beta{x})\mathbf{u}^{\perp}\\
            &=(\alpha{x}+\beta{y})(x,\,y)+(-\alpha{y}+\beta{x})(-y,\,x)
        \end{align}
        The two components expand to become:
        \begin{align}
            \alpha{x}^{2}+\beta{xy}+\alpha{y^{2}}-\beta{xy}
            &=\alpha(x^{2}+y^{2})\\
            \alpha{xy}+\beta{y}^{2}-\alpha{xy}+\beta{x}^{2}
            &=\beta(x^{2}+y^{2})
        \end{align}
        Now we recall that $\mathbf{u}$ is a \textit{unit vector}, and hence:
        \begin{equation}
            ||\mathbf{u}||^{2}=||(x,\,y)||^{2}=x^{2}+y^{2}=1
        \end{equation}
        and so $\alpha(x^{2}+y^{2})=\alpha$ and $\beta(x^{2}+y^{2})=\beta$.
        Piecing all of this together, we have:
        \begin{equation}
            (\alpha{x}+\beta{y})\mathbf{u}+
                (-\alpha{y}+\beta{x})\mathbf{u}^{\perp}
            =(\alpha,\,\beta)
        \end{equation}
        confirming our claim that $\mathbf{u}$ and $\mathbf{u}^{\perp}$ span
        $\mathbb{R}^{2}$. Since $f:\mathbb{R}^{2}\rightarrow\mathbb{R}^{2}$ is
        linear, and since we know how it behaves on a basis, we know everything
        there is to know about the function. Given $\mathbf{x}\in\mathbb{R}^{2}$
        we can write:
        \begin{equation}
            \mathbf{x}=a\mathbf{u}+b\mathbf{u}^{\perp}
        \end{equation}
        for some scalars $a,b\in\mathbb{R}$. We have the following constraints:
        \begin{align}
            f(\mathbf{u})&=-\mathbf{u}\\
            f(\mathbf{u}^{\perp})&=\mathbf{u}^{\perp}
        \end{align}
        Since $f$ is linear we then know that:
        \begin{align}
            f(\mathbf{x})
            &=f(a\mathbf{u}+b\mathbf{u}^{\perp})\\
            &=a\,f(\mathbf{u})+b\,f(\mathbf{u}^{\perp})\\
            &=-a\mathbf{u}+b\mathbf{u}^{\perp}
        \end{align}
        Now we manipulate this final expression to make it look a little nicer.
        We have:
        \begin{align}
            -a\mathbf{u}+b\mathbf{u}^{\perp}
            &=-a\mathbf{u}+(-a\mathbf{u}+a\mathbf{u})+b\mathbf{u}^{\perp}\\
            &=-2a\mathbf{u}+(a\mathbf{u}+b\mathbf{u}^{\perp})\\
            &=-2a\mathbf{u}+\mathbf{x}
        \end{align}
        Since $\mathbf{u}$ and $\mathbf{u}^{\perp}$ are orthogonal, we have:
        \begin{align}
            \langle\mathbf{x}\,|\,\mathbf{u}\rangle
            &=\langle
                a\mathbf{u}+b\mathbf{u}^{\perp}\,|\,\mathbf{u}
            \rangle\\
            &=a\,\langle\mathbf{u}\,|\,\mathbf{u}\rangle
                +b\,\langle\mathbf{u}\,|\,\mathbf{u}^{\perp}\rangle\\
            &=a\,\langle\mathbf{u}\,|\,\mathbf{u}\rangle\\
            &=a\,||\mathbf{u}||^{2}\\
            &=a
        \end{align}
        With this we can write our reflection formula as:
        \begin{equation}
            f(\mathbf{x})
            =\mathbf{x}-2\,\langle\mathbf{x}\,|\,\mathbf{u}\rangle\,\mathbf{u}
        \end{equation}
        This discussion assumed that $\mathbf{u}$ is a \textit{unit vector}.
        If it is not, then the argument needs to be modified by
        \textit{normalizing} the vector. This yields the following:
        \begin{equation}
            f(\mathbf{x})
            =\mathbf{x}-
            2\frac{\langle\mathbf{x}\,|\,\mathbf{u}\rangle}{||\mathbf{u}||^{2}}
            \mathbf{u}
        \end{equation}
    \end{example}
    \begin{example}[\textbf{Reflection Matrix}]
        Every linear function $f:\mathbb{R}^{n}\rightarrow\mathbb{R}^{n}$ can
        be represented by an $n\times{n}$ matrix $A$ with real-valued entries
        in such a way so that $f(\mathbf{x})=A\mathbf{x}$ for every vector
        $\mathbf{x}\in\mathbb{R}^{n}$. The nature of matrix multiplication
        makes this construction very explicit. If we have a basis
        $\{\,\mathbf{e}_{1},\,\dots,\,\mathbf{e}_{n}\,\}$ for $\mathbb{R}^{n}$,
        and if $f:\mathbb{R}^{n}\rightarrow\mathbb{R}^{n}$ is linear, then we
        know $f$ is determined uniquely by what it does to the basis set.
        Suppose we know the vectors $\mathbf{v}_{k}\in\mathbb{R}^{n}$ such that
        $f(\mathbf{e}_{k})=\mathbf{v}_{k}$ for each $k=1,\,\cdots,\,n$.
        Since we want $f(\mathbf{x})=A\mathbf{x}$ for \textit{all} $\mathbf{x}$,
        it better be true for $\mathbf{x}=\mathbf{e}_{k}$. This says:
        \begin{equation}
            f(\mathbf{e}_{k})
            =A\mathbf{e}_{k}
            =\begin{bmatrix}
                a_{1,\,1}&a_{2,\,1}&\cdots&a_{n-1,\,1}&a_{n,\,1}\\
                a_{1,\,2}&a_{2,\,2}&\cdots&a_{n-1,\,2}&a_{n,\,2}\\
                \vdots&\vdots&\ddots&\vdots&\vdots\\
                a_{1,\,n-1}&a_{2,\,n-1}&\cdots&a_{n-1,\,n-1}&a_{n,\,n-1}\\
                a_{1,\,n}&a_{2,\,n}&\cdots&a_{n-1,\,n}&a_{n,\,n}
            \end{bmatrix}
            \begin{bmatrix}
                0\\
                \vdots\\
                1\\
                \vdots\\
                0
            \end{bmatrix}
            =\mathbf{v}_{k}
        \end{equation}
        where the $1$ is in the $k^{th}$ slot of the column vector
        $\mathbf{e}_{k}$. The way matrix multiplication works tells us that
        $\mathbf{v}_{k}$ is just the $k^{\small\textrm{th}}$
        column of the matrix. That is, we can uniquely define $A$ via:
        \begin{align}
            A&=
            \begin{bmatrix}
                f(\mathbf{e}_{1})&\cdots&f(\mathbf{e}_{n})
            \end{bmatrix}\\
            &=
            \begin{bmatrix}
                \mathbf{v}_{1}&\cdots&\mathbf{v}_{n}
            \end{bmatrix}\\
            &=
            \begin{bmatrix}
                v_{1,\,1}&\cdots&v_{n,\,1}\\
                \vdots&\ddots&\vdots\\
                v_{1,\,n}&\cdots&v_{n,\,n}
            \end{bmatrix}
        \end{align}
        where $v_{k,\,m}$ is the $m^{th}$ entry of $\mathbf{v}_{k}$. That is,
        to form $A$ we simply make the vectors $\mathbf{v}_{k}$ into column
        vectors and stand them up next to each other. If
        $f:\mathbb{R}^{2}\rightarrow\mathbb{R}^{2}$ is a reflection about
        $\mathbf{u}$, then $f(\mathbf{u})=-\mathbf{u}$ and
        $f(\mathbf{u}^{\perp})=\mathbf{u}^{\perp}$. If
        $\mathbf{u}=(a,\,b)$ and $\mathbf{u}^{\perp}=(-b,\,a)$, the reflection
        matrix is then:
        \begin{equation}
            A=
            \begin{bmatrix}
                f(\mathbf{u})&f(\mathbf{u}^{\perp})
            \end{bmatrix}
            =
            \begin{bmatrix}
                -a&-b\\
                -b&a
            \end{bmatrix}
        \end{equation}
        We can form a similar matrix for reflections in $\mathbb{R}^{n}$ if we
        are given a basis of mutually orthogonal vectors. Indeed, we can
        perform this construction so long as we are given a basis for
        the orthogonal subspace for $\mathbf{u}$, together with
        $\mathbf{u}$ itself.
    \end{example}
    \begin{example}[\textbf{Euler's Formula}]
        Many spend their first year in calculus attempting to memorize all
        sorts of formulas. This is quite unfortunate, especially since many
        then think that calculus is just a master class in trigonometric
        identities. Students may then be surprised to learn that most
        mathematicians do \textit{not} know the trigonometric identities by
        heart, and that many have \textit{forgotten} them. How could this be!
        Well, there's a trick so that you never need to memorize these
        equations ever again. You just need to do some (fairly simple)
        arithmetic in your head.
        \par\hfill\par
        The imaginary unit is some object $i$ that satisfies $i^{2}=-1$. No
        \textit{real} number will do this, but that's fine. Now,
        if we know about Taylor series we may have seen the following:
        \begin{equation}
            \exp(x)=\sum_{n=0}^{\infty}\frac{1}{n!}x^{n}
        \end{equation}
        What happens if we plug in $x=i\theta$? We get:
        \begin{equation}
            \exp(i\theta)=\sum_{n=0}^{\infty}\frac{1}{n!}(i\theta)^{n}
            =\sum_{n=0}^{\infty}\frac{1}{n!}i^{n}\theta^{n}
        \end{equation}
        What does $i^{n}$ look like for integer values of $n$? Well,
        by definition, $i^{0}=1$ and $i^{1}=i$. We know that $i^{2}=-1$, and
        hence $i^{3}=i\cdot{i}^{2}=-i$. This then tells us that $i^{4}=1$ and
        then we loop back around. So $i^{n}$ cycles around in periods of 4.
        If $n$ is even we have:
        \begin{equation}
            i^{n}=i^{2k}=(i^{2})^{k}=(-1)^{k}
        \end{equation}
        If $n$ is odd we get:
        \begin{equation}
            i^{n}=i^{2k+1}=i^{2k}\cdot{i}=(-1)^{k}\cdot{i}
        \end{equation}
        Let's split our Taylor series about in to even and odd parts. We get:
        \begin{align}
            \exp(i\theta)
            &=\sum_{n=0}^{\infty}\frac{1}{n!}i^{n}\theta^{n}\\
            &=\sum_{n=0}^{\infty}\frac{(-1)^{n}}{(2n)!}\theta^{2n}
            +i\sum_{n=0}^{\infty}\frac{(-1)^{n}}{(2n+1)!}\theta^{2n+1}
        \end{align}
        But wait a minute, I recognize those sums! Those are the Taylor series
        for $\cos(\theta)$ and $\sin(\theta)$! So we have:
        \begin{equation}
            \exp(i\theta)=\cos(\theta)+i\sin(\theta)
        \end{equation}
        This is Euler's formula, and with it we can get just about every
        trigonometric identity you've ever cared about. Let's try to simplify
        the double angle formulas. If we double the angle in Euler's formula
        we get:
        \begin{equation}
            \exp(i2\theta)=\cos(2\theta)+i\sin(2\theta)
        \end{equation}
        But exponential rules say that
        $\exp(i2\theta)=\big(\exp(i\theta)\big)^{2}$. We then have:
        \begin{align}
            \cos(2\theta)+i\sin(2\theta)
            &=\Big(\cos(\theta)+i\sin(\theta)\Big)^{2}\\
            &=\cos^{2}(\theta)-\sin^{2}(\theta)+i2\cos(\theta)\sin(\theta)
        \end{align}
        where we have used the fact that $i^{2}=-1$. We compare the
        \textit{real} parts (the bits without $i$) and the
        \textit{imaginary} parts (those with $i$) and get:
        \begin{align}
            \cos(2\theta)&=\cos^{2}(\theta)-\sin^{2}(\theta)\\
            \sin(2\theta)&=2\cos(\theta)\sin(\theta)
        \end{align}
        Now you may say \textit{you want me to do this in my head?!?} It's
        easier than it looks. All we are doing is using \textit{FOIL} from
        high school (first-outside-inner-last). We have
        $\cos(n\theta)+i\sin(n\theta)$ on the left and
        $\big(\cos(\theta)+i\sin(\theta)\big)^{n}$ on the right. To get the
        desired formula we just need to FOIL out the right hand side.
        The real part is then $\cos(n\theta)$ and the imaginary part is
        $\sin(n\theta)$. Since we usually want $n=2$ or $n=3$ in our formula,
        with a bit of practice you can get this done very quickly. You can
        similarly work out the angle sum formula. We have:
        \begin{align}
            \cos(x+y)+i\sin(x+y)
            &=\exp\big(i(x+y)\big)\\
            &=\exp(ix)\exp(iy)\\
            &=\big(\cos(x)+i\sin(x)\big)\big(\cos(y)+i\sin(y)\big)\\
            &=\big(\cos(x)\cos(y)-\sin(x)\sin(y)\big)\nonumber\\
                &\hspace{2em}+i\big(\cos(x)\sin(y)+\sin(x)\cos(y)\big)
        \end{align}
        Comparing real and imaginary parts we get:
        \begin{align}
            \cos(x+y)&=\cos(x)\cos(y)-\sin(x)\sin(y)\\
            \sin(x+y)&=\cos(x)\sin(y)+\sin(x)\cos(y)
        \end{align}
        Now in this formula I always forget where the minus goes, and which
        formula is cos cos, sin sin and which is cos sin, sin cos. It'd be
        quite unfortunate to get the wrong answer on account of a simple slip
        up here. But Euler's formula comes to the rescue. It becomes straight
        forward to do this quickly in your head after some practice.
    \end{example}
    \begin{example}[\textbf{Change of Basis, Reflections Using Angles}]
        Suppose we want to reflect the plane about the angle $\theta$ made
        with the positive $x$ axis. That is, we want to reflect using
        unit vector $\mathbf{u}^{\perp}=\big(\cos(\theta),\,\sin(\theta)\big)$.
        The vector $\mathbf{u}$ we are reflecting about can be obtained by
        rotating this by $\pi/2$, which gives us
        $\mathbf{u}=\big(-\sin(\theta),\,\cos(\theta)\big)$. The reflection
        formula is:
        \begin{equation}
            f(\mathbf{x})=\mathbf{x}-2\langle\mathbf{x}\,|\,\mathbf{u}\rangle
                \mathbf{u}
        \end{equation}
        The representing matrix is given by $f(\mathbf{u})$ and
        $f(\mathbf{u}^{\perp})$. We have:
        \begin{equation}
            A=
            \begin{bmatrix}
                \sin(\theta)&\cos(\theta)\\
                -\cos(\theta)&\sin(\theta)
            \end{bmatrix}
        \end{equation}
        Let's do this computation again and see if we get the same answer.
        Let's see what the matrix does to
        $\hat{\mathbf{x}}=(1,\,0)$. We have:
        \begin{equation}
            A\hat{\mathbf{x}}
            =\begin{bmatrix}
                a&b\\
                c&d
            \end{bmatrix}
            \begin{bmatrix}
                1\\
                0
            \end{bmatrix}
            =\begin{bmatrix}
                a\\
                c
            \end{bmatrix}
        \end{equation}
        This is also equal to:
        \begin{equation}
            f(\hat{\mathbf{x}})
            =\hat{\mathbf{x}}-2\langle\,\hat{\mathbf{x}}\,|\,\mathbf{u}\,\rangle
                \mathbf{u}
        \end{equation}
        Comparing, we get:
        \begin{align}
            \begin{bmatrix}
                a\\
                c
            \end{bmatrix}
            &=\begin{bmatrix}
                1\\
                0
            \end{bmatrix}
            -2\langle\,(1,\,0)\,|\,\big(-\sin(\theta),\,\cos(\theta)\big)\,
            \rangle
            \begin{bmatrix}
                -\sin(\theta)\\
                \cos(\theta)
            \end{bmatrix}\\
            &=
            \begin{bmatrix}
                1\\
                0
            \end{bmatrix}
            +2\sin(\theta)
            \begin{bmatrix}
                -\sin(\theta)\\
                \cos(\theta)
            \end{bmatrix}\\
            &=
            \begin{bmatrix}
                1-2\sin^{2}(\theta)\\
                2\sin(\theta)\cos(\theta)
            \end{bmatrix}\\
            &=
            \begin{bmatrix}
                \cos(2\theta)\\
                \sin(2\theta)
            \end{bmatrix}
        \end{align}
        Performing the same computation with $\hat{\mathbf{y}}=(0,\,1)$ we
        get:
        \begin{equation}
            \begin{bmatrix}
                b\\
                c
            \end{bmatrix}
            =\begin{bmatrix}
                \sin(2\theta)\\
                -\cos(2\theta)
            \end{bmatrix}
        \end{equation}
        So our representative matrix is:
        \begin{equation}
            A=
            \begin{bmatrix}
                \cos(2\theta)&\sin(2\theta)\\
                \sin(2\theta)&-\cos(2\theta)
            \end{bmatrix}
        \end{equation}
        Uh oh, we get something very different before. What went wrong?
        The problem is that before we computed the representative matrix with
        respect to $\mathbf{u}$ and $\mathbf{u}^{\perp}$ whereas now we are
        computing the representative matrix with respect to
        $\hat{\mathbf{x}}$ and $\hat{\mathbf{y}}$. That is,
        \textbf{the representative matrix depends on the chosen basis}.
        \par\hfill\par
        Let's now show how to change from one basis to another.
        Let's suppose $g:\mathbb{R}^{n}\rightarrow\mathbb{R}^{n}$ is a
        linear function that takes takes a basis
        $\{\,\mathbf{e}_{1},\,\cdots,\,\mathbf{e}_{n}\,\}$ to another basis
        $\{\,\tilde{\mathbf{e}}_{1},\,\cdots,\,\tilde{\mathbf{e}}_{n}\,\}$.
        That is, $g(\mathbf{e}_{k})=\tilde{\mathbf{e}}_{k}$. Let $T$ be the
        representative matrix. Then we have:
        \begin{equation}
            A\mathbf{e}_{n}
            =f(\mathbf{e}_{n})
            =f\big(g^{-1}(\tilde{\mathbf{e}}_{n})\big)
            =AT^{-1}\tilde{\mathbf{e}}_{n}
        \end{equation}
        So the change of basis matrix is given by the inverse of $T$.
        For our example we have
        $f\big((1,\,0)\big)=\big(-\sin(\theta),\,\cos(\theta)\big)$ and
        $f\big((0,\,1)\big)=\big((\cos(\theta),\,\sin(\theta)\big)$, so the
        representative matrix is:
        \begin{equation}
            T=\begin{bmatrix}
                -\sin(\theta)&\cos(\theta)\\
                \cos(\theta)&\sin(\theta)
            \end{bmatrix}
        \end{equation}
        This matrix has the pleasant property that $T=T^{-1}$. The
        representative matrix in the $\{\,\mathbf{u},\,\mathbf{u}^{\perp}\,\}$
        basis is then given by $AT^{-1}$. We get:
        \begin{align}
            A&T^{-1}
            =\begin{bmatrix}
                \cos(2\theta)&\sin(2\theta)\\
                \sin(2\theta)&-\cos(2\theta)
            \end{bmatrix}
            \begin{bmatrix}
                -\sin(\theta)&\cos(\theta)\\
                \cos(\theta)&\sin(\theta)
            \end{bmatrix}\\
            &=\begin{bmatrix}
                -\cos(2\theta)\sin(\theta)+\sin(2\theta)\cos(\theta)&
                \cos(2\theta)\cos(\theta)+\sin(2\theta)\sin(\theta)\\
                -\sin(2\theta)\sin(\theta)-\cos(2\theta)\cos(\theta)&
                \sin(2\theta)\cos(\theta)-\cos(2\theta)\sin(\theta)
            \end{bmatrix}\\
            &=\begin{bmatrix}
                \sin(\theta)&\cos(\theta)\\
                -\cos(\theta)&\sin(\theta)
            \end{bmatrix}
        \end{align}
        Precisely the representative matrix we got in the beginning. Note,
        this last equality can be obtained by applying the angle sum formula to
        $\cos(\theta)=\cos(2\theta-\theta)$ and
        $\sin(\theta)=\sin(2\theta-\theta)$.
    \end{example}
    \begin{example}[\textbf{Image of Linear Subspaces}]
        A linear subspace of $\mathbb{R}^{n}$ is a subset
        $\mathcal{C}\subseteq\mathbb{R}^{n}$ that is the span of some
        collection of vectors. That is, if we have vectors
        $\{\,\mathbf{e}_{1},\,\cdots,\,\mathbf{e}_{k}\,\}$, the set:
        \begin{equation}
            \mathcal{C}=
            \{\,\sum_{\ell=1}^{k}a_{\ell}\mathbf{e}_{\ell}\;|\;
                a_{\ell}\in\mathbb{R}\,\}
        \end{equation}
        is a linear subspace of $\mathbb{R}^{n}$. These types of sets have the
        property that if $\mathbf{x},\mathbf{y}\in\mathcal{C}$, then
        so is $a\mathbf{x}+b\mathbf{y}$, for any real-valued scalars
        $a,b\in\mathbb{R}$. You could similar define a linear subspace to be a
        subset of $\mathbb{R}^{n}$ that is \textit{closed} under vector
        addition and scalar multiplication, and then prove that all such
        subsets are the span of some collection of vectors.
        \par\hfill\par
        Linear subspaces play nicely with linear functions. If
        $f:\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}$ is a linear function, and
        if $\mathcal{C}\subseteq\mathbb{R}^{n}$ is a linear subspace, then
        $f[\mathcal{C}]\subseteq\mathbb{R}^{m}$, the image of
        $\mathcal{C}$ under $f$, is a linear subspace as well. For given
        $\tilde{\mathbf{x}},\tilde{\mathbf{y}}\in{f}[\mathcal{C}]$, by
        definition there are some $\mathbf{x},\mathbf{y}\in\mathcal{C}$
        such that $f(\mathbf{x})=\tilde{\mathbf{x}}$ and
        $f(\mathbf{y})=\tilde{\mathbf{y}}$. But then, since $f$ is linear,
        we have:
        \begin{equation}
            f(\mathbf{x}+\mathbf{y})
            =f(\mathbf{x})+f(\mathbf{y})
            =\tilde{\mathbf{x}}+\tilde{\mathbf{y}}
        \end{equation}
        Since $\mathcal{C}$ is a linear subspace, we know that
        $\mathbf{x}+\mathbf{y}\in\mathcal{C}$, and therefore
        $\tilde{\mathcal{x}}+\tilde{\mathcal{y}}\in{f}[\mathcal{C}]$.
        A similar argument works for scalar multiplication, and therefore
        $f[\mathcal{C}]$ is a linear subspace.
    \end{example}
    \begin{example}[\textbf{Affine Transformations}]
        When first learning geometry and algebra in high school we are told
        the function $f(x)=ax+b$ is given by a \textit{linear equation}.
        This makes sense since the graph of $f$ is a \textit{line}. But now
        we are being told that for $f(x)=ax+b$ to be linear we must require
        that $b=0$. This is because a linear function must satisfy
        $f(ax)=af(x)$, and hence $f(0)=0$. Functions are the form
        $f(x)=ax+b$ is still plenty useful even for non-zero $b$, so we give
        them a new name. An \textbf{affine} function is a function
        $f:\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}$ such that there is a
        \textit{linear} function $L:\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}$
        and a constant vector $\mathbf{b}\in\mathbb{R}^{m}$ such that, for
        all $\mathbf{x}\in\mathbb{R}^{n}$, we have:
        \begin{equation}
            f(\mathbf{x})
            =L(\mathbf{x})+\mathbf{b}
        \end{equation}
        That is, an affine function is a linear function plus a shift. Affine
        subspaces can be similarly described, and these are \textit{shifted}
        linear subspaces. With linear subspaces we know that the image by a
        linear function will again be a linear subspace. In a similar manner,
        the image of an affine subspace by an affine function will again be
        an affine subspace.
    \end{example}
\end{document}
