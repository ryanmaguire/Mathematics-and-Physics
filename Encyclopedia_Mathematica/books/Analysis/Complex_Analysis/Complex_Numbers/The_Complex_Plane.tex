\section{Complex Numbers}
    A \gls{complex number} is a point in the plane $z=(x,\,y)$, but we often
    write:
    \begin{equation}
        z=x+iy
    \end{equation}
    and call $i$ the \textit{imaginary unit}. We call $x$ the \textit{real part}
    and $y$ the \textit{imaginary part}, denoted $\Re(z)$ and $\Im(z)$,
    respectively. The planar representation is shown in
    Fig.~\ref{fig:Cart_Rep_of_Comp_Num}. The arithmetic goes as follows:
    \begin{subequations}
        \begin{align}
            \label{eqn:Complex_Addition}%
            (a+ib)+(c+id)&=(a+c)+i(b+d)\\
            \label{eqn:Complex_Multiplication}%
            (a+ib)\cdot(c+id)&=(ac-bd)+i(bc+ad)
        \end{align}
    \end{subequations}
    We'd hope this definition preserves the arithmetic of the \textit{real}
    numbers, and indeed it does. Setting $b$ and $d$ to zero, we see that
    elementary arithmetic is recovered.
    \par\hfill\par
    The arithmetic of the complex numbers arises when one studies equation
    like $y(x)=x^{2}+1$. For a real variable $x$, there is no root to this
    equation. That is, there is no real number $x$ such that $x^{2}+1=0$.
    We can invent such a number and give that the property that it's square
    is $\minus{1}$. This is what the imaginary unit does.
    \begin{figure}[H]
        \centering
        \captionsetup{type=figure}
        \input{tikz/Complex_Plane_Cartesian_Representation.tex}
        \caption{Cartesian Representation of Complex Numbers}
        \label{fig:Cart_Rep_of_Comp_Num}
    \end{figure}
    It should be clear  that addition and multiplication are commutative
    operations ($z+w=w+z$ and $z\cdot{w}=w\cdot{z}$). That addition is
    associative is also straight forward. What is not obvious is the
    associativity of multiplication.
    \begin{theorem}
        \label{thm:Complex_Multiplication_Associative}%
        If $z$, $w$, and $v$ are complex numbers, then:
        \begin{equation}
            z\cdot(w\cdot{v})=(z\cdot{w})\cdot{v}
        \end{equation}
        That is, complex multiplication is associative.
    \end{theorem}
    \begin{proof}
        For let $z=a+ib$, $w=c+id$, and $v=e+if$. Then:
        \begin{subequations}
            \begin{align}
                z\cdot(w\cdot{v})
                    &=(a+ib)\cdot\Big((c+id)\cdot(e+if)\Big)\\
                    &=(a+ib)\cdot\Big((ce-df)+i(cf+de)\Big)\\
                    &=a(ce-df)-b(cf+de)+i\big(a(cf+de)+b(ce-df)\big)\\
                    &=(ace-adf-bcf-bde)+i(acf+ade+bce-bdf)\\
                    &=(ac-bd)e-(ad+bc)f+i\big((ad+bc)e+(ac-bd)f\big)\\
                    &=\Big((a+ib)\cdot(c+id)\Big)\cdot(e+if)
            \end{align}
        \end{subequations}
        This completes the proof.
    \end{proof}
    \begin{theorem}
        If $i$ is the imaginary unit, then $i^{2}=\minus{1}$.
    \end{theorem}
    \begin{proof}
        For $i=0+1i$, and thus by Eqn.~\ref{eqn:Complex_Multiplication}:
        \begin{equation}
            i^{2}=(0+1i)\cdot(0+1i)
                 =(0\cdot{0}-1\cdot{1})+i(1\cdot{0}+0\cdot{1})
                 =\minus{1}+i\cdot{0}
                 =\minus{1}
        \end{equation}
        This completes the proof.
    \end{proof}
    The complex numbers are \textit{algebraically closed}:
    Every non-constant polynomial has a \textit{root}, or a zero.
    Moreover, given a polynomial of degree $n$ there are at most
    $n$ roots. This result is called the
    \textit{Fundamental Theorem of Algebra}. The real numbers lack this
    feature, for consider the graph of $y(x)=x^{2}+1$. Many attempts at
    proving this theorem were made between 1608 and 1799, and the likes of
    Euler, Lagrange, Laplace, Gauss, and d'Alambert failed in their
    attempts. In 1806 Jean Robert-Argand published a rigorous proof, and
    due to this the complex plane is occasionally called the Argand plane.
    \par\hfill\par
    There are two fundamental notions worth mentioning: The complex
    conjugate and the modulus of a complex number.
    \begin{ldefinition}{Complex Conjugate}{Complex_Conjugate}
        The \gls{complex conjugate} a complex number $z=x+iy$ is:
        \begin{equation}
            \overline{z}=x-iy
        \end{equation}
        That is, the reflection of $z$ across the $x$ axis.
    \end{ldefinition}
    A visual for the complex conjugate of a complex number is given in
    Fig.~\ref{fig:Conj_and_Mod_of_Com_Num}. There are various
    arithmetic properties of the complex conjugate that ease the
    process of computation.
    \begin{theorem}
        If $z$ is a complex number, then $z\cdot\overline{z}$ is
        a non-negative real number.
    \end{theorem}
    \begin{proof}
        For let $z=x+iy$, where $x$ and $y$ are real numbers. Then, by
        the definition of the complex conjugate
        (Def.~\ref{def:Complex_Conjugate}) and of
        complex multiplication (Eqn.~\ref{eqn:Complex_Multiplication}):
        \begin{equation}
            z\cdot\overline{z}=(x+iy)\cdot(x-iy)
                              =x^{2}+y^{2}
        \end{equation}
        This is the sum of the squares of two real numbers, and is
        therefore a real and non-negative number. Therefore, etc.
    \end{proof}
    \begin{figure}[H]
        \centering
        \captionsetup{type=figure}
        \input{tikz/Complex_Number_Conjugate_and_Modulus.tex}
        \caption{Modulus and Conjugate of a Complex Number}
        \label{fig:Conj_and_Mod_of_Com_Num}
    \end{figure}
    \begin{theorem}
        If $z$ and $w$ are complex numbers, then:
        \begin{equation}
            \overline{z+w}=\overline{z}+\overline{w}
        \end{equation}
    \end{theorem}
    \begin{proof}
        For let $z=a+ib$ and $w=c+id$. Then, by
        Eqn.~\ref{eqn:Complex_Addition}, we have:
        \begin{equation}
            \overline{z+w}=\overline{(a+ib)+(c+id)}
                          =\overline{(a+c)+i(b+d)}
        \end{equation}
        Invoking the definition of complex conjugate
        (Def.~\ref{def:Complex_Conjugate}), we obtain:
        \begin{equation}
            \overline{z+w}=(a+c)-i(b+d)
                          =(a-ib)+(c-id)
                          =\overline{z}+\overline{w}
        \end{equation}
        Therefore, etc.
    \end{proof}
    \begin{theorem}
        If $z$ and $w$ are complex numbers, then:
        \begin{equation}
            \overline{z\cdot{w}}=\overline{z}\cdot\overline{w}
        \end{equation}
    \end{theorem}
    \begin{proof}
        For let $z=a+ib$ and $w=c+id$. Then, by
        Eqn.~\ref{eqn:Complex_Multiplication}, we obtain:
        \begin{equation}
            \overline{z\cdot{w}}=\overline{(a+ib)\cdot(c+id)}
                                =\overline{(ac-bd)+i(ad+bc)}
        \end{equation}
        Invoking Def.~\ref{def:Complex_Conjugate}, we have:
        \begin{equation}
            \overline{z\cdot{w}}=(ac-bd)-i(ad+bc)
                                =(a-ib)\cdot(c-id)
                                =\overline{z}\cdot\overline{w}
        \end{equation}
        Therefore, etc.
    \end{proof}
    From the geometry shown in Fig.~\ref{fig:Conj_and_Mod_of_Com_Num},
    one would expect adding a complex number to its conjugate would
    eliminate the imaginary component, and subtracting would eliminate
    the real part. This is indeed true.
    \begin{theorem}
        \label{thm:Sum_with_Conj_is_Real}%
        If $z$ is a complex number, then:
        \begin{equation}
            z+\overline{z}=2\Re(z)
        \end{equation}
    \end{theorem}
    \begin{proof}
        For let $z=x+iy$. Then, by Def.~\ref{def:Complex_Conjugate}
        and Eqn.~\ref{eqn:Complex_Addition}:
        \begin{equation}
            z+\overline{z}=(x+iy)+(x-iy)
                          =(x+x)+i(y-y)
                          =2x
                          =2\Re(z)
        \end{equation}
        Therefore, etc.
    \end{proof}
    \begin{theorem}
        If $z$ is a complex number, then:
        \begin{equation}
            z-\overline{z}=2i\Im(z)
        \end{equation}
    \end{theorem}
    \begin{proof}
        For let $z=x+iy$. Then, by Def.~\ref{def:Complex_Conjugate}
        and Eqn.~\ref{eqn:Complex_Addition}:
        \begin{equation}
            z-\overline{z}=(x+iy)-(x-iy)
                          =(x-x)+i(y+y)
                          =2iy
                          =2i\Im(z)
        \end{equation}
        Therefore, etc.
    \end{proof}
    Lastly, taking the complex conjugate twice is equivalent to performing
    two reflection across the $x$ axis and thus should result in
    no change.
    \begin{theorem}
        \label{thm:Conj_of_Conj}%
        If $z$ is a complex number, then $\overline{\overline{z}}=z$.
    \end{theorem}
    \begin{proof}
        For let $z=x+iy$. Then:
        \begin{equation}
            \overline{\overline{z}}=\overline{\overline{(x+iy)}}
                                   =\overline{(x-iy)}
                                   =x+iy
                                   =z
        \end{equation}
        Therefore, etc.
    \end{proof}
    The complex conjugate can be used to define the modulus, or
    absolute value, of a complex number by simply taking the (positive)
    square root of $z\overline{z}$.
    \begin{ldefinition}{Modulus of a Complex Number}{Modulus_of_Comp_Num}
        The \gls{modulus} of a complex number $z=x+iy$ is:
        \begin{equation}
            |z|=\sqrt{x^{2}+y^{2}}
        \end{equation}
        We can also write $|z|=\sqrt{z\overline{z}}$, where
        $\overline{z}$ is the complex conjugate of $z$.
    \end{ldefinition}
    This is the size, or magnitude, of a complex number in the plane,
    using the Euclidean notion of distance: We compute the length via
    the Pythagorean formula.
    \begin{theorem}
        \label{thm:Mod_of_z_is_mod_of_conj}%
        If $z$ a complex number, then $|z|=|\overline{z}|$.
    \end{theorem}
    \begin{proof}
        For let $z=x+iy$. Then:
        \begin{equation}
            |z|=\sqrt{x^{2}+y^{2}}=\sqrt{x+(\minus{y})^{2}}=|\overline{z}|
        \end{equation}
        Therefore, etc.
    \end{proof}
    There is one particular theorem that is vital to all areas of
    mathematical analysis which dates back to Euclid: The Triangle
    Inequality. To prove this we will need a few results about the
    modulus of a complex number. Firstly, it is preserved by products,
    and secondly that the modulus of the real part of complex number
    is not greater than the entire modulus. That is, the projection of
    a complex number $z$ onto the $x$ axis is less than or equal to the
    magnitude of $z$.
    \begin{theorem}
        \label{thm:Mod_Preserves_Products}%
        If $z$ and $w$ are complex numbers, then:
        \begin{equation}
            |z\cdot{w}|=|z|\cdot|w|
        \end{equation}
    \end{theorem}
    \begin{proof}
        For let $z=a+ib$ and $w=c+id$. By
        Eqn.~\ref{eqn:Complex_Multiplication}, we have:
        \begin{equation}
            |z\cdot{w}|=|(a+ib)\cdot(c+id)|
                       =|(ac-bd)+i(ad+bc)|
        \end{equation}
        Using Def.~\ref{def:Modulus_of_Comp_Num}, we obtain:
        \begin{equation}
            |z\cdot{w}|=\sqrt{(ac-bd)^{2}+(ad+bc)^{2}}
                       =\sqrt{(ac)^{2}+(bd)^{2}+(ad)^{2}+(bc)^{2}}
        \end{equation}
        Factoring this gives us the result:
        \begin{equation}
            |z\cdot{w}|=\sqrt{(a^{2}+b^{2})(c^{2}+d^{2})}
                       =\sqrt{a^{2}+b^{2}}\sqrt{c^{2}+d^{2}}
                       =|z|\cdot|w|
        \end{equation}
        Therefore, etc.
    \end{proof}
    \begin{theorem}
        \label{thm:Mod_of_Real_Part_LEQ_Mod}%
        If $z$ is a complex number, then:
        \begin{equation}
            |\Re(z)|\leq|z|
        \end{equation}
    \end{theorem}
    \begin{proof}
        For let $z=a+ib$. Using Def.\ref{def:Modulus_of_Comp_Num},
        we have:
        \begin{equation}
            |\Re(z)|^{2}=|\Re(a+ib)|^{2}
                        =|a|^{2}
                        \leq|a|^{2}+|b|^{2}
                        =|z|^{2}
        \end{equation}
        Taking the square root of both sides completes the proof.
    \end{proof}
    \begin{ltheorem}{The Triangle Inequality}{Triangle_Inequality}
        If $z$ and $w$ are complex numbers, then $|z+w|\leq|z|+|w|$.
    \end{ltheorem}
    \begin{proof}
        Invoking Def.~\ref{def:Modulus_of_Comp_Num},
        Thms.~\ref{thm:Sum_with_Conj_is_Real}, \ref{thm:Conj_of_Conj},
        \ref{thm:Mod_of_z_is_mod_of_conj},
        \ref{thm:Mod_Preserves_Products}, and
        \ref{thm:Mod_of_Real_Part_LEQ_Mod}, we obtain:
        \par
        \begin{subequations}
            \begin{minipage}[b]{0.55\textwidth}
                \centering
                \begin{align}
                    |z+w|^{2}&=(z+w)\cdot\overline{(z+w)}\\
                             &=z\overline{z}+z\overline{w}+\overline{z}w
                                            +w\overline{w}\\
                             &=|z|^{2}+z\overline{w}
                                      +\overline{z}w+|w|^{2}\\
                             &=|z|^{2}+z\overline{w}
                                      +\overline{z\overline{w}}+|w|^{2}
                \end{align}
            \end{minipage}
            \hfill
            \begin{minipage}[b]{0.44\textwidth}
                \centering
                \begin{align}
                    &=|z|^{2}+2\Re(z\overline{w})+|w|^{2}\\
                    &\leq|z|^{2}+2|z||\overline{w}|+|w|^{2}\\
                    &=|z|^{2}+2|z||w|+|w|^{2}\\
                    &=(|z|+|w|)^{2}
                \end{align}
            \end{minipage}
        \end{subequations}
        \par
        \vspace{2.5ex}
        Taking the square root of both sides completes the proof.
    \end{proof}
    It would be nonsensical to call something the triangle inequality
    if triangles weren't involved. In Euclid's \textit{Elements} he
    proves that, given any triangle, the length of one side is less
    than the sum of the other two. This can be realized in the
    complex plane by thinking of $z$, $w$, and $z+w$ as points on a
    triangle (Fig.~\ref{fig:Triangle_Inequality}). The triangle inequality
    states that it is shorter to walk from the origin to the point $z+w$,
    than it is to walk from the origin to $z$, and then $z$ to $z+w$.
    \begin{figure}[H]
        \centering
        \captionsetup{type=figure}
        \input{tikz/Complex_Plane_Triangle_Inequality.tex}
        \caption{Visual Representation of the Triangle Inequality}
        \label{fig:Triangle_Inequality}
    \end{figure}
    The complex conjugate and the modulus of a complex number can
    combine to form the \textit{inverse} of a non-zero complex number.
    That, the complex numbers form something called a \textit{field}. A
    field is a set with two operations, usually called addition
    and multiplication, such that the operations are commutative,
    associative, and such that multiplication \textit{distributes}
    over addition. There is also the requirement of the existence of an
    \textit{additive identity} and a \textit{multiplicative identity}.
    Lastly, every number needs an \textit{additive inverse}, and every
    non-zero number needs a \textit{multiplicative inverse}. It is not
    difficult to see that the first eight properties are satisfied by
    the complex numbers, given $z=x+iy$,
    $\minus{z}=(\minus{x})+i(\minus{y})$ serves as the additive inverse.
    The last property is tricky, but vital for computations.
    \begin{theorem}
        \label{thm:Inverses_Are_Unique_In_Group}%
        If $G$ is a set, if $*$ is an associative operation on $G$ with an
        identity element $e$, and if $x$ has an inverse $x^{\minus{1}}$,
        then $x^{\minus{1}}$ is unique.
    \end{theorem}
    \begin{proof}
        For suppose $x'^{\minus{1}}$ is a different inverse. Then:
        \begin{equation}
            x'^{\minus{1}}=x'^{\minus{1}}*e
                          =x'^{\minus{1}}*(x*x^{\minus{1}})
                          =(x'^{\minus{1}}*x)*x^{\minus{1}}
                          =e*x^{\minus{1}}
                          =x^{\minus{1}}
        \end{equation}
        And thus $x'^{\minus{1}}=x^{\minus{1}}$. Therefore,
        the inverse is unique.
    \end{proof}
    From uniquness, once we've found a candidate for an inverse, we know
    that this is indeed the inverse. We now prove
    that non-zero complex numbers have multiplicative inverses.
    \begin{theorem}
        \label{thm:Complex_Inverse}%
        If $z$ is a non-zero complex number, then there is a unique
        $z^{\minus{1}}$ such that $z\cdot{z}^{\minus{1}}=1$.
        The inverse of $z=a+ib$ is:
        \begin{equation}
            \label{eqn:Mult_Inv_of_Complex}%
            z^{\minus{1}}=\frac{a-ib}{a^{2}+b^{2}}
        \end{equation}
    \end{theorem}
    \begin{proof}
        If $a+ib\ne{0}$, then $a^2+b^2\ne{0}$, so
        $(a-ib)/(a^2+b^2)$ is well defined. But:
        \begin{equation}
            (a+ib)\cdot\frac{a-ib}{a^2+b^2}=\frac{(a+ib)(a-ib)}{a^2+b^2}
                                           =\frac{a^2+b^2}{a^2+b^2}=1
        \end{equation}
        The uniqueness of inverses
        (Thm.~\ref{thm:Inverses_Are_Unique_In_Group}) gives us our result.
    \end{proof}
    If $|z|$ is the modulus of $z$, and $\overline{z}$ is it's complex
    conjugate, $z^{\minus{1}}$ can be written as:
    \begin{equation}
        \label{eqn:Complex_Addition_Alt}%
        z^{\minus{1}}=\frac{\overline{z}}{|z|^{2}}
    \end{equation}
    We will make use of these formulae often, so they are
    good to keep in mind.
    \begin{lexample}{}{Inverse_of_i}
        Consider the complex number $z=i$. We can use
        Eqn.~\ref{eqn:Mult_Inv_of_Complex} to compute it's
        multiplicative inverse, and we obtain $i^{\minus{1}}=\minus{i}$.
        We can also see this since $i^{\minus{1}}\cdot{i}=1$, and we
        know that $i^{2}=\minus{1}$. Multiplying by $\minus{1}$,
        we have $\minus{i}^{2}=i^{\minus{1}}\cdot{i}$. Dividing by $i$
        obtains the result again.
    \end{lexample}
    \begin{lexample}{}{Inverse_of_Comp_Num}
        Now let $z=(1+i)/F$, where $F$ is a non-zero real number.
        The multiplicative inverse of this is:
        \begin{equation}
            \Big(\frac{1+i}{F}\Big)^{\minus{1}}=F(1+i)^{\minus{1}}
                                               =F\frac{1-i}{2}
        \end{equation}
        Invoking Pythagoras, we see that $|z|=\sqrt{2}/F$.
        Using Eqn.~\ref{eqn:Complex_Addition_Alt}, we obtain:
        \begin{equation}
            z^{\minus{1}}=\frac{\overline{z}}{|z|^{2}}
                         =\frac{\frac{1-i}{F}}{\frac{2}{F^{2}}}
                         =F\frac{1-i}{2}
        \end{equation}
        In agreement with our previous calculation.
    \end{lexample}
\subsection{Polar Representation of Complex Numbers}
    In Walter Rudin's classic text on real and
    complex analysis, he opens with a prologue on the
    \textit{exponential} function and calls it
    ``Undoubtedly the most important function in
    mathematics.'' We take a moment to study this function.
    \begin{ldefinition}{Exponential Function}{Exp_Func}
        The complex exponential function is the function
        $\exp:\mathbb{C}\rightarrow\mathbb{C}$ defined by:
        \begin{equation}
            \exp(z)=\sum_{n=0}^{\infty}\frac{z^{n}}{n!}
        \end{equation}
        where $n!$ denotes the factorial of $n$,
        $n!=n\cdot(n-1)!$, and where $0!\equiv{1}$.
    \end{ldefinition}
    One useful result about the exponential function is that it relates
    multiplication and addition in a convenient way. We will need
    Cauchy's product theorem.
    \begin{ltheorem}{Cauchy's Product Theorem}{Cauchy_Product_Theorem}
        If $a:\mathbb{N}\rightarrow\mathbb{C}$ and
        $b:\mathbb{N}\rightarrow\mathbb{C}$ are sequences, if
        $\sum{a}_{n}$ converge absolutely, and if $\sum{b}_{n}$
        converges, then:
        \begin{equation}
            \Big(\sum_{j=0}^{\infty}a_{j}\Big)
            \Big(\sum_{k=0}^{\infty}b_{k}\Big)
                =\sum_{n=0}^{\infty}\sum_{m=0}^{n}a_{m}b_{n-m}
        \end{equation}
    \end{ltheorem}
    \begin{proof}
        Since the two sums $\sum{a}_{n}$ and $\sum{b}_{n}$ converge,
        let $A$ and $B$ be their limits, respectively. For all
        $n\in\mathbb{N}$, define the following partial sums:
        \par
        \begin{subequations}
            \begin{minipage}[b]{0.49\textwidth}
                \centering
                \begin{equation}
                    A_{n}=\sum_{k=0}^{n}a_{k}
                \end{equation}
            \end{minipage}
            \hfill
            \begin{minipage}[b]{0.49\textwidth}
                \centering
                \begin{equation}
                    B_{n}=\sum_{k=0}^{n}b_{k}
                \end{equation}
            \end{minipage}
        \end{subequations}
        \par\vspace{2.5ex}
        Furthermore, let $c_{n}$ be the Cauchy product and $C_{n}$ be
        the partial sums:
        \par
        \begin{subequations}
            \begin{minipage}[b]{0.49\textwidth}
                \centering
                \begin{equation}
                    c_{n}=\sum_{k=0}^{n}a_{k}b_{n-k}
                \end{equation}
            \end{minipage}
            \hfill
            \begin{minipage}[b]{0.49\textwidth}
                \centering
                \begin{equation}
                    C_{n}=\sum_{m=0}^{n}c_{m}
                \end{equation}
            \end{minipage}
        \end{subequations}
        \par\vspace{2.5ex}
        And finally, let $\beta_{n}=B_{n}-B$. Then, for all
        $n\in\mathbb{N}$:
        \begin{equation}
            C_{n}=\sum_{j=0}^{n}\sum_{k=0}^{j}a_{k}b_{j-k}
                 =\sum_{j=0}^{n}a_{j}B_{n-j}
                 =A_{n}B+\sum_{j=0}^{n}a_{j}\beta_{n-j}
        \end{equation}
        Let $d_{n}$ be the remainder term. That is:
        \begin{equation}
            d_{n}=\sum_{j=0}^{n}a_{j}\beta_{n-j}
        \end{equation}
        Since $\sum{a}_{n}$ is absolutely convergent, and thus
        $\sum|a_{n}|$ converges. Let $A'$ be the limit. Also, by the
        definition of $\beta_{n}$, $\beta_{n}$ converges to zero. That
        is, given any $\varepsilon>0$ there is an $N\in\mathbb{N}$ such
        that, for all $n>N$, we have $|\beta_{n}|<\varepsilon$.
        But then:
        \begin{equation}
            |d_{n}|=\Big|\sum_{j=0}^{N}a_{j}\beta_{n-j}+
                         \sum_{j=N+1}^{n}a_{j}\beta_{n-j}\Big|
                \leq\Big|\sum_{j=0}^{N}a_{j}\beta_{n-j}\Big|
                   +\Big|\sum_{j=N+1}^{n}a_{j}\beta_{n-j}\Big|
        \end{equation}
        Where this last step comes from the triangle inequality.
        Simplifying, we have:
        \begin{equation}
            |d_{n}|<|\sum_{j=0}^{N}a_{j}\beta_{n-j}|+\varepsilon{A}'
        \end{equation}
        The first term can be made small since $\beta_{n-j}$ is small
        for large $n$ (And since $j<N$), and the second term can also
        be made small since $\varepsilon$ is arbitrary. So we see that
        $d_{n}$ converges to zero. Thus:
        \begin{equation}
            \underset{n\rightarrow\infty}{\lim}C_{n}
            =\underset{n\rightarrow\infty}{\lim}A_{n}B+d_{n}
            =AB
        \end{equation}
        This completes the proof.
    \end{proof}
    The immediate application of this is the power
    rule for the exponential function. We will need the
    \textit{binomial theorem}. Let $\binom{n}{k}$ (Which reads as
    $n$ \textit{choose} $k$) denote the \textit{binomial coefficient}:
    \begin{equation}
        \binom{n}{k}=\frac{n!}{k!(n-k)!}
    \end{equation}
    The binomial theorem then says the following:
    \begin{ltheorem}{The Binomial Theorem}{Binomial_Theorem}
        If $x$ and $y$ are real numbers, and if $n\in\mathbb{N}$, then:
        \begin{equation}
            (x+y)^{n}=\sum_{k=0}^{n}\binom{n}{k}x^{n-k}y^{k}
        \end{equation}
        Where $\binom{n}{k}$ denotes the binomial coefficient.
    \end{ltheorem}
    \begin{proof}
        We prove by induction. When $n=0$ or $n=1$ we can evaluate the
        validity of this by hand. When $n=2$ this is commonly known
        as the FOIL rule. Suppose it is true for $n\in\mathbb{N}$. We must
        now show this implies it is true for $n+1$. We have:
        \begin{equation}
            (x+y)^{n+1}=(x+y)(x+y)^{n}
                       =(x+y)\sum_{k=0}^{n}\binom{n}{k}x^{n-k}y^{k}
        \end{equation}
        We can further simplify, and perform a shift of index, to obtain:
        \begin{subequations}
            \begin{align}
                (x+y)^{n+1}&=(x+y)\sum_{k=0}^{n}\binom{n}{k}x^{n-k}y^{k}\\
                           &=x\sum_{k=0}^{n}\binom{n}{k}x^{n-k}y^{k}
                            +y\sum_{k=0}^{n}\binom{n}{k}x^{n-k}y^{k}\\
                           &=\sum_{k=0}^{n}\binom{n}{k}x^{n+1-k}y^{k}
                            +\sum_{k=1}^{n+1}\binom{n}{k-1}
                                x^{n+1-k}y^{k}\\
                    \label{eqn:Binomial_Theorem_Pascal_Identity}%
                    &=x^{n+1}+\sum_{k=1}^{n}
                        \Big[\binom{n}{k}+\binom{n}{k-1}\Big]
                        x^{n+1-k}y^{k}+y^{n+1}
            \end{align}
        \end{subequations}
        The sum of these two binomial coefficients is known as
        Pascal's Identity. We have:
        \begin{subequations}
            \begin{align}
                \binom{n}{k}+\binom{n}{k-1}
                    &=\frac{n!}{k!(n-k)!}+\frac{n!}{(k-1)!(n+1-k)!}\\
                    &=n!\frac{(n+1-k)+k}{k!(n+1-k)!}\\
                    &=\frac{(n+1)!}{k!(n+1-k)!}\\
                    &=\binom{n+1}{k}
            \end{align}
        \end{subequations}
        Thus, returning to
        Eqn.~\ref{eqn:Binomial_Theorem_Pascal_Identity}, we obtain:
        \begin{subequations}
            \begin{align}
                (x+y)^{n+1}
                    &=x^{n+1}+\sum_{k=1}^{n}\binom{n+1}{k}x^{n+1-k}y^{k}
                     +y^{n+1}\\
                    &=\sum_{k=0}^{n+1}\binom{n+1}{k}x^{n+1-k}y^{k}
            \end{align}
        \end{subequations}
        This completes the proof.
    \end{proof}
    \begin{theorem}
        \label{thm:Expo_Product_Formula}%
        If $a$ and $b$ are complex numbers, then:
        \begin{equation}
            \exp(a+b)=\exp(a)\exp(b)
        \end{equation}
    \end{theorem}
    \begin{proof}
        Invoking the \textit{binomial theorem}
        (Thm.~\ref{thm:Binomial_Theorem}), we have:
        \begin{equation}
            \exp(a+b)=\sum_{n=0}^{\infty}\frac{(a+b)^{n}}{n!}
                     =\sum_{n=0}^{\infty}\frac{1}{n!}
                      \sum_{k=0}^{n}\frac{n!}{k!(n-k)!}a^{k}b^{n-k}
        \end{equation}
        But by Cauchy's product theorem
        (Thm.~\ref{thm:Cauchy_Product_Theorem}), this last double
        sum can be written as the product of two sums of the form:
        \begin{equation}
            \sum_{n=0}^{\infty}\sum_{k=0}^{n}
                \frac{1}{k!(n-k)!}a^{k}b^{n-k}
                =\Big(\sum_{j=0}^{\infty}\frac{a^{j}}{j!}\Big)
                 \Big(\sum_{m=0}^{\infty}\frac{b^{m}}{m!}\Big)
        \end{equation}
        But this is just the product of $\exp(a)$
        and $\exp(b)$. Therefore, etc.
    \end{proof}
    We next prove one of the most important theorems of complex analysis:
    Euler's Theorem. This is a crucial part of the theory and allows
    one to define the \textit{polar representation} of a complex number.
    It relates the exponential function to the trigonometric functions.
    \newpage
    \begin{ftheorem}{Euler's Exponential Formula}{Euler_Expo_Formula}
        If $\theta$ is a real number, then:
        \begin{equation}
            \exp(i\theta)=\cos(\theta)+i\sin(\theta)
        \end{equation}
    \end{ftheorem}
    \begin{bproof}
        Using the definition of the exponential function
        (Def.~\ref{def:Exp_Func}) and evaluating $i\theta$ into this
        equation, we obtain:
        \begin{equation}
            \label{def:Exp_Func_With_it}%
            \exp(i\theta)=\sum_{n=0}^{\infty}i^{n}\frac{\theta^{n}}{n!}
        \end{equation}
        But $i^{n}$ cycles between $i$, $\minus{1}$, $\minus{i}$, and $1$.
        So we may split this sum into two parts, a real part and an
        imaginary part, to obtain:
        \begin{equation}
            \sum_{n=0}^{\infty}\exp(i\theta)
                =\sum_{n=0}^{\infty}(\minus{1})^{n}\frac{x^{2n}}{(2n)!}
               +i\sum_{n=0}^{\infty}(\minus{1})^{n}
                    \frac{x^{2n+1}}{(2n+1)!}
        \end{equation}
        But the left sum is the Taylor expansion for $\cos(\theta)$,
        and the right sum is the Taylor expansion for $i\sin(\theta)$.
        This completes the proof.
    \end{bproof}
    Euler's Theorem can also be proved by showing the two expressions
    satisfy the same initial value problem: $\ddot{z}+z=0$, $z(0)=1$,
    $\dot{z}(0)=i$. A corollary of this is often hailed as the most
    beautiful result in mathematics. This is Euler's Identity:
    \begin{equation}
        e^{i\pi}+1=0
    \end{equation}
    Combining Euler's Exponential Formula and the product rule for the
    exponential function, we see that given any complex number $z=a+ib$,
    the following holds:
    \begin{equation}
        \exp(z)=\exp(a)\big(\cos(b)+i\sin(b)\big)
    \end{equation}
    Many of the identities from trigonometry are short corollaries of this
    theorem, rendering memorization of these formulae redundant.
    \begin{ltheorem}{DeMoivre's Theorem}{DeMoivres_Theorem}
        If $n\in\mathbb{N}$ and if $\theta\in\mathbb{R}$, then:
        \begin{equation}
            \big(\cos(\theta)+i\sin(\theta)\big)^{n}
            =\cos(n\theta)+i\sin(n\theta)
        \end{equation}
    \end{ltheorem}
    \begin{proof}
        By Euler's formula (Thm.~\ref{thm:Euler_Expo_Formula}), and by
        Thm.~\ref{thm:Expo_Product_Formula}, we have:
        \begin{equation}
            \big(\cos(\theta)+i\sin(\theta)\big)^{n}
                =\big(\exp(i\theta)\big)^{n}
                =\exp(in\theta)
                =\cos(n\theta)+i\sin(n\theta)
        \end{equation}
        Therefore, etc.
    \end{proof}
    Combining the binomial theorem (Thm.~\ref{thm:Binomial_Theorem})
    and DeMoivre's Theorem allows one to quickly compute any
    trigonometric identity one might need. Letting $n=2$, we obtain the
    double-angle formula:
    \begin{equation}
        \cos(2\theta)+i\sin(2\theta)=\cos^{2}(\theta)-\sin^{2}(\theta)
                                    +2i\cos(\theta)\sin(\theta)
    \end{equation}
    Equating real and imaginary parts, we have:
    \par
    \begin{subequations}
        \begin{minipage}[b]{0.49\textwidth}
            \centering
            \begin{equation}
                \cos(2\theta)=\cos^{2}(\theta)-\sin^{2}(\theta)
            \end{equation}
        \end{minipage}
        \hfill
        \begin{minipage}[b]{0.49\textwidth}
            \centering
            \begin{equation}
                \sin(2\theta)=2\cos(\theta)\sin(\theta)
            \end{equation}
        \end{minipage}
    \end{subequations}
    \par
    \vspace{2.5ex}
    The important thing is that we can now define the polar form of a
    complex number.
    \begin{theorem}
        \label{thm:Polar_Form_Comp_Num}%
        If $z$ is a complex number, then there is a unique real number
        $r\geq{0}$ and a real number $\theta\in[0,2\pi)$ such that:
        \begin{equation}
            \label{eqn:Polar_Form_Comp_Num}%
            z=r\exp(i\theta)
        \end{equation}
    \end{theorem}
    \begin{proof}
        Let $z=x+iy$. Define $r$ and $\theta$ as:
        \begin{subequations}
            \begin{equation}
                r=\sqrt{x^{2}+y^{2}}
            \end{equation}
            \begin{equation}
                \theta=
                \begin{cases}
                    \arctan\big(\frac{y}{x}\big),&x>0,y\geq{0}\\
                    \frac{\pi}{2}+\arctan\big(\frac{y}{|x|}\big),
                        &x<0,y\geq{0}\\
                    \pi+\arctan\big(\frac{y}{x}\big),
                        &x<0,y\leq{0}\\
                    \frac{3\pi}{2}+\arctan\big(\frac{|y|}{x}\big),
                        &x<0,y\geq{0}\\
                    \frac{\pi}{2}\sgn(y),&x=0
                \end{cases}
            \end{equation}
        \end{subequations}
        Here $\sgn(y)$ is the sign of $y$. Euler's Theorem completes
        the proof. Uniqueness of $r$ comes from the fact that
        $|\exp(i\theta)|=1$, so if $z=r_{1}\exp(i\theta_{1})$
        and $z=r_{2}\exp(i\theta_{2})$, then $|r_{1}|=|r_{2}|$. But
        $r_{1}$ and $r_{2}$ are non-negative, and thus $r_{1}=r_{2}$.
    \end{proof}
    Eqn.~\ref{eqn:Polar_Form_Comp_Num} is the definition of the polar
    form of a complex number. This gives geometrical interpretations of
    many aspects of complex arithmetic. Multiplication can be seen
    as rotations and scaling in the complex plane. For if
    $z=r_{1}\exp(i\theta_{1})$ and if $w=z_{2}\exp(i\theta_{2})$,
    then we have:
    \begin{equation}
        z\cdot{w}=r_{1}r_{2}\exp\big(i(\theta_{1}+\theta_{2})\big)
    \end{equation}
    That is, multiplying $z$ by $w$ scales $z$ by the magnitude of $w$,
    and rotates it in the plane by the angle $\theta_{2}$. This also
    allows us to define square roots. We define the $n^{th}$ root of a
    complex number to be:
    \begin{equation}
        \label{eqn:Sqrt_of_Comp_Num}%
        \sqrt[n]{z}=\sqrt[n]{r}\exp\Big(\frac{i\theta}{n}\Big)
    \end{equation}
    This is well defined for all complex numbers since the $n^{th}$
    root of a non-negative real number $r$ is well defined, and
    $\exp(i\theta/n)$ is well defined for all real $\theta$. Thus we
    have avoided the messiness of square roots that occurs in the real
    world. By $\sqrt[n]{r}$, we still mean the positive root.
    So $\sqrt{4}=2$, and not $\minus{2}$.
    \begin{figure}[H]
        \centering
        \captionsetup{type=figure}
        \input{tikz/Complex_Plane_Polar_Form.tex}
        \caption{Polar Representation of a Complex Number}
        \label{fig:Comp_Num_Polar}
    \end{figure}
    \begin{lexample}{}{Square_Root_of_i}
        Consider the square root of $i$. Using Euler's formula, we
        have that $i=\exp(i\pi/2)$. Using
        Eqn.~\ref{eqn:Sqrt_of_Comp_Num}, we obtain:
        \begin{equation}
            \sqrt{i}=\exp\big(\frac{i\pi}{4}\big)
                    =\cos\big(\frac{\pi}{4}\big)
                        +i\sin\big(\frac{\pi}{4}\big)
                    =\frac{1+i}{\sqrt{2}}
        \end{equation}
        We can check this solution by squaring:
        \begin{equation}
            \Big(\frac{1+i}{\sqrt{2}}\Big)^{2}=\frac{(1-1)+i(1+1)}{2}
                                              =\frac{2i}{2}
                                              =i
        \end{equation}
        in agreement with the definition of square roots.
    \end{lexample}
    We must be careful when evaluating square roots. We define the polar
    representation as $z=r\exp(i\theta)$, where
    $0\leq\theta<2\pi$. Problems can occur if we allow $\theta$ to be
    any real number. For note that $\exp(2\pi{i})=1=\exp(0i)$. Thus, we
    may naively perform the following computation:
    \begin{equation}
        1=\sqrt{1}=\exp(2\pi{i}/2)=\exp(\pi{i})=\minus{1}
    \end{equation}
    The angle $\theta\in[0,2\pi)$ that we use to represent $z$ is called
    the \textit{principal value of the argument}, and is often denoted
    $\mathrm{Arg}(z)$.
    \begin{lexample}{}{Roots_of_Unity}
        Let $f(z)=z^{n}-1$. This has a trivial root
        at $z=1$, and by the Fundamental Theorem of Algebra there are
        at most $n$ roots. The roots of this polynomial are called the
        \textit{roots of unity}. The real solutions are $1$ for odd $n$,
        and $\pm{1}$ for even $n$. In the complex world, there are
        always $n$ solutions. Interestingly enough, these points form an
        $n\textrm{-gon}$ around the origin of the complex plane. The
        solutions are:
        \begin{equation}
            z_{k}=\exp\Big(\frac{2\pi{i}{k}}{n}\Big)
            \quad\quad
            k=0,\,1,\,2,\,\dots,\,n-1
        \end{equation}
        Let's plot these solutions for various $n$.
        \begin{figure}[H]
            \centering
            \captionsetup{type=figure}
            \input{tikz/Complex_Roots_of_Unity}
            \caption{Roots of Unity for Degrees 3 to 6.}
            \label{fig:Comp_Roots_Unity}
        \end{figure}
        It should be clear from the definition of $f$ that the roots lie
        on the unit circle centered at the origin.
        While this is certainly an interesting and aesthetically
        appealing bit of mathematics, it also spells trouble for
        certain methods of numerical analysis. We'll return to
        this later when we discuss root finding algorithms.
    \end{lexample}
\subsection{Analytic Functions}
    We take a brief moment to talk about what it means
    to be analytic, the Cauchy-Riemann Equations, and
    Green's Theorem. The results here are counter-intuitive, and
    it is easy to apply certain results where they do not hold.
    \begin{ldefinition}{Entire Function}{Entire_Func}
        An entire function is a function
        $f:\mathbb{C}\rightarrow\mathbb{C}$ such that
        for all $z_{0}\in\mathbb{C}$, the following limit
        exists:
        \begin{equation}
            f'(z_{0})=\underset{z\rightarrow{z_{0}}}{\lim}
                      \frac{f(z)-f(z_{0})}{z-z_{0}}
        \end{equation}
        where $f'$ is called the derivative of $f$.
    \end{ldefinition}
    An entire function is simply a complex function that
    is \textit{differentiable} at every point in the
    complex plane. We can weaken this definition to include
    only some parts of the complex plane, and these are
    called \textit{holomorphic} functions.
    A function $f$ is analytic about the point $z_{0}$
    if its Taylor Series converges for all $z$ to $f(z)$ in some
    neighborhood of $z_{0}$:
    \begin{equation}
        f(z)=\sum_{n=0}^{\infty}\frac{f^{(n)}(z_{0})}{n!}(z-z_{0})^{n}
    \end{equation}
    \begin{lexample}{}{Exp_Func_Is_Entire}
        The exponential function is analytic, since we've  defined it
        as a power series. It is indeed entire as well, since:
        \begin{equation}
            \underset{z\rightarrow{z_{0}}}{\lim}
                \frac{\exp(z)-\exp(z_{0})}{z-z_{0}}
            =\exp(z_{0})\underset{z\rightarrow{z_{0}}}{\lim}
             \frac{\exp(z-z_{0})-1}{z-z_{0}}
        \end{equation}
        Letting $w=z-z_{0}$, we have:
        \begin{subequations}
            \begin{align}
                \underset{z\rightarrow{z_{0}}}{\lim}
                    \frac{\exp(z)-\exp(z_{0})}{z-z_{0}}
                &=\exp(z_{0})\underset{w\rightarrow{0}}{\lim}
                  \frac{\exp(w)-1}{w}\\
                &=\exp(z_{0})\underset{w\rightarrow{0}}{\lim}
                  \Big(1+\sum_{n=2}^{\infty}\frac{w^{n-1}}{n!}\Big)\\
                &=\exp(z_{0})
            \end{align}
            This proves $\exp$ is differentiable at every point
            $z_{0}\in\mathbb{C}$, and is thus entire.
        \end{subequations}
    \end{lexample}
    The remarkable fact of entire functions is that they are automatically
    analytic. This is certainly not true for real valued functions. One
    only need consider the example $f(x)=x|x|$. The derivative is
    $f'(x)=2|x|$, and this has no derivative at the
    origin. Similarly, there are functions with two derivatives,
    but not three. In the real world, having $n$ derivatives does
    not imply having $n+1$ derivatives. For complex functions, one
    derivative implies \textit{all} higher derivatives exist.
    \begin{lexample}{}{Smooth_Non_Analytic}%
        It is often believed that \textit{most} functions of a
        real variable are analytic, but the opposite is true. Real valued
        functions can be quite messy, and we need not construct overly
        pathological examples to show this. For consider the following:
        \begin{equation}
            f(x)=
            \begin{cases}
                \exp\big(\!\minus\!\frac{1}{x^{2}}\big),&x\ne{0}\\
                0,&x=0
            \end{cases}
        \end{equation}
        This is a function that most students of calculus can understand,
        and is everywhere \textit{smooth}: For all $x_{0}\in\mathbb{R}$,
        and for all $n\in\mathbb{N}$, the $n^{th}$ derivative
        $f^{(n)}(x_{0})$ exists.
        \begin{figure}[H]
            \centering
            \captionsetup{type=figure}
            \input{tikz/Smooth_Non_Analytic_Function_1.tex}
            \caption{A Smooth Function That is Not Analytic at the Origin}
            \label{fig:Smooth_Not_Analytic_At_Origin}
        \end{figure}
        However, this function is not analytic at the origin. This
        function approaches zero so quickly at the origin that for all
        $n\in\mathbb{N}$ we have:
        \begin{equation}
            \frac{\diff^{n}f}{\diff{x}^{n}}(0)=0
        \end{equation}
        The Taylor expansion is thus zero, the radius of convergence
        is infinite, but $f$ is not the zero function. Thus $f$ is a
        function that is smooth but not analytic.
    \end{lexample}
    \begin{lexample}{}{Smooth_Nowhere_Analytic}
        Further study of Ex.~\ref{ex:Smooth_Non_Analytic} reveals that
        $f$ is analytic \textit{everywhere else}, so one might expect
        smooth functions must be \textit{somewhere} analytic, but
        this is false. Consider:
        \begin{equation}
            F(x)=\sum_{k=0}^{\infty}\exp(\minus\sqrt{2^{k}})\cos(2^{k}x)
        \end{equation}
        Application of the $M$ test from calculus shows that this sum
        converges, and that all of its derivatives exist. However, for
        all $x\in\mathbb{R}$, the Taylor series:
        \begin{equation}
            \sum_{n=0}^{\infty}
                F^{(n)}(x_{0})\frac{(x-x_{0})^{n}}{n!}
        \end{equation}
        diverges for all $x\ne{x}_{0}$. So this function is smooth
        and \textit{nowhere} analytic.
    \end{lexample}
    \begin{figure}[H]
        \centering
        \input{tikz/Smooth_Non_Analytic_Function_2.tex}
        \caption{A Smooth and Nowhere Analytic Function}
        \label{fig:Smooth_But_Non_Analytic}
    \end{figure}
    The difficulties shown in Ex.~\ref{ex:Smooth_Non_Analytic} and
    Ex.~\ref{ex:Smooth_Nowhere_Analytic} vanish when we study functions
    of a complex variable. Given a function
    $f:\mathbb{C}\rightarrow\mathbb{C}$, differentiable at $z_{0}$
    implies twice differentiable at $z_{0}$, which further implies
    smooth at $z_{0}$, and this implies analytic at $z_{0}$. There is
    one step that is missing here: Continuous does \textit{not} imply
    differentiable. And, unfortunately, there are functions that
    \textit{look} differentiable (Meaning the formula used to represent
    them would make us think at first glance that they are
    differentiable), but are not. Again, as we will see, we need not
    construct overly pathological examples to show this.
    \begin{lexample}{}{Comp_Conj_is_Continuous}
        Let $f:\mathbb{C}\rightarrow\mathbb{C}$ be defined by
        $f(z)=\overline{z}$. That is, $f$ maps $x+iy$ to $x-iy$.
        Then $f$ is continuous at all $z\in\mathbb{C}$. For let
        $\varepsilon>0$ be given, and let $\delta=\varepsilon/2$.
        Then, for all $z_{0}$ such that $|z-z_{0}|<\delta$, we have:
        \begin{subequations}
            \begin{align}
                |f(z)-f(z_{0})|&=|x-iy-(x_{0}-iy_{0})|\\
                               &=|(x-x_{0})+i(y_{0}-y)|\\
                               &\leq|x-x_{0}|+|y-y_{0}|\\
                               &<\frac{\varepsilon}{2}
                                +\frac{\varepsilon}{2}=\varepsilon
            \end{align}
        \end{subequations}
        where we have applied the triangle inequality
        (Thm.~\ref{thm:Triangle_Inequality}) and
        Thm.~\ref{thm:Mod_of_Real_Part_LEQ_Mod} to derive these
        inequalities. Thus $f$ is a continuous function.
        We will soon see that $f$ is \textit{nowhere} differentiable.
    \end{lexample}
    To reveal such functions we will need to present the
    \textit{Cauchy-Riemann} equations. This set of equations provides both
    a \textit{necessary} and a \textit{sufficient} condition for a
    function $f:\mathbb{C}\rightarrow\mathbb{C}$ to be entire.
    We will prove the easy part: Entire functions satisfy the
    Cauchy-Riemann equations.
    \newpage
    \begin{ftheorem}{Cauchy-Riemann Theorem}{Cauchy_Riemann}
        If $f:\mathbb{C}\rightarrow\mathbb{C}$ is an entire function
        defined by:
        \begin{equation}
            f(z)=u(x,\,y)+iv(x,\,y)
        \end{equation}
        Then:
        \par
        \begin{subequations}
            \begin{minipage}{0.49\textwidth}
                \centering
                \begin{equation}
                    \frac{\partial{u}}{\partial{x}}=
                    \frac{\partial{v}}{\partial{y}}
                \end{equation}
            \end{minipage}
            \hfill
            \begin{minipage}{0.49\textwidth}
                \centering
                \begin{equation}
                    \frac{\partial{u}}{\partial{y}}=
                    -\frac{\partial{v}}{\partial{x}}
                \end{equation}
            \end{minipage}
        \end{subequations}
        \par
        \vspace{2.5ex}
    \end{ftheorem}
    \begin{bproof}
        As $f$ is entire, its complex derivative exists at every point
        in the complex plane. Let $z_{0}=x_{0}+iy_{0}$, and let
        $z=x+iy_{0}$. Taking the limit, we have:
        \begin{subequations}
            \begin{align}
                f'(z_{0})&=\underset{x\rightarrow{x_{0}}}{\lim}
                    \frac{u(x,\,y_{0})+iv(x,\,y_{0})
                         -u(x_{0},\,y_{0})-iv(x_{0},\,y_{0})}{x-x_{0}}\\
                &=\underset{x\rightarrow{x_{0}}}{\lim}
                    \frac{\big(u(x,\,y_{0})-u(x_{0},\,y_{0})\big)
                        +i\big(v(x,\,y_{0})-iv(x_{0},\,y_{0})\big)}
                            {x-x_{0}}\\
                &=\underset{x\rightarrow{x_{0}}}{\lim}
                    \Big(\frac{u(x,\,y_{0})-u(x_{0},\,y_{0})}
                              {x-x_{0}}\Big)
                +i\underset{x\rightarrow{x_{0}}}{\lim}
                    \Big(\frac{v(x,\,y_{0})-v(x_{0},\,y_{0})}
                              {x-x_{0}}\Big)
                \end{align}
        \end{subequations}
        Using the definition of \textit{partial derivatives}, we obtain:
        \begin{equation}
            \label{eqn:Cauchy_Riemann_x_Limit}%
            f'(z_{0})=
            \frac{\partial{u}}{\partial{x}}+
            i\frac{\partial{v}}{\partial{x}}
        \end{equation}
        Next we evaluate the limit along the path $z=x_{0}+iy$. Since the
        function is complex differentiable, any path as
        $z\rightarrow{z_{0}}$ will give the same value. Therefore:
        \begin{subequations}
            \begin{align}
                f'(z_{0})&=\underset{y\rightarrow{y_{0}}}{\lim}
                \frac{u(x_{0},\,y)+iv(x_{0},\,y)-
                      u(x_{0},\,y_{0})-iv(x_{0},\,y_{0})}{i(y-y_{0})}\\
                &=\underset{y\rightarrow{y_{0}}}{\lim}
                \frac{\big(u(x_{0},\,y)-u(x_{0},\,y_{0})\big)+
                     i\big(v(x_{0},\,y)-iv(x_{0},\,y_{0})\big)}
                        {i(y-y_{0})}\\
                &=\frac{1}{i}\underset{y\rightarrow{y_{0}}}{\lim}
                    \Big(\frac{u(x_{0},\,y)-u(x_{0},\,y_{0})}
                              {i(y-y_{0})}\Big)
                +\underset{y\rightarrow{y_{0}}}{\lim}
                    \Big(\frac{v(x_{0},\,y)-v(x_{0},\,y_{0})}
                              {(y-y_{0})}\Big)
            \end{align}
        \end{subequations}
        Recalling our result from Thm.~\ref{thm:Complex_Inverse}, the
        inverse of $i$ is $\minus{i}$. Again using the definition of
        partial derivatives:
        \begin{equation}
            \label{eqn:Cauchy_Riemann_y_Limit}%
            f'(z_{0})=-i\frac{\partial{u}}{\partial{y}}+
                        \frac{\partial{v}}{\partial{y}}
        \end{equation}
        Thus, equating
        Eqn.~\ref{eqn:Cauchy_Riemann_x_Limit} and
        Eqn.~\ref{eqn:Cauchy_Riemann_y_Limit}, we obtain:
        \begin{equation}
              \frac{\partial{u}}{\partial{x}}
            +i\frac{\partial{v}}{\partial{x}}
            = \frac{\partial{v}}{\partial{y}}
            -i\frac{\partial{u}}{\partial{y}}
        \end{equation}
        Comparing real and imaginary parts completes the proof.
    \end{bproof}
    This theorem excludes many functions from being analytic.
    \begin{lexample}{}{Real_Valued_Analytic_Func_Is_Constant}
        Let $f:\mathbb{C}\rightarrow\mathbb{C}$ be an analytic function,
        and suppose for all $z\in\mathbb{C}$, $f(z)$ is a \textit{real}
        number. That is, $f$ is a real-valued function. Then $f$
        must be a constant. For:
        \begin{equation}
            f(z)=u(x,\,y)+0i
        \end{equation}
        And from the Cauchy-Riemann equations, we have:
        \par
        \begin{subequations}
            \begin{minipage}[b]{0.49\textwidth}
                \centering
                \begin{equation}
                    \frac{\partial{u}}{\partial{x}}=0
                \end{equation}
            \end{minipage}
            \hfill
            \begin{minipage}[b]{0.49\textwidth}
                \centering
                \begin{equation}
                    \frac{\partial{u}}{\partial{y}}=0
                \end{equation}
            \end{minipage}
        \end{subequations}
        \par\hfill\par
        And thus we conclude that $u(x,\,y)=const$.Given a non-constant
        real function $f:\mathbb{R}\rightarrow\mathbb{R}$ that is
        analytic (Has a Taylor series), the complex extension
        $F(x+iy)=f(x)$ is \textit{not} analytic. Indeed, it is nowhere
        differentiable. Lastly, consider the function $f(z)=z$. This
        is indeed complex analytic. However:
        \begin{equation}
            \overline{f(z)}=x-iy
        \end{equation}
        is \textit{nowhere-analytic}. Indeed, it is nowhere
        differentiable. This is counterintuitive and reveals the bizarre
        nature of complex functions. It is worth recalling
        Ex.~\ref{ex:Comp_Conj_is_Continuous} where we showed that
        $\overline{f}$ is continuous. Thus, we see that continuity
        does not imply differentiability, even for complex valued
        functions.
    \end{lexample}
    We will return to this later when we discuss convolutions and the
    Hilbert transform. The Cauchy-Riemann equations seem to give some
    information for free. If we know $f(z)=u(x,\,y)+iv(x,\,y)$ is
    analytic, and we know $u(x,\,y)$, then we can determine $v(x,\,y)$, up
    to an additive constant. In the theory of signal processing, given a
    real valued function $u:\mathbb{R}\rightarrow\mathbb{R}$, also called
    a \textit{signal}, it will often be the case that we seek a real
    valued function $v:\mathbb{R}\rightarrow\mathbb{R}$, called the
    harmonic conjugate of $u$, such that $u+iv$ is the boundary of some
    analytic function. Imposing certain criteria on $u$ reveals that $v$
    is unique. Thus, given a complex signal where the imaginary part has
    been lost but the real part exists, we can recover the imaginary
    component by computing the harmonic conjugate of $u$. This is the
    \textit{Hilbert Transform}, and we return to it in the section about
    Fourier Analysis.
\subsection{Contour Integrals}
    Next we introduce contour integrals. Throughout this section,
    integration is meant in the sense of the Riemann integral.
    We start by defining \textit{Jordan Curves}.
    \begin{ldefinition}{Jordan Curve}{Jordan_Curve}
        A Jordan Curve in the Complex Plane is a continuous function
        $\Gamma:[0,1]\rightarrow\mathbb{C}$ such that
        $\Gamma(0)=\Gamma(1)$, and there are no values $0<x_{1}<x_{2}<1$
        such that $\Gamma(x_{1})=\Gamma(x_{2})$.
    \end{ldefinition}
    A simple example of a Jordan curve is a circle. Jordan curves are
    \textit{closed}, meaning they start where they end, and do not
    self-intersect. A Figure-8 is thus \textbf{not} a Jordan curve, but
    an ellipse is. An example of a Jordan curve is given below in
    Fig.~\subref{fig:Ex_of_Smooth_Jordan_Curve}. Much the way
    the closed unit interval $[0,1]$ has an ordering on it, a Jordan
    curve has a direction associated with it. Given a Jordan curve
    $\Gamma(t)$, one may change directions by defining
    $\reflectbox{\ensuremath{\Gamma}}(t)=\Gamma(1-t)$.
    While this will plot out the same curve in the complex plane, the
    direction is different and thus it represents a different path. When
    evaluating contour integrals, the direction matters.
    \begin{figure}[H]
        \captionsetup{type=figure}
        \centering
        \begin{subfigure}[b]{0.49\textwidth}
            \centering
            \input{tikz/Jordan_Curve_In_Complex_Plane}
            \subcaption{A Smooth Jordan Curve.}
            \label{fig:Ex_of_Smooth_Jordan_Curve}
        \end{subfigure}
        \begin{subfigure}[b]{0.49\textwidth}
            \centering
            \input{tikz/Jordan_Curve_In_Complex_Plane_Sector}
            \subcaption{An Example of a Sector.}
            \label{fig:Ex_of_Jordan_Curve_Sector}
        \end{subfigure}
        \caption{Jordan Curves in the Complex Plane}
        \label{fig:Ex_of_Jordan_Curve}
    \end{figure}
    For the sake of computation, we will stick to Jordan curves that are
    differentiable at all but finitely many points. A \textit{sector},
    which is the region contained within an arc of a circle, is an example
    of a Jordan curve that is differentiable at all but three points
    (Fig.~\subref{fig:Ex_of_Jordan_Curve_Sector}). We prove Green's
    Theorem for such curves, particularly curves that can be broken into a
    \textit{top} part and a \textit{bottom} part. While we wish to avoid
    presenting theorems without proof, some results are too difficult to
    include. We state the \textit{Jordan Curve Theorem}, but do not prove
    it. The proof can be found in a textbook on algebraic topology.
    \begin{theorem}
        If $\Gamma:\mathbb{R}\rightarrow\mathbb{R}^{2}$ is a Jordan curve,
        then $\Gamma$ separates the plane in to two disjoint parts: The
        interior, denoted $\Int(\Gamma)$, and the exterior. The interior
        is bounded, the exterior is unbounded, and
        $\Gamma$ is their common boundary.
    \end{theorem}
    A quick look at Fig.~\subref{fig:Ex_of_Smooth_Jordan_Curve} can
    convince one of the validity of this statement. We use the fact that a
    Jordan curve has an interior to state Green's Theorem, which is useful
    for the evaluation of complex integrals. A student of electromagnetism
    will already understand the importance and usefulness of Green's
    Theorem. The Weak Green's Theorem applies to \textit{simple} regions.
    There are two types of simple regions: Horizontal and Vertical.
    \begin{definition}
        A vertically simple region is a subset $D$ of
        the plane $\mathbb{R}^{2}$ such that there are
        two functions
        $g_{1},g_{2}:[a,b]\rightarrow\mathbb{R}$ such that:
        \begin{equation}
            D=\{\;(x,\,y)\,:\,a\leq{x}\leq{b},\,
                              g_{1}(x)\leq{y}\leq{g}_{2}(x)\;\}
        \end{equation}
    \end{definition}
    \begin{definition}
        A horizontally simple region is a subset $D$
        of the plane
        $\mathbb{R}^{2}$ such that there are two functions
        $g_{1},g_{2}:[a,b]\rightarrow\mathbb{R}$ such that:
        \begin{equation}
            D=\{\;(x,\,y)\,:\,a\leq{y}\leq{b},\,
                              g_{1}(y)\leq{x}\leq{g}_{2}(y)\;\}
        \end{equation}
    \end{definition}
    A vertically simple region is a subset of the plane bounded by two
    vertical lines, whereas a horizontally simple region is
    bounded by two horizontal lines (Fig.~\ref{fig:Simply_Regions}).
    \begin{figure}[H]
        \centering
        \captionsetup{type=figure}
        \begin{subfigure}[b]{0.49\textwidth}
            \centering
            \captionsetup{type=figure}
            \input{tikz/Complex_Plane_Vertically_Simple.tex}
            \subcaption{A Vertically Simple Region}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.49\textwidth}
            \centering
            \captionsetup{type=figure}
            \input{tikz/Complex_Plane_Horizontally_Simple.tex}
            \subcaption{A Horizontally Simple Region}
        \end{subfigure}
        \caption{Examples of Simple Regions}
        \label{fig:Simply_Regions}
    \end{figure}
    We now prove special cases of Green's Theorem for vertically and
    horizontally simple regions, and then tie this together for the weak
    form of Green's Theorem.
    \begin{theorem}
        \label{thm:Greens_Theorem_Simple_t1_region}%
        If $M:\mathbb{R}^{2}\rightarrow\mathbb{R}$ is
        a differentiable function and if $\Gamma$ is a Jordan curve such
        that $\Int(\Gamma)$ is vertically simple, then:
        \begin{equation}
            \iint_{\Int(\Gamma)}\frac{\partial{M}}{\partial{y}}\diff{A}
            =-\oint_{\Gamma}M\diff{x}
        \end{equation}
    \end{theorem}
    \begin{proof}
        Since the interior of $\Gamma$ is a vertically simple region,
        there are two functions
        $g_{1}$, $g_{2}:[a,b]\rightarrow\mathbb{R}$ such that:
        \begin{equation}
            \Gamma=\{\;(x,y)\,:\,a\leq{x}\leq{b},\,
                                 g_{1}(x)\leq{y}\leq{g}_{2}(x)\;\}
        \end{equation}
        But then:
        \begin{subequations}
            \label{eqn:weak_type1_greens_theorem_eq1}%
            \begin{align}
                \iint_{\Int(\Gamma)}
                    \frac{\partial{M}}{\partial{y}}\diff{A}
                &=\int_{a}^{b}\int_{g_{1}(x)}^{g_{2}(x)}
                    \frac{\partial{M}}{\partial{y}}
                    \diff{y}\diff{x}\\
                &=\int_{a}^{b}
                    \Big(M\big(x,g_{2}(x)\big)
                    -M\big(x,g_{1}(x)\big)\Big)\diff{x}\\
                &=-\int_{a}^{b}
                    \Big(M\big(x,g_{1}(x)\big)
                    -M\big(x,g_{2}(x)\big)\Big)\diff{x}
            \end{align}
        \end{subequations}
        But since $\Int(\Gamma)$ is simple, the path at $x=a$ is either a
        point or a vertical straight line. But then the integral along
        this portion with respect to $x$ is zero. Similarly for
        $x=b$, and therefore:
        \begin{equation}
            \label{eqn:weak_type1_greens_theorem_eq2}%
            \oint_{\Gamma}M\diff{x}
            =\int_{a}^{b}\Big(M\big(x,g_{1}(x)\big)
                -M\big(x,g_{2}(x)\big)\Big)\diff{x}
        \end{equation}
        Comparing Eqn.~\ref{eqn:weak_type1_greens_theorem_eq1}
        and Eqn.~\ref{eqn:weak_type1_greens_theorem_eq2}
        completes the proof.
    \end{proof}
    \begin{theorem}
        \label{thm:Greens_Theorem_Simple_t2_region}%
        If $N:\mathbb{R}^{2}\rightarrow\mathbb{R}$ is a differentiable
        function and if $\Gamma$ is a Jordan curve such that
        $\Int(\Gamma)$ is horizontally simple, then:
        \begin{equation}
            \iint_{\Int(\Gamma)}\frac{\partial{M}}{\partial{x}}\diff{A}
            =\oint_{\Gamma}N\diff{y}
        \end{equation}
    \end{theorem}
    \begin{proof}
        The proof is a mimicry of the proof for
        Thm.~\ref{thm:Greens_Theorem_Simple_t1_region},
        but since the orientation of the path changes
        since we are now integrating with respect to $y$,
        we pick up a minus sign in the contour integral.
    \end{proof}
    \begin{theorem}[Weak Green's Theorem]
        If $M:\mathbb{R}^{2}\rightarrow\mathbb{R}$ and
        $N:\mathbb{R}^{2}\rightarrow\mathbb{R}$ are
        differentiable functions, and if
        $\Gamma$ is a Jordan curve such that the
        interior of $\Gamma$ is vertically and horizontally
        simple (A rectangular region), then:
        \begin{equation}
            \oint_{\Gamma}(M\diff{x}+N\diff{y})=
            \iint_{\Int(\Gamma)}\Big(
            \frac{\partial{N}}{\partial{x}}-
            \frac{\partial{M}}{\partial{y}}\Big)\diff{A}
        \end{equation}
    \end{theorem}
    \begin{proof}
        Since $\Int(\Gamma)$ is both vertically and
        horizontally simple, we may sum the results from
        Thm.~\ref{thm:Greens_Theorem_Simple_t1_region}
        and
        Thm.~\ref{thm:Greens_Theorem_Simple_t2_region},
        completing the proof.
    \end{proof}
    While this is not quite what we want, since most
    regions we wish to integrate over will not be simple,
    we can approximate the interior of a smooth Jordan
    curve arbitrarily well by a finite collection of
    simple regions. The full proof will get into the
    mechanics of this approximation, and show that in
    the \textit{limit} we obtain the result. We'll state
    Green's Theorem, but neglect the full proof.
    \begin{ltheorem}{Green's Theorem}{Greens_Theorem}
        If $M:\mathbb{R}^{2}\rightarrow\mathbb{R}$ and
        $N:\mathbb{R}^{2}\rightarrow\mathbb{R}$ are
        differentiable functions, and if
        $\Gamma$ is a Jordan curve that is differentiable at
        all but finitely many points, then:
        \begin{equation}
            \oint_{\Gamma}(M\diff{x}+N\diff{y})=
            \iint_{\Int(\Gamma)}\Big(
            \frac{\partial{N}}{\partial{x}}-
            \frac{\partial{M}}{\partial{y}}\Big)\diff{A}
        \end{equation}
    \end{ltheorem}
    We return to complex analysis and prove one of the central results of
    the theory: Cauchy's Integral Theorem.
    \newpage
    \begin{ftheorem}{Cauchy's Integral Theorem}{Cauchy_Int_Theorem}
        If $f:\mathbb{C}\rightarrow\mathbb{C}$ is an entire function, and
        if $\Gamma:[0,1]\rightarrow\mathbb{C}$ is a Jordan curve
        differentiable at all but finitely many points, then:
        \begin{equation}
            \oint_{\Gamma}f(z)\diff{z}=0
        \end{equation}
    \end{ftheorem}
    \begin{bproof}
        For let $f(z)=u(x,\,y)+iv(x,\,y)$. Then:
        \begin{subequations}
            \begin{align}
                \oint_{\Gamma}f(z)\diff{z}
                &=\oint_{\Gamma}\big(u(x,\,y)+iv(x,\,y)\big)
                    \big(\diff{x}+i\diff{y}\big)\\
                \nonumber&=\oint_{\Gamma}
                    \big(u(x,\,y)\diff{x}-v(x,\,y)\diff{y}\big)\\
                &\hspace{2cm}+i\oint_{\Gamma}
                    \big(v(x,\,y)\diff{x}+u(x,\,y)\diff{y}\big)
            \end{align}
        \end{subequations}
        As $f$ is entire, $u$ and $v$ are differentiable.
        Applying Green's Theorem, we obtain:
        \begin{equation}
            \oint_{\Gamma}f(z)\diff{z}
            =\iint_{\Int(\Gamma)}
            \Big(\frac{\partial{u}}{\partial{y}}+
                 \frac{\partial{v}}{\partial{x}}\Big)
                 \diff{A}+
            i\iint_{\Int(\Gamma)}
            \Big(\frac{\partial{u}}{\partial{x}}-
                 \frac{\partial{v}}{\partial{y}}\Big)\diff{A}
        \end{equation}
        But since $f$ is entire, $u$ and $v$ satisfy
        the Cauchy-Riemann equations. That is:
        \par\hfill\par
        \begin{subequations}
            \begin{minipage}{0.49\textwidth}
                \centering
                \begin{equation}
                    \frac{\partial{u}}{\partial{x}}-
                    \frac{\partial{v}}{\partial{y}}=0
                \end{equation}
            \end{minipage}
            \hfill
            \begin{minipage}{0.49\textwidth}
                \centering
                \begin{equation}
                    \frac{\partial{u}}{\partial{y}}+
                    \frac{\partial{v}}{\partial{x}}=0
                \end{equation}
            \end{minipage}
        \end{subequations}
        \par\hfill\par
        Thus the integrands of both double integrals are zero,
        and hence the integrals are zero. This completes the proof.
    \end{bproof}
    Finally we prove Jordan's Lemma. This is used
    in conjunction with Cauchy's Integral Theorem to
    provide a powerful means of computing the integrals
    of difficult functions. In particular, this is used
    to evaluate the limits of the \textit{Fresnel Integrals}.
    \begin{theorem}[Jordan's Inequality]
        \label{thm:Jordan_Inequality}%
        If $x\in[0,\,\frac{\pi}{2}]$, then:
        \begin{equation}
            \frac{2}{\pi}x\leq\sin(x)
        \end{equation}
    \end{theorem}
    \begin{proof}
        For let $f:[0,\,\frac{\pi}{2}]\rightarrow\mathbb{R}$ be defined
        by $f(x)=\frac{2}{\pi}x$. Then $f(0)=\sin(0)$ and
        $f(\frac{\pi}{2})=\sin(\frac{\pi}{2})$. But since
        $\sin$ is concave down on the interval $[0,\,\frac{\pi}{2}]$,
        it is impossible for $\sin(x)<f(x)$ on the open interval
        $(0,\,\frac{\pi}{2})$, and therefore we have that
        $\sin(x)\geq{f}(x)$. Therefore, etc.
    \end{proof}
    This simple theorem is best understood by graphing the two functions.
    We can use this to prove Jordan's Lemma, and this will conclude our
    discussion of complex analysis.
    \begin{ltheorem}{Jordan's Lemma}{Jordans_Lemma}
        If $g:\mathbb{C}\rightarrow\mathbb{C}$ is a continuous function,
        if $\theta_{0}\in[0,\,\pi]$, if $R$ and $a$ are positive
        real numbers, and if $\gamma_{R}$ is the arc from
        $R$ to $R\exp(i\theta_{0})$, then:
        \begin{equation}
            \Big|\int_{\gamma_{R}}\exp(iaz)g(z)\diff{z}\Big|
            \leq\frac{\pi}{a}M_{R}
        \end{equation}
        Where $M_{R}=\max\,\{\;|g(z)|\,:\,z\in\gamma_{R}\;\}$.
    \end{ltheorem}
    \begin{proof}
        Applying the triangle inequality
        (Thm.~\ref{thm:Triangle_Inequality}) for integrals,
        Euler's Theorem (Thm.~\ref{thm:Euler_Expo_Formula}) and
        integrating in polar coordinates, we have:
        \begin{subequations}
            \begin{align}
                \Big|\int_{\gamma_{R}}\exp(iaz)g(z)\diff{z}\Big|
                &=\Big|\int_{\gamma_{R}}\exp(iaz)g(z)iR\exp(i\theta)
                    \diff{\theta}\Big|\\
                &\leq\int_{\gamma_{R}}\big|\exp(iaz)g(z)iR\exp(i\theta)
                    \big|\diff{\theta}\\
                &=R\int_{\gamma_{R}}\big|\exp(iaz)g(z)\big|\diff{\theta}\\
                &=R\int_{\gamma_{R}}\big|
                    \exp\big[iaR\big(\cos(\theta)+i\sin(\theta)\big)\big]
                    g(z)\big|\diff{\theta}\\
                &=R\int_{\gamma_{R}}\big|
                    \exp\big[aR\big(i\cos(\theta)-\sin(\theta)\big)\big]
                    g(z)\big|\diff{\theta}\\
                &=R\int_{\gamma_{R}}\big|
                    \exp\big(\!\minus\!aR\sin(\theta)\big)g(z)
                \big|\diff{\theta}\\
                &\leq{R}M_{R}\int_{\gamma_{R}}
                    \exp\big(\!\minus\!aR\sin(\theta)\big)\diff{\theta}
            \end{align}
        \end{subequations}
        Finally, applying Jordan's inequality
        (Thm.~\ref{thm:Jordan_Inequality}), we have:
        \begin{subequations}
            \begin{align}
                \Big|\int_{\gamma_{R}}\exp(iaz)g(z)\diff{z}\Big|
                &\leq{R}M_{R}\int_{\gamma_{R}}
                    \exp\Big(\!\minus\!\frac{2aR\theta}{\pi}\Big)
                \diff{\theta}\\
                &\leq\frac{\pi}{a}\big(1-\exp(\minus{a}R)\big)M_{R}\\
                &\leq\frac{\pi}{a}M_{R}
            \end{align}
        \end{subequations}
        Therefore, etc.
    \end{proof}
    \newpage
\section{Complex Variables}
    A complex function is a function whose argument is a complex
    variable $z=x+iy$, where $i$ is the imaginary unit. Complex
    functions can have the problem of being multi-valued, which
    is a cause for caution when dealing with them. For example,
    in the complex realm every non-zero complex number $z$
    has two square roots $\sqrt{z}$. So the square root
    function is multi-valued. Any complex function $f(z)$ can
    be written as $f(z)=u(x,y)+iv(x,y)$, where $u$ and $v$ are
    purely real functions. The function $w=f(z)$ can be seen
    as a mapping, or transformation, of the $z$ plane to
    the $w$ plane. That is, $f$ is a transformation of
    its domain onto its range, or image. A compound complex
    function is one of the form $F(z)=g(f(z))$. Since complex
    functions are functions of two variables, in a sense, one
    must be careful when considering limits of complex functions.
    \begin{example}
        What is the limit of $z/\overline{z}$ as $z\rightarrow{0}$?
        This is undefined. For:
        \begin{equation*}
            \frac{z}{\overline{z}}=\frac{x+iy}{x-iy}
        \end{equation*}
        Letting $x=0$ and taking the limit on $y$,
        we get:
        \begin{equation*}
            \frac{0+iy}{0-iy}=-1
        \end{equation*}
        Letting $y=0$ and taking the limit on $x$,
        we get:
        \begin{equation*}
            \frac{x+0i}{x-0i}=1
        \end{equation*}
        So the limit does not exist.
    \end{example}
    Continuity and the various properties of limits
    are defined similarly on $\mathbb{C}$ as for
    $\mathbb{R}$, with distance between points being
    defined by
    $d(z_{1},z_{2})=\sqrt{(x_{2}-x_{1})^{2}+(y_{2}-y_{1})^{2}}$.
    Differentiation is defined as:
    \begin{equation*}
        f'(z_{0})=\lim_{z\rightarrow{z_{0}}}\frac{f(z)-f(z_{0})}{z-z_{0}}
    \end{equation*}
    \begin{theorem}
        A complex function $f(z)=u(x,y)+iv(x,y)$ is
        differentiable if and only if it satisfies
        the Cauchy-Riemann equations:
        \begin{align*}
            \frac{\partial{u}}{\partial{x}}
            &=\frac{\partial{v}}{\partial{y}}
            &
            \frac{\partial{u}}{\partial{y}}
            &=-\frac{\partial{v}}{\partial{x}}
        \end{align*}
    \end{theorem}
    \begin{theorem}
        If $f(z)=u(x,y)+iv(x,y)$ is differentiable,
        then:
        \begin{equation*}
            f'(z)=u_{x}(x,y)+iv_{y}(x,y)
        \end{equation*}
    \end{theorem}
    \begin{definition}
        A complex function $f(z)$ is analytic,
        or holomorphic, at a point $z_{0}$ if
        it is differentiable in some neighborhood of
        $z_{0}$.
    \end{definition}
    \begin{definition}
        An entire function is a complex function
        $f(z)$ such that $f$ is analytic at every
        point $z\in\mathbb{C}$.
    \end{definition}
    \begin{definition}
        A harmonic function is a function
        $A(x,y)$ such that all of its second
        partial derivatives exists, and it
        satisfies the Laplace Equation:
        \begin{equation*}
            \nabla^{2}A
            =A_{xx}(x,y)+A_{yy}(x,y)
            =0
        \end{equation*}
    \end{definition}
    \begin{theorem}
        If $f(z)=u(x,y)+iv(x,y)$ is differentiable
        on a domain $D$, then $u$ and $v$ are
        harmonic on the domain.
    \end{theorem}
    \begin{theorem}
        A function $f(z)$ is analytic if and only if
        its real and complex parts are harmonic
        conjugates of each other.
    \end{theorem}
    \begin{definition}
        A level curve of a function $f(x,y)$ is
        a curve in $\mathbb{R}^{2}$ such that
        $f$ is constant on that curve.
    \end{definition}
    One of the most basic and fundamental results from
    complex variables is Euler's Formula:
    \begin{equation*}
        \exp(i\theta)=\cos(\theta)+i\sin(\theta)
    \end{equation*}