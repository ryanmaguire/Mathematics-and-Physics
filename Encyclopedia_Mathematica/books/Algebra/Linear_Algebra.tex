\chapter{Linear Algebra}
    \section{Linear Algebra}
        \begin{lexample}
            If $V$ and $W$ are $2-$dimensional subspaces in
            $\mathbb{R}^{4}$, what are the possible dimensions of
            $V\cap W$. If $V$ and $W$ are subspaces, then
            ${V}\cap{W}$ is subspace, and
            $\dim({V}\cap{W}\leq\min(\{\dim(V),\dim(W)\})$
            we have in our problem that $\dim\{V\cap W\}\leq 2$. We
            now must show that $V\cap W$ can have dimensions 0,1, or
            2. If $V=\{(x,y,0,0):x,y\in\mathbb{R}\}$ and
            $W=\{(0,0,z,w):z,w\in \mathbb{R}\}$, then
            ${V}\cap{W}=\{(0,0,0,0)\}$ which has dimension $0$.
            If $V=\{(x,y,0,0):x,y\in\mathbb{R}\}$ and
            $W=\{(0,y,z,0):y,z\in\mathbb{R}\}$,
            then ${V}\cap{W}=\{(0,y,0,0):y\in\mathbb{R}\}$
            which has dimension $1$. Finally, if $V=W$ then
            ${V}\cap{W}=V$, which has dimension $2$. So, the only
            possibilities are $0,1$, or $2$.
        \end{lexample}
        A system of linear equations can be written
        using an equivalent matrix notation.
        \begin{example}
            \begin{align*}
                a_{11}x_{1}+a_{12}x_{2}+a_{13}x_{3}&=b_{1}\\
                a_{21}x_{1}+a_{22}x_{2}+a_{23}x_{3}&=b_{2}\\
                a_{31}x_{1}+a_{32}x_{2}+a_{33}x_{3}&=b_{3}\\
            \end{align*}
            This is equivalent to either of the following:
            \begin{align*}
                \begin{bmatrix}
                    a_{11}&a_{12}&a_{13}\\
                    a_{21}&a_{22}&a_{31}\\
                    a_{31}&a_{32}&a_{33}
                \end{bmatrix}
                \begin{bmatrix}
                    x_{1}\\
                    x_{2}\\
                    x_{3}
                \end{bmatrix}
                &=
                \begin{bmatrix}
                    b_{1}\\
                    b_{2}\\
                    b_{3}
                \end{bmatrix}
                &
                \mathbf{A}\mathbf{x}
                &=\mathbf{b}
            \end{align*}
        \end{example}
        Matrices can also be written as $\mathbf{A}=(a_{ij})$.
        The following rules are used to
        define matrix arithmetic.
        \begin{enumerate}
            \item Addition: To add two matrices, add their
                corresponding elements. That is, if
                $\mathbf{A}=(a_{ij})$ and $\mathbf{B}=(b_{ij})$,
                then $\mathbf{A}+\mathbf{B}=(a_{ij}+b_{ij})$.
                Matrix addition is only defined on matrices of
                the same size.
            \item Scale multiplication: To multiply a
                matrix by a real or complex number $c$,
                multiply this number to every element. That is,
                if $\mathbf{A}=(a_{ij})$, then
                $c\mathbf{A}=({c}\cdot{a_{ij}})$
            \item Matrix Multiplication: The product of
                and ${M}\times{N}$ matrix with an
                ${N}\times{P}$ matrix is defined by
                $\mathbf{C}=\mathbf{A}\mathbf{B}$, where
                $(c_{ij})=(\sum_{k=1}^{N}a_{ik}b_{kj})$. Note
                that it is possible for
                $\mathbf{A}\mathbf{B}\ne\mathbf{B}\mathbf{A}$.
                Indeed, it is possible for
                $\mathbf{A}\mathbf{B}$ to be defined, whereas
                $\mathbf{B}\mathbf{A}$ is undefined.
        \end{enumerate}
        \begin{example}
            Let the following be true:
            \begin{align*}
                A&=
                \begin{bmatrix}
                    1&2\\
                    3&4
                \end{bmatrix}
                &
                B&=
                \begin{bmatrix}
                    5&6\\
                    7&8
                \end{bmatrix}
            \end{align*}
            Then, using the defined rules, we have:
            \begin{align*}
                A+B&=
                \begin{bmatrix}
                    6&8\\
                    10&12
                \end{bmatrix}
                &
                5A&=
                \begin{bmatrix}
                    5&10\\
                    15&20
                \end{bmatrix}
                \\
                AB&=
                \begin{bmatrix}
                    19&22\\
                    43&50
                \end{bmatrix}
                &
                BA&=
                \begin{bmatrix}
                    23&34\\
                    31&46
                \end{bmatrix}
            \end{align*}
        Note that even in this trivial example,
        ${AB}\ne{BA}$.
        \end{example}
        \begin{definition}
            The ${n}\times{n}$ identity matrix is the matrix
            $I_{n}=(I_{ij})$, where
            $I_{ij}=%
            \begin{cases}%
             0,&{i}\ne{j}\\%
             1,&{i}={j}%
            \end{cases}$
        \end{definition}
        \begin{definition}
            An inverse matrix of an ${n}\times{n}$ matrix
            $A$ is a matrix $A^{-1}$ such that
            $AA^{-1}=A^{-1}A=I_{n}$
        \end{definition}
        Not every matrix has an inverse matrix. If one
        does exists, there are many properties it contains.
        \begin{theorem*}
            The following are true:
            \begin{enumerate}
                \item If $\mathbf{A}$ and $\mathbf{B}$
                    are invertible ${n}\times{n}$ matrices,
                    then $\mathbf{A}\mathbf{B}$ is invertible
                    and
                    $\mathbf{A}\mathbf{B}^{-1}%
                     =\mathbf{B}^{-1}\mathbf{A}^{-1}$
                \item If $\mathbf{A}$ is an invertible matrix,
                    then $\mathbf{A}^{-1}$ is an invertible
                    matrix and
                    $(\mathbf{A}^{-1})^{-1}=\mathbf{A}$
            \end{enumerate}
        \end{theorem*}
        \begin{definition}
            The trace of an ${n}\times{n}$ matrix
            $A$ is the sum of
            it's diagonal: $\Tr(A)=\sum_{i=1}^{n}a_{ii} $
        \end{definition}
        \begin{example}
            \begin{equation*}
                \Tr\Bigg(
                \begin{bmatrix}
                    4&5&6\\
                    1&2&3\\
                    8&8&3
                \end{bmatrix}
                \Bigg)
                =4+2+3=9
            \end{equation*}
        \end{example}
        \begin{definition}
            The determinant of a ${2}\times{2}$ matrix
            $A=%
             \begin{bmatrix}%
                a&b\\%
                c&d%
             \end{bmatrix}$
            is $\det(A)=ad-bc$
        \end{definition}
        \begin{definition}
            The minor of the $i^{th}$ row and $j^{th}$
            column of an ${n}\times{n}$ matrix $\mathbf{A}$,
            denoted $M_{ij}$, is the determinant of the
            ${(n-1)}\times{(n-1)}$ matrix formed by
            removing the $i^{th}$ row and $j^{th}$ column
            from $\mathbf{A}$.
        \end{definition}
        \begin{definition}
            The cofactor of the minor $M_{ij}$ of an
            ${n}\times{n}$ matrix $\mathbf{A}$,
            denoted $C_{ij}$, is $(-1)^{i+j}M_{ij}$.
        \end{definition}
        \begin{example}
            \begin{align*}
                A&=
                \begin{bmatrix}
                    7&1&3\\
                    1&3&5\\
                    17&4&20
                \end{bmatrix}
                &
                M_{11}
                &=
                \det\Bigg(\begin{bmatrix}
                         3&5\\
                         4&20
                     \end{bmatrix}
                    \Bigg)
                =40
                &
                C_{11}
                &=(-1)^{1+1}M_{11}=40
            \end{align*}
        \end{example}
        \begin{definition}
            The determinant of an ${n}\times{n}$ matrix
            $\mathbf{A}$ is
            $\det(A)=\sum_{j=1}^{n}a_{1j}C_{1j}$
        \end{definition}
        \begin{theorem*}
            If $\mathbf{A}$ is an ${n}\times{n}$ matrix
            and ${1}\leq{i}\leq{n}$, then
            $\det(A)=\sum_{j=1}^{n}a_{ij}C_{ij}$
        \end{theorem*}
        \begin{definition}
            The transpose of an ${n}\times{m}$ matrix
            $\mathbf{A}$, denoted $\mathbf{A}^{T}$,
            is the ${m}\times{n}$ matrix formed by
            swapping the rows and columns of $\mathbf{A}$
            with each other. That is $(a_{ij})^{T}=(a_{ji})$.
        \end{definition}
        \begin{definition}
            A symmetric matrix is a matrix $\mathbf{A}$
            such that $\mathbf{A}^{T}=\mathbf{A}$
        \end{definition}
        \begin{theorem*}
            If $\mathbf{A}$ is an ${n}\times{m}$ matrix and
            $\mathbf{B}$ is an ${m}\times{p}$ matrix, then
            the following are true:
            \begin{enumerate}
                \begin{multicols}{2}
                    \item $(\mathbf{A}^{T})^{T}=\mathbf{A}$
                    \item $(\mathbf{A}+\mathbf{B})^{T}%
                           =\mathbf{A}^{T}+\mathbf{B}^{T}$
                    \item $(k\mathbf{A})^{T}=k\mathbf{A}^{T}$
                    \item $(\mathbf{A}\mathbf{B})^{T}%
                           =\mathbf{B}^{T}\mathbf{A}^{T}$
                \end{multicols}
            \end{enumerate}
        \end{theorem*}
        \begin{definition}
            The adjoint of an ${n}\times{n}$ matrix
            $\mathbf{A}$, denoted $\adjoint(\mathbf{A})$,
            is the matrix $(C_{ij})^{T}$.
        \end{definition}
        \begin{theorem*}
            If ${\det(\mathbf{A})}\ne{0}$, then $\mathbf{A}$
            is invertible and
            $\mathbf{A}^{-1}=%
             \frac{1}{\det(\mathbf{A})}\adjoint(\mathbf{A})$
        \end{theorem*}
        \begin{theorem*}
            If $\mathbf{A}$ and $\mathbf{B}$ are
            ${n}\times{n}$ matrices, then the following
            are true:
            \begin{enumerate}
                \begin{multicols}{3}
                    \item $\det(\mathbf{A})%
                           =\det(\mathbf{A}^{T})$
                    \item $\det(k\mathbf{A})%
                           =k^{n}\det(\mathbf{A})$
                    \item $\det(\mathbf{A}\mathbf{B})%
                           =\det(\mathbf{A})\det(\mathbf{B})$
                \end{multicols}
            \end{enumerate}
        \end{theorem*}
        \begin{theorem*}
            A matrix $\mathbf{A}$ is invertible if and only
            if ${\det(\mathbf{A})}\ne{0}$
        \end{theorem*}
        \begin{theorem*}
            If $\mathbf{A}$ is invertible, then
            $\det(\mathbf{A}^{-1})=\frac{1}{\det(\mathbf{A})}$
        \end{theorem*}
        The differential equation
        $\sum_{k=0}^{n}a_{k}y^{(k)}(x)$ Can be expression
        in terms of the characteristic polynomial
        $\sum_{k=0}^{n}a_{k}D^{k}$. Factoring this linear
        operator into $\Pi_{k=0}^{n}(D-r_{k})$,
        the general solution is
        $y(x)=\sum_{k=1}^{n}c_{k}e^{r_{k}x}$. If some of the
        $r_{k}$ repeat, we have $c_{k}x^{m_{k}-1}e^{r_{k}x}$,
        where $m_{k}$ is the number of repetitions.
        In general, if we have
        $\Pi_{k=0}^{n}(D-r_{k})^{m_{k}}$, the general
        solution is
        $y(x)=%
         \sum_{k=1}^{n}c_{k}e^{r_{k}x}%
         (\sum_{j=0}^{m_{k}-1}x^{j})$
        \begin{example}
            \
            \begin{enumerate}
                \item $y'''-4y''+4y'=0$ has the characteristic
                    polynomial $D(D-2)^{2}$, so
                    $y(x)=c_{1}+c_{2}e^{2x}+c_{3}xe^{2x}$
            \end{enumerate}
        \end{example}
        In linear algebra, the determinant
        $\det(\mathbf{A}-\lambda{I})$ is the characteristic
        polynomial of the square matrix $\mathbf{A}$.
        \begin{definition}
            A vector space $V$ over a Field (Set of scalars)
            $F$ is a set $V$ with two operations
            $+$ and $\cdot$
            such that the following are true:
            \begin{enumerate}
                \begin{multicols}{3}
                    \item $\forall_{{\mathbf{a},%
                                     \mathbf{b}}\in{V}}$
                          ${\mathbf{a}+\mathbf{b}}\in{V}$
                    \item $\mathbf{a}+\mathbf{b}%
                           =\mathbf{b}+\mathbf{a}$
                    \item $\mathbf{a}+(\mathbf{b}+\mathbf{c})%
                           =(\mathbf{a}+\mathbf{b})+\mathbf{c}$
                    \item $\forall_{\mathbf{a}\in{V}}%
                           \exists_{\mathbf{b}\in{V}}:%
                           \mathbf{a}+\mathbf{b}=\mathbf{0}$
                    \item $\forall_{{k}\in{F},\mathbf{a}\in{V}}$
                          $k\mathbf{a}\in{V}$
                    \item $k(\mathbf{a}+\mathbf{b})%
                           =k\mathbf{a}+k\mathbf{b}$
                    \item $(k_{1}+k_{2})\mathbf{a}%
                           =k_{1}\mathbf{a}+k_{2}\mathbf{a}$
                    \item $1\mathbf{a}=\mathbf{a}$
                    \item $k_{1}(k_{2}\mathbf{a})%
                           =(k_{1}k_{2})\mathbf{a}$
                \end{multicols}
            \end{enumerate}
        \end{definition}
        \begin{theorem*}
            If $V$ is a vector space, then there is a
            $\mathbf{0}\in{V}$ such that for all
            $\mathbf{a}\in{V}$,
            $\mathbf{a}+\mathbf{0}=\mathbf{a}$
        \end{theorem*}
        \begin{definition}
            A linearly dependent subset of a vector space
            $V$ (Over $\mathbb{R}$)
            is a subset ${S}\subset{V}$ such that
            there exists an $N\in\mathbb{N}$, a non-zero
            $a_{n}:\mathbb{Z}_{N}\rightarrow\mathbb{R}$
            and an injective function
            $\mathbf{v}_{n}:\mathbb{Z}_{N}\rightarrow{V}$
            such that
            $\sum_{k=1}^{N}a_{n}\mathbf{v}_{n}=\mathbf{0}$
        \end{definition}
        \begin{definition}
            A linearly independent subset of a vector space
            $V$ is a subset ${S}\subset{V}$ that is not
            linearly dependent.
        \end{definition}
        \begin{theorem*}
            If $V\subset\mathbb{R}^{n}$ has more than
            $n$ vectors, then $V$ is linearly dependent.
        \end{theorem*}
        \begin{definition}
            The rank of a matrix is the number
            of linearly independent columns of
            the matrix.
        \end{definition}
        \begin{example}
            Let
            $\mathbf{A}=[A_{1}\ A_{2}]$
            where $A_{1}=(1,2)^{T}$ and
            $A_{2}=(2,4)^{T}$. So
            $2A_{1}+(-1)A_{2}=(0,0)^{T}=\mathbf{0}$. Therefore
            $\{A_{1},A_{2}\}$ is a linearly independent
            subset. Thus, $\rk(\mathbf{A})=1$.
        \end{example}
        \begin{definition}
            A matrix with full rank is a square
            ${n}\times{n}$ matrix $\mathbf{A}$ such that
            $\rk(\mathbf{A})=n$.
        \end{definition}
        \begin{theorem*}
            If $\mathbf{A}$ is a square matrix and
            $\det(\mathbf{A})\ne{0}$, then $\mathbf{A}$
            has full rank.
        \end{theorem*}
        \begin{theorem*}
            If $\mathbf{A}$ is a square matrix
            with full rank, then it is invertible.
        \end{theorem*}
        \begin{definition}
            A finite basis of a vector space $V$ is a
            linearly independent subset ${S}\subset{V}$
            where
            $S=\{\mathbf{v}_{k}\}_{k=0}^{n}$
            and for all
            $\mathbf{x}\in{V}$ there is an
            $a_{n}:\mathbb{Z}_{n}\rightarrow\mathbb{R}$
            such that
            $\mathbf{x}=\sum_{k=1}^{n}a_{k}\mathbf{v}_{k}$
        \end{definition}
        \begin{theorem*}
            All bases of a vector space $V$ have the
            same number of elements.
        \end{theorem*}
        \begin{definition}
            The dimension of a vector space
            $V$ is the number of elements in any
            basis of $V$.
        \end{definition}
        \begin{definition}
            An inner product on a vector space $V$ is a function
            $\langle|\rangle:{V}\times{V}\rightarrow\mathbb{R}$
            such that:
            \begin{enumerate}
                \begin{multicols}{3}
                    \item $\langle{v_{1},v_{2}}\rangle%
                           =\langle{v_{2},v_{3}}\rangle$
                    \item $\langle{v_{1},v_{1}}\rangle\geq{0}$
                    \item $\langle{v_{1}+v_{2},v_{3}}\rangle%
                           =\langle{v_{1},v_{3}}\rangle%
                           +\langle{v_{2},v_{3}}\rangle$
                \end{multicols}
            \end{enumerate}
        \end{definition}
        \begin{definition}
            The Euclidean inner product on $\mathbb{R}^{n}$
            is defined as:
            $\langle{\mathbf{x},\mathbf{y}}\rangle%
             =\sum_{k=1}^{n}x_{k}y_{k}$
        \end{definition}
        \begin{definition}
            An eigenvector of an ${n}\times{n}$ matrix
            $\mathbf{A}$ is a vector
            $\mathbf{x}\in\mathbb{R}^{n}$
            such that there exists a $\lambda\in\mathbb{R}$
            such that
            $\mathbf{A}\mathbf{x}=\lambda\mathbf{x}$
        \end{definition}
        \begin{definition}
            An eigenvalue of an ${n}\times{n}$ matrix
            $\mathbf{A}$ is a real number
            $\lambda\in\mathbb{R}$ such that there is
            a vector $\mathbf{x}\in\mathbf{R}^{n}$ such
            that $\mathbf{A}\mathbf{x}=\lambda\mathbf{x}$
        \end{definition}
        \begin{definition}
            The characteristic equation, or
            the characteristic polynomial, of an
            ${n}\times{n}$ matrix $\mathbf{A}$
            is $\det(\lambda{I}-\mathbf{A})=0$
        \end{definition}
        \begin{definition}
            A diagonalizable matrix is an
            ${n}\times{n}$ matrix
            $\mathbf{A}$ such that there exists
            an invertible matrix $\mathbf{B}$
            such that
            $\mathbf{A}=\mathbf{B}^{-1}\mathbf{A}\mathbf{B}$
        \end{definition}
        \begin{theorem*}
            The following are true:
            \begin{enumerate}
                \item If $\mathbf{A}$ is an ${n}\times{n}$
                    diagonable matrix, then $\mathbf{A}$
                    has $n$ linearly independent
                    eigenvectors.
                \item If $\mathbf{A}$ is an ${n}\times{n}$
                    matrix with $n$ linearly independent
                    eigenvectors, then $\mathbf{A}$
                    is diagonalizable.
                \item A symmetric matrix has all real
                    eigenvalues.
            \end{enumerate}
        \end{theorem*}
    \section{Miscellaneous Lecture Notes}
        \subsection{Orthogonal Projections}
        \begin{definition}
        The span of
        $\mathcal{W}=\{X_{i}\}_{1}^{k}\subset\mathbb{R}^n$
        is the set
        $\Span(\mathcal{W})=\{\sum_{i=1}^{k}a_{i}X_{i}:a_{i}\in \mathbb{R}\}$.
        \end{definition}
        \begin{definition}
        A linearly dependent subset of $\mathbb{R}^{n}$ is a subset $S\subset\mathbb{R}^{n}$ such that there exists a finite subset $\{X_{i}\}_{i=1}^{k}\subset S$ and a subset $\{a_{i}\}_{i=1}^{k}\subset \mathbb{R}\setminus \{0\}$ such that $\sum_{i=1}^{k}a_{i}X_{i}=\mathbf{0}$
        \end{definition}
        \begin{definition}
        A linearly independent subset of $\mathbb{R}^{n}$ is a subset $S\subset \mathbb{R}^{n}$ that is not linearly dependent.
        \end{definition}
        \begin{theorem}
        If $S\subset\mathbb{R}^{n}$ is linearly independent, then $|S|\leq n$.
        \end{theorem}
        \begin{theorem}
        If $\mathcal{W}\subset\mathbb{R}^{n}$ is linearly independent and $|\mathcal{W}| = k$, then $\Span(\mathcal{W})$ is a $k$ dimensional subspace of $\mathbb{R}^n$.
        \end{theorem}
        If we have a linearly independent subset $\mathcal{W}=\{X_1,\hdots, X_k\}\subset\mathbb{R}^n$, and some other vector $Y$, we may wish to consider the orthogonal projection of $Y$ onto the $k$ dimensional subspace spanned by $\mathcal{W}$. That is, we may wish to find a vector $Y'\in\Span(X_1,\hdots, X_k)$ such that $Y-Y'$ is orthogonal to $\Span(X_1,\hdots, X_k)$.
        \begin{theorem}
        If $\{X_1,\hdots, X_k\}\subset\mathbb{R}^n$ is linearly independent, $\mathcal{W} = \Span(X_1,\hdots, X_n)$, and if $Y\in \mathbb{R}^n$ is such that $Y\perp X_i$ for $i=1,2,\hdots, k$, then $\forall_{Z\in \mathcal{W}}$, $Y\perp Z$.
        \end{theorem}
        \begin{proof}
        For let $Y\in \mathbb{R}^n$ be such that for $i=1,2,\hdots, k$, $Y\perp X_i$. Let $Z\in \mathcal{W}$. Then $Z= \sum_{i=1}^{k} a_i X_i$, where $a_i\in \mathbb{R}$. But then $\langle Y, Z\rangle = \sum_{i=1}^{k} a_i \langle Y, X_i\rangle = \sum_{i=1}^{k} a_i\cdot 0 = 0$. 
        \end{proof}
        \begin{lemma}
        If $P$ is an $n\times k$ matrix whose columns are linearly independent, then $P^TP$ is invertible.
        \end{lemma}
        \begin{proof}
        If $P^TPX = 0$, then $PX$ is orthogonal to the columns of $P$. But $PX$ is a linear combination of the columns of $P$, and thus $PX$ is orthogonal to itself. Thus, $PX = 0$. But the columns of $P$ are linearly independent, if $PX = 0$, then $X=0$. Thus $P^TPX = 0$ if and only if $X=0$. $P^TP$ is invertible.
        \end{proof}
        We need $X_{i}^T(Y-Y')=0$. Let $X_{i}=(x_{1i},x_{2i},\hdots,x_{ni})^{T},P=(x_{ij})$. Then $P^T(Y-Y') = 0$, so $P^TY = P^T Y'$. But $Y'\in \mathcal{W}$, so $Y' = \sum_{i=1}^{k} c_i X_i = P(c_1,\hdots, c_k)^T = PC$. Thus, $C = (P^TP)^{-1}P^TY$. Therefore $Y'=P(P^TP)^{-1}P^T Y$.
        \begin{definition}
        The projection matrix of $\Span(X_{1},\hdots,X_{k})$ is $P(P^TP)^{-1}P^T$, where $P=(x_{ij})$.
        \end{definition}
        \begin{theorem}
        If $Q = P(P^TP)^{-1}P^T$ is a projection matrix for a subspace $\mathcal{W}$, then $Q^T =Q$.
        \end{theorem}
        \begin{proof}
        $Q^T=(P(P^{T}P)^{-1}P^{T})^{T}=(P^{T})^{T}(P(P^{T}P)^{-1})^{T}= P((P^{T}P)^{-1}g)^{T}P^{T}=P(P^{T}P)^{-1}P^{T}=Q$
        \end{proof}
        \begin{theorem}
        If $Q = P(P^TP)^{-1}P^T$ is a projection matrix for a subspace $\mathcal{W}$, then $Q^2 = Q$.
        \end{theorem}
        \begin{proof}
        $Q^{2}=P(P^TP)^{-1}P^TP(P^TP)^{-1}P^T= P((P^{T}P)^{-1}(P^{T}P))(P^{T}P)^{-1}P^{T}=P(P^{T}P)^{-1}P^{T}=Q$
        \end{proof}
        \begin{theorem}
        If $Q$ is an $n\times n$ matrix, $Q = Q^{2}$, and $Q=Q^{T}$, then there is a subspace $\mathcal{W}\subset \mathbb{R}^{n}$ such that $Q$ is the projection matrix of $\mathcal{W}$.
        \end{theorem}
        \subsection{Reflections}
        Let $\mathcal{W}$ be a plane passing through the origin, and suppose we want to reflect a vector $v$ across this plane. Let $u$ be a unit vector along $\mathcal{W}^{\perp}$. That is, $u$ is normal to the plane. The projection of $v$ along the line through $u$ is then given by $\hat{v} = Proj_{u}(v) = u(u^Tu)^{-1}u^Tv$. But $u$ is a unit vector, and therefore $u^Tu = 1$, so $\hat{v} = uu^T v$. Let $Q_u = uu^T$. The definition of the reflection of $v$ across $\mathcal{W}$ is the vector $\Refl_{\mathcal{W}}(v)$ such that has the same magnitude as $v$ lying on the opposite side of $\mathcal{W}$. Thus $v-\Refl_{\mathcal{W}}(v) = 2Q_u v$, and so we have:
        \begin{equation*}
            \Refl_{\mathcal{W}}(v) = v-2Q_u v = (I-2Q_u)v =(I-2uu^T)v
        \end{equation*}
        \begin{definition}
        $H_{\mathcal{W}} = I-2uu^T$ is called the Reflection (Householder) Matrix for $\mathcal{W}$.
        \end{definition}
        \begin{definition}
        An orthogonal matrix is a matrix $P$ such that $P^TP = I$.
        \end{definition}
        \subsection{Lecture Notes on Orthogonal Matrices}
        \begin{definition}
        An orthoganal matrix is a $n\times n$ matrix $A$ such that $A^{T}A = I$.
        \end{definition}
        \begin{theorem}
        If $A$ is an orthogonal matrix, then $A^T = A^{-1}$.
        \end{theorem}
        \begin{proof}
        For $A^TA = I$, and inverses are unique. Thus $A^T = A^{-1}$.
        \end{proof}
        If we let $A_{i} = Ae_{i}$, then $A^TA = (A_{i}^{T}A_{j}) = I$. Therefore $A_i^TA_j = \delta_{ij}$.
        \begin{theorem}
        If $A$ is an $n\times n$ real-valued matrix and $A_i = Ae_i$, $i=1,2,\hdots, n$, then $A$ is orthogonal if and only if $\{A_1,\hdots, A_n\}$ is an orthonormal set of vectors.
        \end{theorem}
        \begin{proof}
        If $A$ is orthogonal, then $A_{i}^{T}A_{j} = \delta_{ij}$, and from this we have orthonormality. If $\{A_1,\hdots, A_n\}$ is orthonormal, then $A^TA = I$ and is therefore orthogonal.
        \end{proof}
        \begin{theorem}
        The following statements are equivalent:
        \begin{enumerate}
        \begin{multicols}{3}
            \item $A$ is orthogonal
            \item $\forall_{X\in\mathbb{R}^{n}}$, $\norm{AX} = \norm{X}$
            \item $\forall_{X,Y\in\mathbb{R}^{n}}$, $\langle AX, AY\rangle = \langle X, Y\rangle$
        \end{multicols}
        \end{enumerate}
        \end{theorem}
        \begin{proof}
        We show $1\Rightarrow 2 \Rightarrow 3 \Rightarrow 1$.
        \begin{enumerate}
            \item If $A^TA = I$, then $\norm{AX}^{2}=(AX)^{T}AX=X^{T}A^{T}AX=X^{T}X=\norm{X}^{2}$. Therefore, $\norm{AX}=\norm{X}$.
            \item If $A$ is a square matrix such that $\forall_{X\in\mathbb{R}^{n}}$, $\norm{AX} = \norm{X}$, then:
            \begin{equation*}
                \norm{X+Y}^{2}=(X+Y)^T(X+Y)=X^TX+X^TY+Y^TX+Y^TY=\norm{X}^2+2X^TY+\norm{Y}^2
            \end{equation*}
            But:
            \begin{equation*}
                \norm{A(X+Y)}^{2}=\norm{AX+AY}^{2}=\norm{AX}^{2}+2(AX)^{T}AY+\norm{AY}^2=\norm{X}^{2}+2(AX)^{T}AY+\norm{Y}^2
            \end{equation*}
            Therefore $(AX)^TAY = X^TY$. That is, $\langle AX, AY\rangle = \langle X, Y\rangle$.
            \item If $A$ is a square matrix such that $\forall_{X,Y\in \mathbb{R}^n}$, $\langle AX, AY\rangle = \langle X, Y\rangle$, then $\langle Ae_{i}, Ae_{j}\rangle=\langle e_i,e_j\rangle=\delta_{ij}$
            Therefore, $A$ is orthogonal.
        \end{enumerate}
        \end{proof}
        \begin{theorem}
        If $A$ and $B$ are $n\times n$ orthogonal matrices, then $AB$ is orthogonal.
        \end{theorem}
        \begin{proof}
        For if $A^{T}A = I$ and $B^{T}B = I$, then $(AB)^{T}AB = B^{T}A^{T}AB = B^{T}IB = B^{T}B = I$. $AB$ is orthogonal.
        \end{proof}
        \begin{theorem}
        \label{theorem:LINEAR_ALGEBRA_orthogonal_matrices_have_determinant_pm_1}
        If $A$ is an $n\times n$ orthogonal matrix, then $\det(A) = 1$ or $-1$.
        \end{theorem}
        \begin{proof}
        For $\det(I) = \det(A^TA) = \det(A^T)\det(A) = \det(A)^2$. Thus, $\det(A) = \pm 1$.
        \end{proof}
        \begin{remark}
        The converse of Thm.~\ref{theorem:LINEAR_ALGEBRA_orthogonal_matrices_have_determinant_pm_1} is false.
        \end{remark}
        Recall that if $u\in \mathbb{R}^n$ is a unit vector and $W = u^{\perp}$, then $H=2uu^T$ is the reflection matrix for $W$. Reflections preserve distance, and therefore $H$ must be orthogonal.
        \newpage
        \begin{theorem}
        If $A$ is an $n\times n$ orthogonal matrix, then there exist $k$ $n\times n$ reflection matrices $H_1,\hdots, H_k$, $0\leq k \leq n$, such that $A = \prod_{i=1}^{j}H_i$.
        \end{theorem}
        \begin{proof}
        By induction. The base case is trivial. Suppose it holds for $n-1$. Let $z = Ae_n$, and let $H$ be the reflection matrix that exchanges $z$ and $e_n$. Then $HAe_n = Hz = e_n$, so $HA$ fixes $e_n$. But $HA$ is an orthogonal matrix, and thus preserves distances and angles. Thus $HA$ maps $\mathbb{R}^{n-1}$ onto itself, and thus by induction there are $H_2,\hdots, H_k$ such that $HA = \prod_{i=2}^{k} H_i$. Letting $H_{1}=H$, we have $A = HHA = \prod_{i=1}^{k}H_i$.
        \end{proof}
        \begin{theorem}
        If $H$ is a reflection matrix, then $\det(H) = -1$.
        \end{theorem}
        \begin{theorem}
        If $A$ is an orthogonal matrix and $A=\prod_{i=1}^{k} H_i$, then $\det(A) = (-1)^k$.
        \end{theorem}
        If $A$ is an orthogonal $2\times 2$ matrix, then we know that columns must be unit vectors that are also orthogonal (Orthonormal). That is, the two columns must lie on the unit circle about the origin. So we may express the first column as $(\cos(\theta),\sin(\theta))$ for some angle $\theta$. There are then two options for the seconds column: $(-\sin(\theta),\cos(\theta))$ or $(\sin(\theta),-\cos(\theta))$. The first is the rotation matrix which rotates $\mathbb{R}^2$ counterclockwise around the origin, and the second is the reflection matrix that makes a reflection across the line that makes an angle $\frac{\theta}{2}$ with the $x-$axis. 
        \begin{theorem}
        If $A$ is a $3\times 3$ orthogonal matrix and $\det(A) = 1$, then $A$ is a rotation matrix.
        \end{theorem}
        \begin{proof}
        $A$ must be the product of $0,1,2,$ or $3$ reflection matrices. If $\det(A) = 1$, then $A$ is the product of an even number of reflections, and thus either $A=I$ or $A$ is the product of two reflections, and is thus a rotation matrix.
        \end{proof}
        \begin{theorem}
        If $A$ is a $3\times 3$ orthogonal matrix, $\det(A) = -1$, and $A=A^T$, then either $A=-I$ or $A$ is a reflection matrix.
        \end{theorem}
        \begin{proof}
        If $\det(A)=-1$, then $A$ is the product of an odd number of reflections, either $1$ or $3$. If $A$ is a single reflection, then $A=H$ for some Householder matrix $H$. Thus $A^T = A$. Conversely, if $A = A^T$ and $\det(A) = -1$, then $\det(-A) = 1$, and $-A^T = -A = -A^{-1}$. Therefore $-A$ is a rotation whose square is the identity. If $A\ne I$, then $A$ must be a rotation of $\pi/2$ around some axis, and thus $A$ is a reflection.
        \end{proof}
        \begin{theorem}
        If $A$ is a $3\times 3$ orthogonal matrix, $\det(A) = -1$, and $A\ne A^T$, then $A$ is the product of three reflections.
        \end{theorem}
        \begin{proof}
        If $\det(A) = -1$, and $A\ne A^T$, then $A$is not a rotation or a pure reflection, and is thus a product of $3$ reflection matrices.
        \end{proof}
        \begin{theorem}
        If $A$ and $B$ are $3\times 3$ rotation matrices, then $AB$ is a rotation matrix.
        \end{theorem}
        \begin{proof}
        For $A$ and $B$ must be orthogonal, and thus $AB$ is orthogonal. But $\det(AB) = \det(A)\det(B) = 1\cdot 1 = 1$, and thus $AB$ is an orthogonal matrix with determinant equal to $1$, and is therefore a rotation matrix.
        \end{proof}
        \subsection{Rotations}
        The $2\times 2$ matrix $A_{\theta}$ rotates the plane $\mathbb{R}^2$ counterclockwise by $\theta$ around the origin. The question that then arises is, ``Is there a similar way to do this for $\mathbb{R}^3$?" The simple case would be rotating by an angle $\theta$ about the $z-$axis, analogous the rotating the Earth by $\theta$ about the North Pole. This fixes the $z-$axis and acts on the $xy$ plane only. This can be represented by the matrix $S_{\theta}$.
        \begin{equation*}
         A_{\theta}=\begin{bmatrix*}[r]\cos(\theta) & -\sin(\theta) \\ \sin(\theta) & \cos(\theta)\end{bmatrix*}\quad\quad\quad\quad S_{\theta}=\begin{bmatrix*}\cos(\theta) & -\sin(\theta) & \phantom{sin}0 \\ \sin(\theta) & \phantom{-}\cos(\theta) & \phantom{sin}0\\ 0 & \phantom{-}0 & \phantom{sin}1 \end{bmatrix*}   
        \end{equation*}
        $S_{\theta}$ is an orthogonal matrix. That is, $S_{\theta} S_{\theta}^T = I$, and therefore $S_{\theta}^T = S_{\theta}^{-1}$. Suppose we want to rotate by an angle $\theta$ about a different axis. Let $\mathbf{u}$ be a unit vector pointing in the direction of the axis of rotation and let $R_{\theta,\mathbf{u}}$ be the new rotation matrix. To compute $R_{\theta,\mathbf{u}}$ choose any unit vector $\mathbf{v}$ that is orthogonal to $\mathbf{u}$. Let $\mathbf{w} = \mathbf{u}\times \mathbf{v}$. Then $\{\mathbf{u},\mathbf{v},\mathbf{w}\}$ is an orthonormal basis of $\mathbb{R}^3$ such that $\mathbf{v}\times \mathbf{w} = \mathbf{u}$. Let:
        \begin{equation*}
            P = \begin{bmatrix} v_1 & w_1 & u_1 \\ v_2 & w_2 & u_2 \\ v_3 & w_3 & u_3 \end{bmatrix}
        \end{equation*}
        The columns of $P$ form an orthonormal set, and therefore $P$ is orthogonal. In particular:
        \begin{align*}
            P^{T}\mathbf{v}&=e_{1}&P^{T}\mathbf{w}&=e_{2}&P^{T}\mathbf{u}=e_{3}
        \end{align*}
        \begin{theorem}
        If $\theta \in [0,2\pi]$ and $\mathbf{u}\in \mathbb{R}^3$ is a unit vector, then $R_{\theta, \mathbf{u}} = PS_{\theta}P^T$.
        \end{theorem}
        \begin{proof}
        For $PS_{\theta}P^T\mathbf{u}=\mathbf{u}$, $PS_{\theta}P^{T}\mathbf{v}=\cos(\theta)\mathbf{v}+\sin(\theta)\mathbf{w}$, and $PS_{\theta}P^{T}\mathbf{w}=-\sin(\theta)\mathbf{v}+\cos(\theta)\mathbf{w}$
        Thus, if $X = a\mathbf{v}+b\mathbf{w}+c\mathbf{u}$, then $PS_{\theta}P^TX=a(\cos(\theta)\mathbf{v}+\sin(\theta)\mathbf{w})+b(-\sin(\theta)\mathbf{v}+\cos(\theta)\mathbf{w})+c\mathbf{u}=R_{\theta,\mathbf{u}}X$
        \end{proof}
        From the orthogonality of $P$ and $S_{\theta}$ we have that $R_{\theta,\mathbf{u}}$ is also orthogonal.
        \begin{theorem}
        \label{theorem:LINEAR_ALGEBRA_a_rotation_matrix_is_an_orthoganal_matrix_with_determinant_1}
        A rotation matrix $R$ is an orthogonal matrix with determinant $1$.
        \end{theorem}
        \begin{proof}
        For $R^{T}R=(PS_{\theta}P^{T})^{T}PS_{\theta}P^{T}=PS_{\theta}^{T}P^{T}PS_{\theta}P^{T}=PS_{\theta}^{T}S_{\theta}P^{T}=PP^{T}=I$. But also we have $\det(R)=\det(PS_{\theta}P^T)=\det(P)\det(S_{\theta})\det(P^T)=\det(P)\det(P^{-1})=1$
        \end{proof}
        \begin{remark}
        The converse of Thm.~\ref{theorem:LINEAR_ALGEBRA_a_rotation_matrix_is_an_orthoganal_matrix_with_determinant_1} is also true.
        \end{remark}
        We now turn to the question of how to compute the rotation of $\mathbb{R}^3$ represented by a given orthogonal matrix. If $R$ is an orthogonal matrix such that $\det(R) = 1$, how do we compute the angle of rotation? First recall that the trace of a matrix is the sum of the diagonal components, $\Tr(A) = \sum_{i=1}^{n}a_{ii}$.
        \begin{theorem}
        If $A$ and $B$ are $n\times n$ matrices, then $\Tr(AB) = \Tr(BA)$
        \end{theorem}
        \begin{theorem}
        If $R$ is a rotation matrix of angle $\theta$, then $\cos(\theta) = \frac{\Tr(R) - 1}{2}$.
        \end{theorem}
        \begin{proof}
        For $\Tr(R)=\Tr(PS_{\theta}P^{-1})=\Tr(PP^{-1}S_{\theta}) = \Tr(S_{\theta})=1+2\cos(\theta)\Rightarrow \cos(\theta)=\tfrac{\Tr(R)-1}{2}$
        \end{proof}
        This doesn't tell us everything, as we still don't know $\mathbf{u}$, and $\cos(\theta) = \cos(-\theta)$, so we still don't know the sign of $\theta$. Since $R$ is an orthogonal matrix, $R^T = R^{-1}$. So if $\mathbf{u}$ lies on the axis of rotation, then $(R-R^T)\mathbf{u} = (R-R^{-1})\mathbf{u} = 0$. We can find the axis of rotation by determining the null space of $R-R^T$. 
        \begin{equation*}
            R = \begin{bmatrix*}[c] r_{11} & r_{12} & r_{13} \\ r_{21} & r_{22} & r_{23} \\ r_{31} & r_{32} & r_{33} \end{bmatrix*} \Rightarrow R-R^{T} = \begin{bmatrix*}[c] 0 & r_{12} - r_{21} & r_{13} - r_{31} \\ r_{21} - r_{12} & 0 & r_{23}-r_{32} \\ r_{31} - r_{13} & r_{32} - r_{23} & 0 \end{bmatrix*}\equiv \begin{bmatrix*}[r] 0 & \alpha & \beta \\ -\alpha & 0 & \gamma \\ -\beta & -\gamma & \phantom{-}0 \end{bmatrix*}
        \end{equation*}
        This suggests that $\mathbf{u}$ is parallel to $(-\gamma, \beta, -\alpha)^{T} = (r_{32}-r_{23}, r_{13}-r_{31}, r_{21}-r_{12})^{T}$.
        \begin{theorem}
        If $R$ is a rotation matrix such that $R\ne R^T$, then the axis of rotation of $R$ is parallel to $\mathbf{q}=(-\gamma, \beta, -\alpha)^{T} = 2\sin(\theta)\mathbf{u}$, where $\mathbf{u}$ is a unit vector about the axis of rotation.
        \end{theorem}
        \begin{proof}
        Let $R = PS_{\theta}P^T$. Then:
        \begin{equation*}
            R-R^{T}=PS_{\theta}P^{T}-PS_{\theta}^{T}P^{T}=P(S_{\theta}-S_{\theta}^{T})P^{T}=2P\begin{bmatrix}0 & -\sin(\theta) & 0 \\ \sin(\theta) & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix}P^{T}= 2\sin(\theta)(\mathbf{w}\mathbf{v}^{T} - \mathbf{v}\mathbf{w}^{T})
        \end{equation*} 
        Where $\mathbf{v}$ is orthogonal to $\mathbf{u}$ and $\mathbf{w} = \mathbf{u}\times \mathbf{v}$. Thus, $\mathbf{q}=(-\gamma,\beta,-\alpha)^{T}=2\sin(\theta)\mathbf{v}\times\mathbf{w}=2\sin(\theta)\mathbf{u}$
        \end{proof}
        \begin{remark}
        What about the case when $R-R^T = 0$? When this happens either $\theta = 0$ or $\theta = \pi$. If $\theta = 0$, then this is the identity rotation and thus $R = I$, and we are done. If $R\ne I$, then $\theta = \pi$. To find out the axis of rotation, we have that:
        \begin{equation*}
            R = PS_{\pi}P^T = \begin{bmatrix*}[r]-1 & 0 & \phantom{-}0 \\ 0 & -1 & 0 \\ 0 & 0 & 1 \end{bmatrix*} = -\mathbf{v}\mathbf{v}^T-\mathbf{w}\mathbf{w}^T +\mathbf{u}\mathbf{u}^T    
        \end{equation*}
        But $\mathbf{v},\mathbf{w},$ and $\mathbf{u}$ form an orthonormal basis, and therefore $\mathbf{v}\mathbf{v}^T + \mathbf{w}\mathbf{w}^T+\mathbf{u}\mathbf{u}^T = I$. Thus, $R = -I+2\mathbf{u}\mathbf{u}^T$, so $\mathbf{u} \mathbf{u}^T = \frac{1}{2}(R+I)$. But the columns of $\mathbf{u}\mathbf{u}^T$ are parallel to $\mathbf{u}$, and therefore we can determine $\mathbf{u}$ by normalizing one of the columns of $\frac{1}{2}(R+I)$.
        \end{remark}
        \newpage
        \subsection{The Matrix Exponential}
        \begin{definition}
        If $A$ is an $n\times n$ matrix, then the exponential of $A$ is $e^{A} =\sum_{k=0}^{\infty} \frac{A^k}{k!}$.
        \end{definition}
        \begin{remark}
        Notationally, we write $A^0 = I$. For any complex-valued matrix $A$ of finite dimension, it can be shown that this sum converges.
        \end{remark}
        \begin{theorem}
        If $A$ and $P$ are complex $n\times n$ matrices and $P$ invertible, then $e^{P^{-1}AP} = P^{-1}e^{A}P$.
        \end{theorem}
        \begin{proof}
        For all $m\in \mathbb{N}$, $(P^{-1}AP)^{m} = P^{-1}A^mP$. Thus:
        \begin{equation*}
            e^{P^{-1}AP} = \sum_{k=0}^{\infty} P^{-1}\frac{A^k}{k!}P = P^{-1}\big(\sum_{k=0}^{\infty} \frac{A^k}{k!}\big)P = P^{-1}e^A P
        \end{equation*}
        \end{proof}
        \begin{theorem}
        If $0$ is the zero matrix, then $e^0 = I$.
        \end{theorem}
        \begin{theorem}
        If $A$ is an $n\times n$ matrix and $m\in \mathbb{N}$, then $A^{m} e^{A} = e^{A} A^{m}$.
        \end{theorem}
        \begin{proof}
        For $A^{m} e^{A} = A^{m} \sum_{k=0}^{\infty} \frac{A^{k}}{k!} = \sum_{k=0}^{\infty} \frac{A^{k+m}}{k!} = \big(\sum_{k=0}^{\infty} \frac{A^k}{k!}\big)A^{m}$.
        \end{proof}
        \begin{theorem}
        If $A$ is an $n\times n$ matrix, then $e^{A^{T}} = (e^{A})^{T}$.
        \end{theorem}
        \begin{proof}
        For $e^{A^{T}} = \sum_{k=0}^{\infty} \frac{(A^{T})^{k}}{k!} = \sum_{k=0}^{\infty} \frac{(A^{k})^{T}}{k!} = \big(\sum_{k=0}^{\infty} \frac{A^{k}}{k!}\big)^{T} = (e^{A})^{T}$.
        \end{proof}
        \begin{theorem}
        If $A$ and $B$ are $n\times n$ matrices and if $AB = BA$, then $Ae^{B} = e^{B} A$.
        \end{theorem}
        \begin{proof}
        For $Ae^{B} = A\sum_{k=0}^{\infty} \frac{B^{k}}{k!} = \sum_{k=0}^{\infty} A\frac{B^{k}}{k!} = \sum_{k=0}^{\infty} \frac{B^{k}}{k!}A = \big(\sum_{k=0}^{\infty} \frac{B^{k}}{k!}\big)A = e^{B}A$.
        \end{proof}
        \begin{theorem}
        If $A$ and $B$ are $n\times n$ matrices and $AB = BA$, then $e^{A}e^{B} = e^{B}e^{A}$.
        \end{theorem}
        \begin{proof}
        For:
        \begin{align*}
            e^A e^B &= e^A\sum_{k=0}^{\infty}\frac{B^k}{k!}=\sum_{k=0}^{\infty} e^A\frac{B^k}{k!}= \sum_{k=0}^{\infty} \big(\sum_{j=0}^{\infty} \frac{A^j}{j!}\big) \frac{B^k}{k!}= \sum_{k=0}^{\infty}\big(\sum_{j=0}^{\infty} \frac{A^j}{j!}\frac{B^k}{k!}\big)\\
            &=\sum_{k=0}^{\infty}\big(\sum_{j=0}^{\infty} \frac{B^k}{k!}\frac{A^j}{j!}\big)=\sum_{k=0}^{\infty}\big(\sum_{j=0}^{\infty} \frac{B^k}{k!}\big)\frac{A^j}{j!}= \sum_{k=0}^{\infty}\frac{B^k}{k!}\sum_{j=0}^{\infty}\frac{A^j}{j!}=e^{B}e^{A}
        \end{align*}
        \end{proof}
        It is NOT true that $e^{A+B}=e^{A}e^{B}$, in general. Matrix exponentiation lacks this feature.
        \begin{theorem}
        If $A$ is an $n\times n$ matrix and $s,t\in \mathbb{C}$, then $e^{A(s+t)} = e^{As}e^{At}$.
        \end{theorem}
        \begin{proof}
        For $e^{As}e^{At} = \sum_{j=0}^{\infty} \sum_{k=0}^{\infty} \frac{A^{j+k}s^jt^k}{j!k!}$. Letting $n = j+k$, so $j = n-k$, we have:
        \begin{equation*}
            \sum_{n=0}^{\infty} \sum_{k=0}^{\infty} \frac{A^n}{n!}\frac{n!}{k!(n-k)!}s^{n-k}t^k = \sum_{n=0}^{\infty}\frac{A^n}{n!}\big(\sum_{k=0}^{\infty} \frac{n!}{k!(n-k)!}s^{n-k}t^k\big)    
        \end{equation*}
        From the binomial theorem, the expression inside the parenthesis is equal to $(s+t)^n$. So we have $e^{As}e^{At}=\sum_{n=0}^{\infty} \frac{A^n(t+s)^n}{n!} = e^{A(s+t)}$.
        \end{proof}
        \begin{theorem}
        If $A$ is an $n\times n$ matrix, then $e^A$ is invertible and $(e^A)^{-1} = e^{-A}$.
        \end{theorem}
        \begin{proof}
        For $I = e^{0} = e^{A(1-1)} = e^Ae^{-A}$. Thus $(e^{A})^{-1} = e^{-A}$.
        \end{proof}
        \begin{theorem}
        If $A$ is an $n\times n$ matrix and $t\in \mathbb{R}$, then $\frac{d}{dt}\big(e^{At}\big) = Ae^{At}$.
        \end{theorem}
        \begin{proof}
        For $\underset{h\rightarrow 0}\lim \frac{e^{A(t+h)}-e^{At}}{h} = e^{At}\underset{h\rightarrow 0}\lim \frac{e^{Ah}-I}{h} = e^{At}\underset{h\rightarrow 0}\lim\big[A+\frac{A^2}{2!}h+\hdots\big] = e^{At}A = Ae^{At}$.
        \end{proof}
        \begin{theorem}
        If $A$ and $B$ are $n\times n$ matrices and $AB=BA$, then $e^{A+B} = e^{A}e^{B}$.
        \end{theorem}
        \begin{proof}
        For let $g(t) = e^{(A+B)t}e^{-Bt}e^{-At}$. Then from commutativity of $A$ and $B$, $g'(t) = 0$. But then $g(t)$ is a constant. From the definition, $g(0) = I$, and thus $g(t) = I$. So $e^{(A+B)t}e^{-Bt}e^{-At} = I$, and therefore $e^{(A+B)t} = e^{At}e^{Bt}$.
        \end{proof}
        \begin{theorem}
        If $A^{2} = 0$, then $e^{A} = I+A$.
        \end{theorem}
        \begin{proof}
        For $e^{A} = I+A+A^{2}\big(\frac{I}{2!}+\frac{A}{3!}+\hdots\big) = I+A+0 = I+A$.
        \end{proof}
        \subsection{Linear Systems of Ordinary Differential Equations}
        Consider the equation $y' = ky$, where $k$ is some constant. We can solve this via calculus using separation of variables:
        \begin{equation*}
            \frac{y'}{y} = k\Rightarrow \int \frac{y'}{y}dx = \int kdx \Rightarrow \ln(y) = kx+c \Rightarrow y = e^c e^{kx}    
        \end{equation*}
        Setting $x=0$, we have $e^c = y_0$. So $y = y_0e^{kx}$. Let us solve this a different way: Let $F(x) = e^{-kx}y$, and let $y'=kx$. Differentiating we have:
        \begin{equation*}
            F'(x)=-ke^{kx}y+e^{-kx}y'=-kye^{-kx}+e^{-kx}ky=0    
        \end{equation*}
        So $F'(x) = 0$, and therefore $F(x)$ is a constant. Setting $x=0$, we have $F(x) = y_0$. So $y = y_0e^{kx}$. This shows us that $y_0e^{kx}$ is the $only$ solution to this problem. Let:
        \begin{equation*}
            Y(t) = \begin{bmatrix} y_1(t) \\ y_2(t)\end{bmatrix}    
        \end{equation*}
        Consider $Y'(t) = AY(t)$, where $A$ is an $n\times n$ matrix. Let $F(t) = e^{-At}Y(t)$. Then $F'(t) = 0$, and $Y(t) = Y_0 e^{At}$.
        \begin{theorem}
        If $Y:\mathbb{R}\rightarrow \mathbb{R}^n$ is a differentiable function such that $Y'(t) = AY(t)$, where $A$ is a diagonalizable matrix with eigenvalues $\lambda_1,\hdots, \lambda_n$ and eigenvectors $v_1,\hdots, v_n$, then $Y(t) = \sum_{k=1}^{n} \lambda_k e^{\lambda_k t}v_k$
        \end{theorem}
    \section{Problem Sets}
        \subsection{Problem Set I}
        \begin{problem}
        Find the point on the line $y=4x$ which is closest to the point $(2,5)$.
        \end{problem}
        \begin{proof}[Solution 1]
        Given a vector $\mathbf{v}$ that is parallel to the line $y$, we know that the vector $\mathbf{w}$ from $(2,5)$ to the point $(x,y)$ that minimizes the distance from $y=4x$ to the point $(2,5)$ will satisfy $\langle \mathbf{v}, \mathbf{w}\rangle = 0$. That is:
        \begin{equation*}
            \big\langle (1,4), (2-x,5-y)\big\rangle = 0\Rightarrow 2-x+4(5-y) = 0 \Rightarrow 22 - x - 4 y = 0    
        \end{equation*}
        But $y = 4x$, and thus $22-17x = 0 \Rightarrow x= \frac{22}{17}$. The point of least distance is $\frac{22}{17}(1,4)$.
        \end{proof}
        \begin{proof}[Solution 2]
        This point is the projection of the vector $(2,5)^T$ onto $(1,4)^T$. That is:
        \begin{equation*}
            \mathbf{P} = \frac{\begin{bmatrix}2 & 5 \end{bmatrix} \begin{bmatrix} 1 \\ 4 \end{bmatrix}}{\begin{bmatrix} 1 & 4 \end{bmatrix} \begin{bmatrix} 1 \\ 4 \end{bmatrix}} \begin{bmatrix} 1 \\ 4 \end{bmatrix} = \frac{22}{17} \begin{bmatrix} 1 \\ 4\end{bmatrix}
        \end{equation*}
        \end{proof}
        \begin{problem}
        Show that $\mathbf{x}\mathbf{y}^T + \mathbf{y}\mathbf{x}^T$ is symmetric.
        \end{problem}
        \begin{proof}[Solution]
        Recall that a matrix is symmetric if it is equal to its transpose. Thus, we must show $A = A^T$. But for any $n\times n$ matrices $A$ and $B$, $(A+B)^T = A^T + B^T$, and $(AB)^T = B^T A^T$, and $(A^T)^T = A$. Thus, given our matrix $A= \mathbf{x}\mathbf{y}^T + \mathbf{y}\mathbf{x}^T$, we have that $A^T = (\mathbf{x}\mathbf{y}^T + \mathbf{y}\mathbf{x}^T)^T = (\mathbf{x}\mathbf{y}^T)^T + (\mathbf{y}\mathbf{x}^T)^T = (\mathbf{y}^T)^T\mathbf{x}^T + (\mathbf{x}^T)^T\mathbf{y}^T = \mathbf{y}\mathbf{x}^T + \mathbf{x}\mathbf{y}^T = \mathbf{x}\mathbf{y}^T + \mathbf{y}\mathbf{x}^T = A$
        \end{proof}
        \begin{problem}
        Compute the product $\begin{bmatrix*}[r] 2 & -1 \\ 3 & 1\end{bmatrix*} \begin{bmatrix*}[r] -1 & \phantom{-}2 & \phantom{-}3 & \phantom{-}1 \\ 2 & -2 & 1 & -1 \end{bmatrix*}$
        \end{problem}
        \begin{proof}[Solution]
        \begin{align*}
            \begin{bmatrix*}[r] 2 & -1 \\ 3 & 1\end{bmatrix*} \begin{bmatrix*}[r] -1 & \phantom{-}2 & \phantom{-}3 & \phantom{-}1 \\ 2 & -2 & 1 & -1 \end{bmatrix*}&=\begin{bmatrix} 2(-1)+(-1)2 & 2\cdot 2 + (-1)(-2) & 2\cdot 3 + (-1)1 & 2\cdot 1 + (-1)(-1) \\ 3(-1)+1\cdot 2 & 3\cdot 2 + 1(-2) & 3\cdot 3 + 1\cdot 1 & 3\cdot 1 + 1(-1)\end{bmatrix}\\
            &=\begin{bmatrix} -4 & 6 & 5 &3 \\ -1 & 4 & 10 & 2\end{bmatrix}
        \end{align*}
        \end{proof}
        \begin{problem}
        Find the equation of the plane that contains $P_{1}(2,2,1),P_{2}(2,3,2)$, and $P_{3}(-1,3,1)$.
        \end{problem}
        \begin{proof}[Solution]
        It suffices to find a vector normal to this plane. We have that:
        \begin{equation*}
            \overrightarrow{P_1P_2} = (0,1,1)^T \quad\quad\quad\quad \overrightarrow{P_1P_3} = (-3,1,0)^T
        \end{equation*}
        Then both vectors are parallel to the plane, and thus $\overrightarrow{P_1P_2}\times \overrightarrow{P_1P_3}=(-1,3,3)^T$ is perpendicular to the plane. Suppose $Q=(x,y,z)$ is a point in the plane. Then the relative position vector $P_1 Q = (x-2,y-2,z-1)^T$ is orthogonal to $(-1,3,3)^T$. Thus:
        \begin{align*}
            (x-2,y-2,z-1)(-1,3,3)^T &= 0\\
            \Rightarrow 2-x+3y-6+3z-3 &= 0\\
            \Rightarrow x-3y-3z +7 &= 0   
        \end{align*}
        This is the equation of the plane.
        \end{proof}
        \begin{problem}
        Let $S=\Span(\mathbf{x}_1,\mathbf{x}_{2})$, where $\mathbf{x}_{1}=(1,-1,2)^{T}$, $\mathbf{x}_{2}=(-1,2,2)^{T}$. Find a basis for $S^{\perp}$
        \end{problem}
        \begin{proof}[Solution]
        We seek a vector in $\mathbf{x}_3\in\mathbb{R}^3$ such that $\langle \mathbf{x}_3, \mathbf{x}_{i}\rangle = 0$, $i=1,2$. That is:
        \begin{equation*}
            \begin{bmatrix*}[r] 1 & -1 & \phantom{-}2 \\ 0 & 1 & 4 \end{bmatrix*}\begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} = 0    
        \end{equation*}
        Solving gives us $x_2 = -4x_3$, $x_1=-6x_3$. $\{(-6,-4,1)\}$ is a basis.
        \end{proof}
        \begin{problem}
        For the matrix $A = \begin{bmatrix} 1 & 2 & 2 \\ -1 & -1 & 0 \end{bmatrix}$, find a basis for the following:
        \begin{enumerate}
        \begin{multicols}{4}
            \item $R(A^T)$
            \item $N(A)$
            \item $R(A)$
            \item $N(A^T)$
        \end{multicols}
        \end{enumerate}
        \end{problem}
        \begin{proof}[Solution]
        The row-echelon form of $A$ and $A^{T}$ are given below:
        \begin{align*}
            A'&=\begin{bmatrix}1&1&0\\0&1&2\end{bmatrix} & (A^{T})'&=\begin{bmatrix*}[r]1&0\\0&-1\\0&0\end{bmatrix*}
        \end{align*}
        \begin{enumerate}
            \item The rows of $A'$ give us a basis for $R(A^T)$ of $\{(1,1,0),(0,1,2)\}$
            \item $N(A) = \{x\in \mathbb{R}^3: Ax = 0\}$. Solving $A'x=0$ gives us a basis of $\{(2,-2,1)\}$
            \item The non-zero rows of $(A^{T})'$ give us a basis of $\{(1,0), (0,-1)\}$.
            \item $N(A^T)= \{x\in \mathbb{R}^2: A^T x = 0\}$. $A'x = 0$ gives us $x_1 = 0$ and $-x_2 = 0$. $N(A^T) = \{(0,0)\}$.
        \end{enumerate}
        \end{proof}
        \subsection{Problem Set II}
        \begin{problem}
        Find a point on the line $y=5x$ that is closest to the point $(1,3)$.
        \end{problem}
        \begin{proof}[Solution]
        Pick a point on the line, say $\mathbf{w} = (1,5)^T$. The point $P$ is the projection of $\mathbf{v} = (1,3)^T$ onto the line $y=5x$, and thus:
        \begin{equation*}
            P = \frac{v^T w}{w^T w} = \frac{\begin{bmatrix}1 & 5 \end{bmatrix}\begin{bmatrix}1 \\ 5\end{bmatrix}}{\begin{bmatrix}1 & 5 \end{bmatrix}\begin{bmatrix}1 \\ 5\end{bmatrix}}(1,5)^T = \frac{8}{13}(1,5)^T
        \end{equation*}
        \end{proof}
        \begin{problem}
        Is $A = xy^T - yx^T$ symmetric? ($x$ and $y$ are $n\times 1$ vectors)
        \end{problem}
        \begin{proof}[Solution]
        In general, no. For if it were, then $A-A^T = 0$. But:
        \begin{align*}
            0&=A-A^T=xy^{T}-yx^{T}-(xy^{T}-yx^{T})^{T}=xy^{T}-yx^{T}-[(xy^{T})^{T}-(yx^{T})^{T}]\\
            &=xy^T - yx^T - [yx^T - xy^T]=2xy^T-2yx^T=2A\Rightarrow xy^{T}-yx^{T}=0\Rightarrow xy^{T}=yx^{T} 
        \end{align*}
        As this is not, in general, true, $A$ is not necessarily symmetric.
        \end{proof}
        \begin{problem}
        Compute the product $\begin{bmatrix*}[r] -1 & \phantom{-}3 \\ 4 & 2 \end{bmatrix*} \begin{bmatrix*}[r] -1 & \phantom{-}1 & \phantom{-}2 & -2 \\ 2 & 3 & 1 & 1 \end{bmatrix*}$
        \end{problem}
        \begin{proof}[Solution]
        \begin{align*}
            \begin{bmatrix*}[r] -1 & \phantom{-}3 \\ 4 & 2 \end{bmatrix*} \begin{bmatrix*}[r] -1 & \phantom{-}1 & \phantom{-}2 & -2 \\ 2 & 3 & 1 & 1 \end{bmatrix*}=\begin{bmatrix*}[r] \phantom{-}1+6 & -1+9 & -2+3 & \phantom{-}2+3 \\ -4+4 & \phantom{-}4+6 & \phantom{-}8+2 & -8+2 \end{bmatrix*}=\begin{bmatrix*}[r] 7 & 8 & 1 & 5 \\ 0 & 10 & 10 & -6 \end{bmatrix*}
        \end{align*}
        \end{proof}
        \begin{problem}
        Find the equation of the plane that passes through $P_1(2,2,2), P_2(2,3,4), P_3(-1,3,3)$.
        \end{problem}
        \begin{proof}[Solution]
        $\overrightarrow{P_1P_2} = (0,1,2)^{T}$, $\overrightarrow{P_1 P_3} = (-3,1,1)^{T}$. So:
        \begin{equation*}
            \overrightarrow{N} = \begin{vmatrix*}[r] \hat{\mathbf{i}} & \hat{\mathbf{j}} & \hat{\mathbf{k}} \\ 0 & 1 & 2 \\ -3 & \phantom{-}1 & \phantom{-}1 \end{vmatrix*} = \hat{\mathbf{i}}(1-2) + \hat{\mathbf{j}}(0+6) + \hat{\mathbf{k}}(0+3)=\begin{bmatrix*}[r]-1 \\ -6 \\ 3\end{bmatrix*}   
        \end{equation*}
        For a point $P=(x,y,z)$ in the plane, $\langle \overrightarrow{P_1P}, \overrightarrow{N}\rangle = 0$. Thus, $x + 6y - 3z =0$
        \end{proof}
        \begin{problem}
        Let $S=\Span(\{(2,1,2)^T, (-2,-1,3)^T\})$. Find a basis for $S^{\perp}$.
        \end{problem}
        \begin{proof}[Solution]
        Let $A$ and it's row-echelon form be the matrices shown below. Then $S^{\perp} = N(A)$.
        \begin{align*}
            A&=\begin{bmatrix*}[r] 2 & 1 & 2 \\ -2 & -1 & \phantom{-}3\end{bmatrix*} & A'&=\begin{bmatrix} 2 & 1 & 2 \\ 0 & 0 & 5 \end{bmatrix}
        \end{align*}
        Solving for $A'x = 0$ gives us a basis of $\{(1,-2,0)\}$
        \end{proof}
        \begin{problem}
        For the matrix $A = \begin{bmatrix*}[r] 2 & 3 & 4 \\ -2 & -2 & \phantom{-}0 \end{bmatrix*}$, find a basis for the following:
        \begin{enumerate}
        \begin{multicols}{4}
            \item $R(A^T)$
            \item $N(A)$
            \item $R(A)$
            \item $N(A^T)$
        \end{multicols}
        \end{enumerate}
        \end{problem}
        \begin{proof}[Solution]
        $A$ and $A^{T}$ have the following row-echelon forms:
        \begin{align*}
            A'&=\begin{bmatrix}1&1&0\\0&1&4\end{bmatrix} & (A^{T})'=&\begin{bmatrix}1&0\\0&1\\0&0\end{bmatrix}
        \end{align*}
        \begin{enumerate}
            \item Putting $A$ into row-echelon  form and reading off the rows, we obtain the basis $\{(1,1,0),(0,1,4)\}$
            \item $N(A) = \{x\in \mathbb{R}^3:  Ax = 0\}$. This gives us a basis of $\{(4,-4,1)\}$
            \item The non-zero rows of $(A^{T})'$ give us a basis of $\{(1,0),(0,1)\}$
            \item $N(A^T) = \{x\in \mathbb{R}^2: A^Tx = 0\}$. Solving $A'^{T}x=0$ gives us $x_1 = 0$ and $x_2 = 0$. $N(A^T) = \{(0,0)\}$
        \end{enumerate}
        \end{proof}
        \subsection{Problem Set III}
        \begin{problem}
        Let $A,B,C$ be $n\times n$ matrices. Is $A = BC^T + CB^T$ symmetric?
        \end{problem}
        \begin{proof}[Solution]
        A matrix is symmetric if $A = A^T$. If $A = BC^T+CB^T$, then:
        \begin{align*}
            A^{T}=(BC^{T}+CB^{T})^{T}=(BC^{T})^{T}+(CB^{T})^{T}=(C^{T})^{T}B^{T}+(B^{T})^{T}C^{T}=CB^{T}+BC^{T}=A
        \end{align*}
        $A$ is symmetric.
        \end{proof}
        \begin{problem}
        Compute $\norm{x}_1, \norm{x}_2, \norm{x}_3$ for $x = (2,-3,1)^T$
        \end{problem}
        \begin{proof}[Solution]
        By definition, for $x\in \mathbb{R}^n$, $\norm{x}_p = (\sum_{k=1}^{n}|x_k|^p)^{1/p}$. So we have the following:
        \begin{enumerate}
            \item $\norm{x}_1 = |2|+|-3|+|1| = 2+3+1 = 6$
            \item $\norm{x}_2 = (|2|^2+|-3|^2+|1|^2)^{1/2} = (4+9+1)^{1/2} = \sqrt{14}$
            \item $\norm{x}_3 = (|2|^3+|-3|^3+|1|^3)^{1/3} = (8+27+1)^{1/3} = \sqrt[3]{36}$
        \end{enumerate}
        \end{proof}
        \newpage
        \begin{problem}
        For the matrix $A = \begin{bmatrix*}[r] \phantom{-}2 & -2 & \phantom{-}4 \\ -1 & 1 & -2 \end{bmatrix*}$, find a basis for the following:
        \begin{enumerate}
        \begin{multicols}{4}
            \item $R(A^T)$
            \item $N(A)$
            \item $R(A)$
            \item $N(A^T)$
        \end{multicols}
        \end{enumerate}
        \end{problem}
        \begin{proof}[Solution]
        $A$ and $A^{T}$ have the following row-echelon forms:
        \begin{align*}
           A'&=\begin{bmatrix*}[r]1&-1&2\\0&0&0\end{bmatrix*} & (A^{T})'&=\begin{bmatrix*}[r]-2&1\\0&0\\0&0\end{bmatrix*}
        \end{align*}
        \begin{enumerate}
            \item The non-zero rows of $A'$ give a basis of $\{(1,-1,2)\}$
            \item $N(A) = \{x\in \mathbb{R}^3: Ax = 0\}$. Solving $A'x=0$ gives a basis of $\{(1,1,0),(-2,0,1)\}$
            \item The non-zero rows of $(A^{T})'$ give a basis of $\{(-2,1)\}$
            \item $N(A^T) = \{x\in \mathbb{R}^2: A^T x = 0\}$. Solving $(A^{T})'x=0$ gives a basis of $\{(1,2)\}$
        \end{enumerate}
        \end{proof}
        \begin{problem}
        Find the least-squares solution to the following system:
        \begin{align*}
            x_{1}-x_{2}\phantom{2} &=2\\
            x_{1}+x_{2}\phantom{2} &= 0\\
            x_{1}+2x_{2} &=-1
        \end{align*}
        \end{problem}
        \begin{proof}[Solution]
        We want the solution to $A^T A x = A^T b$. We have:
        \begin{align*}
            A&= \begin{bmatrix*}[r]1&-1\\1&1\\1&2\end{bmatrix*} & b&=\begin{bmatrix*}[r]2\\0\\-1\end{bmatrix*} & A^{T}Ax&=A^{T}b\Rightarrow\begin{bmatrix}3&1\\1&9\end{bmatrix} \begin{bmatrix}x_{1}\\x_{2}\end{bmatrix}=\begin{bmatrix*}[r]1\\-6\end{bmatrix*}
        \end{align*}
        The solution is $x = \frac{1}{26}(15,-19)^T$
        \end{proof}
        \begin{problem}
        Let $\theta\in\mathbb{R}$ and let $\mathbf{x}_1 = (\cos(\theta), \sin(\theta))^{T}$, $\mathbf{x}_2 = (-\sin(\theta), \cos(\theta))^{T}$. Show that $\{\mathbf{x}_1,\mathbf{x}_2\}$ is an orthonormal basis for $\mathbb{R}^2$. Write $\mathbf{y}=(-2,3)^{T}$ as a linear combination $\mathbf{y}=c_{1} \mathbf{x}_{1}+c_{2}\mathbf{x}_{2}$
        \end{problem}
        \begin{proof}[Solution]
        They are orthonormal for $\mathbf{x}_1^T \mathbf{x}_2 = -\cos(\theta)\sin(\theta) + \cos(\theta)\sin(\theta) = 0$, and since $\norm{\mathbf{x}_1}=\norm{\mathbf{x}_2}= (\sin^2(\theta)+\cos^2(\theta))^{1/2}=1$. Let $c_{1}=\langle\mathbf{y},\mathbf{x}_{1}\rangle$ and $c_{2}=\langle\mathbf{y},\mathbf{x}_{2}\rangle$. Then $c_1 = -2\cos(\theta)+3\sin(\theta)$ and $c_2 = 2\sin(\theta)+3\cos(\theta)$. Therefore, $\mathbf{y}=(-2\cos(\theta)+3\sin(\theta))\mathbf{x}_1+(2\sin(\theta)+3\cos(\theta)\mathbf{x}_2)$
        \end{proof}
        \subsection{Problem Set IV}
        \begin{problem}
        Find the eigenvalues and associated eigenspaces of $A = \begin{bmatrix}4 & 5 \\ 2 & 1 \end{bmatrix}$
        \end{problem}
        \begin{proof}[Solution]
        We need to compute $\det(A-\lambda I)=0$. This gives us:
        \begin{equation*}
            \begin{vmatrix} 4-\lambda & 5 \\ 2 & 1-\lambda \end{vmatrix} = (4-\lambda)(1-\lambda)-10 = 0
        \end{equation*}
        The solutions to this are $\lambda_1 = 6, \lambda_2 = -1$. Solving $Ax = \lambda x$ yields the eigenspaces. We have:
        \begin{equation*}
            \begin{bmatrix} 4 & 5 \\ 2 & 1 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}=-\begin{bmatrix} x_1 \\ x_2 \end{bmatrix}\quad\quad\quad\quad\begin{bmatrix} 4 & 5 \\ 2 & 1 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}=6\begin{bmatrix} x_1 \\ x_2 \end{bmatrix}
        \end{equation*}
        These give solutions $x_2(-1,1)^T$ and $x_2 (\frac{5}{2},1)^T$, where $x_2$ is a free variable.
        \end{proof}
        \begin{problem}
        Show that for a $2\times 2$ matrix $A$, $\lambda^2 - \Tr(A)\lambda + \det(A) = 0$, where $\lambda$ is an eigenvalue of $A$.
        \end{problem}
        \begin{proof}[Solution]
        For we have that $\det(A-\lambda I) = 0$. But:
        \begin{equation*}
            \det(A-\lambda I)=\begin{vmatrix} a-\lambda & b \\ c & d-\lambda \end{vmatrix}=(a-\lambda)(d-\lambda)-bc=\lambda^2-(a+d)\lambda+ad-bc=\lambda^{2}-\Tr(A)\lambda+\det(A)
        \end{equation*}
        Therefore, if $\lambda$ is an eigenvalue of $A$, then $\lambda^2 - \Tr(A) \lambda + \det(A) = 0$.
        \end{proof}
        \begin{problem}
        Find the eigenvalues and associated eigenspaces for $A = \begin{bmatrix} 1 & 1 & 1 \\ 0 & 2 & 1 \\ 0 & 0 & 3\end{bmatrix}$
        \end{problem}
        \begin{proof}[Solution]
        Recall that the determinant expansion can be done along any row. Thus:
        \begin{align*}
            \det(A-\lambda I) &= \begin{vmatrix} 1-\lambda & 1 & 1 \\ 0 & 2-\lambda & 1 \\ 0 & 0 & 3-\lambda \end{vmatrix}=0\begin{vmatrix} 1 & 1 \\ 2-\lambda & 1 \end{vmatrix}-0 \begin{vmatrix} 1-\lambda & 1 \\ 0 & 1 \end{vmatrix} + (3-\lambda)\begin{vmatrix} 1-\lambda & 1 \\ 0 & 2-\lambda\end{vmatrix}\\
            &= (3-\lambda)(1-\lambda)(2-\lambda)    
        \end{align*}
        The solutions are $\lambda_1 = 1,\ \lambda_2 = 2,\ \lambda_3 = 3$. The eigenspaces correspond to the solutions of the equation $Ax = \lambda x$. Thus we get:
        \begin{equation*}
            \begin{bmatrix} 1 & 1 & 1 \\ 0 & 2 & 1 \\ 0 & 0 & 3 \end{bmatrix}\begin{bmatrix} x \\ y \\ z \end{bmatrix} = \lambda \begin{bmatrix}x \\ y \\ z\end{bmatrix}    
        \end{equation*}
        This gives 3 different equations for each value of $\lambda$.
        \begin{equation*}
            Ax=x\Rightarrow x=(1,0,0)^{T}\quad\quad\quad\quad Ax=2x\Rightarrow x=(1,1,0)^{T}\quad\quad\quad\quad Ax=3x\Rightarrow x=(1,1,1)^{T}
        \end{equation*}
        \end{proof}
        \subsection{Problem Set V}
        \begin{problem}
        Factor $\begin{bmatrix} 4 & 2 \\ 2 & 1 \end{bmatrix}$ into the form $PDP^T$, where $D$ is a diagonal and $P$ is orthogonal.
        \end{problem}
        \begin{proof}[Solution]
        The eigenvalues of $A$ are the solutions to $(4-\lambda)(1-\lambda)-4=0$: $\lambda_1 = 0$, $\lambda_2 = 5$. The eigenvectors are solutions to:
        \begin{equation*}
            \begin{bmatrix} 4 & 2 \\ 2 & 1 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} = \lambda \begin{bmatrix} x \\ y \end{bmatrix}
        \end{equation*}
        Which gives us $\frac{1}{\sqrt{5}}(2,1)^T$ and $\frac{1}{\sqrt{5}}(-1,2)^T$. Thus:
        \begin{equation*}
            P = \frac{1}{\sqrt{5}}\begin{bmatrix} -1 & 2 \\ 2 & 1 \end{bmatrix}\quad\quad D = \begin{bmatrix} 0 & 0 \\ 0 & 5 \end{bmatrix}\quad\quad P^{T} = \frac{1}{\sqrt{5}}\begin{bmatrix} -1 & 2 \\ 2 & 1 \end{bmatrix}
        \end{equation*}
        \end{proof}
        \begin{problem}
        Solve the differential equation $Y'(t) = \begin{bmatrix} 4 & 2 \\ 2 & 1 \end{bmatrix} Y(t)$ with $Y(0) = \begin{bmatrix} -1 \\ 4 \end{bmatrix}$
        \end{problem}
        \begin{proof}[Solution]
        We know from the previous problem that the eigenvalues and eigenvectors are distinct, and thus $Y(t) = \alpha V_1 e^{\lambda_1 t} + \beta V_2 e^{\lambda_2 t}$ where $\lambda_{i}$ are the distinct eigenvalues, and $V_{i}$ are the distinct eigenvectors. Solving for the initial condition:
        \begin{equation*}
            \frac{1}{\sqrt{5}}\begin{bmatrix} 2 & -1 \\ 1 & 2 \end{bmatrix}\begin{bmatrix} \alpha \\ \beta \end{bmatrix}=\begin{bmatrix} -1 \\ 4 \end{bmatrix}\Rightarrow \begin{bmatrix} \alpha \\ \beta \end{bmatrix}=\frac{1}{\sqrt{5}}\begin{bmatrix} -1 & 2 \\ 2 & 1 \end{bmatrix}\begin{bmatrix} -1 \\ 4 \end{bmatrix}=\frac{1}{\sqrt{5}} \begin{bmatrix} 9 \\ 2 \end{bmatrix}
        \end{equation*}
        Thus, $Y(t) = \frac{9}{5}(-1,2)^T + \frac{2}{5} (2,1)^T e^{5t}$ 
        \end{proof}
        \begin{problem}
        Solve the following:
        \begin{enumerate}
            \item Let $A$ be an $n\times n$ complex Hermitian matrix such that $A^4=I$. What are the possible eigenvalues of $A$?
            \item If $A$ is an $n\times n$ complex matrix and $A^4 = I$, what are the possible eigenvalues?
        \end{enumerate}
        \end{problem}
        \begin{problem}
        Using least squares, find the line in $\mathbb{R}^2$ that best fits $\{(2,1),\ (3,2),\ (4,2),\ (5,3)\}$
        \end{problem}
        \begin{proof}[Solution]
        We want a line $y=mx+b$ that best fits the points. Setting up the problem, we get:
        \begin{equation*}
            \begin{bmatrix} 1 & 2 \\ 1 & 3 \\ 1 & 4 \\ 1 & 5 \end{bmatrix} \begin{bmatrix} b \\ m \end{bmatrix} = \begin{bmatrix} 1 \\ 2 \\ 2 \\ 3\end{bmatrix}   
        \end{equation*}
        This has no solution. Let $A$ be the left-most matrix. Then:
        \begin{equation*}
            A^T = \begin{bmatrix} 1 & 1 & 1 & 1 \\ 2 & 3 & 4 & 5 \end{bmatrix}\Rightarrow A^{T}A = \begin{bmatrix} 4 & 14 \\ 14 & 54 \end{bmatrix}
        \end{equation*}
        We now solve $A^{T}AX$:
        \begin{equation*}
            \begin{bmatrix} 4 & 14 \\ 14 & 54 \end{bmatrix} \begin{bmatrix} b \\ m \end{bmatrix} =  A^T \begin{bmatrix} 1 \\ 2 \\ 2 \\ 3 \end{bmatrix} = \begin{bmatrix} 8 \\ 31 \end{bmatrix}   
        \end{equation*}
        The solution gives us $y = \frac{3}{5}x-\frac{1}{10}$
        \end{proof}
        \begin{problem}
        Find the projection matrix $P$ that projects $\mathbb{R}^4$ onto the line through the origin spanned by the vector $(2,1,-1,-1)$.
        \end{problem}
        \begin{problem}
        Consider the rotation matrix $R$ shown below. Compute the axis vector $\textbf{u}$ and both the sine and cosine of the counterclockwise angle $\theta$ such that $R = R_{\theta,\textbf{u}}$
        \begin{equation*}
            R = \begin{bmatrix*}[r] -\frac{4}{9} & -\frac{7}{9} & \frac{4}{9} \\ \frac{1}{9} & \frac{4}{9} & \frac{8}{9} \\ -\frac{8}{9} & \frac{4}{9} & -\frac{1}{9} \end{bmatrix*}
        \end{equation*}
        \end{problem}
        \begin{problem}
        Find an orthonormal basis for the column space of the matrix:
        \begin{equation*}
            A = \begin{bmatrix*}[r] 1 & 1 & 1 \\ 0 & 3 & 1 \\ 2 & 2 & 2 \\ 2 & 4 & 3 \\ -1 & \phantom{-}2 & \phantom{-}0 \end{bmatrix*}
        \end{equation*}
        \end{problem}
        \begin{proof}[Solution]
        We use Gram-Schmidt to do this. Let $v_{1}=(1,0,2,2,-1)$. Normalizing gives us:
        \begin{equation*}
            e_{1} = \frac{1}{\sqrt{10}}(1,0,2,2,-1)^T    
        \end{equation*}
        We then compute:
        \begin{align*}
            (1,3,2,4,2)^T-\tfrac{(1,3,2,4,2)^T(1,0,2,2,-1)}{(1,0,2,2,-1)^T (1,0,2,2,-1)}(1,0,2,2,-1)^{T}&=(1,3,2,4,2)^{T}-\tfrac{11}{10}(1,0,2,2,-1)^{T}\\
            &=(-\tfrac{1}{10},3,-\tfrac{2}{10},\tfrac{18}{10},\tfrac{33}{10})^{T}=\tfrac{1}{10}(-1,30,-2,18,33)^{T}
        \end{align*}
        Thus:
        \begin{equation*}
            e_{2}=\tfrac{\frac{1}{10}(-1,30,-2,18,33)}{\norm{\frac{1}{10}(-1,30,-2,18,33)}}=\frac{1}{\sqrt{2318}}(-1,30,-2,18,33)
        \end{equation*}
        Finishing off, we compute:
        \begin{equation*}
            \mathbf{v}_{3}=(1,1,2,3,0)^{T}-\tfrac{(1,1,2,3,0)^T(1,0,2,2,-1)}{10}(1,0,2,2,-1)^T-\tfrac{(1,1,2,3,0)^T(1,3,2,4,2)}{34}(1,3,2,4,2,0)^{T}
        \end{equation*}
        Finally, $e_3=\frac{\textbf{v}_{3}}{\norm{\textbf{v}_{3}}}$
        \end{proof}
        \begin{problem}
        Eliminate crossterms and classify the conic section $6x^2 - 4xy+3y^2 = 1$
        \end{problem}
        \newpage
        \subsection{Problem Set VI}
        \begin{problem}
        Let $\begin{bmatrix*}[r] 1 & 0 & 3 & \vline & 1 \\ 0 & \phantom{-}1 & -2 & \vline & 3 \\ 1 & 2 & 0 & \vline & 0 \end{bmatrix*}$ be an augmented matrix.
        \begin{enumerate}
            \item Solve the system using Gaussian elimination.
            \item Express $(1,3,0)^{T}$ as a linear combination of the column vectors of the coefficient matrix.
            \item Use elementary matrices to find the LU decomposition of the coefficient matrix.
        \end{enumerate}
        \end{problem}
        \begin{proof}[Solution]
        In order,
        \begin{enumerate}
            \item 
            \begin{align*}
                \begin{bmatrix*}[r] 1 & 0 & 3 & \vline & 1 \\ 0 & \phantom{-}1 & -2 & \vline & 3 \\ 1 & 2 & 0 & \vline & 0 \end{bmatrix*} &\underset{r_{2}\leftrightarrow r_{3}\phantom{2}}{\longrightarrow} \begin{bmatrix*}[r] 1 & 0 & 3 & \vline & \phantom{-}1 \\ 1 & 2 & 0 & \vline & 0 \\ 0 & \phantom{-}1 & -2 & \vline & 3 \end{bmatrix*} \underset{r_{2}-r_{1}\phantom{3}}{\longrightarrow} \begin{bmatrix*}[r] 1 & 0 & 3 & \vline & 1 \\ 0 & \phantom{-} 2 & -3 & \vline & -1 \\ 0 & 1 & -2 & \vline & 3 \end{bmatrix*}\\
                &\underset{r_{2}\div 2\phantom{2_{2}}}{\longrightarrow} \begin{bmatrix*}[r] 1 & 0 & 3 & \vline & 1 \\ 0 & 1 & -\tfrac{3}{2} & \vline & -\tfrac{1}{2} \\ 0 & \phantom{-}1 & -2 & \vline & 3\end{bmatrix*} \underset{r_{3}-r_{2}\phantom{3}}{\longrightarrow} \begin{bmatrix*}[r] 1 & 0 & 3 & \vline & 1 \\ 0 & 1 & -\tfrac{3}{2} & \vline & -\tfrac{1}{2} \\ 0 & \phantom{-}0 & -\tfrac{1}{2} & \vline & \tfrac{7}{2} \end{bmatrix*}\\
                &\underset{r_{3}\cdot(-2)}{\longrightarrow} \begin{bmatrix*}[r] 1 & 0 & 3 & \vline & 1 \\ 0 & \phantom{-}1 & -\tfrac{3}{2} & \vline & -\tfrac{1}{2} \\ 0 & 0 & 1 & \vline & -7 \end{bmatrix*} \underset{r_{1}-3r_{3}}{\longrightarrow} \begin{bmatrix*}[r] 1 & 0 & 0 & \vline & 22 \\ 0 & 1 & -\tfrac{3}{2} & \vline & -\tfrac{1}{2} \\ 0 & \phantom{-}0 & \phantom{-}1 & \vline & -7 \end{bmatrix*}\\
                &\underset{r_{2}+\frac{3}{2}r_{3}}{\longrightarrow} \begin{bmatrix*}[r] 1 & 0 & 0 & \vline & 22 \\ 0 & 1 & 0 & \vline & -11 \\ 0 & 0 & 1 & \vline & -7 \end{bmatrix*}
            \end{align*}
            \item $(1,3,0)^{T}=22(1,0,1)^{T}-11(0,1,2)^{T}-7(3,-2,0)^{T}$
            \item
            \begin{equation*}
                A = \begin{bmatrix*}[r] 1 & 0 & 0 \\ 0 & 1 & 0 \\ 1 & 2 & 1 \end{bmatrix*} \begin{bmatrix*}[r] 1 & 0 & 3 \\ 0 & \phantom{-}1 & -2 \\ 0 & 0 & 1 \end{bmatrix*}
            \end{equation*}
        \end{enumerate}
        \end{proof} 
        \begin{problem}
        Let $A = \begin{bmatrix*}[r] 1 & 0 & 0 \\ 2 & 1 & 0 \\ 3 & 4 & 1 \end{bmatrix*}$, $B=\begin{bmatrix*}[r]1 & 0 & 0 \\ -2 & 1 & \phantom{-}0 \\ 5 & -4 & 1 \end{bmatrix*}$, and $C = \begin{bmatrix*}[r] 2 & 3 \\ -1 & 0 \\ 1 & 1 \end{bmatrix*}$. 
        \begin{enumerate}
        \begin{multicols}{3}
            \item Solve $AC+BC$
            \item Solve $AB$
            \item Does $A = B^{-1}$?
        \end{multicols}
        \end{enumerate}
        \end{problem}
        \begin{proof}[Solution]
        In order,
        \begin{enumerate}
            \item $AC+BC = (A+B)C = \begin{bmatrix*}[r] 2 & 0 & 0 \\ 0 & 2 & 0 \\ 8 & 0 & 2 \end{bmatrix*} \begin{bmatrix*}[r] 2 & 3 \\ -1 & 0 \\ 1 & 1 \end{bmatrix*} = \begin{bmatrix*}[r] 4 & 6 \\ -2 & 0 \\ 18 & 26 \end{bmatrix*}$
            \item $AB = \begin{bmatrix*}[r] 1 & 1 & 0 \\ 0 & -15 & \phantom{-}0 \\ 0 & 0 & 1 \end{bmatrix*}$
            \item No, for if $A=B^{-1}$ then $AB=I$, but this is not true.
        \end{enumerate}
        \end{proof}
        \begin{problem}
        If $A$ and $B$ are $n\times n$ invertible matrices, what is $(AB)^{-1}$?
        \end{problem}
        \begin{proof}[Solution]
        As $A^{-1}$ and $B^{-1}$ exist, and as $A$ and $B$ are of the same dimension, $B^{-1}A^{-1}$ exists. But $(B^{-1}A^{-1})(AB) = B^{-1}(A^{-1}A)B = B^{-1}IB = B^{-1}B = I$. As inverses are unique, $(AB)^{-1} = B^{-1}A^{-1}$.
        \end{proof}
        \begin{problem}
        If $A$ and $B$ are $n\times n$ matrices, what is $(A+B)^2$?
        \end{problem}
        \begin{proof}[Solution]
        $(A+B)^2 =(A+B)(A+B) = A(A+B)+B(A+B)=A^2+AB+BA+B^2$. Note: It is not true in general that $AB=BA$, and thus we cannot simplify further.
        \end{proof}
        \begin{problem}
        If $A$ and $A^T$ are $n\times n$ invertible matrices, show that $(A^T)^{-1} = (A^{-1})^T$
        \end{problem}
        \begin{proof}[Solution]
        For $A^T(A^{-1})^T = (A^{-1}A)^T = I^T = I$. As inverses are unique, $(A^T)^{-1} = (A^{-1})^T$
        \end{proof}
        \begin{problem}
        What are the solutions of:
        \begin{enumerate}
        \begin{multicols}{2}
            \item $\begin{bmatrix*}[r] 1 & 1 & 0 & 0 & \vline & -1 \\ 0 & 1 & 0 & 0 & \vline & 3 \\ 0 & 0 & 1 & 1 & \vline & 2 \\ 0 & 0 & 1 & 1 & \vline & 1 \end{bmatrix*}$
            \item $\begin{bmatrix*}[r] 1 & 1 & 0 & 0 & \vline & -1 \\ 0 & 1 & 0 & 0 & \vline & 3 \\ 0 & 0 & 1 & 1 & \vline & 1 \\ 0 & 0 & 1 & 1 & \vline & 1 \end{bmatrix*}$
        \end{multicols}
        \end{enumerate}
        \end{problem}
        \begin{proof}[Solution]
        In order,
        \begin{enumerate}
            \item No solution as the bottom two rows say $x_3 + x_4 = 2$ and $x_3 + x_4 = 1$. An impossibility.
            \item The entire space $S = \{(-4,3,x,1-x):x\in \mathbb{R}\}$.
        \end{enumerate}
        \end{proof}
        \begin{problem}
        If $A,B,$ and $C$ are $n\times n$ invertible matrices, then solve the following equations for $X$:
        \begin{enumerate}
        \begin{multicols}{3}
            \item $XA+B=C$
            \item $AX+B=X$
            \item $XA+C=X$
        \end{multicols}
        \end{enumerate}
        \end{problem}
        \begin{proof}
        In order,
        \begin{enumerate}
            \item $XA +B=C\Rightarrow XA = C-B \Rightarrow X = (C-B)A^{-1}$
            \item $AX+B = X\Rightarrow AX-X=-B \Rightarrow (A-I)X=-B \Rightarrow X = -(A-I)^{-1}B$
            \item $XA+C = X \Rightarrow XA-X = -C \Rightarrow X(A-I) = -C \Rightarrow X = -C(A-I)^{-1}$
        \end{enumerate}
        \end{proof}
        \subsection{Problem Set VII}
        \begin{problem}
        Determine the basis of the given vector space over the given field.
        \begin{enumerate}
        \begin{multicols}{3}
            \item $V=\mathbb{R}$ over $K=\mathbb{R}$
            \item $V=\mathbb{C}$ over $K=\mathbb{C}$
            \item $V=\mathbb{C}$ over $K=\mathbb{R}$
        \end{multicols}
        \end{enumerate}
        \end{problem}
        \begin{proof}[Solution]
        In order,
        \begin{enumerate}
            \item The set $\{1\}$ is a basis. Let $r \in \mathbb{R}$. Then $r=1\cdot r$.
            \item The set $\{(1,0)\}$ is a basis. Let $z\in \mathbb{Z}$. Then $z\cdot(1,0) = z$
            \item The set $\{(1,0),(0,1)\}$ is a basis. Let $z=a+bi\in \mathbb{Z}$. Then $z = a(1,0)+b(0,1)$.
        \end{enumerate}
        \end{proof}
        \begin{problem}
        What is the nullspace of an $n\times n$ matrix $A$ with real entries?
        \end{problem}
        \begin{proof}[Solution]
        The nullspace is the set $N(A) = \{X\in \mathbb{R}^n: AX = 0\}$
        \end{proof}
        \begin{problem}
        A matrix $A$ and its row reduced form $A'$ are shown below. What is the rank of $A$?
        \begin{equation*}
            A=\begin{bmatrix*}[r] 1 & 2 & 3 & 4 \\ -1 & -1 & -4 & -2 \\ 3 & 4 & 11 & 8 \end{bmatrix*} \quad\quad\quad\quad A' = \begin{bmatrix} 1 & 0 & 5 & 0 \\ 0 & 1 & -1 & 2 \\ 0 & 0 & 0 & 0 \end{bmatrix}
        \end{equation*}
        \end{problem}
        \begin{proof}[Solution]
        The rank is the dimension of the space spanned by the column vectors of the matrix. Using the row-reduced form, we see that these columns span $\mathbb{R}^2$ and thus the matrix has rank $2$.
        \end{proof}
        \begin{problem}
        What is the rank-nullity theorem?
        \end{problem}
        \begin{proof}[Solution]
        For an $n\times n$ matrix $A$, $\rk(A)+\nul(A) = n$.
        \end{proof}
        \newpage
        \subsection{Problem Set VIII}
        \begin{problem}
        Let $T:\mathbb{R}^3\rightarrow \mathbb{R}^2$ be defined by $T\begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} = \begin{bmatrix} x_3 \\ x_1+x_2 \end{bmatrix}$.
        \begin{enumerate}
            \item Determine $\ker(T)$.
            \item Determine the dimensions of $\ker(T)$.
            \item Using the Nullity Theorem, determine the dimension of im$(T)$.
        \end{enumerate}
        \end{problem}
        \begin{proof}[Solution]
        In order,
        \begin{enumerate}
            \item If $T(x_{1},x_{2},x_{3})^{T} = 0$, then $x_3=0$ and $x_{1}+x_{2}=0$. $\ker(T)=\{(x,-x,0):x\in \mathbb{R}\}$
            \item This is a line through the origin, so the dimension is $1$ 
            \item The Nullity Theorem states that $\dim(\ker(T))+\dim(im(T)) = \dim(\mathbb{R}^3) = 3$. Thus $\dim(im(T)) = 2$.
        \end{enumerate}
        \end{proof}
        \begin{problem}
        Find the matrix representation of $T$ (Previous problem) in the standard basis of $\mathbb{R}^3$.
        \end{problem}
        \begin{proof}[Solution]
        $Te_1 = (0,1)^T$, $T e_2 = (0,1)^T$, and $Te_3 = (1,0)^T$. The matrix representation is $T=\begin{bmatrix} 0 & 0 & 1 \\ 1 & 1 & 0 \end{bmatrix}$
        \end{proof}
        \begin{problem}
        Let $P_n$ be the set of all polynomials with real coefficients of degree less than $n$. The standard basis is $\{1,x,\hdots, \ x^{n-1}\}$. Let $D:P_3 \rightarrow P_2$ be defined by $D(p) = 5\frac{dp}{dx}$. Determine the matrix representation of $D$ with respect to the standard basis.
        \end{problem}
        \begin{proof}[Solution]
        We need only check how $D$ acts on the basis vectors. $D(1) = 0+0x$, $D(x) = 1+0x$, $D(x^2) = 0+2x$. So, we have $D = \begin{bmatrix} 0 & 2 & 0 \\ 1 & 0 & 0 \end{bmatrix}$
        \end{proof}
        \begin{problem}
        Let $V$ be a vector space over $\mathbb{R}$ and let $S$ be a subspace of $V$.
        \begin{enumerate}
        \begin{multicols}{2}
            \item Define $S^{\perp}$.
            \item If $S=\Span\{ (1,2,1)^T, (1-1,2)^T\}$, what is $S^{\perp}$?
        \end{multicols}
        \end{enumerate}
        \end{problem}
        \begin{proof}[Solution]
        In order,
        \begin{enumerate}
            \item $S^{\perp} = \{x\in V: \forall y\in S, x^T y = 0\}$.
            \item Using the definition, the equations below give us $S^{\perp}=\{x_{3}(-\frac{5}{3},\frac{1}{3},1):x_{3}\in \mathbb{R}\}$
            \begin{equation*}
                \begin{bmatrix}1&2&1\\1&-1&2\end{bmatrix}\begin{bmatrix}x_1\\x_2\\x_3\end{bmatrix}=\begin{bmatrix}0\\0\end{bmatrix}\Leftrightarrow\begin{bmatrix}1&0&\frac{5}{3} \\0&1&\frac{-1}{3}\end{bmatrix}\begin{bmatrix}x_1\\x_2\\x_3\end{bmatrix}=\begin{bmatrix}0\\0\end{bmatrix}
            \end{equation*}
        \end{enumerate}
        \end{proof}
        \begin{problem}
        \
        \begin{enumerate}
            \item Let $V$ be a vector space over $\mathbb{R}$. Define an inner product.
            \item What is the difference between the standard dot product in $\mathbb{R}^n$ and an inner product? Can a vector space have more than one inner product?
            \item If $\langle x,y \rangle = xy$, what is $\norm{x}$?
        \end{enumerate}
        \end{problem}
        \begin{proof}[Solution]
        In order,
        \begin{enumerate}
            \item An inner product is a function from $\mathbb{R}\times \mathbb{R}\rightarrow \mathbb{R}$ with the following properties:
            \begin{enumerate}
                \item $\langle ax+by,z\rangle = a\langle x,z\rangle+b\langle y,z\rangle$
                \item $\langle x,y\rangle = \langle y,x \rangle$
                \item $\langle x,x\rangle \geq 0$
            \end{enumerate}
            \item An inner product is a generalization of the standard dot product. The dot product is itself an inner product, but not all inner products are dot products. There are infinitely many inner products for $\mathbb{R}$. Let $n\in \mathbb{N}$ be arbitrary, then $\langle x,y \rangle = nxy$ is an inner product.
            \item $\norm{x} = \sqrt{\langle x,x \rangle } = \sqrt{x^2}= |x|$.
        \end{enumerate}
        \end{proof}
        \begin{problem}
        Let $V = C[-1,1]$ and let $\langle f,g\rangle = \int_{-1}^{1} f(x)g(x)dx$.
        \begin{enumerate}
            \item Show that $f(x)=1$ and $g(x) = x$ are orthogonal with respect to this inner product.
            \item Determine $\norm{f}$ and $\norm{g}$.
            \item Show that $f$ and $g$ satisfy the Pythagorean Law.
        \end{enumerate}
        \end{problem}
        \begin{proof}[Solution]
        In order,
        \begin{enumerate}
        \begin{multicols}{2}
            \item $\langle 1,x\rangle=\int_{-1}^{1}xdx=0$
            \item $\norm{1} = \sqrt{ \int_{-1}^{1} dx} = \sqrt{2}$, $\norm{x} = \sqrt{\int_{-1}^{1}x^2dx} = \sqrt{\frac{2}{3}}$
        \end{multicols}
            \item $\norm{1+x}^2 = \langle 1+x,1+x\rangle = \langle 1,1\rangle + 2\langle 1,x \rangle + \langle x,x\rangle = \norm{1}^2 + \norm{x}^2$
        \end{enumerate}
        \end{proof}
        \begin{problem}
        Let $V$ be any inner product space. State and prove the Pythagorean Theorem for inner product spaces.
        \end{problem}
        \begin{proof}[Solution]
        The Pythagorean Theorem for Inner Product Spaces state that if $V$ is an inner product space with inner product $\langle, \rangle$, and if $\langle x,y\rangle = 0$, then $\norm{x}^2+\norm{y}^2 = \norm{x+y}^2$. For $\norm{x+y}^2 = \langle x+y,x+y\rangle = \langle x,x\rangle + 2\langle x,y\rangle +\langle y,y\rangle$. But as $x$ and $y$ are orthogonal, $\langle x,y \rangle = 0$. Thus $\norm{x+y}^2 = \langle x,x\rangle + \langle y,y\rangle = \norm{x}^2+\norm{y}^2$. $\norm{x+y}^2 =\norm{x}^2+\norm{y}^2$.
        \end{proof}
        \begin{problem}
        Prove that if $V$ is an inner product space and $S$ is a subspace of $V$, then $S^{\perp}$ is a subspace of $V$.
        \end{problem}
        \begin{proof}[Solution]
        We must check that $0\in S^{\perp}$ and that $S^{\perp}$ is closed under addition and scalar multiplication.
        \begin{enumerate}
            \item For all $x\in S$, $\langle 0,x \rangle = 0$, and thus $0\in S^{\perp}$.
            \item If $x,y\in S^{\perp}$ and $z\in S$, then $\langle x+y,z\rangle = \langle x,z\rangle + \langle y,z\rangle = 0+0=0$. Thus $x+y\in S^{\perp}$.
            \item If $x\in S^{\perp}$, $y\in S$, and $\alpha$ is a scalar, then $\langle \alpha x,y \rangle = \alpha \langle x,y \rangle = \alpha \cdot 0 = 0$. Thus $\alpha x \in S^{\perp}$. $S^{\perp}$ is a subspace.
        \end{enumerate}
        \end{proof}
    \section{Problems from Salem State}