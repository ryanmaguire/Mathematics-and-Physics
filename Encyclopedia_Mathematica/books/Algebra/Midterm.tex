%------------------------------------------------------------------------------%
\documentclass[crop=false,class=article]{standalone}                           %
%------------------------------Preamble----------------------------------------%
\makeatletter                                                                  %
    \def\input@path{{../../../}}                                               %
\makeatother                                                                   %
\input{preamble.tex}                                                           %
%----------------------------Main Document-------------------------------------%
\begin{document}
    \title{Midterm}
    \author{Ryan Maguire}
    \date{\vspace{-5ex}}
    \maketitle
    \section{}
        Let $(k,+,\cdot)$ be a commutative ring,
        $(M,\boldsymbol{+},\boldsymbol{\cdot})$ be a module over $k$, and let
        $k[x]$ be the ring of polynomials. That is, $k[x]$ is the space of
        finitely supported sequences of ring elements in $k$ equipped with
        sequence addition and the Cauchy product of two sequences. Let $M[x]$ be
        the same space but for sequences of elements in $M$. Give $M[x]$ the
        same pointwise addition and let $\star:k[x]\times{M}[x]\rightarrow{M}[x]$
        be the following product:
        \begin{equation}
            r(x)\star{m}(x)=
            \sum_{i=0}^{N}\sum_{j=0}^{M}(r_{i}\boldsymbol{\cdot}m_{j})x^{i+j}
        \end{equation}
        This makes $M[x]$ a module over $k[x]$. For we have:
        \begin{equation}
            1(x)\star{m}(x)=\sum_{i=0}^{N}(1\boldsymbol{\cdot}m_{i})x^{i}
            =\sum_{i=0}^{N}m_{i}x^{i}
            =m(x)
        \end{equation}
        For the distributive law let $n$ and $m$ be polynomials in $M[x]$ and
        extend $n$ or $m$ with zeros so that we may write them as a sum over the
        same powers. Then we have:
        \begin{equation}
            r(x)\star(m(x)+n(x))=
            \sum_{i=0}^{N}\sum_{j=0}^{M}r_{i}\boldsymbol{\cdot}(m_{j}+n_{j})x^{i+j}
            =\sum_{i=0}^{N}\sum_{j=0}^{M}
                (r_{i}\boldsymbol{m}_{j}+r_{i}\boldsymbol{\cdot}n_{j})x^{i+j}
        \end{equation}
        Thinking of these as finitely supported sequences we'd be done here, but
        alas, they are polynomials. So:
        \begin{subequations}
            \begin{align}
                r(x)\star(m(x)+n(x))
                &=\sum_{i=0}^{N}\sum_{j=0}^{M}r_{i}\boldsymbol{\cdot}m_{j}x^{i+j}
                    +\sum_{i=0}^{N}\sum_{j=0}^{M}
                    r_{i}\boldsymbol{\cdot}n_{j}x^{i+j}\\
                &=(r(x)\star{m}(x))+(r(x)\star{n}(x))
            \end{align}
        \end{subequations}
        Similarly for the distributive law over modules. Lastly, checking the
        compatibility of multiplication, we have:
        \begin{subequations}
            \begin{align}
                r(x)\star(s(x)\boldsymbol{\cdot}m(x))
                &=r(x)\star\sum_{i=0}^{N}\sum_{j=0}^{M}s_{i}m_{j}x^{i+j}\\
                &=\sum_{\ell=0}^{L}\sum_{i=0}^{N}\sum_{j=0}^{M}
                    r_{\ell}s_{i}m_{j}x^{\ell+i+j}\\
                &=\Big(\sum_{\ell=0}^{L}\sum_{i=0}^{N}
                    r_{\ell}s_{i}x^{\ell+i}\Big)\star{m}(x)\\
                &=(r(x)\cdot{s}(x))\star{m}(x)
            \end{align}
        \end{subequations}
        Thus $M[x]$ is a $k[x]$ module. Let $\iota:M\rightarrow{M}[x]$ be the
        inclusion mapping. That is $m\mapsto{m}(x)$, where $m(x)$ is the
        constant polynomial $m(x)=m$. Suppose $N$ is a $k$ module and
        $\varphi:M\rightarrow{N}$ is a module homomorphism. If $\tilde{\varphi}$
        is a $k[x]$ module homomorphism from $M[x]$ to $N$ such that the diagram
        commutes thatn $(\tilde{\varphi}\circ\iota)(m)=\varphi(m)$. But:
        \begin{equation}
            (\tilde{\varphi}\circ\iota)(m)
            =\tilde{\varphi}(\iota(m))
            =\tilde{\varphi}(m)
            =\varphi(m)
        \end{equation}
        Thus let $\tilde{\varphi}(m(x))=\varphi(m_{0})$, where $m_{0}$ is the
        constant term of the polynomial $m(x)$. Then $\tilde{\varphi}$ makes the
        diagram commute. Moreover, from our chain of equalities, we have that
        such a map must be unique. Thus $\iota$ is universal.
    \section{}
        Put $A$ into rational canonical form. That is, the matrix $R_{A}$ where:
        \begin{equation}
            R_{A}=\textrm{diag}(C(f_{1}),\dots,C(f_{n}))
        \end{equation}
        where the $C(f_{1})$ are the companion matrices to the polynomials
        $f_{i}$ such that $f_{i}|f_{i+1}$ are minimal and whose product is the
        characteristic polynomial. $A$ is similar to $R_{A}$. But the rational
        canonical form for $A^{T}$ is also $R_{A}$ since $A$ and $A^{T}$ have
        the same characteristic polynomial and since:
        \begin{equation}
            \prod_{k=1}^{N}(\lambda_{k}I-A)=0
            \Leftrightarrow(\lambda_{k}I-A^{T})=0
        \end{equation}
        and therefore $A$ and $A^{T}$ will have the same minimial polynomial,
        and thus their rational canonical forms will be made from the same
        block diagonal matrices. But if $A$ is similar to $R_{A}$ and
        $A^{T}$ is similar to $R_{A}$, then $A$ is similar to $A^{T}$ since
        similarity is an equivalence relation. Thus, $A$ is similar to its
        transpose.
    \section{}
    \section{}
        For let $\mathscr{B}=\{e_{1},\dots,e_{n}\}$ be a basis of $V$. Note that
        $J$ is invertible since:
        \begin{equation}
            JJ^{3}=(J^{2})^{2}=(\minus\textrm{Id})^{2}=\textrm{Id}
        \end{equation}
        Since $J$ is a real valued matrix, $\det(A^{2})=\det(A)^{2}\geq{0}$.
        But if $n$ is odd, then $\det(\minus{I})=(\minus{1})^{n}=\minus{1}$,
        a contradiction since $\det(A^{2})=\det(\minus{I})$. Therefore $n$ is
        even. From the fact that the characteristic polynomial of $J^{2}$ is
        $(\xi+1)^{n}$ we have that the characteristic polynomial of $J$ is
        $(\lambda^{2}+1)^{n}$. Over $\mathbb{R}$ the minimial polynomial is then
        $\lambda^{2}+1$. The rational canonical form is therefore:
        \begin{equation}
            R_{J}=
            \begin{bmatrix}
                1&0&\dots&0&1\\
                0&1&\dots&0&1\\
                \vdots&\vdots&\ddots&\vdots&\vdots\\
                0&0&\dots&1&1\\
                0&0&\dots&0&1
            \end{bmatrix}
        \end{equation}
    \section{}
    \section{}
        Let $\textrm{Nil}(A)$ be the set of nilpotent element of $A$. This is an
        ideal. For if $r\in{A}$ and $n\in{N}$, then since $A$ is commutative we
        have that $(rn)^{k}=r^{k}n^{k}=r^{k}0=0$ for some $k\in\mathbb{N}$, and
        therefore $rn$ is nilpotent. Similarly, if $n,m\in\textrm{Nil}(A)$, then
        from the binomial theorem:
        \begin{equation}
            (n+m)^{k}=\sum_{i=1}^{k}\binom{k}{i}n^{k-i}y^{i}
        \end{equation}
        For large enough $k$ every term in this expansion is zero, and thus
        $n+m$ is nilpotent. If $\mathfrak{p}$ is a prime ideal, then
        $\textrm{Nil}(A)\subseteq\mathfrak{p}$. For if $n\in\textrm{Nil}(A)$,
        then $n^{k}=0$, which is contained in $\mathfrak{p}$, and therefore
        $n^{k-1}n=0$ so either $n\in\mathfrak{p}$ or $n^{k-1}\in\mathfrak{p}$.
        Continuing by induction we find that $n\in\mathfrak{p}$. Therefore
        $\textrm{Nil}(\mathfrak{p})=\textrm{Nil}(A)$ for any prime ideal.
        Therefore $A$ is reduced if and only if for every prime ideal, the
        only nilpotent element is zero.
        \par\hfill\par
        Let $R=\mathbb{Z}_{2}^{2}$. The prime ideals of this are the ideals
        generated by $(1,0)$ and $(0,1)$, both of which are domains
        (since they are essentially $\mathbb{Z}_{2}$). However
        $\mathbb{Z}_{2}^{2}$ is not a domain since $(1,0)\cdot(0,1)=(0,0)$,
        yet neither $(1,0)$ nor $(0,1)$ are the zero element.
        \section{}
        The characteristic polynomial is:
        \begin{equation}
            (\lambda-4)^{3}
        \end{equation}
        And from this we have eigenvalues $4$ with triple multiplicity. From
        this we compute the Jordan canonical form by:
        \begin{equation}
            J_{A}=
            \begin{bmatrix}
                4&1&0\\
                0&4&1\\
                0&0&4
            \end{bmatrix}
        \end{equation}
        For the rational canonical form we have again that the characteristic
        polynomial is $(\lambda-4)^{3}$. We have that $A-4I\ne{0}$ and
        $(A-4I)^{2}\ne{0}$, and lastly $(A-4I)^{3}=0$. So the minimal
        polynomial is simply:
        \begin{equation}
            (\lambda-4)^{3}=x^{3}-12x^{2}+48x-64
        \end{equation}
        Reading this off, we have that the rational canonical form is:
        \begin{equation}
            R_{A}=
            \begin{bmatrix*}[r]
                0&0&64\\
                1&0&\minus{48}\\
                0&1&12
            \end{bmatrix*}
        \end{equation}
        To find an invertible matrix $P$ such that $A=P^{\minus{1}}J_{A}P$ we
        set up the following augmented matrix:
        \begin{equation}
            \begin{bmatrix*}[r]
                3&1&0&\vline&4&1&0\\
                1&4&1&\vline&0&4&1\\
                3&\minus{2}&5&\vline&0&0&4
            \end{bmatrix*}
        \end{equation}
        Interchanging the two with elementary row operations gives us:
        \begin{equation}
            P^{\minus{1}}=
            \begin{bmatrix*}[r]
                0&0&1\\
                3&\minus{2}&1\\
                \minus{2}&1&\minus{1}
            \end{bmatrix*}
        \end{equation}
        This is indeed intervtible since it has non-zero determinant (and since
        it was obtain via elementary operations) and the inverse is:
        \begin{equation}
            P=
            \begin{bmatrix*}[r]
                \minus{1}&\minus{1}&\minus{2}\\
                \minus{1}&\minus{2}&\minus{3}\\
                1&0&0
            \end{bmatrix*}
        \end{equation}
\end{document}