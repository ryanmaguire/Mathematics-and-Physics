%------------------------------------------------------------------------------%
\documentclass[crop=false,class=article]{standalone}                           %
%----------------------------Preamble------------------------------------------%
\input{preamble.tex}                                                           %
%--------------------------Main Document---------------------------------------%
\begin{document}
    \title{Topics in Algebra}
    \author{}
    \date{\vspace{-5ex}}
    \maketitle
    \tableofcontents
    \clearpage
    \section{Preliminaries}
        Firstly, recall from group/set theory that a binary operation on a
        set $A$ is a function $*:A\times{A}\rightarrow{A}$. Rather than
        writing the image of $(a,b)\in{A}\times{A}$ as $*(a,b)$, we write
        $a*b$. Some common examples are addition $(+)$ and multiplication
        $(\cdot)$ of real or complex numbers.
        \begin{fdefinition}{Fields}{Fields}
            A field is a set $\mathbb{F}$ with two binary operations $+$ and
            $\cdot$, usually called addition and multiplication, such that
            the following are true:
            \begin{enumerate}
                \item $a+b=b+a$
                      \hfill[Commutativity of Addition]
                \item $a\cdot{b}=b\cdot{a}$
                      \hfill[Commutativity of Multiplication]
                \item $a+(b+c)=(a+b)+c$
                      \hfill[Associativity of Addition]
                \item $a\cdot(b\cdot{c})=(a\cdot{b})\cdot{c}$
                      \hfill[Associativity of Multiplication]
                \item There exists $0\in\mathbb{F}$ such that $0+a=a$
                      \hfill[Additive Identity]
                \item There exists $1\in\mathbb{F}$ such that $1\cdot{a}=1$
                      \hfill[Multiplicative Identity]
                \item For all $a$ there is a $b$ such that $a+b=0$
                      \hfill[Additive Inverses]
                \item For all $a\ne{0}$ there is a $b$ such that
                      $a\cdot{b}=1$
                      \hfill[Multiplicative Inverses]
                \item $a\cdot(b+c)=a\cdot{b}+a\cdot{c}$
                      \hfill[Distributive Law]
            \end{enumerate}
        \end{fdefinition}
        Given an element $a\in\mathbb{F}$, if $b$ is such that
        $a+b=0$ then we write $b=\minus{a}$. Subtraction of two elements
        $a$ and $c$, denoted $a-c$, is defined as $a+(\minus{c})$. The
        structure $(\mathbb{F},+)$ forms an Abelian group. From this we have
        that the identity is unique, as are additive inverses.
        It is common in the definition of a field to require that
        $0\ne{1}$. This is because if $0=1$ then we have $\mathbb{F}=\{0\}$.
        This comes from the following.
        \begin{ltheorem}{Multiplication by Zero}{Multiplication_by_Zero}
            If $(\mathbb{F},\,+,\,\cdot\,)$ is a field, and if $a\in\mathbb{F}$,
            then $a\cdot{0}=0$.
        \end{ltheorem}
        \begin{proof}
            For we have:
            \begin{equation}
                0=a\cdot(0)-a\cdot(0)=a\cdot(0-0)=a\cdot{0}
            \end{equation}
            This simply combines the distributive law with the additive
            property of zero, completing the proof.
        \end{proof}
        \begin{theorem}
            If $(\mathbb{F},\,+,\,\cdot\,)$ is a field, and if $0=1$, then
            $\mathbb{F}=\{0\}$.
        \end{theorem}
        \begin{proof}
            For suppose not, and let $a\in\mathbb{F}$ be such that $a\ne{0}$.
            But then, by Thm.~\ref{thm:Multiplication_by_Zero}:
            \begin{equation}
                a=a\cdot{1}=a\cdot{0}=0
            \end{equation}
            And thus $a=0$, a contradiction. Therefore,
            $\mathbb{F}$ is trivial.
        \end{proof}
        It is thus common to either call such a field a trivial field, or
        to require that $0\ne{1}$.
        \begin{lexample}{Examples of Fields}{Examples_of_Fields}
            There are several fields that should be familiar to the reader.
            If we let $\mathbb{R}$ denote the real numbers and $+$ and $\cdot$
            be the usual notations of addition and multiplication, then
            $(\mathbb{R},\,+,\,\cdot\,)$ is a field. Similarly, letting
            $\mathbb{Q}$ denote the rational numbers and $\mathbb{C}$ denote
            the complex numbers, $(\mathbb{Q},\,+,\,\cdot\,)$ is a field, as
            is $(\mathbb{C},\,+,\,\cdot\,)$. There are finite fields as well.
            Let $\mathbb{F}_{2}=\{0,\,1\}$ and define multiplication and
            addition as follows:
            \par\hfill\par
            \begin{table}[H]
                \centering
                \captionsetup{type=table}
                \parbox{.45\linewidth}{%
                    \centering
                    \begin{tabular}{c|cc}
                        $+$&0&1\\
                        \hline
                        0&0&1\\
                        1&1&0
                    \end{tabular}
                }
                \parbox{.45\linewidth}{%
                    \centering
                    \begin{tabular}{c|cc}
                        $\cdot$&0&1\\
                        \hline
                        0&0&0\\
                        1&0&1
                    \end{tabular}
                }
                \caption{The Arithmetic of $\mathbb{F}_{2}$}
            \end{table}
            $(\mathbb{F}_{2},\,+,\,\cdot)$ forms a field. Finally, if
            $p\in\mathbb{N}$ is prime, and if $+$ and $\cdot$ are addition
            and multiplication mod $p$, respectively, then
            $(\mathbb{Z}_{p},\,+,\,\cdot\,)$ is a field.
        \end{lexample}
        \begin{fdefinition}{Vector Spaces}{Vector_Spaces}
            A vector space over a field $(\mathbb{F},\,+,\,\cdot\,)$ is a
            set $V$ and a function
            $\boldsymbol{\cdot}:\mathbb{F}\times{V}\rightarrow{V}$ and
            a binary operation $\boldsymbol{+}$ on $V$, usuall called
            scalar multiplication and vector addition, respectively, 
            such that for all $\mathbf{x},\mathbf{y},\mathbf{z}\in{V}$,
            and all $a,b\in\mathbf{F}$, the following is true:
            \begin{enumerate}
                \item $\mathbf{x}\boldsymbol{+}%
                       (\mathbf{y}\boldsymbol{+}\mathbf{z})=%
                       (\mathbf{x}\boldsymbol{+}\mathbf{y})%
                       \boldsymbol{+}\mathbf{z}$
                      \hfill[Associative of Vector Addition]
                \item $\mathbf{x}\boldsymbol{+}\mathbf{y}=%
                       \mathbf{y}\boldsymbol{+}\mathbf{x}$
                      \hfill[Commutativity of Vector Addition]
                \item There is a $\mathbf{0}\in{V}$ such that
                      $\mathbf{0}\boldsymbol{+}\mathbf{x}=\mathbf{x}$
                      \hfill[Existence of Zero Vector]
                \item For all $\mathbf{x}$ there is a $\mathbf{y}$ such that
                      $\mathbf{x}\boldsymbol{+}\mathbf{y}=\mathbf{0}$
                      \hfill[Additive Inverses]
                \item $(a\cdot{b})\boldsymbol{\cdot}\mathbf{x}=%
                        a\boldsymbol{\cdot}(b\boldsymbol{\cdot}\mathbf{x})$
                      \hfill[Compatibility of Multiplication]
                \item $(a+b)\boldsymbol{\cdot}\mathbf{x}=%
                       (a\boldsymbol{\cdot}\mathbf{x})\boldsymbol{+}%
                       (b\boldsymbol{\cdot}\mathbf{x})$
                      \hfill[Distributive Law for Field Addition]
                \item $a\boldsymbol{\cdot}(\mathbf{x}\boldsymbol{+}\mathbf{y})=%
                       (a\boldsymbol{\cdot}\mathbf{x})\boldsymbol{+}%
                       (a\boldsymbol{\cdot}\mathbf{y})$
                      \hfill[Distributive Law for Vector Addition]
            \end{enumerate}
        \end{fdefinition}
        It is quite common not to distinguish between scalar multiplication
        $\boldsymbol{\cdot}$ and field multiplication $\cdot$, which may cause
        confusion. It is also common to drop the use of a symbol altogether and
        simply representation multiplication by concatenation of the the
        two variables, for example $a\mathbf{x}$ or $ab$, which represents
        scalar multiplication and field multiplication, respectively.
        \begin{lexample}{}{}
            If we let $\mathbb{F}=\mathbb{R}$ and let
            $V=\mathbb{R}^{n}$, where addition, multiplication, scalar
            multiplication, and vector addition are defined in their usual
            manner, then this forms a vector space. Similarly, the space
            $C([a,b])$ of continuous functions forms a vector space over
            $\mathbb{R}$, as does $L^{2}(\mathbb{R})$, the space of
            square integrable functions.
        \end{lexample}
        \begin{fdefinition}{Bilinear Operations}{Bilinear_Operations}
            A bilinear operation on a vector space
            $(V,\,\boldsymbol{+},\,\boldsymbol{\cdot}\,)$ over a field
            $(\mathbf{F},\,+,\,\cdot\,)$ is a function
            $[\,]:V\times{V}\rightarrow{V}$ such that, for all
            $\mathbf{x},\mathbf{y},\mathbf{z}\in{V}$, and for all
            $a,b\in\mathbf{F}$, the following is true:
            \begin{enumerate}
                \item $[\mathbf{x}\boldsymbol{+}\mathbf{y}, \mathbf{z}]=%
                       [\mathbf{x},\mathbf{z}]\boldsymbol{+}%
                       [\mathbf{y},\mathbf{z}]$
                      \hfill[Right Distributive Law]
                \item $[\mathbf{x},\mathbf{y}\boldsymbol{+}\mathbf{z}]=%
                       [\mathbf{x},\mathbf{y}]\boldsymbol{+}%
                       [\mathbf{x},\mathbf{z}]$
                      \hfill[Left Distributive Law]
                \item $[a\boldsymbol{\cdot}\mathbf{x},%
                        b\boldsymbol{\cdot}\mathbf{y}]=%
                       (a\cdot{b})\boldsymbol{\cdot}[\mathbf{x},\mathbf{y}]$
                      \hfill[Compatibility with Scalars]
            \end{enumerate}
        \end{fdefinition}
        \begin{lexample}{Examples of Bilinear Operations}
                        {Examples_of_Bilinear_Operation}
            The quintessential example of a bilinear operation is the
            cross product that one encounters in a multivariable calculus
            course. That is, for any three vectors
            $\mathbf{x},\mathbf{y},\mathbf{z}$, we have:
            \begin{equation}
                \mathbf{x}\times(\mathbf{y}+\mathbf{z})=
                \mathbf{x}\times\mathbf{y}+\mathbf{x}\times\mathbf{z}
            \end{equation}
            Similarly for right sided multiplication. The compatibility of
            the cross product with scalar multiplication is also true:
            \begin{equation}
                (a\mathbf{x})\times(b\mathbf{y})=ab(\mathbf{x}\times\mathbf{y})
            \end{equation}
            This serves somewhat as a motivating example for bilinear
            operations. If we think of the field of invertible matrices,
            then multiplication forms a bilinear operation as well, with
            scalar multiplication being the usual entry wise operation that
            is done on matrices. Lastly, if $\langle\,\rangle$ is an inner
            product on $\mathbb{R}$ or $\mathbb{C}$, then this is a bilinear
            operation, the vector space being the underlying field itself.
        \end{lexample}
        \begin{fdefinition}{Algebra over a Field}{Algebra_over_a_Field}
            An algebra of a field $(\mathbf{F},\,+,\,\cdot\,)$ is a
            vector space $(\mathbf{V},\,\boldsymbol{+},\,\boldsymbol{\cdot}\,)$
            and a bilinear operation $[\,]:V\times{V}\rightarrow{V}$.
        \end{fdefinition}
        \begin{fdefinition}{Associative Algebra over a Field}
                           {Associative_Algebra_over_a_Field}
            An associative algebra over a field $(\mathbb{F},\,+,\,\cdot\,)$
            is an algebra $(V,[\,])$ over $\mathbb{F}$ such that, for all
            $r\in\mathbb{F}$ and for all $\mathbf{x},\mathbf{y}\in{V}$,
            the following is true:
            \begin{equation}
                r[\mathbf{x},\,\mathbf{y}]=[r\mathbf{x},\,\mathbf{y}]
                                          =[\mathbf{x},\,r\mathbf{y}]
            \end{equation}
        \end{fdefinition}
        \begin{fdefinition}{Derivation on an Algebra}{Derivation_on_an_Algebra}
            A derivation on an algebra $(V,\,[\,])$ is a function
            $D:V\rightarrow{V}$ such that for all $\mathbf{x},\mathbf{y}\in{V}$,
            the following (Liebniz's Rule) is true:
            \begin{equation}
                D([\mathbf{x},\mathbf{y}])
                =[\mathbf{x},D(\mathbf{y})]+[D(\mathbf{x}),\mathbf{y}]
            \end{equation}
        \end{fdefinition}
    \section{Lie Algebras}
        For most purposes, $\mathbb{F}$ will be either the field $\mathbb{R}$
        or $\mathbb{C}$, where $+$ and $\cdot$ are the usual notions of
        addition and multiplication.
        \begin{fdefinition}{Lie Algebras}{Lie_Algebras}
            A Lie algebra over a field $(\mathbb{F},\,+,\,\cdot\,)$
            is vector space $(V,\,\boldsymbol{+},\,\boldsymbol{\cdot}\,)$
            over $\mathbb{F}$ and a bilinear map
            $[\,]:V\times{V}\rightarrow{V}$, denoted $(X,Y)\mapsto[X,Y]$,
            satisfying:
            \begin{enumerate}
                \item $[\mathbf{x},\mathbf{x}]=0$
                      \hfill[Alternating Property]
                \item $[\mathbf{x},[\mathbf{y},\mathbf{z}]]%
                       \boldsymbol{+}[\mathbf{y},[\mathbf{z},\mathbf{x}]]%
                       \boldsymbol{+}[\mathbf{z},[\mathbf{x},\mathbf{y}]]=0$
                      \hfill[Jacobi Identity]
            \end{enumerate}
        \end{fdefinition}
        \begin{theorem}
            \label{thm:Lie_Bracket_Anti_Commutes}%
            If $(\mathbb{F},\,+,\,\cdot\,)$ is a field, if $(V,\,[])$ is a
            Lie algebra over $\mathbb{F}$, and if
            $\mathbf{x},\mathbf{y}\in{V}$, then:
            \begin{equation}
                [\mathbf{x},\mathbf{y}]=\minus[\mathbf{x},\mathbf{y}]
            \end{equation}
        \end{theorem}
        \begin{proof}
            Applying bilinearity and the alternating property, we have:
            \begin{equation}
                \mathbf{0}=[\mathbf{x}\boldsymbol{+}\mathbf{y},\,
                            \mathbf{x}\boldsymbol{+}\mathbf{y}]
                          =[\mathbf{x},\,\mathbf{x}]\boldsymbol{+}
                           [\mathbf{x},\,\mathbf{y}]\boldsymbol{+}
                           [\mathbf{y},\,\mathbf{x}]\boldsymbol{+}
                           [\mathbf{y},\,\mathbf{y}]
            \end{equation}
            But from the alternating property,
            $[\mathbf{x},\mathbf{x}]=\mathbf{0}$ and
            $[\mathbf{y},\mathbf{y}]=\mathbf{0}$, and thus:
            \begin{equation}
                \mathbf{0}=[\mathbf{x},\,\mathbf{y}]\boldsymbol{+}
                           [\mathbf{y},\,\mathbf{x}]
            \end{equation}
            This completes the proof.
        \end{proof}
        \begin{theorem}
            \label{thm:alt_def_of_jacobi_identity}
            If $(\mathbb{F},\,+,\,\cdot\,)$ is a field, if $(V,\,[])$ is a
            Lie algebra over $\mathbb{F}$, and if $X,Y\in{V}$, then:
            \begin{equation}
                [X,[Y,Z]]=[[X,Y],Z]+[Y,[X,Z]]
            \end{equation}
        \end{theorem}
        \begin{proof}
            Applying the Jacobi identity and
            Thm.~\ref{thm:Lie_Bracket_Anti_Commutes} completes the proof.
        \end{proof}
        \begin{ltheorem}{Lie Brackets Form a Derivation}
                        {Lie_Brackets_Form_a_Derivation}
            If $(V,[\,])$ is a Lie algebra, if $\mathbf{x}\in{V}$, and if
            $D:V\rightarrow{V}$ is defined by:
            \begin{equation}
                D(\mathbf{y})=[\mathbf{x},\mathbf{y}]
            \end{equation}
            Then $D$ is a derivation on $V$.
        \end{ltheorem}
        \begin{proof}
            For by Thm.~\ref{thm:alt_def_of_jacobi_identity}, we have:
            \begin{equation}
                D([\mathbf{y},\mathbf{z}])=
                [\mathbf{x},[\mathbf{y},\mathbf{z}]]=
                [[\mathbf{x},\mathbf{y}],\mathbf{z}]\boldsymbol{+}
                [\mathbf{y},[\mathbf{x},\mathbf{z}]]=
                [D(\mathbf{y}),\mathbf{z}]\boldsymbol{+}
                [\mathbf{y},D(\mathbf{z})]
            \end{equation}
            And thus $D$ is a derivation.
        \end{proof}
        \begin{lexample}{Examples of Lie Algebras}
            Again, the most elementary example is the cross product on
            $\mathbb{R}^{3}$, with scalar multiplication and vector addition
            having their usual definitions. The cross product is
            anti-commutative:
            \begin{equation}
                \mathbf{x}\times\mathbf{y}=\minus\mathbf{y}\times\mathbf{x}
            \end{equation}
            And from this we obtain the alternating law. Similarly, it obeys
            the following identity:
            \begin{equation}
                \mathbf{x}\times(\mathbf{y}\times\mathbf{z})=
                (\mathbf{x}\times\mathbf{y})\times\mathbf{z}+
                \mathbf{y}\times(\mathbf{x}\times\mathbf{z})
            \end{equation}
            And from this the Jacobi identity is recovered.
            If $(V,\,\boldsymbol{+},\,\boldsymbol{\cdot}\,)$ is a vector space
            and if $[\,]:V\times{V}\rightarrow{V}$ is defined by
            $[\mathbf{x},\mathbf{y}]=\mathbf{0}$ for all
            $\mathbf{x},\mathbf{y}\in{V}$, then $(V,[\,])$ is a Lie algebra.
            \par\hfill\par
            Given a ring $R$, the Heisenberg group $H_{3}(R)$ is defined by
            considering matrices of the following form:
            \begin{equation}
                A=\begin{bmatrix}
                    1&a&b\\
                    0&1&c\\
                    0&0&1
                \end{bmatrix}
            \end{equation}
            Where $a,b,c\in{R}$. If we consider this over $\mathbb{R}$, then
            $A$ will be invertible for any such $a,b,c\in\mathbb{R}$ since
            $\textrm{det}(A)=1$, and thus the set of all such matrices forms
            a group under matrix multiplication. The Lie algebra associated
            with the Heisenberg group is the set of matrices of the form
            $A-I$, where $I$ is the $3\times{3}$ identity matrix. We form as
            a basis the following three matrices:
            \par\hfill\par
            \begin{subequations}
                \begin{minipage}[b]{0.32\textwidth}
                    \begin{equation}
                        X=\begin{bmatrix}
                            0&1&0\\
                            0&0&0\\
                            0&0&0
                        \end{bmatrix}
                    \end{equation}
                \end{minipage}
                \hfill
                \begin{minipage}[b]{0.32\textwidth}
                    \begin{equation}
                        Y=\begin{bmatrix}
                            0&0&1\\
                            0&0&0\\
                            0&0&0
                        \end{bmatrix}
                    \end{equation}
                \end{minipage}
                \hfill
                \begin{minipage}[b]{0.32\textwidth}
                    \begin{equation}
                        Z=\begin{bmatrix}
                            0&0&0\\
                            0&0&1\\
                            0&0&0
                        \end{bmatrix}
                    \end{equation}
                \end{minipage}
            \end{subequations}
            \par\vspace{2.5ex}
            We then define the Lie bracket from this basis:
            \par\hfill\par
            \begin{subequations}
                \begin{minipage}[b]{0.32\textwidth}
                    \begin{equation}
                        [X,Y]=Z
                    \end{equation}
                \end{minipage}
                \hfill
                \begin{minipage}[b]{0.32\textwidth}
                    \begin{equation}
                        [X,Z]=0
                    \end{equation}
                \end{minipage}
                \hfill
                \begin{minipage}[b]{0.32\textwidth}
                    \begin{equation}
                        [Y,Z]=0
                    \end{equation}
                \end{minipage}
            \end{subequations}
            \par\vspace{2.5ex}
            $(H_{3}(\mathbb{R}),[\,])$ then forms a Lie algebra.
        \end{lexample}
        \begin{fdefinition}{Lie Algebra Homomorphism}{Lie_Algebra_Homomorphis}
            A Lie algebra homomorphism from a Lie algebra $(X,[\,]_{X})$ to a
            Lie algebra $(Y,[\,]_{Y})$ is a linear function
            $f:X\rightarrow{Y}$ such that, for all
            $\mathbf{x}_{1},\mathbf{x}_{2}\in{X}$, the following is true:
            \begin{equation}
                f\big([\mathbf{x}_{1},\,\mathbf{x}_{2}]_{X}\big)
                =\big[f(\mathbf{x}_{1}),\,f(\mathbf{x}_{2})\big]_{Y}
            \end{equation}
        \end{fdefinition}
        \begin{fdefinition}{Lie Algebra Isomorphism}{Lie_Algebra_Isomorphism}
            A Lie algebra isomorphism between two Lie algebras $G$ and $H$
            is a homomorphism $f:G\rightarrow{H}$ such that $f$ is a bijection.
        \end{fdefinition}
        \begin{ltheorem}{Equivalent Definition of Isomorphism (Part I)}
                        {Equivalent_Definition_of_Isomorphism_Part_I}
            If $(X,[\,]_{X})$ and $(Y,[\,]_{Y}$ are Lie algebras, and if
            $f:X\rightarrow{Y}$ is a Lie algebra isomorphism, then $f$ is
            a Lie algebra homomorphism such that
            $f^{\minus{1}}:Y\rightarrow{X}$ is a Lie algebra homomorphism.
        \end{ltheorem}
        \begin{proof}
            For if $f:X\rightarrow{Y}$ is a Lie algebra isomorphism, then it
            is a homomorphism and a bijection
            (Def.~\ref{def:Lie_Algebra_Isomorphism}). But then the inverse
            function $f^{\minus{1}}:Y\rightarrow{X}$ is well defined. It thus
            suffices to show that this a homomorphism as well. Let
            $\mathbf{y}_{1},\mathbf{y}_{2}\in{Y}$. Then:
            \begin{equation}
                f\Big(\big[f^{\minus{1}}(\mathbf{y}_{1}),\,
                           f^{\minus{1}}(\mathbf{y}_{2})\big]_{Y}\Big)
                =\Big[f\big(f^{\minus{1}}(\mathbf{y}_{1})\big),\,
                      f\big(f^{\minus{1}}(\mathbf{y}_{2})\big)\Big]_{Y}
                =[\mathbf{y}_{1},\,\mathbf{y}_{2}]
            \end{equation}
            Since $f$ is a bijection, we may take the inverse of
            this to obtain:
            \begin{equation}
                [f^{\minus{1}}(\mathbf{y}_{1}),
                 f^{\minus{1}}(\mathbf{y}_{2})]_{X}=
                f^{\minus{1}}\Big([\mathbf{y}_{1},\mathbf{y}_{2}]_{Y}\Big)
            \end{equation}
            Thus completing the proof.
        \end{proof}
        \begin{ltheorem}{Equivalent Definition of Isomorphism (Part II)}
                        {Equivalent_Definition_of_Isomorphism_Part_II}
            If $(X,[\,]_{X})$ and $(Y,[\,]_{Y})$ are Lie algebras, and if
            $f:X\rightarrow{Y}$ is a Lie algebra homomorphism such that
            $f^{\minus{1}}:Y\rightarrow{X}$ exists and is a homomorphism,
            then $f$ is a Lie algebra homomorphism.
        \end{ltheorem}
        \begin{proof}
            For if $f:X\rightarrow{Y}$ is a Lie algebra homomorphism such that
            $f^{\minus{1}}:X\rightarrow{Y}$ exists and is a Lie algebra
            homomorphism, then $f$ is bijective. Therefore, $f$ is a
            Lie algebra isomorphism (Def.~\ref{def:Lie_Algebra_Isomorphism}).
        \end{proof}
        \begin{fdefinition}{Commutative Elements of a Lie Algebra}
                           {Commutative Elements of a Lie Algebra}
            Commutative elements of a Lie algebra $(X,[\,])$ are elements
            $\mathbf{x},\mathbf{y}$ such that:
            \begin{equation}
                [\mathbf{x},\,\mathbf{y}]=\mathbf{0}
            \end{equation}
        \end{fdefinition}
        \begin{fdefinition}{Abelian Lie Algebra}{Abelian Lie Algebra}
            An Abelian Lie Algebra is a Lie algebra $(X,[\,])$ such that for
            all $\mathbf{x},\mathbf{y}\in{X}$, $\mathbf{x}$ and $\mathbf{y}$
            are commutative elements.
        \end{fdefinition}
        It will be seen that these come from Abelian Lie groups.
        \begin{lexample}{}{}
            Let $(A,\cdot)$ be an associative $\mathbb{F}$ algebra. We can
            define a Lie algebra $\textrm{Lie}(A)$ as a vector space
            $A$ and by setting the Lie bracket to be the commutator:
            \begin{equation}
                [\mathbf{x},\mathbf{y}]
                =\mathbf{x}\cdot\mathbf{y}-\mathbf{y}\cdot\mathbf{x}
            \end{equation}
            Where $\cdot$ is the multiplicative operation that comes from the
            associative $\mathbb{F}$ algebra $(A,\,\cdot\,)$.
            The commutator is anticommutative:
            \begin{equation}
                [\mathbf{x},\mathbf{y}]
                =\mathbf{x}\cdot\mathbf{y}-\mathbf{y}\cdot\mathbf{x}
                =\minus(\mathbf{y}\cdot\mathbf{x}-\mathbf{x}\cdot\mathbf{y})
                =\minus[\mathbf{y},\mathbf{x}]
            \end{equation}
            And thus the alternating property is satisfied. Similarly, the
            Jacobi identity is valid and thus $\textrm{Lie}(A)$ is a Lie
            algebra. A special case of this is when we have
            $A=M_{n}(\mathbb{F})$. Then $\textrm{Lie}(M_{n}(\mathbb{F})$ is
            denoted by $\textrm{GL}_{n}(\mathbb{F})$.
        \end{lexample}
        We will use the following notation.
        \begin{fnotation}{Lie Bracket of Vector Subspaces}{}
            Given a Lie Algebra $(V,[\,])$ and two vector subspaces
            $W_{1},W_{2}\subseteq{V}$, we write $[W_{1},W_{2}]$ to
            denote the following:
            \begin{equation}
                [W_{1},W_{2}]=\textrm{Span}\big\{\,
                [\mathbf{w}_{1},\mathbf{w}_{2}]\,:\,
                    \mathbf{w_{1}}\in{W}_{1},\,\mathbf{w}_{2}\in{W}_{2}\,\big\}
            \end{equation}
        \end{fnotation}
        With this we can define Lie Subalgebras.
        \begin{fdefinition}{Lie Subalgebra}{Lie_Subalgebra}
            A Lie subalgebra of a Lie algebra $(V,[\,])$ is a subset
            $W\subseteq{V}$ such that:
            \begin{equation}
                [W,W]\subseteq{W}
            \end{equation}
        \end{fdefinition}
        That is to say, a Lie subalgebra of a Lie algebra is a subspace that is
        closed under the Lie bracket.
        \begin{fdefinition}{Ideal of a Subspace}{Ideal_of_a_Subspace}
            An ideal of a Lie algebra $(V,[\,])$ is a subset $W\subseteq{V}$
            such that:
            \begin{equation}
                [V,W]\subseteq{W}
            \end{equation}
        \end{fdefinition}
        By the anticommutativity of the Lie bracket, there is no distinction
        between left ideals and right ideals.
        \begin{theorem}
            If $(X,[\,]_{X})$ and $(Y,[\,])_{Y}$ are Lie algebras, and if
            $f:X\rightarrow{Y}$ is a Lie algebra homomorphism, then
            $\ker(f)$ is an ideal of $X$.
        \end{theorem}
        \begin{proof}
            For if $\mathbf{x}\in\ker(f)$ and $\mathbf{y}\in{X}$, then:
            \begin{equation}
                f([\mathbf{x},\mathbf{y}])
                =[f(\mathbf{x}),f(\mathbf{y})]
                =[\mathbf{0},\mathbf{y}]
                =\mathbf{0}
            \end{equation}
            And therefore $[\mathbf{x},\mathbf{y}]\in\ker(f)$. Thus,
            $ker(f)$ is an ideal of $X$.
        \end{proof}
        \begin{theorem}
            If $(V,[\,])$ is a Lie algebra, and if $W\subseteq{V}$ is an
            ideal of $V$, then $G/H$ has a Lie algebra structure such that the
            projection mapping $\pi:V\rightarrow{V}/W$, defined by
            $\mathbf{x}\mapsto\mathbf{x}+W$, is a Lie algebra homomorphism.
        \end{theorem}
        \begin{proof}
            For define $[\mathbf{x}+H,\mathbf{y}+H]$ as follows:
            \begin{equation}
                [\mathbf{x}+H,\mathbf{y}+H]=[\mathbf{x},\mathbf{y}]+H
            \end{equation}
            Then:
            \begin{equation}
                [\mathbf{x}+H,\mathbf{y}+H]
                =[\pi(\mathbf{x}),\pi(\mathbf{y})]
                =[\mathbf{x},\mathbf{y}]+H
                =\pi([\mathbf{x},\mathbf{y}])
            \end{equation}
            And therefore:
            \begin{equation}
                \big[\pi(\mathbf{x}),\pi(\mathbf{y})\big]
                =\pi\big([\mathbf{x},\mathbf{y}]\big)
            \end{equation}
            Let $\overline{X}=X+H$. If $\overline{X}'=\overline{X}$ and
            $\overline{Y}'=\overline{Y}$, then
            $\overline{[X,Y]}=\overline{[X',Y']}$. Thus:
            \begin{equation}
                X-X'=H_{1}\in{H}\quad\quad
                Y-Y'=H_{2}\in{H}
            \end{equation}
            And:
            \begin{equation}
                [X,Y]=[X'+H_{1},Y'+H_{2}]
                =[X',Y']+[H_{1},Y']+[X',H_{2}]+[H_{1},H_{2}]
            \end{equation}
            And this last sum of three is in $H$.
        \end{proof}
        \begin{theorem}
            If $(X,[\,])_{X}$ is a Lie algebra, and if $W\subseteq{X}$ is an
            ideal of $X$, then there is a Lie algebra $(Y,[\,]_{Y})$ and a
            homomorphism $f:X\rightarrow{Y}$ such that $W=\ker(f)$.
        \end{theorem}
        \begin{proof}
            For let $Y=X/W$ and let $[\,]_{Y}$ be the Lie bracket:
            \begin{equation}
                [\mathbf{x}+W,\mathbf{y}+W]_{Y}=[\mathbf{x},\mathbf{y}]+W
            \end{equation}
            Let $\pi:X\rightarrow{X/W}$ be the projection mapping
            $\pi(x)=x+W$. From the previous theorem,
            $(X/W,[\,]_{Y})$ is a Lie algebra and $\pi$ is a Lie algebra
            homomorphism. Moreover, $\ker(\pi)=W$. Therefore, etc.
        \end{proof}
        \begin{theorem}
            Let $f:G\rightarrow{H}$ be a map of Lie algebras, and let
            $K=\ker(f)$. Then there is a unique map
            $\overline{f}:G/K\rightarrow{H}$ such that $\overline{f}$ is
            injective and such that some commutative diagram thing.
        \end{theorem}
        \begin{example}
            \begin{enumerate}
                \item $0,G(TRIANGLE))G$
                \item $Z(G)=\{Z\in{G}:[X,Z]=0\}$ is called the center of $G$.
                \item $[G,G]=\textrm{Span}\{[X,Y]:X,Y\in{G}\}(TRIANGLE)G$
                \item Fuck it.
                \item For ideals $a,b(TRIANGLE)G$, $a+b(TRIANGLE)G$.
                \item Similarly for subtraction.
            \end{enumerate}
        \end{example}
        \begin{example}
            \begin{enumerate}
                \item V finite dimensional vector space over $\mathbb{F}$,
                      $A=\textrm{End}_{\mathbb{F}}(V)$ an associative
                      $\mathbb{F}$ algebra, then
                      $GL(V)=Lie(End_{\mathbb{F}}(V))$
                      with $[X,Y]=XY-YX$.
                \item $Tr:gl(V)\rightarrow\mathbb{F}$ abelian is a Lie algebra
                      homomorphism. $Tr([X,Y])=Tr(X,Y-YX)=0=[Tr(X),Tr(Y)]$.
                      $\ker(Tr)=SL(V)=\{X\in{GL}(V):Tr(X)=0\}$
            \end{enumerate}
        \end{example}
        If $V=\mathbb{F}^{n}$, denote $GL(V)$ by $GL_{n}(\mathbb{F})$ and $SL(V)$
        by $SL_{n}(\mathbb{F})$. Special cases: $SL_{2}(\mathbb{F})$. This has basis
        $(E^{ij})_{k\ell}=\delta^{i}_{k}\delta^{j}_{\ell}$.
        \begin{example}
            Compute $[X,Y]=[E^{12},E^{21}]=E^{11}-E^{22}=H$.
            And $[H,X]=2X$, $[H,Y]=\minus{2}Y$.
        \end{example}
        \begin{ldefinition}{Simple Lie Algebra}{Simple_Lie_Algebra}
            A Lie algebra is simple if it is non-abelian and its only ideals are
            $0$ and itself.
        \end{ldefinition}
        \begin{theorem}
            $SL_{2}(\mathbb{F})$ is simple for $\mathbb{F}=\mathbb{R}$ or
            $\mathbb{F}=\mathbb{C}$.
        \end{theorem}
        \begin{proof}
            Bracket notation says that $X$ is a 2-eigenvector of $[H,\cdot]$
            and $Y$ is a -2-eigenvector of $[H,\cdot]$. Let $A$ be a non-trivial
            ideal. We must show that $A=SP_{2}(\mathbb{F})$. Then
            $[X,U]\in{G}$. But:
            \begin{equation}
                [X,[X,U]]=[X,[X,xX+yY+hH]]
                =[X,gH-2hX]=\minus{2}yX
            \end{equation}
            So thus, either $X\in{A}$ or $Y=0$. Doing the same for
            $[Y,[Y,U]]$ shows that $\minus{2}xY=[Y,[Y,U]]$ and thus either
            $Y\in{A}$ or $X=0$. There are two cases now. If If $x=y=0$ then
            $h\ne{0}$ since $U$ is non-zero. This would imply $H\in{A}$. But
            $2X=[X,H]\in{A}$ so $X\in{A}$. Also, $Y\in{A}$.
            Similarly if $y\ne{0}$.
        \end{proof}
        This can be generalized for $SL_{n}(\mathbb{F})$ as well.
        \begin{ldefinition}{Normalizer of a Lie Algebra Subspace}
                           {Normalizer_of_Lie_Alg_Subspace}
            The Normalizer of $H\subset{G}$ of a Lie Algebra $G$ is the set:
            \begin{equation}
                N_{G}(H)=\{X\in{G}:[X,h]\in{H}\}
            \end{equation}
        \end{ldefinition}
        By the Jacobian identity, the normalizer of a subspace is also a
        subalgebra. Moreover, by def, $H$ is an ideal of its normalizer.
        \begin{ldefinition}{Centralizer of a Lie Algebra Subspace}
                           {Centralizer_of_a_Lie_Algebra_Subspace}
            The centralizer of $H\subset{G}$ is the set:
            \begin{equation}
                C_{G}(H)=\{X\in{G}:[X,H]=0\}
            \end{equation}
        \end{ldefinition}
        \begin{ldefinition}{Product Lie Algebras}{Product_Lie_Algebra}
            For Lie algebras $G_{1},G_{2}$, their product
            $G_{1}\times{G}_{2}$, also written $G_{1}\oplus{G}_{2}$, is the set
            $G_{1}\oplus{G}_{2}$ as a vector space with the bracket:
            \begin{equation}
                [(X_{1},X_{2}),(Y_{1},Y_{2})]=([X_{1},Y_{1}],[X_{2},Y_{2}]
            \end{equation}
        \end{ldefinition}
        $G_{1}\oplus{G}_{2}$ has ideals $\overline{G}_{1}=G_{1}\oplus{0}$ and
        $\overline{G}_{2}=0\oplus{G}_{2}$, and moreover
        $\overline{G}_{1}+\overline{G}_{2}=G_{1}\oplus{G}_{2}$
        \begin{theorem}
            If $G$ is a lie algebra, $a$, $b$ ideals of $G$ satisfying:
            \begin{enumerate}
                \item $a+b=G$
                \item $a\cap{b}=\emptyset$
                \item $[a,b]=0$
            \end{enumerate}
            Then $G$ is isomorphic to $a\oplus{b}$.
        \end{theorem}
        \begin{proof}
            Define $\varphi:a\oplus{b}\rightarrow{G}$ by
            $\varphi(A,B)=A+B$. By vector space theory, this is a vector
            space isomorphism. We need to check that it preserves brackets.
            But:
            \begin{equation}
                \varphi([(A,B),(A',B')])=\varphi([A,A'],[B,B'])
                =[A,A']+[B,B']
            \end{equation}
            But also:
            \begin{equation}
                [\varphi(A,B),\varphi(A',B')]=[A+B,A'+B']
                =[A,A']+[B,A']+[A,B']+[B,B']
            \end{equation}
            But $[B,A']=[A,B']=0$, completing the proof.
        \end{proof}
    \section{Review of Differentiation}
        If $\mathcal{U}\subseteq\mathbb{R}^{n}$ is open, and if
        $f:\mathcal{U}\rightarrow\mathbb{R}^{n}$ is a function, then
        $Df(p)$ (if it is exists) is a linear map
        $Df(p):\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}$ such that:
        \begin{equation}
            \underset{h\rightarrow{0}}{\lim}
                \frac{f(p+h)-(f(p)+Df(p)(h)}{\norm{h}}=0
        \end{equation}
        That is, $Df(p)$ is the best linear affine approximation to $f$ at
        $p$.
        \begin{theorem}
            If $f\in{C}^{1}$, then $Df(p)$ exists and:
            \begin{equation}
                Df(p)(v)=\underset{t\rightarrow{0}}{\lim}
                \frac{f(p+tr)-f(p)}{t}
            \end{equation}
        \end{theorem}
        Chain rule, if $p\in\mathcal{U}$,
        $f:\mathcal{U}\rightarrow\mathbb{R}$,
        then $D(g\circ{f})(o)=Dg(f(p))Df(p)$
        \begin{theorem}
            If $V_{1}$, $V_{2}$, $W$ are $\mathbb{R}$ vector spaces, and
            if $B:V_{1}\times{V}_{2}\rightarrow{W}$ is bilinear, then
            for $p_{1},v_{1}\in{V}_{1}$, $p_{2},v_{2}\in{V}_{2}$:
            \begin{equation}
                DB(p_{1},p_{2})(v_{1},v_{2})=B(p_{1},v_{2})+B(v_{1},p_{2})
            \end{equation}
        \end{theorem}
        This can be generalized to multilinear maps.
        \begin{theorem}
            if $B:V_{1}\times\cdots\times{V}_{n}\rightarrow{W}$ is
            multilinear, then:
            \begin{equation}
                DB(p_{1},\dots,p_{n})(v_{1},\dots,v_{n})=
                \sum_{j=1}^{n}B(p_{1},\dots,p_{j-1},v_{j},p_{j+1},\dots,p_{n})
            \end{equation}
        \end{theorem}
        \begin{example}
            Let $det:M_{n}(\mathbb{R})\rightarrow\mathbb{R}$ be the
            determinant function. Then $D(det)(x)(H)=Tr(X^{Thing}H)$,
            where $X^{Thing}$ is the classical adjugate matrix:
            \begin{equation}
                (X^{Thing})_{ij}=(\minus{1})^{i+j}det(M_{ij}(X)
            \end{equation}
            Where $M_{ij}(X)$ is the minor of $X$ formed by crossing out the
            $i^{th}$ row and $j^{th}$ column.
        \end{example}
        \begin{example}
            Let $F_{k}:M_{n}(\mathbb{R})\rightarrow{M}_{n}(\mathbb{R})$ be
            defined by $F_{k}(X)=X^{k}$. Then:
            \begin{equation}
                DF_{k}(X)(H)=X^{k-1}H+x^{k-2}HX+\cdots+XHX^{k-1}+HX^{k-1}
            \end{equation}
            If $X$ and $H$ commute, then:
            \begin{equation}
                DF_{k}(X)(H)=kX^{k-1}H
            \end{equation}
        \end{example}
        \begin{theorem}
            If $V:GL_{n}(\mathbb{R})\rightarrow{M}_{n}(\mathbb{R})$ is
            defined by $V(X)=X^{\minus{1}}$, then:
            \begin{equation}
                DV(X)(H)=\minus{X}^{\minus{1}}HX^{\minus{1}}
            \end{equation}
        \end{theorem}
    \section{Lie Groups}
        Let $\mathbb{F}$ denote either $\mathbb{R}$ or $\mathbb{C}$.
        \begin{ldefinition}{Real Lie Groups}{Real_Lie_Group}
            A real Lie group is a $C^{\infty}$ manifold $C$ with a group
            structure such that the operation is smooth. That is,
            $m:G\times{G}\rightarrow{G}$ which maps $(x,y)\mapsto{x}*y$ is
            smooth, and $v:G\rightarrow{G}$ which maps
            $x\mapsto{x}^{\minus{1}}$ is also smooth.
        \end{ldefinition}
        \begin{ldefinition}{Lie Group Homomorphism}{}
            If $G$ and $H$ are Lie groups, a Lie group homomorphism is a
            smooth group homomorphism $f:G\rightarrow{H}$.
        \end{ldefinition}
        \begin{example}
            A Complex Lie group is a complex manifold with a group structure
            whose operations are holomorphic. Let $\mathbb{R}$ or
            $\mathbb{F}$ be a finite dimensional $\mathbb{F}$ vector space.
            Then $G:(V)\subseteq\textrm{End}(V)$ is an open subset of
            the vector space $\textrm{End}_{\mathbb{F}}(V)$. So this is
            a Lie group (real if $\mathbb{F}=\mathbb{R}$, complex if
            $\mathbb{F}=\mathbb{C}$) and
            $GL_{n}(\mathbb{F})=GL(\mathbb{F}^{n})$. As another example,
            $SL_{n}(\mathbb{F})=\{g\in{GL}_{n}(\mathbb{F}):det(g)=1\}$.
            We now how to show that this is a Lie group. We have that
            $SL_{n}(\mathbb{F})=det^{\minus{1}}(1)$. By the implicit
            function theorem, $det^{\minus{1}}(1)$ is a smooth
            manifold provided that
            $D(det)(X):M_{n}(\mathbb{F})\rightarrow\mathbb{F}$ is a
            surjective map for any $X\in{det}^{\minus{1}}(1)$. But
            $D(det)(X)H)=Tr(X^{Thing}H)$, so:
            \begin{equation}
                D(det)(X)(X)=Tr(X^{Thing}X)=Tr(det(X)I)=n\ne{0}
            \end{equation}
        \end{example}
        \begin{example}
            Let $T_{n}(\mathbb{F})$ be the subgroup of $GL_{n}(\mathbb{F})$
            stabilizing the standard flag:
            \begin{equation}
                0=V_{0}\subseteq{V}_{1}
                \subseteq\dots\subseteq{V}_{n}=\mathbb{F}^{n}
            \end{equation}
            This is often called the Borel subgroup. Let $V_{j}$ be
            the span of $\{e_{1},\dots,e_{j}\}$. There is a special case when
            $\mathbb{F}=\mathbb{R}$ and $n=3$. This is called the
            Heisenberg group, and is the set of all matrices of the following:
            \begin{equation}
                A=\begin{bmatrix}
                    1&x&y\\
                    0&1&z\\
                    0&0&1
                \end{bmatrix}
            \end{equation}
            Topologically this is homeomorphic to $\mathbb{R}^{3}$.
        \end{example}
        Let $<>$ be a bilinear form on $\mathbb{F}$. Pick a basis $B$ of $V$.
        Define $G$ by $G_{ij}=<v_{i},v_{j}>$.
\end{document}