%------------------------------------------------------------------------------%
\documentclass[crop=false,class=article]{standalone}                           %
%----------------------------Preamble------------------------------------------%
\input{../../../preamble.tex}                                                  %
%--------------------------Main Document---------------------------------------%
\begin{document}
    \title{Topics in Algebra}
    \author{}
    \date{\vspace{-5ex}}
    \maketitle
    \section{Preliminaries}
        Firstly, recall from group/set theory that a binary operation on a
        set $A$ is a function $*:A\times{A}\rightarrow{A}$. Rather than
        writing the image of $(a,b)\in{A}\times{A}$ as $*(a,b)$, we write
        $a*b$. Some common examples are addition $(+)$ and multiplication
        $(\cdot)$ of real or complex numbers.
        \begin{fdefinition}{Fields}{Fields}
            A field is a set $\mathbb{F}$ with two binary operations $+$ and
            $\cdot$, usually called addition and multiplication, such that
            the following are true:
            \begin{enumerate}
                \item $a+b=b+a$
                      \hfill[Commutativity of Addition]
                \item $a\cdot{b}=b\cdot{a}$
                      \hfill[Commutativity of Multiplication]
                \item $a+(b+c)=(a+b)+c$
                      \hfill[Associativity of Addition]
                \item $a\cdot(b\cdot{c})=(a\cdot{b})\cdot{c}$
                      \hfill[Associativity of Multiplication]
                \item There exists $0\in\mathbb{F}$ such that $0+a=a$
                      \hfill[Additive Identity]
                \item There exists $1\in\mathbb{F}$ such that $1\cdot{a}=1$
                      \hfill[Multiplicative Identity]
                \item For all $a$ there is a $b$ such that $a+b=0$
                      \hfill[Additive Inverses]
                \item For all $a\ne{0}$ there is a $b$ such that
                      $a\cdot{b}=1$
                      \hfill[Multiplicative Inverses]
                \item $a\cdot(b+c)=a\cdot{b}+a\cdot{c}$
                      \hfill[Distributive Law]
            \end{enumerate}
        \end{fdefinition}
        Given an element $a\in\mathbb{F}$, if $b$ is such that
        $a+b=0$ then we write $b=\minus{a}$. Subtraction of two elements
        $a$ and $c$, denoted $a-c$, is defined as $a+(\minus{c})$. The
        structure $(\mathbb{F},+)$ forms an Abelian group. From this we have
        that the identity is unique, as are additive inverses.
        It is common in the definition of a field to require that
        $0\ne{1}$. This is because if $0=1$ then we have $\mathbb{F}=\{0\}$.
        This comes from the following.
        \begin{ltheorem}{Multiplication by Zero}{Multiplication_by_Zero}
            If $(\mathbb{F},\,+,\,\cdot\,)$ is a field, and if $a\in\mathbb{F}$,
            then $a\cdot{0}=0$.
        \end{ltheorem}
        \begin{proof}
            For we have:
            \begin{equation}
                0=a\cdot(0)-a\cdot(0)=a\cdot(0-0)=a\cdot{0}
            \end{equation}
            This simply combines the distributive law with the additive
            property of zero, completing the proof.
        \end{proof}
        \begin{theorem}
            If $(\mathbb{F},\,+,\,\cdot\,)$ is a field, and if $0=1$, then
            $\mathbb{F}=\{0\}$.
        \end{theorem}
        \begin{proof}
            For suppose not, and let $a\in\mathbb{F}$ be such that $a\ne{0}$.
            But then, by Thm.~\ref{thm:Multiplication_by_Zero}:
            \begin{equation}
                a=a\cdot{1}=a\cdot{0}=0
            \end{equation}
            And thus $a=0$, a contradiction. Therefore,
            $\mathbb{F}$ is trivial.
        \end{proof}
        It is thus common to either call such a field a trivial field, or
        to require that $0\ne{1}$.
        \begin{lexample}{Examples of Fields}{Examples_of_Fields}
            There are several fields that should be familiar to the reader.
            If we let $\mathbb{R}$ denote the real numbers and $+$ and $\cdot$
            be the usual notations of addition and multiplication, then
            $(\mathbb{R},\,+,\,\cdot\,)$ is a field. Similarly, letting
            $\mathbb{Q}$ denote the rational numbers and $\mathbb{C}$ denote
            the complex numbers, $(\mathbb{Q},\,+,\,\cdot\,)$ is a field, as
            is $(\mathbb{C},\,+,\,\cdot\,)$. There are finite fields as well.
            Let $\mathbb{F}_{2}=\{0,\,1\}$ and define multiplication and
            addition as follows:
            \par\hfill\par
            \begin{table}[H]
                \centering
                \captionsetup{type=table}
                \parbox{.45\linewidth}{%
                    \centering
                    \begin{tabular}{c|cc}
                        $+$&0&1\\
                        \hline
                        0&0&1\\
                        1&1&0
                    \end{tabular}
                }
                \parbox{.45\linewidth}{%
                    \centering
                    \begin{tabular}{c|cc}
                        $\cdot$&0&1\\
                        \hline
                        0&0&0\\
                        1&0&1
                    \end{tabular}
                }
                \caption{The Arithmetic of $\mathbb{F}_{2}$}
            \end{table}
            $(\mathbb{F}_{2},\,+,\,\cdot)$ forms a field. Finally, if
            $p\in\mathbb{N}$ is prime, and if $+$ and $\cdot$ are addition
            and multiplication mod $p$, respectively, then
            $(\mathbb{Z}_{p},\,+,\,\cdot\,)$ is a field.
        \end{lexample}
        \begin{fdefinition}{Vector Spaces}{Vector_Spaces}
            A vector space over a field $(\mathbb{F},\,+,\,\cdot\,)$ is a
            set $V$ and a function
            $\boldsymbol{\cdot}:\mathbb{F}\times{V}\rightarrow{V}$ and
            a binary operation $\boldsymbol{+}$ on $V$, usuall called
            scalar multiplication and vector addition, respectively, 
            such that for all $\mathbf{x},\mathbf{y},\mathbf{z}\in{V}$,
            and all $a,b\in\mathbf{F}$, the following is true:
            \begin{enumerate}
                \item $\mathbf{x}\boldsymbol{+}%
                       (\mathbf{y}\boldsymbol{+}\mathbf{z})=%
                       (\mathbf{x}\boldsymbol{+}\mathbf{y})%
                       \boldsymbol{+}\mathbf{z}$
                      \hfill[Associative of Vector Addition]
                \item $\mathbf{x}\boldsymbol{+}\mathbf{y}=%
                       \mathbf{y}\boldsymbol{+}\mathbf{x}$
                      \hfill[Commutativity of Vector Addition]
                \item There is a $\mathbf{0}\in{V}$ such that
                      $\mathbf{0}\boldsymbol{+}\mathbf{x}=\mathbf{x}$
                      \hfill[Existence of Zero Vector]
                \item For all $\mathbf{x}$ there is a $\mathbf{y}$ such that
                      $\mathbf{x}\boldsymbol{+}\mathbf{y}=\mathbf{0}$
                      \hfill[Additive Inverses]
                \item $(a\cdot{b})\boldsymbol{\cdot}\mathbf{x}=%
                        a\boldsymbol{\cdot}(b\boldsymbol{\cdot}\mathbf{x})$
                      \hfill[Compatibility of Multiplication]
                \item $(a+b)\boldsymbol{\cdot}\mathbf{x}=%
                       (a\boldsymbol{\cdot}\mathbf{x})\boldsymbol{+}%
                       (b\boldsymbol{\cdot}\mathbf{x})$
                      \hfill[Distributive Law for Field Addition]
                \item $a\boldsymbol{\cdot}(\mathbf{x}\boldsymbol{+}\mathbf{y})=%
                       (a\boldsymbol{\cdot}\mathbf{x})\boldsymbol{+}%
                       (a\boldsymbol{\cdot}\mathbf{y})$
                      \hfill[Distributive Law for Vector Addition]
            \end{enumerate}
        \end{fdefinition}
        It is quite common not to distinguish between scalar multiplication
        $\boldsymbol{\cdot}$ and field multiplication $\cdot$, which may cause
        confusion. It is also common to drop the use of a symbol altogether and
        simply representation multiplication by concatenation of the the
        two variables, for example $a\mathbf{x}$ or $ab$, which represents
        scalar multiplication and field multiplication, respectively.
        \begin{lexample}{}{}
            If we let $\mathbb{F}=\mathbb{R}$ and let
            $V=\mathbb{R}^{n}$, where addition, multiplication, scalar
            multiplication, and vector addition are defined in their usual
            manner, then this forms a vector space. Similarly, the space
            $C([a,b])$ of continuous functions forms a vector space over
            $\mathbb{R}$, as does $L^{2}(\mathbb{R})$, the space of
            square integrable functions.
        \end{lexample}
        \begin{fdefinition}{Bilinear Operations}{Bilinear_Operations}
            A bilinear operation on a vector space
            $(V,\,\boldsymbol{+},\,\boldsymbol{\cdot}\,)$ over a field
            $(\mathbf{F},\,+,\,\cdot\,)$ is a function
            $[\,]:V\times{V}\rightarrow{V}$ such that, for all
            $\mathbf{x},\mathbf{y},\mathbf{z}\in{V}$, and for all
            $a,b\in\mathbf{F}$, the following is true:
            \begin{enumerate}
                \item $[\mathbf{x}\boldsymbol{+}\mathbf{y}, \mathbf{z}]=%
                       [\mathbf{x},\mathbf{z}]\boldsymbol{+}%
                       [\mathbf{y},\mathbf{z}]$
                      \hfill[Right Distributive Law]
                \item $[\mathbf{x},\mathbf{y}\boldsymbol{+}\mathbf{z}]=%
                       [\mathbf{x},\mathbf{y}]\boldsymbol{+}%
                       [\mathbf{x},\mathbf{z}]$
                      \hfill[Left Distributive Law]
                \item $[a\boldsymbol{\cdot}\mathbf{x},%
                        b\boldsymbol{\cdot}\mathbf{y}]=%
                       (a\cdot{b})\boldsymbol{\cdot}[\mathbf{x},\mathbf{y}]$
                      \hfill[Compatibility with Scalars]
            \end{enumerate}
        \end{fdefinition}
        \begin{lexample}{Examples of Bilinear Operations}
                        {Examples_of_Bilinear_Operation}
            The quintessential example of a bilinear operation is the
            cross product that one encounters in a multivariable calculus
            course. That is, for any three vectors
            $\mathbf{x},\mathbf{y},\mathbf{z}$, we have:
            \begin{equation}
                \mathbf{x}\times(\mathbf{y}+\mathbf{z})=
                \mathbf{x}\times\mathbf{y}+\mathbf{x}\times\mathbf{z}
            \end{equation}
            Similarly for right sided multiplication. The compatibility of
            the cross product with scalar multiplication is also true:
            \begin{equation}
                (a\mathbf{x})\times(b\mathbf{y})=ab(\mathbf{x}\times\mathbf{y})
            \end{equation}
            This serves somewhat as a motivating example for bilinear
            operations. If we think of the field of invertible matrices,
            then multiplication forms a bilinear operation as well, with
            scalar multiplication being the usual entry wise operation that
            is done on matrices. Lastly, if $\langle\,\rangle$ is an inner
            product on $\mathbb{R}$ or $\mathbb{C}$, then this is a bilinear
            operation, the vector space being the underlying field itself.
        \end{lexample}
        \begin{fdefinition}{Algebra over a Field}{Algebra_over_a_Field}
            An algebra of a field $(\mathbf{F},\,+,\,\cdot\,)$ is a
            vector space $(\mathbf{V},\,\boldsymbol{+},\,\boldsymbol{\cdot}\,)$
            and a bilinear operation $[\,]:V\times{V}\rightarrow{V}$.
        \end{fdefinition}
        \begin{fdefinition}{Associative Algebra over a Field}
                           {Associative_Algebra_over_a_Field}
            An associative algebra over a field $(\mathbb{F},\,+,\,\cdot\,)$
            is an algebra $(V,[\,])$ over $\mathbb{F}$ such that, for all
            $r\in\mathbb{F}$ and for all $\mathbf{x},\mathbf{y}\in{V}$,
            the following is true:
            \begin{equation}
                r[\mathbf{x},\,\mathbf{y}]=[r\mathbf{x},\,\mathbf{y}]
                                          =[\mathbf{x},\,r\mathbf{y}]
            \end{equation}
        \end{fdefinition}
        \begin{fdefinition}{Derivation on an Algebra}{Derivation_on_an_Algebra}
            A derivation on an algebra $(V,\,[\,])$ is a function
            $D:V\rightarrow{V}$ such that for all $\mathbf{x},\mathbf{y}\in{V}$,
            the following (Liebniz's Rule) is true:
            \begin{equation}
                D([\mathbf{x},\mathbf{y}])
                =[\mathbf{x},D(\mathbf{y})]+[D(\mathbf{x}),\mathbf{y}]
            \end{equation}
        \end{fdefinition}
    \section{Lie Algebras}
        For most purposes, $\mathbb{F}$ will be either the field $\mathbb{R}$
        or $\mathbb{C}$, where $+$ and $\cdot$ are the usual notions of
        addition and multiplication.
        \begin{fdefinition}{Lie Algebras}{Lie_Algebras}
            A Lie algebra over a field $(\mathbb{F},\,+,\,\cdot\,)$
            is vector space $(V,\,\boldsymbol{+},\,\boldsymbol{\cdot}\,)$
            over $\mathbb{F}$ and a bilinear map
            $[\,]:V\times{V}\rightarrow{V}$, denoted $(X,Y)\mapsto[X,Y]$,
            satisfying:
            \begin{enumerate}
                \item $[\mathbf{x},\mathbf{x}]=0$
                      \hfill[Alternating Property]
                \item $[\mathbf{x},[\mathbf{y},\mathbf{z}]]%
                       \boldsymbol{+}[\mathbf{y},[\mathbf{z},\mathbf{x}]]%
                       \boldsymbol{+}[\mathbf{z},[\mathbf{x},\mathbf{y}]]=0$
                      \hfill[Jacobi Identity]
            \end{enumerate}
        \end{fdefinition}
        \begin{theorem}
            \label{thm:Lie_Bracket_Anti_Commutes}%
            If $(\mathbb{F},\,+,\,\cdot\,)$ is a field, if $(V,\,[])$ is a
            Lie algebra over $\mathbb{F}$, and if
            $\mathbf{x},\mathbf{y}\in{V}$, then:
            \begin{equation}
                [\mathbf{x},\mathbf{y}]=\minus[\mathbf{x},\mathbf{y}]
            \end{equation}
        \end{theorem}
        \begin{proof}
            Applying bilinearity and the alternating property, we have:
            \begin{equation}
                \mathbf{0}=[\mathbf{x}\boldsymbol{+}\mathbf{y},\,
                            \mathbf{x}\boldsymbol{+}\mathbf{y}]
                          =[\mathbf{x},\,\mathbf{x}]\boldsymbol{+}
                           [\mathbf{x},\,\mathbf{y}]\boldsymbol{+}
                           [\mathbf{y},\,\mathbf{x}]\boldsymbol{+}
                           [\mathbf{y},\,\mathbf{y}]
            \end{equation}
            But from the alternating property,
            $[\mathbf{x},\mathbf{x}]=\mathbf{0}$ and
            $[\mathbf{y},\mathbf{y}]=\mathbf{0}$, and thus:
            \begin{equation}
                \mathbf{0}=[\mathbf{x},\,\mathbf{y}]\boldsymbol{+}
                           [\mathbf{y},\,\mathbf{x}]
            \end{equation}
            This completes the proof.
        \end{proof}
        \begin{theorem}
            \label{thm:alt_def_of_jacobi_identity}
            If $(\mathbb{F},\,+,\,\cdot\,)$ is a field, if $(V,\,[])$ is a
            Lie algebra over $\mathbb{F}$, and if $X,Y\in{V}$, then:
            \begin{equation}
                [X,[Y,Z]]=[[X,Y],Z]+[Y,[X,Z]]
            \end{equation}
        \end{theorem}
        \begin{proof}
            Applying the Jacobi identity and
            Thm.~\ref{thm:Lie_Bracket_Anti_Commutes} completes the proof.
        \end{proof}
        \begin{ltheorem}{Lie Brackets Form a Derivation}
                        {Lie_Brackets_Form_a_Derivation}
            If $(V,[\,])$ is a Lie algebra, if $\mathbf{x}\in{V}$, and if
            $D:V\rightarrow{V}$ is defined by:
            \begin{equation}
                D(\mathbf{y})=[\mathbf{x},\mathbf{y}]
            \end{equation}
            Then $D$ is a derivation on $V$.
        \end{ltheorem}
        \begin{proof}
            For by Thm.~\ref{thm:alt_def_of_jacobi_identity}, we have:
            \begin{equation}
                D([\mathbf{y},\mathbf{z}])=
                [\mathbf{x},[\mathbf{y},\mathbf{z}]]=
                [[\mathbf{x},\mathbf{y}],\mathbf{z}]\boldsymbol{+}
                [\mathbf{y},[\mathbf{x},\mathbf{z}]]=
                [D(\mathbf{y}),\mathbf{z}]\boldsymbol{+}
                [\mathbf{y},D(\mathbf{z})]
            \end{equation}
            And thus $D$ is a derivation.
        \end{proof}
        \begin{lexample}{Examples of Lie Algebras}
            Again, the most elementary example is the cross product on
            $\mathbb{R}^{3}$, with scalar multiplication and vector addition
            having their usual definitions. The cross product is
            anti-commutative:
            \begin{equation}
                \mathbf{x}\times\mathbf{y}=\minus\mathbf{y}\times\mathbf{x}
            \end{equation}
            And from this we obtain the alternating law. Similarly, it obeys
            the following identity:
            \begin{equation}
                \mathbf{x}\times(\mathbf{y}\times\mathbf{z})=
                (\mathbf{x}\times\mathbf{y})\times\mathbf{z}+
                \mathbf{y}\times(\mathbf{x}\times\mathbf{z})
            \end{equation}
            And from this the Jacobi identity is recovered.
            If $(V,\,\boldsymbol{+},\,\boldsymbol{\cdot}\,)$ is a vector space
            and if $[\,]:V\times{V}\rightarrow{V}$ is defined by
            $[\mathbf{x},\mathbf{y}]=\mathbf{0}$ for all
            $\mathbf{x},\mathbf{y}\in{V}$, then $(V,[\,])$ is a Lie algebra.
            \par\hfill\par
            Given a ring $R$, the Heisenberg group $H_{3}(R)$ is defined by
            considering matrices of the following form:
            \begin{equation}
                A=\begin{bmatrix}
                    1&a&b\\
                    0&1&c\\
                    0&0&1
                \end{bmatrix}
            \end{equation}
            Where $a,b,c\in{R}$. If we consider this over $\mathbb{R}$, then
            $A$ will be invertible for any such $a,b,c\in\mathbb{R}$ since
            $\textrm{det}(A)=1$, and thus the set of all such matrices forms
            a group under matrix multiplication. The Lie algebra associated
            with the Heisenberg group is the set of matrices of the form
            $A-I$, where $I$ is the $3\times{3}$ identity matrix. We form as
            a basis the following three matrices:
            \par\hfill\par
            \begin{subequations}
                \begin{minipage}[b]{0.32\textwidth}
                    \begin{equation}
                        X=\begin{bmatrix}
                            0&1&0\\
                            0&0&0\\
                            0&0&0
                        \end{bmatrix}
                    \end{equation}
                \end{minipage}
                \hfill
                \begin{minipage}[b]{0.32\textwidth}
                    \begin{equation}
                        Y=\begin{bmatrix}
                            0&0&1\\
                            0&0&0\\
                            0&0&0
                        \end{bmatrix}
                    \end{equation}
                \end{minipage}
                \hfill
                \begin{minipage}[b]{0.32\textwidth}
                    \begin{equation}
                        Z=\begin{bmatrix}
                            0&0&0\\
                            0&0&1\\
                            0&0&0
                        \end{bmatrix}
                    \end{equation}
                \end{minipage}
            \end{subequations}
            \par\vspace{2.5ex}
            We then define the Lie bracket from this basis:
            \par\hfill\par
            \begin{subequations}
                \begin{minipage}[b]{0.32\textwidth}
                    \begin{equation}
                        [X,Y]=Z
                    \end{equation}
                \end{minipage}
                \hfill
                \begin{minipage}[b]{0.32\textwidth}
                    \begin{equation}
                        [X,Z]=0
                    \end{equation}
                \end{minipage}
                \hfill
                \begin{minipage}[b]{0.32\textwidth}
                    \begin{equation}
                        [Y,Z]=0
                    \end{equation}
                \end{minipage}
            \end{subequations}
            \par\vspace{2.5ex}
            $(H_{3}(\mathbb{R}),[\,])$ then forms a Lie algebra.
        \end{lexample}
        \begin{fdefinition}{Lie Algebra Homomorphism}{Lie_Algebra_Homomorphis}
            A Lie algebra homomorphism from a Lie algebra $(X,[\,]_{X})$ to a
            Lie algebra $(Y,[\,]_{Y})$ is a linear function
            $f:X\rightarrow{Y}$ such that, for all
            $\mathbf{x}_{1},\mathbf{x}_{2}\in{X}$, the following is true:
            \begin{equation}
                f\big([\mathbf{x}_{1},\,\mathbf{x}_{2}]_{X}\big)
                =\big[f(\mathbf{x}_{1}),\,f(\mathbf{x}_{2})\big]_{Y}
            \end{equation}
        \end{fdefinition}
        \begin{fdefinition}{Lie Algebra Isomorphism}{Lie_Algebra_Isomorphism}
            A Lie algebra isomorphism between two Lie algebras $G$ and $H$
            is a homomorphism $f:G\rightarrow{H}$ such that $f$ is a bijection.
        \end{fdefinition}
        \begin{ltheorem}{Equivalent Definition of Isomorphism (Part I)}
                        {Equivalent_Definition_of_Isomorphism_Part_I}
            If $(X,[\,]_{X})$ and $(Y,[\,]_{Y}$ are Lie algebras, and if
            $f:X\rightarrow{Y}$ is a Lie algebra isomorphism, then $f$ is
            a Lie algebra homomorphism such that
            $f^{\minus{1}}:Y\rightarrow{X}$ is a Lie algebra homomorphism.
        \end{ltheorem}
        \begin{proof}
            For if $f:X\rightarrow{Y}$ is a Lie algebra isomorphism, then it
            is a homomorphism and a bijection
            (Def.~\ref{def:Lie_Algebra_Isomorphism}). But then the inverse
            function $f^{\minus{1}}:Y\rightarrow{X}$ is well defined. It thus
            suffices to show that this a homomorphism as well. Let
            $\mathbf{y}_{1},\mathbf{y}_{2}\in{Y}$. Then:
            \begin{equation}
                f\Big(\big[f^{\minus{1}}(\mathbf{y}_{1}),\,
                           f^{\minus{1}}(\mathbf{y}_{2})\big]_{Y}\Big)
                =\Big[f\big(f^{\minus{1}}(\mathbf{y}_{1})\big),\,
                      f\big(f^{\minus{1}}(\mathbf{y}_{2})\big)\Big]_{Y}
                =[\mathbf{y}_{1},\,\mathbf{y}_{2}]
            \end{equation}
            Since $f$ is a bijection, we may take the inverse of
            this to obtain:
            \begin{equation}
                [f^{\minus{1}}(\mathbf{y}_{1}),
                 f^{\minus{1}}(\mathbf{y}_{2})]_{X}=
                f^{\minus{1}}\Big([\mathbf{y}_{1},\mathbf{y}_{2}]_{Y}\Big)
            \end{equation}
            Thus completing the proof.
        \end{proof}
        \begin{ltheorem}{Equivalent Definition of Isomorphism (Part II)}
                        {Equivalent_Definition_of_Isomorphism_Part_II}
            If $(X,[\,]_{X})$ and $(Y,[\,]_{Y})$ are Lie algebras, and if
            $f:X\rightarrow{Y}$ is a Lie algebra homomorphism such that
            $f^{\minus{1}}:Y\rightarrow{X}$ exists and is a homomorphism,
            then $f$ is a Lie algebra homomorphism.
        \end{ltheorem}
        \begin{proof}
            For if $f:X\rightarrow{Y}$ is a Lie algebra homomorphism such that
            $f^{\minus{1}}:X\rightarrow{Y}$ exists and is a Lie algebra
            homomorphism, then $f$ is bijective. Therefore, $f$ is a
            Lie algebra isomorphism (Def.~\ref{def:Lie_Algebra_Isomorphism}).
        \end{proof}
        \begin{fdefinition}{Commutative Elements of a Lie Algebra}
                           {Commutative Elements of a Lie Algebra}
            Commutative elements of a Lie algebra $(X,[\,])$ are elements
            $\mathbf{x},\mathbf{y}$ such that:
            \begin{equation}
                [\mathbf{x},\,\mathbf{y}]=\mathbf{0}
            \end{equation}
        \end{fdefinition}
        \begin{fdefinition}{Abelian Lie Algebra}{Abelian Lie Algebra}
            An Abelian Lie Algebra is a Lie algebra $(X,[\,])$ such that for
            all $\mathbf{x},\mathbf{y}\in{X}$, $\mathbf{x}$ and $\mathbf{y}$
            are commutative elements.
        \end{fdefinition}
        It will be seen that these come from Abelian Lie groups.
        \begin{lexample}{}{}
            Let $(A,\cdot)$ be an associative $\mathbb{F}$ algebra. We can
            define a Lie algebra $\textrm{Lie}(A)$ as a vector space
            $A$ and by setting the Lie bracket to be the commutator:
            \begin{equation}
                [\mathbf{x},\mathbf{y}]
                =\mathbf{x}\cdot\mathbf{y}-\mathbf{y}\cdot\mathbf{x}
            \end{equation}
            Where $\cdot$ is the multiplicative operation that comes from the
            associative $\mathbb{F}$ algebra $(A,\,\cdot\,)$.
            The commutator is anticommutative:
            \begin{equation}
                [\mathbf{x},\mathbf{y}]
                =\mathbf{x}\cdot\mathbf{y}-\mathbf{y}\cdot\mathbf{x}
                =\minus(\mathbf{y}\cdot\mathbf{x}-\mathbf{x}\cdot\mathbf{y})
                =\minus[\mathbf{y},\mathbf{x}]
            \end{equation}
            And thus the alternating property is satisfied. Similarly, the
            Jacobi identity is valid and thus $\textrm{Lie}(A)$ is a Lie
            algebra. A special case of this is when we have
            $A=M_{n}(\mathbb{F})$. Then $\textrm{Lie}(M_{n}(\mathbb{F})$ is
            denoted by $\textrm{GL}_{n}(\mathbb{F})$.
        \end{lexample}
        We will use the following notation.
        \begin{fnotation}{Lie Bracket of Vector Subspaces}{}
            Given a Lie Algebra $(V,[\,])$ and two vector subspaces
            $W_{1},W_{2}\subseteq{V}$, we write $[W_{1},W_{2}]$ to
            denote the following:
            \begin{equation}
                [W_{1},W_{2}]=\textrm{Span}\big\{\,
                [\mathbf{w}_{1},\mathbf{w}_{2}]\,:\,
                    \mathbf{w_{1}}\in{W}_{1},\,\mathbf{w}_{2}\in{W}_{2}\,\big\}
            \end{equation}
        \end{fnotation}
        With this we can define Lie Subalgebras.
        \begin{fdefinition}{Lie Subalgebra}{Lie_Subalgebra}
            A Lie subalgebra of a Lie algebra $(V,[\,])$ is a subset
            $W\subseteq{V}$ such that:
            \begin{equation}
                [W,W]\subseteq{W}
            \end{equation}
        \end{fdefinition}
        That is to say, a Lie subalgebra of a Lie algebra is a subspace that is
        closed under the Lie bracket.
        \begin{fdefinition}{Ideal of a Subspace}{Ideal_of_a_Subspace}
            An ideal of a Lie algebra $(V,[\,])$ is a subset $W\subseteq{V}$
            such that:
            \begin{equation}
                [V,W]\subseteq{W}
            \end{equation}
        \end{fdefinition}
        By the anticommutativity of the Lie bracket, there is no distinction
        between left ideals and right ideals.
        \begin{theorem}
            If $(X,[\,]_{X})$ and $(Y,[\,])_{Y}$ are Lie algebras, and if
            $f:X\rightarrow{Y}$ is a Lie algebra homomorphism, then
            $\ker(f)$ is an ideal of $X$.
        \end{theorem}
        \begin{proof}
            For if $\mathbf{x}\in\ker(f)$ and $\mathbf{y}\in{X}$, then:
            \begin{equation}
                f([\mathbf{x},\mathbf{y}])
                =[f(\mathbf{x}),f(\mathbf{y})]
                =[\mathbf{0},\mathbf{y}]
                =\mathbf{0}
            \end{equation}
            And therefore $[\mathbf{x},\mathbf{y}]\in\ker(f)$. Thus,
            $ker(f)$ is an ideal of $X$.
        \end{proof}
        \begin{theorem}
            If $(V,[\,])$ is a Lie algebra, and if $W\subseteq{V}$ is an
            ideal of $V$, then $G/H$ has a Lie algebra structure such that the
            projection mapping $\pi:V\rightarrow{V}/W$, defined by
            $\mathbf{x}\mapsto\mathbf{x}+W$, is a Lie algebra homomorphism.
        \end{theorem}
        \begin{proof}
            For define $[\mathbf{x}+H,\mathbf{y}+H]$ as follows:
            \begin{equation}
                [\mathbf{x}+H,\mathbf{y}+H]=[\mathbf{x},\mathbf{y}]+H
            \end{equation}
            Then:
            \begin{equation}
                [\mathbf{x}+H,\mathbf{y}+H]
                =[\pi(\mathbf{x}),\pi(\mathbf{y})]
                =[\mathbf{x},\mathbf{y}]+H
                =\pi([\mathbf{x},\mathbf{y}])
            \end{equation}
            And therefore:
            \begin{equation}
                \big[\pi(\mathbf{x}),\pi(\mathbf{y})\big]
                =\pi\big([\mathbf{x},\mathbf{y}]\big)
            \end{equation}
            Let $\overline{X}=X+H$. If $\overline{X}'=\overline{X}$ and
            $\overline{Y}'=\overline{Y}$, then
            $\overline{[X,Y]}=\overline{[X',Y']}$. Thus:
            \begin{equation}
                X-X'=H_{1}\in{H}\quad\quad
                Y-Y'=H_{2}\in{H}
            \end{equation}
            And:
            \begin{equation}
                [X,Y]=[X'+H_{1},Y'+H_{2}]
                =[X',Y']+[H_{1},Y']+[X',H_{2}]+[H_{1},H_{2}]
            \end{equation}
            And this last sum of three is in $H$.
        \end{proof}
        \begin{theorem}
            If $(X,[\,])_{X}$ is a Lie algebra, and if $W\subseteq{X}$ is an
            ideal of $X$, then there is a Lie algebra $(Y,[\,]_{Y})$ and a
            homomorphism $f:X\rightarrow{Y}$ such that $W=\ker(f)$.
        \end{theorem}
        \begin{proof}
            For let $Y=X/W$ and let $[\,]_{Y}$ be the Lie bracket:
            \begin{equation}
                [\mathbf{x}+W,\mathbf{y}+W]_{Y}=[\mathbf{x},\mathbf{y}]+W
            \end{equation}
            Let $\pi:X\rightarrow{X/W}$ be the projection mapping
            $\pi(x)=x+W$. From the previous theorem,
            $(X/W,[\,]_{Y})$ is a Lie algebra and $\pi$ is a Lie algebra
            homomorphism. Moreover, $\ker(\pi)=W$. Therefore, etc.
        \end{proof}
        \begin{theorem}
            Let $f:G\rightarrow{H}$ be a map of Lie algebras, and let
            $K=\ker(f)$. Then there is a unique map
            $\overline{f}:G/K\rightarrow{H}$ such that $\overline{f}$ is
            injective and such that some commutative diagram thing.
        \end{theorem}
        \begin{example}
            \begin{enumerate}
                \item $0,G(TRIANGLE))G$
                \item $Z(G)=\{Z\in{G}:[X,Z]=0\}$ is called the center of $G$.
                \item $[G,G]=\textrm{Span}\{[X,Y]:X,Y\in{G}\}(TRIANGLE)G$
                \item Fuck it.
                \item For ideals $a,b(TRIANGLE)G$, $a+b(TRIANGLE)G$.
                \item Similarly for subtraction.
            \end{enumerate}
        \end{example}
        \begin{example}
            \begin{enumerate}
                \item V finite dimensional vector space over $\mathbb{F}$,
                      $A=\textrm{End}_{\mathbb{F}}(V)$ an associative
                      $\mathbb{F}$ algebra, then
                      $GL(V)=Lie(End_{\mathbb{F}}(V))$
                      with $[X,Y]=XY-YX$.
                \item $Tr:gl(V)\rightarrow\mathbb{F}$ abelian is a Lie algebra
                      homomorphism. $Tr([X,Y])=Tr(X,Y-YX)=0=[Tr(X),Tr(Y)]$.
                      $\ker(Tr)=SL(V)=\{X\in{GL}(V):Tr(X)=0\}$
            \end{enumerate}
        \end{example}
        If $V=\mathbb{F}^{n}$, denote $GL(V)$ by $GL_{n}(\mathbb{F})$ and $SL(V)$
        by $SL_{n}(\mathbb{F})$. Special cases: $SL_{2}(\mathbb{F})$. This has basis
        $(E^{ij})_{k\ell}=\delta^{i}_{k}\delta^{j}_{\ell}$.
        \begin{example}
            Compute $[X,Y]=[E^{12},E^{21}]=E^{11}-E^{22}=H$.
            And $[H,X]=2X$, $[H,Y]=\minus{2}Y$.
        \end{example}
        \begin{ldefinition}{Simple Lie Algebra}{Simple_Lie_Algebra}
            A Lie algebra is simple if it is non-abelian and its only ideals are
            $0$ and itself.
        \end{ldefinition}
        \begin{theorem}
            $SL_{2}(\mathbb{F})$ is simple for $\mathbb{F}=\mathbb{R}$ or
            $\mathbb{F}=\mathbb{C}$.
        \end{theorem}
        \begin{proof}
            Bracket notation says that $X$ is a 2-eigenvector of $[H,\cdot]$
            and $Y$ is a -2-eigenvector of $[H,\cdot]$. Let $A$ be a non-trivial
            ideal. We must show that $A=SP_{2}(\mathbb{F})$. Then
            $[X,U]\in{G}$. But:
            \begin{equation}
                [X,[X,U]]=[X,[X,xX+yY+hH]]
                =[X,gH-2hX]=\minus{2}yX
            \end{equation}
            So thus, either $X\in{A}$ or $Y=0$. Doing the same for
            $[Y,[Y,U]]$ shows that $\minus{2}xY=[Y,[Y,U]]$ and thus either
            $Y\in{A}$ or $X=0$. There are two cases now. If If $x=y=0$ then
            $h\ne{0}$ since $U$ is non-zero. This would imply $H\in{A}$. But
            $2X=[X,H]\in{A}$ so $X\in{A}$. Also, $Y\in{A}$.
            Similarly if $y\ne{0}$.
        \end{proof}
        This can be generalized for $SL_{n}(\mathbb{F})$ as well.
        \begin{ldefinition}{Normalizer of a Lie Algebra Subspace}
                           {Normalizer_of_Lie_Alg_Subspace}
            The Normalizer of $H\subset{G}$ of a Lie Algebra $G$ is the set:
            \begin{equation}
                N_{G}(H)=\{X\in{G}:[X,h]\in{H}\}
            \end{equation}
        \end{ldefinition}
        By the Jacobian identity, the normalizer of a subspace is also a
        subalgebra. Moreover, by def, $H$ is an ideal of its normalizer.
        \begin{ldefinition}{Centralizer of a Lie Algebra Subspace}
                           {Centralizer_of_a_Lie_Algebra_Subspace}
            The centralizer of $H\subset{G}$ is the set:
            \begin{equation}
                C_{G}(H)=\{X\in{G}:[X,H]=0\}
            \end{equation}
        \end{ldefinition}
        \begin{ldefinition}{Product Lie Algebras}{Product_Lie_Algebra}
            For Lie algebras $G_{1},G_{2}$, their product
            $G_{1}\times{G}_{2}$, also written $G_{1}\oplus{G}_{2}$, is the set
            $G_{1}\oplus{G}_{2}$ as a vector space with the bracket:
            \begin{equation}
                [(X_{1},X_{2}),(Y_{1},Y_{2})]=([X_{1},Y_{1}],[X_{2},Y_{2}]
            \end{equation}
        \end{ldefinition}
        $G_{1}\oplus{G}_{2}$ has ideals $\overline{G}_{1}=G_{1}\oplus{0}$ and
        $\overline{G}_{2}=0\oplus{G}_{2}$, and moreover
        $\overline{G}_{1}+\overline{G}_{2}=G_{1}\oplus{G}_{2}$
        \begin{theorem}
            If $G$ is a lie algebra, $a$, $b$ ideals of $G$ satisfying:
            \begin{enumerate}
                \item $a+b=G$
                \item $a\cap{b}=\emptyset$
                \item $[a,b]=0$
            \end{enumerate}
            Then $G$ is isomorphic to $a\oplus{b}$.
        \end{theorem}
        \begin{proof}
            Define $\varphi:a\oplus{b}\rightarrow{G}$ by
            $\varphi(A,B)=A+B$. By vector space theory, this is a vector
            space isomorphism. We need to check that it preserves brackets.
            But:
            \begin{equation}
                \varphi([(A,B),(A',B')])=\varphi([A,A'],[B,B'])
                =[A,A']+[B,B']
            \end{equation}
            But also:
            \begin{equation}
                [\varphi(A,B),\varphi(A',B')]=[A+B,A'+B']
                =[A,A']+[B,A']+[A,B']+[B,B']
            \end{equation}
            But $[B,A']=[A,B']=0$, completing the proof.
        \end{proof}
    \section{Review of Differentiation}
        If $\mathcal{U}\subseteq\mathbb{R}^{n}$ is open, and if
        $f:\mathcal{U}\rightarrow\mathbb{R}^{n}$ is a function, then
        $Df(p)$ (if it is exists) is a linear map
        $Df(p):\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}$ such that:
        \begin{equation}
            \underset{h\rightarrow{0}}{\lim}
                \frac{f(p+h)-(f(p)+Df(p)(h)}{\norm{h}}=0
        \end{equation}
        That is, $Df(p)$ is the best linear affine approximation to $f$ at
        $p$.
        \begin{theorem}
            If $f\in{C}^{1}$, then $Df(p)$ exists and:
            \begin{equation}
                Df(p)(v)=\underset{t\rightarrow{0}}{\lim}
                \frac{f(p+tr)-f(p)}{t}
            \end{equation}
        \end{theorem}
        Chain rule, if $p\in\mathcal{U}$,
        $f:\mathcal{U}\rightarrow\mathbb{R}$,
        then $D(g\circ{f})(o)=Dg(f(p))Df(p)$
        \begin{theorem}
            If $V_{1}$, $V_{2}$, $W$ are $\mathbb{R}$ vector spaces, and
            if $B:V_{1}\times{V}_{2}\rightarrow{W}$ is bilinear, then
            for $p_{1},v_{1}\in{V}_{1}$, $p_{2},v_{2}\in{V}_{2}$:
            \begin{equation}
                DB(p_{1},p_{2})(v_{1},v_{2})=B(p_{1},v_{2})+B(v_{1},p_{2})
            \end{equation}
        \end{theorem}
        This can be generalized to multilinear maps.
        \begin{theorem}
            if $B:V_{1}\times\cdots\times{V}_{n}\rightarrow{W}$ is
            multilinear, then:
            \begin{equation}
                DB(p_{1},\dots,p_{n})(v_{1},\dots,v_{n})=
                \sum_{j=1}^{n}B(p_{1},\dots,p_{j-1},v_{j},p_{j+1},\dots,p_{n})
            \end{equation}
        \end{theorem}
        \begin{example}
            Let $det:M_{n}(\mathbb{R})\rightarrow\mathbb{R}$ be the
            determinant function. Then:
            \begin{equation}
                D(det)(x)(H)=\textrm{Tr}(X^{Thing}H)
            \end{equation}
            where $X^{Thing}$ is the classical adjugate matrix:
            \begin{equation}
                (X^{Thing})_{ij}=(\minus{1})^{i+j}det(M_{ij}(X)
            \end{equation}
            Where $M_{ij}(X)$ is the minor of $X$ formed by crossing out the
            $i^{th}$ row and $j^{th}$ column.
        \end{example}
        \begin{example}
            Let $F_{k}:M_{n}(\mathbb{R})\rightarrow{M}_{n}(\mathbb{R})$ be
            defined by $F_{k}(X)=X^{k}$. Then:
            \begin{equation}
                DF_{k}(X)(H)=X^{k-1}H+x^{k-2}HX+\cdots+XHX^{k-1}+HX^{k-1}
            \end{equation}
            If $X$ and $H$ commute, then:
            \begin{equation}
                DF_{k}(X)(H)=kX^{k-1}H
            \end{equation}
        \end{example}
        \begin{theorem}
            If $V:GL_{n}(\mathbb{R})\rightarrow{M}_{n}(\mathbb{R})$ is
            defined by $V(X)=X^{\minus{1}}$, then:
            \begin{equation}
                DV(X)(H)=\minus{X}^{\minus{1}}HX^{\minus{1}}
            \end{equation}
        \end{theorem}
    \section{Lie Groups}
        Let $\mathbb{F}$ denote either $\mathbb{R}$ or $\mathbb{C}$.
        \begin{ldefinition}{Real Lie Groups}{Real_Lie_Group}
            A real Lie group is a $C^{\infty}$ manifold $C$ with a group
            structure such that the operation is smooth. That is,
            $m:G\times{G}\rightarrow{G}$ which maps $(x,y)\mapsto{x}*y$ is
            smooth, and $v:G\rightarrow{G}$ which maps
            $x\mapsto{x}^{\minus{1}}$ is also smooth.
        \end{ldefinition}
        \begin{ldefinition}{Lie Group Homomorphism}{}
            If $G$ and $H$ are Lie groups, a Lie group homomorphism is a
            smooth group homomorphism $f:G\rightarrow{H}$.
        \end{ldefinition}
        \begin{example}
            A Complex Lie group is a complex manifold with a group structure
            whose operations are holomorphic. Let $\mathbb{R}$ or
            $\mathbb{F}$ be a finite dimensional $\mathbb{F}$ vector space.
            Then $G:(V)\subseteq\textrm{End}(V)$ is an open subset of
            the vector space $\textrm{End}_{\mathbb{F}}(V)$. So this is
            a Lie group (real if $\mathbb{F}=\mathbb{R}$, complex if
            $\mathbb{F}=\mathbb{C}$) and
            $GL_{n}(\mathbb{F})=GL(\mathbb{F}^{n})$. As another example,
            $SL_{n}(\mathbb{F})=\{g\in{GL}_{n}(\mathbb{F}):det(g)=1\}$.
            We now how to show that this is a Lie group. We have that
            $SL_{n}(\mathbb{F})=det^{\minus{1}}(1)$. By the implicit
            function theorem, $det^{\minus{1}}(1)$ is a smooth
            manifold provided that
            $D(det)(X):M_{n}(\mathbb{F})\rightarrow\mathbb{F}$ is a
            surjective map for any $X\in{det}^{\minus{1}}(1)$. But
            $D(det)(X)H)=Tr(X^{Thing}H)$, so:
            \begin{equation}
                D(det)(X)(X)=Tr(X^{Thing}X)=Tr(det(X)I)=n\ne{0}
            \end{equation}
        \end{example}
        \begin{example}
            Let $T_{n}(\mathbb{F})$ be the subgroup of $GL_{n}(\mathbb{F})$
            stabilizing the standard flag:
            \begin{equation}
                0=V_{0}\subseteq{V}_{1}
                \subseteq\dots\subseteq{V}_{n}=\mathbb{F}^{n}
            \end{equation}
            This is often called the Borel subgroup. Let $V_{j}$ be
            the span of $\{e_{1},\dots,e_{j}\}$. There is a special case when
            $\mathbb{F}=\mathbb{R}$ and $n=3$. This is called the
            Heisenberg group, and is the set of all matrices of the following:
            \begin{equation}
                A=\begin{bmatrix}
                    1&x&y\\
                    0&1&z\\
                    0&0&1
                \end{bmatrix}
            \end{equation}
            Topologically this is homeomorphic to $\mathbb{R}^{3}$.
        \end{example}
        Let $<>$ be a bilinear form on $\mathbb{F}$. Pick a basis $B$ of $V$.
        Define $G$ by $G_{ij}=<v_{i},v_{j}>$.
    \section{Solvability and Semisimplicity}
        \begin{ltheorem}{Nondegenerate Splitting Lemma}{}
            iF $V$ is a finite dimensional $\mathbb{F}$ vector space with a
            symmetric bilinear form $\langle\cdot|\cdot\rangle$, if
            $W\subseteq{V}$ is a non-degenerate subspace, then
            $V$ is isomorphic to $W\boxplus{W}^{\perp}$.
        \end{ltheorem}
        If $V$ itself is nondegenerate, then $W^{\perp}$ is nondegenerate.
        \begin{ltheorem}{Structure of Semisimple Lie algebras}{}
            If $\mathfrak{g}$ is a semisimple Lie algebra over either
            $\mathbb{R}$ or $\mathbb{C}$, then there exists finitely many ideals
            $\mathfrak{s}_{k}\subseteq\mathfrak{g}$ such that:
            \begin{equation}
                \mathfrak{g}=\bigoplus_{k=1}^{N}\mathfrak{s}_{k}
            \end{equation}
            And if $\mathfrak{h}$ is simple then it is one of the
            $\mathfrak{s}_{k}$ above.
        \end{ltheorem}
        \begin{proof}
            If $\mathfrak{g}$ is simple, then we are done. If not then there is
            some non-zero proper ideal $\mathfrak{h}\subsetneq\mathfrak{g}$.
            But then $\mathfrak{h}^{\perp}$ is an ideal of $\mathfrak{g}$. Let
            $\mathfrak{a}=\mathfrak{h}\cap\mathfrak{h}^{\perp}$. Then the
            killing form on $\mathfrak{a}$ is:
            \begin{equation}
                B_{\mathfrak{a}}
                =B_{\mathfrak{g}}|_{\mathfrak{a}\times\mathfrak{a}}
            \end{equation}
            But for $X,Y\in\mathfrak{a}$, we have $B_{\mathfrak{g}}(X,Y)=0$
            since $X\in\mathfrak{h}$ and $Y\in\mathfrak{h}^{\perp}$, and thus
            $B_{\mathfrak{a}}=0$. Then by Cartan's Solvability criterion,
            $\mathfrak{h}$ is a nondegenerate subspace of $\mathfrak{g}$. By the
            nondegenerate splitting lemma this means that $\mathfrak{g}$ is
            isomorphic to $\mathfrak{h}\oplus\mathfrak{h}^{\perp}$. By
            Cartan's criterion for semisimplicity, $B_{\mathfrak{g}}$ is
            nondegenerate and thus $\mathfrak{h}^{\perp}$ is nondegenerate.
            But since $\mathfrak{h}$ and $\mathfrak{h}^{\perp}$ are ideals of
            $\mathfrak{g}$ we have:
            \begin{equation}
                B_{\mathfrak{h}}
                =B_{\mathfrak{g}}|_{\mathfrak{h}\times\mathfrak{h}}
                \quad\quad
                B_{\mathfrak{h}^{\perp}}
                =B_{\mathfrak{g}}
                    |_{\mathfrak{h^{\perp}}\times\mathfrak{h}^{\perp}}
            \end{equation}
            By Cartan's semisimplicity criterion, $\mathfrak{h}$ and
            $\mathfrak{h}^{\perp}$ are semisimple. But $\mathfrak{h}$ and
            $\mathfrak{h}^{\perp}$ are proper ideals of $\mathfrak{g}$ and thus
            $\textrm{dim}(\mathfrak{h})<\textrm{dim}(\mathfrak{g})$, and
            similarly for $\mathfrak{h}^{\perp}$. So by induction we have that
            $\mathfrak{h}$ and $\mathfrak{h}^{\perp}$ are the products of
            simple ideals, so $\mathfrak{g}$ is the product of simple ideals.
            Moreover, any simple ideal is one of the $\mathfrak{s}_{k}$. For let
            $\mathfrak{h}$ be a non-zero simple ideal of $\mathfrak{g}$. Then
            $[\mathfrak{g},\mathfrak{h}]\subseteq\mathfrak{h}$ is an ideal of
            $\mathfrak{h}$. But this bracket is either zero or all of
            $\mathfrak{h}$. But $[\mathfrak{g},\mathfrak{h}]=0$ means that
            $\mathfrak{h}$ is in the center of $\mathfrak{g}$,
            $Z(\mathfrak{g})$, but since $\mathfrak{h}$ is semisimple we have
            that $[\mathfrak{g},\mathfrak{h}]=\mathfrak{h}$, a contradiction as
            $\mathfrak{h}$ is nonzero. As such
            $[\mathfrak{g},\mathfrak{h}]=\mathfrak{h}$. But then:
            \begin{equation}
                [\mathfrak{s}_{k},\mathfrak{h}]\subseteq
                =\Big[\bigoplus_{k=1}^{N}\mathfrak{s}_{k},\mathfrak{h}\Big]
                =\mathfrak{h}=[\mathfrak{g},\mathfrak{h}]
            \end{equation}
            If $[\mathfrak{s}_{k},\mathfrak{h}]=0$ for all $k$ then
            $[\mathfrak{g},\mathfrak{h}]=0$, a contradiction, and thus there
            is a $k$ such that $[\mathfrak{s}_{k},\mathfrak{h}]\ne{0}$. But then
            we obtain:
            \begin{align}
                0\ne[\mathfrak{s}_{k},\mathfrak{h}]\subseteq\mathfrak{s}_{k}\\
                0\ne[\mathfrak{s}_{k},\mathfrak{h}]\subseteq\mathfrak{h}
            \end{align}
            From simplicity we obtain equality, and thus
            $\mathfrak{h}=\mathfrak{s}_{k}$.
        \end{proof}
        \begin{theorem}
            If $\mathfrak{h}$ is a semisimple ideal in a Lie algebra
            $\mathfrak{h}$, then there is a complementary ideal $\mathfrak{a}$
            of $\mathfrak{g}$ such that
            $\mathfrak{h}=\mathfrak{h}\oplus\mathfrak{a}$.
        \end{theorem}
        \begin{proof}
            By Cartan's semisimplicity criterion, since $\mathfrak{h}$ is
            semisimple the killing form is nondegenerate. But then
            $\mathfrak{h}$ is a nondegenerate subspace of $\mathfrak{g}$. By
            the nondegnerate splitting lemma, $\mathfrak{g}$ is isomorphic as
            a vector space to $\mathfrak{h}\oplus\mathfrak{h}^{\perp}$.
        \end{proof}
    \section{Semidirect Products}
        Let $\mathfrak{g}$ be a Lie algebra with an ideal $\mathfrak{k}$ and a
        subalgebra $\mathfrak{h}$ such that
        $\mathfrak{k}+\mathfrak{h}=\mathfrak{g}$ and
        $\mathfrak{k}\cap\mathfrak{g}=0$. That is,
        $\mathfrak{g}=\mathfrak{k}\oplus\mathfrak{h}$ as vector spaces. Then:
        \begin{subequations}
            \begin{align}
                [K+H,K'+H']&=[K,K']+[H,K']+[K,H']+[H,H']\\
                &=\Big([K,K']+(\textrm{ad}H)(K')-(\textrm{ad}H')(K)\Big)+[H,H']
            \end{align}
        \end{subequations}
        The left term is in $\mathfrak{k}$ and the right term is in
        $\mathfrak{g}$. Abstractly, given two Lie algebras $\mathfrak{k}$ and
        $\mathfrak{h}$ and a Lie algebra homomorphism
        $\delta:\mathfrak{h}\rightarrow\textrm{Der}(\mathfrak{k})$, define the
        Lie algebra on $\mathfrak{k}\oplus\mathfrak{h}$ by the bracket:
        \begin{equation}
            [(K,H),(K',H')]=
            ([K,K]+\delta(H)(K')-\delta(H')(K),[H,H'])
        \end{equation}
        This is a Lie algebra as the bracket will satisfy the Jacobi identity.
        Let $\overline{\mathfrak{k}}=\mathfrak{k}\oplus{0}$, which is isomorphic
        to $\mathfrak{k}$, and let $\overline{\mathfrak{h}}=0\oplus\mathfrak{h}$
        be subalgebras such that
        $\overline{\mathfrak{h}}+\overline{\mathfrak{k}}=\mathfrak{g}$ and
        $\mathfrak{h}\cap\mathfrak{k}=0$. Recalling from the definition of a
        derivation, for any $X\in\mathfrak{g}$, $\textrm{ad}X$ is a derivation.
        A derivation of $\mathfrak{g}$ that is of the form $\textrm{ad}X$ for
        some $X\in\mathfrak{g}$ is called an inner derivation. We denote the
        set of derivations on $\mathfrak{g}$ by:
        \begin{equation}
            \textrm{Der}(\mathfrak{k})=
            \{D\in\textrm{End}_{\mathbb{F}}(\mathfrak{g})
                :D\textrm{ is a derivation}\}
        \end{equation}
        Note that for $D,D'\in\textrm{Der}(\mathfrak{g})$, we have that:
        \begin{equation}
            [D,D']=D\circ{D}'-D'\circ{D}
        \end{equation}
        is also a derivation. Let $D\in\textrm{Der}(\mathfrak{g})$. Then for all
        $X\in\mathfrak{g}$ we have $[D,\textrm{ad}X]=\textrm{ad}(D(x))$. That is
        $\textrm{ad}(\mathfrak{g})$ is the set of inner derivations on
        $\mathfrak{g}$. It is also an ideal of $\textrm{Der}(\mathfrak{g})$.
        \begin{theorem}
            If $\mathfrak{g}$ is a semisimple Lie algebra, then all derications
            of $\mathfrak{g}$ are inner. That is,
            $\textrm{Der}(\mathfrak{g})=\textrm{ad}(\mathfrak{g})$.
        \end{theorem}
        \begin{proof}
            For let $D\in\textrm{Der}(\mathfrak{g})$. Form
            $\tilde{\mathfrak{g}}=\mathfrak{g}\times{_{\delta}\mathbb{F}}$,
            where $\delta:\mathbb{F}\rightarrow\textrm{Der}(\mathfrak{g})$.
            Since $\mathfrak{g}$ is a semisimple ideal in $\tilde{\mathfrak{g}}$
            there is a complementary ideal $\mathfrak{a}$ such that
            $\tilde{\mathfrak{g}}=\mathfrak{g}\oplus\mathfrak{a}$. Thus
            $\mathfrak{a}\cap\mathfrak{g}=0$ and
            \begin{equation}
                textrm{dim}(\mathfrak{a})
                =\textrm{dim}(\tilde{\mathfrak{g}})-\textrm{dim}(\mathfrak{g})
                =1
            \end{equation}
            Some more stuff.
        \end{proof}
        \begin{fdefinition}{Perfect Lie Algebra}{Perfect_Lie_Algebra}
            A perfect Lie algebra is a Lie algebra $\mathfrak{g}$ such that
            $[\mathfrak{g},\mathfrak{g}]=\mathfrak{g}$.
        \end{fdefinition}
        \begin{theorem}
            A semisimple Lie algebra is perfect.
        \end{theorem}
        \begin{proof}
            For if $\mathfrak{g}$ is simple then it is perfect since otherwise
            $[\mathfrak{g},\mathfrak{g}]=0$. If not then $\mathfrak{g}$ is the
            director sum of simple ideals by the structure theorem. Btut then:
            \begin{equation}
                [\mathfrak{g},\mathfrak{g}]
                =\Big[\bigoplus_{k=1}^{n}\mathfrak{s}_{k},
                      \bigoplus_{j=1}^{n}\mathfrak{s}_{j}\Big]
                =\bigoplus_{k=1}^{n}\bigoplus_{j=1}^{n}
                    [\mathfrak{s}_{k},\mathfrak{s}_{j}]
                =\bigoplus_{i=1}^{n}[\mathfrak{s}_{i},\mathfrak{s}_{i}]
                =\mathfrak{g}
            \end{equation}
        \end{proof}
    \section{Abstract Jordan-Chevelay Decomposition}
        Let $\mathfrak{a}$ be a finite dimensional algebra. It need not be
        associative. Let $D$ be a derivation of $\mathfrak{a}$ and let
        $D_{s}$ and $D_{n}$ be the Jordan-Chevelay decomposition of $D$. That
        is, $D=D_{s}+D_{n}$. Then $D_{s}$ and $D_{n}$ are also derivations. Let
        $\mathfrak{g}$ be a semisimple Lie algebra.
        \par\hfill\par
        Let $\varphi:\mathfrak{g}\rightarrow{gl}(V)$ be a representation of a
        Lie algebra $\mathfrak{G}$ and let $A_{\varphi}$ be the associative
        algebra defined by:
        \begin{equation}
            A_{\varphi}=\mathbb{F}[\varphi(\mathfrak{g})]
        \end{equation}
        Then $A_{\varphi}\subseteq\textrm{End}_{\mathbb{F}}(V)$, and this is
        moreover generated by $\varphi(\mathfrak{g})$ Then the
        subrepresentations of $V$, that is the subspaces $W\subseteq{V}$ that
        are invariant under all $\varphi(x)$ in $X(\mathfrak{g})$, are just the
        $A_{\varphi}$ submodules of $V$.
        \begin{ltheorem}{Schur's Lemma}{Schurs_Lemma}
            If $A$ is a finite dimensional $\mathbb{F}$ algebra and if $V$ is
            an irreducible $A$ module, then any non-zero $A$ module
            endomorphism $f:V\rightarrow{V}$ is an automorphism.
        \end{ltheorem}
        \begin{proof}
            Since $V$ is irreducible and $f$ is non-zero, the kernel of $f$ is
            zero and the image is $V$, and therefore $f$ is bijective, and
            therefore $\textrm{End}_{A}(V)$ is a division algebra.
        \end{proof}
        \begin{theorem}
            If $\mathbb{F}$ is algebraically closed then there is a
            $\lambda\in\mathbb{F}$ such that $f=\lambda\cdot\textrm{Id}$.
        \end{theorem}
        \begin{proof}
            If $\mathbb{F}$ is algebraically closed then $f$ has an eigenvalue
            $\lambda\in\mathbb{F}$. Then
            $\textrm{ker}(\lambda\cdot\textrm{Id}-f)\subseteq{V}$ is a non-zero
            submodule so is all of $V$. That is, $f=\lambda\cdot\textrm{Id}$.
        \end{proof}
        \begin{ltheorem}{Weyl's Theorem}{Weyls_Theorem}
            Let $\mathfrak{g}$ be a semi-simple Lie algebra over $\mathbb{C}$
            and let $\varphi:\mathfrak{g}\rightarrow{gl}(V)$ be a finite
            dimensional representation of $\mathfrak{g}$. Then $V$ is
            completely reducible.
        \end{ltheorem}
        \begin{theorem}
            If $V$ is a vector space over $\mathbb{C}$ and if $\mathfrak{g}$ is
            a semisimple subalgebra of $gl_{\mathbb{C}}(V)$, then for all
            $x\in\mathfrak{g}$, $X_{s},X_{n}\in\mathfrak{g}$.
        \end{theorem}
        \begin{theorem}
            If $\mathfrak{g}$ is a semisimple Lie algebra over $\mathbb{C}$,
            if $\varphi:\mathfrak{g}\rightarrow{gl}_{\mathbb{C}}(V)$ is a
            finite dimensional representation of $\mathfrak{g}$, if
            $X=X_{s}+X_{n}$ is the abstract Jordan-Chevelay decomposition of
            $X\in\mathfrak{G}$, then:
            \begin{equation}
                \varphi(X)=\varphi(X_{n})\varphi(X_{s})
            \end{equation}
            Is the Jordan-Chevelay decomposition of
            $\varphi(X)\in\textrm{End}_{\mathbb{C}}(V)$. That is,
            $\varphi(X_{s})=\varphi(X)_{s}$ and $\varphi(X_{n})=\varphi(X)_{n}$.
        \end{theorem}
        \begin{proof}
            Apply a previous theorem to the semisimple subalgebra
            $\overline{\mathfrak{G}}=\varphi(\mathfrak{g})\subseteq{g}(V)$.
        \end{proof}
    \section{Representations of $sl_{2}(\mathbb{C})$}
        $sl_{2}(\mathbb{C})$ is the space of all $X\in{gl}_{2}(\mathbb{C})$ such
        that $\textrm{Tr}(A)=0$. This has the following basis:
        \begin{equation}
            H=
            \begin{bmatrix*}[r]
                1&0\\
                0&\minus{1}
            \end{bmatrix*}
            \quad\quad
            X=
            \begin{bmatrix*}[r]
                0&1\\
                0&0
            \end{bmatrix*}
            \quad\quad
            Y=
            \begin{bmatrix*}[r]
                0&0\\
                1&0
            \end{bmatrix*}
        \end{equation}
        The brackets are:
        \begin{equation}
            [H,X]=2X
        \end{equation}
        Let $\varphi:sl_{2}(\mathbb{C})\rightarrow{gl}(V)$ be a finite
        dimensional representation of $sl_{2}(\mathbb{C})$ on a $\mathbb{C}$
        vector space $V$. Consider the eigenspaces of $\varphi(H)$. Recall that
        $\varphi$ preserves the Jordan-Chevelay decomposition. Let $V_{\lambda}$
        be defined by:
        \begin{equation}
            V_{\lambda}=\big\{v\in{V}\;|\;Hv=\lambda{v}\,\}
        \end{equation}
        If $\lambda\in\textrm{spec}(\varphi(H))$, and $0$ otherwise.
        If $V_{\lambda}\ne{0}$ then $\lambda$ is said to be a weight of $H$ in
        $V$ and $V_{\lambda}$ is the weight space. Consider $0\ne{V}_{0}=V$ and
        let $V_{1}=YV$, $V_{2}=Y^{2}V$, and so on. Let $V_{\minus{1}}=0$ and let
        $\mathcal{V}=\{V_{k}\}$. Note that all weights are equivalent mod 2.
        \begin{theorem}
            If $k\geq{0}$, $HV_{k}=(\lambda-2k)V_{k}$. $YV_{k}=V_{k+1}$.
            $XV_{k}=k(\lambda-k+1)V_{k-1}$.
        \end{theorem}
        \begin{theorem}
            If $V$ is a finite dimensional irreducible representation of
            $sl_{2}(\mathbb{C})$ then:
            \begin{equation}
                V=\bigoplus_{k=\minus{m}}^{m}V_{k}
            \end{equation}
            That is, $V$ is the direct sum of one dimensional weight spaces.
        \end{theorem}
        \begin{theorem}
            If $V$ is a finite dimensional irreducible representation of
            $sl_{2}(\mathbb{C})$, then there is a vector $v\in{V}$ with weight
            $m$ such that $m$ is the highest weight.
        \end{theorem}
        \begin{theorem}
            If $V$ is a finite dimensional irreducible representation of
            $sl_{2}(\mathbb{C})$ then $V$ has a basis $\mathcal{V}=\{v_{k}\}$
            such that $v_{k}\in{V}_{m-2k}$ and such that the
            $sl_{2}(\mathbb{C})$ action is something.
        \end{theorem}
        \begin{theorem}
            If $\varphi:sl_{2}(\mathbb{C})\rightarrow{gl}(V)$ is a finite
            dimensional complex representation of $sl_{2}(\mathbb{C})$, then
            the eigenvalues of $\varphi(H)$ are all integers. If $\lambda$ is
            an eigenvalue of $\varphi(H)$, then $\minus\lambda$ is an eigenvalue
            and has the same multiplicity. Lastly, if:
            \begin{equation}
                V=\bigoplus_{k=1}^{r}W_{k}
            \end{equation}
            where $W_{k}$ is an irreducible representation, then
            $r=\textrm{dim}(E_{0}(V))+\textrm{dim}(E_{1}(V))$, where
            $E_{\lambda}(V)$ is the $\lambda$ eigenspace of $\varphi(H)$.
        \end{theorem}
        \begin{proof}
            By Weyl's theorem, $V$ is completely reducible and each summand is
            described by some previous theorem, so the first two parts are done.
            If $V$ is the direct sum over $W_{k}$, then each $W_{k}$ is
            isomorphic to $V(m)$ for some $m$.
        \end{proof}
        Let $m=1$, then $V(1)=V_{\minus{1}}\oplus{V}_{1}$. This is the defining
        representation $\mathbb{C}^{2}$ of $sl_{2}(\mathbb{C})$. If we let
        $W=\mathbb{C}^{2}$ be the defining representation of
        $sl_{2}(\mathbb{C})$ and consider $\textrm{Sym}^{2}(W)$ where $W$ is a
        $k$ vector space. Something about tensor product. If $\{x,y\}$ is the
        standard basis of $W$ then a basis of $\textrm{Sym}^{2}(W)$ is
        $\{x^{2},xy,y^{2}\}$. We have:
        \begin{equation}
            \textrm{Sym}^{2}(W)=
            \mathbb{C}x^{2}\oplus\mathbb{C}xy\oplus\mathbb{C}y^{2}=
            W_{\minus{2}}\oplus{W}_{0}\oplus{W}_{2}=
            V(2)
        \end{equation}
        The adjoint representation of $sl_{2}(\mathbb{C})$ is irreducible, and
        so the adjoint representation is $V(2)$.
    \section{Root Space Decomposition}
        Let $\mathfrak{g}$ be a nonzero semisimple Lie algebra. The idea is to
        immitate the case of $sl_{2}(\mathbb{C})$ for the adjoint representation
        of $\mathfrak{g}$ with a family $\mathfrak{h}$ of semisimple elements
        instead of just $H\in{sl}_{2}(\mathbb{C})$.
        \begin{fdefinition}{Toral Subalgebra}{Torel_Subalgebra}
            A Toral subalgebra is a subalgebra $\mathfrak{h}$ of a Lie algebra
            $\mathfrak{g}$ such that for every element $H\in\mathfrak{H}$ we
            have that $H$ is semisimple, $H=H_{s}$.
        \end{fdefinition}
        \begin{theorem}
            If $H\in\mathfrak{G}$ is semisimple then $\mathbb{C}H$ is a toral
            subalgebra of $\mathfrak{g}$. If $\mathfrak{a},\mathfrak{b}$ are
            subalgebras of $\mathfrak{g}$ and if one of them is toral, then
            $\mathfrak{a}+\mathfrak{b}$ is toral.
        \end{theorem}
        Toral subalgebras exists since $\mathfrak{g}$ has semisimple elements.
        For if not then every $X\in\mathfrak{g}$ would be ad-nilpotent and thus
        $\mathfrak{g}$ would be a nilpotent Lie algebra, by Engel's theorem.
        \begin{theorem}
            If $V$ is a finite dimensional vector space over $\mathbb{C}$, if
            $T\in\textrm{End}_{\mathbb{C}}(V)$ is a semisimple and singular
            endomorphism, then $\ker(T)\cap{T}(V)=0$.
        \end{theorem}
        \begin{theorem}
            If $\mathfrak{h}\subseteq\mathfrak{g}$ is a toral Abelian
            subalgebra, then $\mathfrak{h}$ is Abelian.
        \end{theorem}
        Since the elements of $\mathfrak{h}$ are commuting semisimple elements,
        the operators $\{\textrm{ad}H:H\in\mathfrak{H}\}$ are simultaneously
        diagonalizable. Recal that if $X$ is a common eigenvector for all
        $\{\textrm{ad}H\}_{H\in\mathfrak{H}}$, then $[H,X]=\lambda_{H}X$ for
        some $\lambda_{H}\in\mathbb{C}$. This $\mathfrak{g}$ ca be decomposed as
        the direct sum of subspaces called weight spaces or root spaces as:
        \begin{equation}
            \mathfrak{g}=\mathfrak{g}_{0}
            \oplus(\oplus_{\alpha\in\Delta}\mathfrak{g}_{\alpha})
        \end{equation}
        Where $\alpha\in\mathfrak{h}^{*}\setminus\{0\}$ and:
        \begin{equation}
            \mathfrak{g}_{\alpha}
            =\{X\in\mathfrak{g}:\forall_{H\in\mathfrak{h}},[H,X]=\alpha(H)X\}
        \end{equation}
        and:
        \begin{equation}
            \Delta=\{\alpha\in\mathfrak{h}\setminus\{0\}:
                \mathfrak{g}_{\alpha}\ne{0}\}
        \end{equation}
        \begin{fdefinition}{Relative Roots}{Relative_Roots}
            The roots of $\mathfrak{g}$ relative to $\mathfrak{h}$ are the
            nonzero $\alpha\in\mathfrak{h}^{*}$ such that
            $\mathfrak{g}_{\alpha}\ne{0}$.
        \end{fdefinition}
        \begin{theorem}
            For all $\alpha,\beta\in\mathfrak{h}^{*}$,
            $[\mathfrak{g}_{\alpha},\mathfrak{g}_{\beta}]\subseteq
            \mathfrak{g}_{\alpha+\beta}$.
        \end{theorem}
        \begin{proof}
            For let $x\in\mathfrak{g}_{\alpha}$ and $y\in\mathfrak{g}_{\beta}$.
            Then for all $H\in\mathfrak{h}$:
            \begin{equation}
                [H,X]=\alpha(H)X
                \quad\quad
                [H,Y]=\beta(H)Y
            \end{equation}
            And therefore:
            \begin{equation}
                [H,[X,Y]]=
                [[H,X],Y]+[X,[H,Y]]
                [\alpha(H)X,Y]+[X,\beta(H)Y]
            \end{equation}
        \end{proof}
        \begin{theorem}
            If $\mathfrak{g}$ is a semisimple Lie algebra over $\mathbb{C}$, if
            $\mathfrak{h}\subseteq\mathfrak{g}$ is a maximal toral subalgebra,
            then:
            \begin{equation}
                \mathfrak{g}\simeq{C}_{\mathfrak{g}}(\mathfrak{h})\oplus
                \Big(\bigoplus_{\alpha\in\Delta}\mathfrak{g}_{\alpha}\Big)
            \end{equation}
            Moreover $C_{\mathfrak{g}}(\mathfrak{h})=\mathfrak{h}$.
        \end{theorem}
        This is the root space decomposition of $\mathfrak{g}$. Let
        $\mathfrak{g}$ be a semisimple complex Lie algebra, and let
        $\mathfrak{h}\subseteq\mathfrak{g}$ be a maximal toral subalgebra.
        By the previous theorem, the killing form restricted to
        $\mathfrak{h}$ is nondegenerate and so it defined a linear isomorphism
        $\flat:\mathfrak{h}\rightarrow\mathfrak{h}^{*}$ by
        $H\mapsto{B}_{\mathfrak{g}}(H,\cdot)|_{\mathfrak{h}}$. Thus for all
        $\psi\in\mathfrak{h}^{*}$ there is a unique $T_{\psi}\in\mathfrak{H}$
        such that $\psi=\flat(T_{\psi})=B_{\mathfrak{g}}(T_{\psi},\cdot)$.
        In particular roots $\alpha\in\Delta$ correspond to vectors
        $T_{\alpha}\in\mathfrak{h}$ called the coroots.
    \section{Geometric Properties of Root Space Decomposition}
        Let $\mathfrak{g}$ be a semisimple Lie algebra over $\mathbb{C}$ and let
        $\mathfrak{h}\subseteq\mathfrak{g}$ be a maximal toral subalgebra.
        \begin{theorem}
            The set of roots $\Delta$ spans $\mathfrak{h}^{*}$.
        \end{theorem}
        \begin{proof}
            If not, let $\{\alpha_{1},\dots,\alpha_{r}\}$ be a basis for the
            span of the roots and enlarge it to a basis of $\mathfrak{h}^{*}$.
            Let $\{H_{1},\dots,H_{n}\}$ be the dual basis of $\mathfrak{h}$.
            That is, $\alpha_{i}(H_{j})=\delta_{ij}$. 
        \end{proof}
\end{document}